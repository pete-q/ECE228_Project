{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "qoyt902J2Enj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivYQWBkf_o2h",
        "outputId": "128d421b-75bc-45ef-bfbb-47bc031f12dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import scipy.io\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "overwrite = False"
      ],
      "metadata": {
        "id": "2Thk17TvLJUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Extraction"
      ],
      "metadata": {
        "id": "n9td75KwOm7G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Original Feature Extraction"
      ],
      "metadata": {
        "id": "s4jX2M-52Kc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_folder = '/content/drive/MyDrive/Machine Learning Project (ECE 228)/Battery Dataset'\n",
        "voltage_cycles = []\n",
        "current_cycles = []\n",
        "features = []\n",
        "log_file = '/content/drive/MyDrive/Machine Learning Project (ECE 228)/Battery Dataset/feature_extraction_regular.log'\n",
        "logfh = open(log_file, 'w')\n",
        "plots = False # NOTE: change this to get plots but also screw up the tqdm\n",
        "from scipy.stats import kurtosis, skew\n",
        "\n",
        "file_count = sum(len(files) for _, _, files in os.walk(base_folder))  # Get the number of files - got this from https://stackoverflow.com/questions/35969433/using-tqdm-on-a-for-loop-inside-a-function-to-check-progress\n",
        "with tqdm(total=file_count) as pbar:  # Do tqdm this way\n",
        "    for root, dirs, files in os.walk(base_folder):\n",
        "      for filename in sorted(files):\n",
        "          pbar.update(1)\n",
        "          if filename.endswith('.mat'):\n",
        "\n",
        "              filepath = os.path.join(root, filename)\n",
        "              tqdm.write(f\"Processing {filename}...\", file=logfh)\n",
        "              mat_data = scipy.io.loadmat(filepath, squeeze_me=True, struct_as_record=False)\n",
        "\n",
        "              var_name = os.path.splitext(filename)[0]\n",
        "              battery = mat_data[var_name]\n",
        "\n",
        "              cycles = battery.cycle\n",
        "              cycle_index = 0\n",
        "              if not isinstance(cycles, (list, tuple, np.ndarray)):\n",
        "                  cycles = [cycles]\n",
        "              for idx, cycle in enumerate(cycles):\n",
        "                #for cycle in cycles:\n",
        "                  if hasattr(cycle, 'type') and cycle.type in ['charge']:\n",
        "                    if hasattr(cycle, 'data'):\n",
        "                                data = cycle.data\n",
        "                                if hasattr(data, 'Time') and hasattr(data, 'Voltage_measured') and hasattr(data, 'Current_measured'):\n",
        "                                    df = pd.DataFrame({\n",
        "                                        'time': data.Time,\n",
        "                                        'voltage': data.Voltage_measured,\n",
        "                                        'current': data.Current_measured\n",
        "                                    })\n",
        "\n",
        "                                    cc_start_idx = df[df['voltage'] >= 4.0].index # when the voltage passes 4.0 for the first time that's the start of CC mode\n",
        "                                    if cc_start_idx.empty:\n",
        "                                        tqdm.write(f\"Skipping {filename}_cycle{idx} due to missing CC start data.\", file=logfh)\n",
        "                                        continue # Skip to the next cycle\n",
        "                                    else:\n",
        "                                        cc_start_idx = cc_start_idx[0]\n",
        "\n",
        "                                    cc_end_idx = df[df['voltage'] >= 4.2].index # when the voltage hits 4.2 for the first time that's the end of CC mode\n",
        "                                    if cc_end_idx.empty:\n",
        "                                        tqdm.write(f\"Skipping {filename}_cycle{idx} due to missing CC end data.\", file=logfh)\n",
        "                                        continue # Skip to the next cycle\n",
        "                                    else:\n",
        "                                        cc_end_idx = cc_end_idx[0]\n",
        "\n",
        "                                    # Check if CC_df can be created\n",
        "                                    if cc_start_idx is not None and cc_end_idx is not None and cc_start_idx <= cc_end_idx:\n",
        "                                        CC_df = df.loc[cc_start_idx:cc_end_idx]\n",
        "                                    else:\n",
        "                                        tqdm.write(f\"Skipping {filename}_cycle{idx} due to invalid CC range.\", file=logfh)\n",
        "                                        continue # Skip to the next cycle\n",
        "\n",
        "                                    # Check if CV start index exists\n",
        "                                    cv_start_indices = df[(df['current'] <= 0.5) & (df['voltage']>=4.0)].index\n",
        "                                    if cv_start_indices.empty:\n",
        "                                        tqdm.write(f\"Skipping {filename}_cycle{idx} due to missing CV start data.\", file=logfh)\n",
        "                                        continue # Skip to the next cycle\n",
        "                                    cv_start_idx = cv_start_indices[0]\n",
        "\n",
        "                                    # Check if CV end index exists\n",
        "                                    cv_end_indices = df[(df['current'] <= 0.1) & (df['voltage']>=4.0)].index\n",
        "                                    if cv_end_indices.empty:\n",
        "                                        tqdm.write(f\"Skipping {filename}_cycle{idx} due to missing CV end data.\", file=logfh)\n",
        "                                        continue # Skip to the next cycle\n",
        "                                    cv_end_idx = cv_end_indices[0]\n",
        "\n",
        "                                    # without the skippin gthere were 1588 rows. we'll see how many there are after 1582 but some of them are still weird\n",
        "                                    # Check if CV_df can be created\n",
        "                                    if cv_start_idx <= cv_end_idx:\n",
        "                                        CV_df = df.loc[cv_start_idx:cv_end_idx]\n",
        "                                    else:\n",
        "                                        tqdm.write(f\"Skipping {filename}_cycle{idx} due to invalid CV range.\", file=logfh)\n",
        "                                        continue # Skip to the next cycle\n",
        "\n",
        "                                    filtered_df = df[\n",
        "                                        (df['voltage'] >= 4.0) & (df['voltage'] <= 4.2) &\n",
        "                                        (df['current'] >= 0.1) & (df['current'] <= 0.5)\n",
        "                                    ] # TODO re-examine this filtering - it's not wrong but apparently the voltage oscillates in this range and we're not capturing the linear portion of the charge cycle\n",
        "                                    # TODO apparently this is indeed wrong - they are separate ranges - voltage data during constant current mode and current data during constant voltage mode\n",
        "                                    # TODO see https://github.com/wang-fujin/Battery-dataset-preprocessing-code-library/blob/main/HUSTBatteryClass.py for a filtering implementation\n",
        "\n",
        "                                    # If filtered data exists, store voltage with time as index\n",
        "                                    # if not filtered_df.empty:\n",
        "                                    #     cycle_id = f\"{filename}_cycle{idx}\"\n",
        "                                    #     voltage_series = pd.Series(\n",
        "                                    #         data=filtered_df['voltage'].values,\n",
        "                                    #         index=filtered_df['time'].values,\n",
        "                                    #         name=cycle_id\n",
        "                                    #     )\n",
        "                                    #     voltage_cycles.append(voltage_series)\n",
        "                                    #     print(voltage_cycles)\n",
        "\n",
        "                                    #apparently this is filtering out a lot of stuff that didn't need to be skipped\n",
        "                                    #if not filtered_df.empty:\n",
        "                                    cycle_id = f\"{filename}_cycle{idx}\"\n",
        "\n",
        "                                    if plots:\n",
        "                                      # Plot voltage vs. time\n",
        "                                      #plt.figure()\n",
        "                                      #plt.plot(filtered_df['time'].values, filtered_df['voltage'].values)\n",
        "                                      #plt.title(f\"Voltage vs Time - {cycle_id}\")\n",
        "                                      #plt.xlabel(\"Time (s)\")\n",
        "                                      #plt.ylabel(\"Voltage (V)\")\n",
        "                                      #plt.grid(True)\n",
        "                                      #plt.tight_layout()\n",
        "                                      #plt.show()\n",
        "\n",
        "                                      plt.figure()\n",
        "                                      plt.plot(CC_df['time'].values, CC_df['voltage'].values)\n",
        "                                      plt.title(f\"Voltage vs Time in Constant Current - {cycle_id}\")\n",
        "                                      plt.xlabel(\"Time (s)\")\n",
        "                                      plt.ylabel(\"Voltage (V)\")\n",
        "                                      plt.grid(True)\n",
        "                                      plt.tight_layout()\n",
        "                                      plt.show()\n",
        "\n",
        "\n",
        "                                      plt.figure()\n",
        "                                      plt.plot(CV_df['time'].values, CV_df['current'].values)\n",
        "                                      plt.title(f\"Current vs Time in Constant Current - {cycle_id}\")\n",
        "                                      plt.xlabel(\"Time (s)\")\n",
        "                                      plt.ylabel(\"Current (A)\")\n",
        "                                      plt.grid(True)\n",
        "                                      plt.tight_layout()\n",
        "                                      plt.show()\n",
        "\n",
        "\n",
        "                                    cc_v = CC_df['voltage'].values\n",
        "                                    cc_t = CC_df['time'].values\n",
        "                                    cc_i = CC_df['current'].values\n",
        "\n",
        "                                    cv_i = CV_df['current'].values\n",
        "                                    cv_t = CV_df['time'].values\n",
        "\n",
        "                                    # some log checking for later\n",
        "                                    if (cc_t[-1] - cc_t[0]) == 0.0:\n",
        "                                      tqdm.write(f\"Skipping {filename}_cycle{idx} due to zero CC time.\", file=logfh)\n",
        "                                      continue #\n",
        "                                    if (cv_t[-1] - cv_t[0]) == 0.0:\n",
        "                                      tqdm.write(f\"Skipping {filename}_cycle{idx} due to zero CV time.\", file=logfh)\n",
        "                                      continue #\n",
        "                                    cycle_index += 1\n",
        "                                    features.append({\n",
        "                                        'charge_CC_mean_V': np.mean(cc_v),\n",
        "                                        'charge_CC_std_V': np.std(cc_v),\n",
        "                                        'charge_CC_kurtosis_V': kurtosis(cc_v),\n",
        "                                        'charge_CC_skew_V': skew(cc_v),\n",
        "                                        'charge_CC_time_V': cc_t[-1] - cc_t[0], #np.shape(cc_v)[0], # basically just the length\n",
        "                                        'charge_CC_charge': np.trapezoid(cc_i, cc_t),\n",
        "                                        'charge_CC_slope_V': (cc_v[-1]-cc_v[0])/(cc_t[-1] - cc_t[0]),\n",
        "                                        'charge_CC_entropy_V': -np.sum(\n",
        "                                                  cc_v/np.sum(cc_v) *\n",
        "                                                  np.log(cc_v/np.sum(cc_v))),\n",
        "                                        'charge_CV_mean_I': np.mean(cv_i),\n",
        "                                        'charge_CV_std_dev_I': np.std(cv_i),\n",
        "                                        'charge_CV_kurtosis_I': kurtosis(cv_i),\n",
        "                                        'charge_CV_skew_I': skew(cv_i),\n",
        "                                        'charge_CV_time_I': cv_t[-1] - cv_t[0],\n",
        "                                        'charge_CV_charge_I': np.trapezoid(cv_i, cv_t),\n",
        "                                        'charge_CV_slope_I': (cv_i[-1]-cv_i[0])/(cv_t[-1] - cv_t[0]),\n",
        "                                        'charge_CV_entropy_I': -np.sum(\n",
        "                                                  cv_i/np.sum(cv_i) *\n",
        "                                                  np.log(cv_i/np.sum(cv_i))),\n",
        "                                        'cycle_index': cycle_index, #what if we used the idx instead of the cycle index? just a thought that I might explore\n",
        "                                        'capacity': np.trapezoid(data.Current_measured, data.Time) / 3600 # the whole charge accrued is current integrated over time divided by seconds in an hour\n",
        "                                    })\n"
      ],
      "metadata": {
        "id": "rx3elqqd_vzg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c36fe00-d903-4f85-84b3-4c1ae682e56d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:51<00:00,  1.28it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features_df = pd.DataFrame(features)\n",
        "features_df.to_csv(\"/content/drive/MyDrive/Machine Learning Project (ECE 228)/Battery Dataset/features.csv\", index=False) # VERY key, with indices it all goes a bit wonky"
      ],
      "metadata": {
        "id": "LDT3zTqOK203"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extended Feature Extraction"
      ],
      "metadata": {
        "id": "flepdPsLPkHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.io\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import kurtosis, skew, entropy\n",
        "from scipy.fft import fft\n",
        "\n",
        "base_folder = '/content/drive/MyDrive/Machine Learning Project (ECE 228)/Battery Dataset'\n",
        "voltage_cycles = []\n",
        "current_cycles = []\n",
        "features = []\n",
        "log_file = '/content/drive/MyDrive/Machine Learning Project (ECE 228)/Battery Dataset/feature_extraction_extended.log'\n",
        "logfh = open(log_file, 'w')\n",
        "plots = False # NOTE: change this to get plots but also screw up the tqdm\n",
        "\n",
        "file_count = sum(len(files) for _, _, files in os.walk(base_folder))  # Get the number of files - got this from https://stackoverflow.com/questions/35969433/using-tqdm-on-a-for-loop-inside-a-function-to-check-progress\n",
        "with tqdm(total=file_count) as pbar:  # Do tqdm this way\n",
        "    for root, dirs, files in os.walk(base_folder):\n",
        "      for filename in sorted(files):\n",
        "          pbar.update(1)\n",
        "          if filename.endswith('.mat'):\n",
        "              filepath = os.path.join(root, filename)\n",
        "              tqdm.write(f\"Processing {filename}...\", file=logfh)\n",
        "              mat_data = scipy.io.loadmat(filepath, squeeze_me=True, struct_as_record=False)\n",
        "              var_name = os.path.splitext(filename)[0]\n",
        "              battery = mat_data[var_name]\n",
        "\n",
        "              cycles = battery.cycle\n",
        "              cycle_index = 0\n",
        "              if not isinstance(cycles, (list, tuple, np.ndarray)):\n",
        "                  cycles = [cycles]\n",
        "\n",
        "              for idx, cycle in enumerate(cycles):\n",
        "                  if hasattr(cycle, 'type') and cycle.type == 'charge' and hasattr(cycle, 'data'):\n",
        "                      data = cycle.data\n",
        "                      if hasattr(data, 'Time') and hasattr(data, 'Voltage_measured') and hasattr(data, 'Current_measured'):\n",
        "                          df = pd.DataFrame({\n",
        "                              'time': data.Time,\n",
        "                              'voltage': data.Voltage_measured,\n",
        "                              'current': data.Current_measured\n",
        "                          })\n",
        "                          cc_start_idx = df[df['voltage'] >= 4.0].index # when the voltage passes 4.0 for the first time that's the start of CC mode\n",
        "                          if cc_start_idx.empty:\n",
        "                            tqdm.write(f\"Skipping {filename}_cycle{idx} due to missing CC start data.\", file=logfh)\n",
        "                            continue # Skip to the next cycle\n",
        "                          else:\n",
        "                            cc_start_idx = cc_start_idx[0]\n",
        "\n",
        "                          cc_end_idx = df[df['voltage'] >= 4.2].index # when the voltage hits 4.2 for the first time that's the end of CC mode\n",
        "                          if cc_end_idx.empty:\n",
        "                            tqdm.write(f\"Skipping {filename}_cycle{idx} due to missing CC end data.\", file=logfh)\n",
        "                            continue # Skip to the next cycle\n",
        "                          else:\n",
        "                            cc_end_idx = cc_end_idx[0]\n",
        "\n",
        "                          # Check if CC_df can be created\n",
        "                          if cc_start_idx is not None and cc_end_idx is not None and cc_start_idx <= cc_end_idx:\n",
        "                            CC_df = df.loc[cc_start_idx:cc_end_idx]\n",
        "                          else:\n",
        "                            tqdm.write(f\"Skipping {filename}_cycle{idx} due to invalid CC range.\", file=logfh)\n",
        "                            continue # Skip to the next cycle\n",
        "\n",
        "                          # Check if CV start index exists\n",
        "                          cv_start_indices = df[(df['current'] <= 0.5) & (df['voltage']>=4.0)].index\n",
        "                          if cv_start_indices.empty:\n",
        "                            tqdm.write(f\"Skipping {filename}_cycle{idx} due to missing CV start data.\", file=logfh)\n",
        "                            continue # Skip to the next cycle\n",
        "                          cv_start_idx = cv_start_indices[0]\n",
        "\n",
        "                          # Check if CV end index exists\n",
        "                          cv_end_indices = df[(df['current'] <= 0.1) & (df['voltage']>=4.0)].index\n",
        "                          if cv_end_indices.empty:\n",
        "                            tqdm.write(f\"Skipping {filename}_cycle{idx} due to missing CV end data.\", file=logfh)\n",
        "                            continue # Skip to the next cycle\n",
        "                          cv_end_idx = cv_end_indices[0]\n",
        "\n",
        "                          # without the skippin gthere were 1588 rows. we'll see how many there are after 1582 but some of them are still weird\n",
        "                          # Check if CV_df can be created\n",
        "                          if cv_start_idx <= cv_end_idx:\n",
        "                            CV_df = df.loc[cv_start_idx:cv_end_idx]\n",
        "                          else:\n",
        "                            tqdm.write(f\"Skipping {filename}_cycle{idx} due to invalid CV range.\", file=logfh)\n",
        "                            continue # Skip to the next cycle\n",
        "                          filtered_df = df[\n",
        "                              (df['voltage'] >= 4.0) & (df['voltage'] <= 4.2) &\n",
        "                              (df['current'] >= 0.1) & (df['current'] <= 0.5)\n",
        "                          ]\n",
        "\n",
        "                          #if not filtered_df.empty:\n",
        "                          cycle_id = f\"{filename}_cycle{idx}\"\n",
        "                          if plots:\n",
        "                            # Plot (optional)\n",
        "                            plt.figure()\n",
        "                            plt.plot(CC_df['time'].values, CC_df['voltage'].values)\n",
        "                            plt.title(f\"Voltage vs Time in Constant Current - {cycle_id}\")\n",
        "                            plt.xlabel(\"Time (s)\")\n",
        "                            plt.ylabel(\"Voltage (V)\")\n",
        "                            plt.grid(True)\n",
        "                            plt.tight_layout()\n",
        "                            plt.show()\n",
        "\n",
        "\n",
        "                            plt.figure()\n",
        "                            plt.plot(CV_df['time'].values, CV_df['current'].values)\n",
        "                            plt.title(f\"Current vs Time in Constant Current - {cycle_id}\")\n",
        "                            plt.xlabel(\"Time (s)\")\n",
        "                            plt.ylabel(\"Current (A)\")\n",
        "                            plt.grid(True)\n",
        "                            plt.tight_layout()\n",
        "                            plt.show()\n",
        "\n",
        "\n",
        "                          # Time-series data\n",
        "                          cc_v = CC_df['voltage'].values\n",
        "                          cc_t = CC_df['time'].values\n",
        "                          cc_i = CC_df['current'].values\n",
        "\n",
        "                          cv_i = CV_df['current'].values\n",
        "                          cv_t = CV_df['time'].values\n",
        "\n",
        "                          # some log checking for later\n",
        "                          if (cc_t[-1] - cc_t[0]) == 0.0:\n",
        "                            tqdm.write(f\"Skipping {filename}_cycle{idx} due to zero CC time.\", file=logfh)\n",
        "                            continue\n",
        "                          if (cv_t[-1] - cv_t[0]) == 0.0:\n",
        "                            tqdm.write(f\"Skipping {filename}_cycle{idx} due to zero CV time.\", file=logfh)\n",
        "                            continue\n",
        "                          cycle_index +=1\n",
        "                          # --- Basic Features ---\n",
        "                          feature_dict = {\n",
        "                              'charge_CC_mean_V': np.mean(cc_v),\n",
        "                              'charge_CC_std_V': np.std(cc_v),\n",
        "                              'charge_CC_kurtosis_V': kurtosis(cc_v),\n",
        "                              'charge_CC_skew_V': skew(cc_v),\n",
        "                              'charge_CC_time_V': cc_t[-1] - cc_t[0], #np.shape(cc_v)[0], # basically just the length\n",
        "                              'charge_CC_charge': np.trapezoid(cc_i, cc_t),\n",
        "                              'charge_CC_slope_V': (cc_v[-1]-cc_v[0])/(cc_t[-1] - cc_t[0]),\n",
        "                              'charge_CC_entropy_V': -np.sum(\n",
        "                                                cc_v/np.sum(cc_v) *\n",
        "                                                np.log(cc_v/np.sum(cc_v))),\n",
        "                              'charge_CV_mean_I': np.mean(cv_i),\n",
        "                              'charge_CV_std_dev_I': np.std(cv_i),\n",
        "                              'charge_CV_kurtosis_I': kurtosis(cv_i),\n",
        "                              'charge_CV_skew_I': skew(cv_i),\n",
        "                              'charge_CV_time_I': cv_t[-1] - cv_t[0],\n",
        "                              'charge_CV_charge_I': np.trapezoid(cv_i, cv_t),\n",
        "                              'charge_CV_slope_I': (cv_i[-1]-cv_i[0])/(cv_t[-1] - cv_t[0]),\n",
        "                              'charge_CV_entropy_I': -np.sum(\n",
        "                                                cv_i/np.sum(cv_i) *\n",
        "                                                np.log(cv_i/np.sum(cv_i))),\n",
        "                          }\n",
        "\n",
        "                          # --- Additional Features ---\n",
        "                          feature_dict.update({\n",
        "                              #'duration': t[-1] - t[0], that's time already\n",
        "                              'voltage_range_CC': np.max(cc_v) - np.min(cc_v),\n",
        "                              'current_range_CV': np.max(cv_i) - np.min(cv_i),\n",
        "                              'voltage_rms_CC': np.sqrt(np.mean(cc_v**2)),\n",
        "                              'current_rms_CV': np.sqrt(np.mean(cv_i**2)),\n",
        "                          })\n",
        "\n",
        "\n",
        "                          # FFT features (voltage)\n",
        "                          if len(cc_v) > 5 and np.std(cc_v) > 1e-6:\n",
        "                              fft_v = fft(cc_v - np.mean(cc_v))\n",
        "                              power_v = np.abs(fft_v[:len(fft_v)//2])**2\n",
        "                              total_power_v = np.sum(power_v)\n",
        "                              if total_power_v > 0:\n",
        "                                  power_v /= total_power_v\n",
        "                                  feature_dict.update({\n",
        "                                      'fft_dominant_freq_v': np.argmax(power_v),\n",
        "                                      'fft_spectral_entropy_v': entropy(power_v)\n",
        "                                  })\n",
        "                              else:\n",
        "                                  feature_dict.update({\n",
        "                                      'fft_dominant_freq_v': np.nan,\n",
        "                                      'fft_spectral_entropy_v': np.nan\n",
        "                                  })\n",
        "                          else:\n",
        "                              feature_dict.update({\n",
        "                                  'fft_dominant_freq_v': np.nan,\n",
        "                                  'fft_spectral_entropy_v': np.nan\n",
        "                              })\n",
        "\n",
        "                          # FFT features (current)\n",
        "                          if len(cv_i) > 5 and np.std(cv_i) > 1e-6:\n",
        "                              fft_i = fft(cv_i - np.mean(cv_i))\n",
        "                              power_i = np.abs(fft_i[:len(fft_i)//2])**2\n",
        "                              total_power_i = np.sum(power_i)\n",
        "                              if total_power_i > 0:\n",
        "                                  power_i /= total_power_i\n",
        "                                  feature_dict.update({\n",
        "                                      'fft_dominant_freq_i': np.argmax(power_i),\n",
        "                                      'fft_spectral_entropy_i': entropy(power_i)\n",
        "                                  })\n",
        "                              else:\n",
        "                                  feature_dict.update({\n",
        "                                      'fft_dominant_freq_i': np.nan,\n",
        "                                      'fft_spectral_entropy_i': np.nan\n",
        "                                  })\n",
        "                          else:\n",
        "                              feature_dict.update({\n",
        "                                  'fft_dominant_freq_i': np.nan,\n",
        "                                  'fft_spectral_entropy_i': np.nan\n",
        "                              })\n",
        "                          feature_dict.update({'cycle_index': cycle_index,\n",
        "                                                'capacity': np.trapezoid(data.Current_measured, data.Time) / 3600})\n",
        "                          features.append(feature_dict)"
      ],
      "metadata": {
        "id": "fu3Qw9_Ny2-e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4bd2db6-7455-4088-fa35-fe4352c7c0d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:29<00:00,  2.23it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features_df = pd.DataFrame(features)\n",
        "print(features_df.columns)\n",
        "features_df.to_csv(\"/content/drive/MyDrive/Machine Learning Project (ECE 228)/Battery Dataset/features_extended.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLbF8emQBQpg",
        "outputId": "3e25ba73-86af-407e-c596-d72ebef21602"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['charge_CC_mean_V', 'charge_CC_std_V', 'charge_CC_kurtosis_V',\n",
            "       'charge_CC_skew_V', 'charge_CC_time_V', 'charge_CC_charge',\n",
            "       'charge_CC_slope_V', 'charge_CC_entropy_V', 'charge_CV_mean_I',\n",
            "       'charge_CV_std_dev_I', 'charge_CV_kurtosis_I', 'charge_CV_skew_I',\n",
            "       'charge_CV_time_I', 'charge_CV_charge_I', 'charge_CV_slope_I',\n",
            "       'charge_CV_entropy_I', 'voltage_range_CC', 'current_range_CV',\n",
            "       'voltage_rms_CC', 'current_rms_CV', 'fft_dominant_freq_v',\n",
            "       'fft_spectral_entropy_v', 'fft_dominant_freq_i',\n",
            "       'fft_spectral_entropy_i', 'cycle_index', 'capacity'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Architecture - Derived from PINN4SOH"
      ],
      "metadata": {
        "id": "RtcgY73-U8Ea"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP Block and Sol U"
      ],
      "metadata": {
        "id": "NFsUCx9u8aWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.autograd import grad\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# apparently there's no torch nn sin activation function so we make one up\n",
        "# https://discuss.pytorch.org/t/sine-activation-missing-in-pytorch-nn/189470\n",
        "class Sin(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Sin, self).__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return torch.sin(x)\n",
        "\n",
        "\n",
        "##### Basic MLP #####\n",
        "class MLPBLock(nn.Module):\n",
        "  def __init__(self, input_dim=17, hidden_dim=50, output_dim=1, num_layers=4, dropout=0.2):\n",
        "    # init super\n",
        "    super(MLPBLock, self).__init__()\n",
        "    # assign all the properties\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim\n",
        "    self.num_layers = num_layers\n",
        "    self.hidden_dim = hidden_dim\n",
        "\n",
        "    # make the layers - first layer is Linear + Sin, hidden layers are Linear+Sin+Dropout and final layer is just Linear\n",
        "    self.layers = []\n",
        "    for i in range(num_layers):\n",
        "      if i == 0: # First layer\n",
        "        self.layers.append(nn.Linear(input_dim, hidden_dim))\n",
        "        self.layers.append(Sin())\n",
        "      elif i == num_layers - 1: # Final layer\n",
        "        self.layers.append(nn.Linear(hidden_dim, output_dim))\n",
        "      else: # Hidden layers\n",
        "        self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "        self.layers.append(Sin())\n",
        "        self.layers.append(nn.Dropout(dropout))\n",
        "    self.net = nn.Sequential(*self.layers)\n",
        "    self._init() # they have a preferred weigh initialization scheme\n",
        "\n",
        "  def _init(self):\n",
        "    for m in self.net: # dude the colab auto predict is so scary it alsmost go this whole block right\n",
        "      if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_normal_(m.weight)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ],
      "metadata": {
        "id": "uSNkMPb1NbwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### Prediction Layer #####\n",
        "# just to separate the ML encoder from the actual layer that makes predictions\n",
        "class Predict(nn.Module):\n",
        "  def __init__(self, input_dim=30):\n",
        "    super(Predict, self).__init__()\n",
        "\n",
        "    # simple drop, linear, activation and linear\n",
        "    self.input_dim = input_dim\n",
        "    self.layer = nn.Sequential(\n",
        "        nn.Dropout(p=0.2),\n",
        "        nn.Linear(self.input_dim, 32),\n",
        "        Sin(),\n",
        "        nn.Linear(32, 1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layer(x)\n",
        "\n",
        "\n",
        "\n",
        "##### Solution model #####\n",
        "# Solution u is the first half of the architecture. This makes a guess as to the capacity which is then checked against the ODE in the loss\n",
        "class Sol_u(nn.Module):\n",
        "  def __init__(self, feature_length=17): # add feature_length as a passable parameter to make the model flexible for extended features\n",
        "    super(Sol_u, self).__init__()\n",
        "    self.encoder = MLPBLock(input_dim=feature_length, hidden_dim=60, output_dim=32, num_layers=3, dropout=0.2) # they said 17 but there's 18 features with the cycle index\n",
        "    self.predictor = Predict(input_dim=32)\n",
        "\n",
        "    # init the weights (used to be a separate method)\n",
        "    for layer in self.modules():\n",
        "      if isinstance(layer, nn.Linear):\n",
        "        nn.init.xavier_normal_(layer.weight)\n",
        "        nn.init.constant_(layer.bias, 0)\n",
        "      elif isinstance(layer, nn.Conv1d): # TODO: This is in the original paper but there's no Conv1d in the model and there's no difference so we can remove that\n",
        "        nn.init.xavier_normal_(layer.weight)\n",
        "        nn.init.constant_(layer.bias, 0)\n",
        "\n",
        "  # we'll see if this is used at all\n",
        "  def get_embedding(self, x):\n",
        "    return self.encoder(x)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.encoder(x)\n",
        "    return self.predictor(x)\n",
        "\n",
        "\n",
        "\n",
        "##### Utility Functions #####\n",
        "def count_parameters(model):\n",
        "  count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  return count\n",
        "\n",
        "# normally I'm not a fan of LR scheduler classes but this is a bit more complicated than the usual so I'm going to use that\n",
        "class LR_sched(object):\n",
        "  def __init__(self, opt, warmup_epochs, warmup_lr, total_epochs, base_lr, final_lr, iter_per_epoch=1, constant_predictor_lr=False):\n",
        "\n",
        "    self.base_lr = base_lr\n",
        "    self.constant_predictor_lr = constant_predictor_lr\n",
        "\n",
        "    # calculate the different stages of the scheduler\n",
        "    warmup_iter = iter_per_epoch * warmup_epochs\n",
        "    warmup_lr_schedule = np.linspace(warmup_lr, base_lr, warmup_iter)\n",
        "\n",
        "    decay_iter = iter_per_epoch * (total_epochs - warmup_epochs)\n",
        "    cosine_lr_schedule = final_lr + 0.5 * (base_lr - final_lr) * (1 + np.cos(np.pi * np.arange(decay_iter) / decay_iter))\n",
        "\n",
        "    # set out schedulers values\n",
        "    self.lr_schedule = np.concatenate((warmup_lr_schedule, cosine_lr_schedule))\n",
        "    self.iter = 0\n",
        "    self.opt = opt\n",
        "    self.current_lr = 0\n",
        "\n",
        "  def step(self):\n",
        "    #if self.iter >= len(self.lr_schedule):\n",
        "    #  return self.current_lr # don't do anyhting if we reached the end\n",
        "    for param_group in self.opt.param_groups:\n",
        "      if self.constant_predictor_lr and param_group['name'] == 'predictor': # we'll see if this working when we run it TODO never used a param_group object before\n",
        "        param_group['lr'] = self.base_lr\n",
        "      else:\n",
        "        lr = param_group['lr'] = self.lr_schedule[self.iter]\n",
        "    self.current_lr = lr\n",
        "    self.iter += 1\n",
        "    return lr\n",
        "\n",
        "  def get_lr(self):\n",
        "    return self.current_lr\n",
        "\n",
        "\n",
        "##### Actual full model #####\n",
        "class PINN(nn.Module):\n",
        "  def __init__(self, arguments):\n",
        "    super(PINN, self).__init__()\n",
        "    self.args = arguments\n",
        "\n",
        "    # the original has some logging stuff here but I'm going to skip that for now\n",
        "\n",
        "    # the actual guesser\n",
        "    self.solv_u = Sol_u(self.args.feature_length).to(device) # pass variable size to solU\n",
        "    # the ODE simulator\n",
        "    # NOTE: original 35\n",
        "    if self.args.second_derivatives:\n",
        "      # if the model uses second derivatives figure out the size think x3 + 1\n",
        "      # should work retain the original +1 and then add 1x input feature because\n",
        "      # you took a derivative with respect to each of those\n",
        "      self.dynamic_F = MLPBLock(input_dim=self.args.feature_length*3 + 1,\n",
        "                              output_dim=1,\n",
        "                              num_layers=self.args.F_layers_num,\n",
        "                              hidden_dim=self.args.F_hidden_dim,\n",
        "                              dropout=0.2\n",
        "                         ).to(device)\n",
        "    else:\n",
        "      # with no second derivatives the model 17 features to 35 dynamic F features (double check the formula for extended features)\n",
        "      self.dynamic_F = MLPBLock(input_dim=self.args.feature_length*2 + 1,\n",
        "                              output_dim=1,\n",
        "                              num_layers=self.args.F_layers_num,\n",
        "                              hidden_dim=self.args.F_hidden_dim,\n",
        "                              dropout=0.2\n",
        "                         ).to(device)\n",
        "\n",
        "    # use to optimizers, one for solvU and one for dynamic_F\n",
        "    self.opt_solvU = torch.optim.Adam(self.solv_u.parameters(), lr=self.args.warmup_lr)\n",
        "    self.opt_F = torch.optim.Adam(self.dynamic_F.parameters(), lr=self.args.lr_F)\n",
        "\n",
        "    self.scheduler = LR_sched(opt=self.opt_solvU,\n",
        "                              warmup_epochs=self.args.warmup_epochs,\n",
        "                              warmup_lr=self.args.warmup_lr,\n",
        "                              total_epochs=self.args.epochs,\n",
        "                              base_lr=self.args.lr,\n",
        "                              final_lr=self.args.final_lr)\n",
        "    self.loss_func = nn.MSELoss()\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    self.best_model = None\n",
        "    # these are for scaling the loss terms\n",
        "    self.alpha = self.args.alpha\n",
        "    self.beta = self.args.beta\n",
        "\n",
        "  # NOTE: missing a bunch of logger functions but I don't really want those right now\n",
        "  def load_model(self, model_path):\n",
        "    chkpt = torch.load(model_path)\n",
        "    self.solv_u.load_state_dict(chkpt['solv_u'])\n",
        "    self.dynamic_F.load_state_dict(chkpt['dynamic_F'])\n",
        "    for param in self.solv_u.parameters():\n",
        "      param.requires_grad = True\n",
        "\n",
        "  def predict(self, xt):\n",
        "    return self.solv_u(xt)\n",
        "\n",
        "  # run a test\n",
        "  def Test(self, testloader):\n",
        "    self.eval()\n",
        "    true_label = []\n",
        "    pred_label = []\n",
        "    with torch.no_grad():\n",
        "      for (index, (x1,_,y1,_)) in enumerate(testloader):\n",
        "        x1 = x1.to(device)\n",
        "        u1 = self.predict(x1)\n",
        "        true_label.append(y1)\n",
        "        pred_label.append(u1.cpu().detach().numpy()) # dk why all this stuff but it works\n",
        "    true_label = np.concatenate(true_label)\n",
        "    pred_label = np.concatenate(pred_label)\n",
        "\n",
        "    return true_label, pred_label\n",
        "\n",
        "  def Validate(self, validloader):\n",
        "    self.eval()\n",
        "    true_label = []\n",
        "    pred_label = []\n",
        "    with torch.no_grad():\n",
        "      for (index, (x1,_,y1,_)) in enumerate(validloader):\n",
        "        x1 = x1.to(device)\n",
        "        u1 = self.predict(x1)\n",
        "        true_label.append(y1)\n",
        "        pred_label.append(u1.cpu().detach().numpy()) # dk why all this stuff but it works\n",
        "    true_label = np.concatenate(true_label)\n",
        "    pred_label = np.concatenate(pred_label)\n",
        "    mse = self.loss_func(torch.tensor(true_label), torch.tensor(pred_label))\n",
        "\n",
        "    return mse.item()\n",
        "\n",
        "  def forward(self, x_full):\n",
        "    x_full.requires_grad = True\n",
        "    x = x_full[:,0:-1]\n",
        "    t = x_full[:,-1:]\n",
        "\n",
        "    u = self.solv_u(torch.cat((x,t), dim=1))\n",
        "    u_t = grad(u.sum(),t,\n",
        "               create_graph=True,\n",
        "               only_inputs=True,\n",
        "               allow_unused=True)[0]\n",
        "\n",
        "    u_x = grad(u.sum(), x,\n",
        "               create_graph=True,\n",
        "               only_inputs=True,\n",
        "               allow_unused=True)[0]\n",
        "\n",
        "    if self.args.second_derivatives:\n",
        "      u_xx = grad(u_x.sum(), x,\n",
        "                  create_graph=True,\n",
        "                  only_inputs=True,\n",
        "                  allow_unused=True)[0]\n",
        "      u_tt = grad(u_t.sum(), t,\n",
        "                  create_graph=True,\n",
        "                  only_inputs=True,\n",
        "                  allow_unused=True)[0]\n",
        "      F = self.dynamic_F(torch.cat([x_full, u, u_x, u_t, u_xx, u_tt], dim=1))\n",
        "    else:\n",
        "      # THIS is where the magic happens. feed the solution of the regular predictor and the derivatives of the solution to the ODE simulator\n",
        "      F = self.dynamic_F(torch.cat([x_full, u, u_x, u_t], dim=1))\n",
        "\n",
        "    # d/dt (solution) = F so the difference should be 0\n",
        "    f = u_t - F\n",
        "    return u,f\n",
        "\n",
        "\n",
        "  def one_epoch_training(self, epoch, dataloader):\n",
        "    self.train()\n",
        "\n",
        "    # keep track of the losses\n",
        "    loss1_tracker = []\n",
        "    loss2_tracker = []\n",
        "    loss3_tracker = [] # they have average meters but whatever\n",
        "\n",
        "    for iter, (x1,x2,y1,y2) in enumerate(dataloader): # note uiqte sure why it does two ata a time but ok\n",
        "      x1 = x1.to(device)\n",
        "      x2 = x2.to(device)\n",
        "      y1 = y1.to(device)\n",
        "      y2 = y2.to(device)\n",
        "\n",
        "      u1,f1 = self.forward(x1)\n",
        "      u2,f2 = self.forward(x2)\n",
        "\n",
        "\n",
        "      # NOTE: without the physics loss the total loss improves significantly. It's because our dataset has multiple batteries in\n",
        "      # one file so sometimes the capacity will spike back up. Doesn't make sense to enforce that loss\n",
        "\n",
        "      # data loss\n",
        "      l1 = 1*self.loss_func(u1,y1) #+ 0.5*self.loss_func(u2,y2) NOTE: trying without the sequence info\n",
        "\n",
        "      # pde loss\n",
        "      f_target = torch.zeros_like(f1)\n",
        "      # the PDE and the guesser should be predicting the same output, so the f1 and f2 should be 0\n",
        "      l2 = 1*self.loss_func(f1,f_target) #+ 0.5*self.loss_func(f2,f_target) NOTE trying without the sequence info\n",
        "\n",
        "      # physics loss\n",
        "      # the capacity should be decreasing all the time\n",
        "      l3 = 0 #self.relu(torch.mul(u2-u1, y1-y2)).sum() NOTE trying without the sequence info\n",
        "\n",
        "      # total loss\n",
        "      loss = l1 + self.alpha*l2 #+ self.beta*l3\n",
        "\n",
        "      self.opt_solvU.zero_grad()\n",
        "      self.opt_F.zero_grad()\n",
        "      loss.backward()\n",
        "      self.opt_solvU.step()\n",
        "      self.opt_F.step()\n",
        "\n",
        "      loss1_tracker.append(l1.item())\n",
        "      loss2_tracker.append(l2.item())\n",
        "      loss3_tracker.append(l3) #, x1.size(0)) NOTE trying without the sequence info\n",
        "\n",
        "      if (iter+1)%50 == 0:\n",
        "        print(\"[epoch:{} iter:{}] data_loss:{:.6f} PDE loss:{:.6f}, physics loss:{:.6f}\".format(epoch, iter+1, l1, l2, l3))\n",
        "\n",
        "    return sum(loss1_tracker)/len(loss1_tracker), sum(loss2_tracker)/len(loss2_tracker), sum(loss3_tracker)/len(loss3_tracker)\n",
        "\n",
        "  def Train(self, trainloader, testloader, validloader, debug=False):\n",
        "    min_mse = 10\n",
        "    valid_mse = 10\n",
        "    early_stop = 0\n",
        "    mae = 10\n",
        "    losses = []\n",
        "    for epoch in range(1,self.args.epochs+1):\n",
        "      early_stop += 1\n",
        "      train_loss1, train_loss2, train_loss3 = self.one_epoch_training(epoch, trainloader)\n",
        "      current_lr = self.scheduler.step()\n",
        "\n",
        "      state = '[Train] epoch:{}, lr:{:.6f}, total loss:{:.6f}'.format(epoch, current_lr, train_loss1 + self.alpha*train_loss2 + self.beta*train_loss3)\n",
        "      if debug:\n",
        "        print(state)\n",
        "      losses.append(train_loss1 + self.alpha*train_loss2 + self.beta*train_loss3)\n",
        "\n",
        "      if epoch%1 ==0 and validloader is not None:\n",
        "        valid_mse = self.Validate(validloader)\n",
        "        info = '[Validation] epoch:{}, MSE:{}'.format(epoch,valid_mse)\n",
        "\n",
        "      if valid_mse < min_mse and testloader is not None:\n",
        "        min_mse = valid_mse\n",
        "        true_label, pred_label = self.Test(testloader)\n",
        "        # missing a logger thing here\n",
        "        early_stop = 0\n",
        "        self.best_model = {'solv_u': self.solv_u.state_dict(),\n",
        "                             'dynamic_F': self.dynamic_F.state_dict()}\n",
        "\n",
        "      if self.args.early_stop is not None and early_stop > self.args.early_stop:\n",
        "        print(\"Early stopping at: \", epoch)\n",
        "        break\n",
        "    return losses"
      ],
      "metadata": {
        "id": "dDAHHFxjU_fs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data loaders"
      ],
      "metadata": {
        "id": "zMrSCr5p8l_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# no clue how the trainloader works for their end so I'm gonna take a break and go swim\n",
        "# back now\n",
        "# messing about with their files to see how they loaded everything\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "class DF():\n",
        "  def __init__(self, args):\n",
        "    self.normalization = True\n",
        "    self.normalization_method = args.normalization_method # min-max or z-score\n",
        "    self.args = args\n",
        "  # very unclear on this\n",
        "  def _3_sigma(self, Ser1):\n",
        "    rule = (Ser1.mean() - 3*Ser1.std() > Ser1) | (Ser1.mean() + 3*Ser1.std() < Ser1)\n",
        "    index = np.arange(Ser1.shape[0])[rule]\n",
        "    return index\n",
        "\n",
        "  # think this is just drop entries where the values are 3 sigma off the average. interesting\n",
        "  def delete_3_sigma(self,df):\n",
        "    df = df.replace([np.inf, -np.inf], np.nan)\n",
        "    df = df.dropna()\n",
        "    df = df.reset_index(drop=True)\n",
        "    out_index = []\n",
        "    for col in df.columns:\n",
        "      index = self._3_sigma(df[col])\n",
        "      out_index.extend(index)\n",
        "    out_index = list(set(out_index))\n",
        "    df = df.drop(out_index)\n",
        "    df = df.reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "  def read_one_csv(self, file_name, nominal_capacity=None):\n",
        "\n",
        "    df = pd.read_csv(file_name)\n",
        "    #df.insert(df.shape[1]-1, 'cycle index', np.arange(df.shape[0])) # add the cycle index in the penultimate column\n",
        "    df = self.delete_3_sigma(df)\n",
        "\n",
        "    if nominal_capacity is not None:\n",
        "      df['capacity'] = df['capacity']/nominal_capacity # normalize the capacity\n",
        "      features_df = df.iloc[:, :-1] # get all the columns bar the last one\n",
        "      if self.normalization_method == 'min-max':\n",
        "        features_df = 2*(features_df - features_df.min())/(features_df.max() - features_df.min()) - 1\n",
        "      elif self.normalization_method == 'z-score':\n",
        "        features_df = (features_df - features_df.mean())/features_df.std()\n",
        "    df = df.astype({col: float for col in df.columns[:-1]}) # apparently putting everything back complains because everything just became floats\n",
        "    df.iloc[:,:-1] = features_df.astype(float) # put everything back\n",
        "    return df\n",
        "\n",
        "  # they have a bunch of separate batteries but we only have one big file\n",
        "  def load_battery(self, path, nominal_capacity=None):\n",
        "    df = self.read_one_csv(path, nominal_capacity)\n",
        "    print(df)\n",
        "    # get the x and y\n",
        "    x = df.iloc[:, :-1].values\n",
        "    y = df.iloc[:,-1].values\n",
        "\n",
        "    #x1 and y1 are all but the last and x2,y2 are all but the first\n",
        "    # they're basically shifted version of each other at every point x2 is the next time step of x1\n",
        "    x1 = x[:-1]\n",
        "    x2 = x[1:]\n",
        "    y1 = y[:-1]\n",
        "    y2 = y[1:]\n",
        "\n",
        "    return (x1,y1), (x2,y2)\n",
        "\n",
        "  def load_all(self, path_list, nominal_capacity):\n",
        "    X1, X2, Y1, Y2 = [], [], [], []\n",
        "    for path in path_list:\n",
        "      (x1,y1), (x2,y2) = self.load_battery(path, nominal_capacity)\n",
        "      X1.append(x1)\n",
        "      X2.append(x2)\n",
        "      Y1.append(y1)\n",
        "      Y2.append(y2)\n",
        "\n",
        "    X1 = np.concatenate(X1, axis=0)\n",
        "    X2 = np.concatenate(X2, axis=0)\n",
        "    Y1 = np.concatenate(Y1, axis=0)\n",
        "    Y2 = np.concatenate(Y2, axis=0)\n",
        "\n",
        "    tensorX1 = torch.from_numpy(X1).float()\n",
        "    tensorX2 = torch.from_numpy(X2).float()\n",
        "    tensorY1 = torch.from_numpy(Y1).float().view(-1,1)\n",
        "    tensorY2 = torch.from_numpy(Y2).float().view(-1,1)\n",
        "\n",
        "    # they used multiple train/validate/test setups\n",
        "\n",
        "    split = int(tensorX1.shape[0] * 0.8)\n",
        "    trainX1, testX1 = tensorX1[:split], tensorX1[split:]\n",
        "    trainX2, testX2 = tensorX2[:split], tensorX2[split:]\n",
        "    trainY1, testY1 = tensorY1[:split], tensorY1[split:]\n",
        "    trainY2, testY2 = tensorY2[:split], tensorY2[split:]\n",
        "\n",
        "\n",
        "\n",
        "    trainX1, validX1, trainX2, validX2, trainY1, validY1, trainY2, validY2 = train_test_split(trainX1, trainX2, trainY1, trainY2, test_size=0.2, random_state=420)\n",
        "\n",
        "    # setup 1 - regular 80/20 split for train/test and 80/20 split inside train for train/valid\n",
        "    trainloader = DataLoader(TensorDataset(trainX1, trainX2, trainY1, trainY2), batch_size=self.args.batch_size, shuffle=True)\n",
        "    validloader = DataLoader(TensorDataset(validX1, validX2, validY1, validY2), batch_size=self.args.batch_size, shuffle=True)\n",
        "    testloader = DataLoader(TensorDataset(testX1, testX2, testY1, testY2), batch_size=self.args.batch_size, shuffle=True)\n",
        "\n",
        "    # setup 2 - no test set\n",
        "    trainX1, validX1, trainX2, validX2, trainY1, validY1, trainY2, validY2 = train_test_split(tensorX1, tensorX2, tensorY1, tensorY2, test_size=0.2, random_state=420)\n",
        "    trainloader2 = DataLoader(TensorDataset(trainX1, trainX2, trainY1, trainY2), batch_size=self.args.batch_size, shuffle=True)\n",
        "    validloader2 = DataLoader(TensorDataset(validX1, validX2, validY1, validY2), batch_size=self.args.batch_size, shuffle=True)\n",
        "\n",
        "    # setup 3 - only test set\n",
        "    testloader3 = DataLoader(TensorDataset(tensorX1, tensorX2, tensorY1, tensorY2), batch_size=self.args.batch_size, shuffle=False)\n",
        "\n",
        "    loader = {\n",
        "          'train':trainloader,\n",
        "          'valid':validloader,\n",
        "          'test':testloader,\n",
        "          'train_2':trainloader2,\n",
        "          'valid_2':validloader2,\n",
        "          'test_3':testloader3\n",
        "    }\n",
        "    return loader\n"
      ],
      "metadata": {
        "id": "s3JW52RYOQ-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "LhrQdMgo8tWR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regular Features"
      ],
      "metadata": {
        "id": "ULoTBJCd8vPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# little bit of hee hee haha data manipulation\n",
        "df = pd.read_csv('/content/drive/MyDrive/Machine Learning Project (ECE 228)/Battery Dataset/features.csv')\n",
        "df = df[df['charge_CC_time_V']!=0.0] # these were giving strange results\n",
        "df = df[df['charge_CV_time_I']!=0.0]\n",
        "df.to_csv(\"clean_features.csv\", index=False)"
      ],
      "metadata": {
        "id": "BnYDA3yPtaW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class args:\n",
        "  pass\n",
        "\n",
        "# hehe tricks\n",
        "arg = args()\n",
        "\n",
        "arg.data = 'our_stuff'\n",
        "arg.batch = 10\n",
        "arg.batch_size = 256\n",
        "arg.normalization_method = 'z-score'\n",
        "arg.epochs = 10000\n",
        "arg.lr = 1e-3\n",
        "arg.warmup_epochs = 10\n",
        "arg.warmup_lr = 5e-4\n",
        "arg.final_lr = 1e-4\n",
        "arg.lr_F = 1e-3\n",
        "arg.iter_per_epoch = 1\n",
        "arg.F_layers_num = 3\n",
        "arg.F_hidden_dim = 60\n",
        "arg.alpha = 1\n",
        "arg.beta = 1\n",
        "arg.early_stop = 500\n",
        "arg.feature_length = 17 # use 17 for regular feature count\n",
        "arg.second_derivatives = False # use to not include second derivatives\n"
      ],
      "metadata": {
        "id": "wifZofk2MQ79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ok let's load a dataset\n",
        "reader = DF(arg)"
      ],
      "metadata": {
        "id": "pCU7xgx2zcU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's get a loader back\n",
        "loader = reader.load_all([\"./clean_features.csv\"], nominal_capacity=2)"
      ],
      "metadata": {
        "id": "LwsCIhys6wlI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69d7bddf-db04-4aa7-c130-ffbaab56a56d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      charge_CC_mean_V  charge_CC_std_V  charge_CC_kurtosis_V  \\\n",
            "0             1.051718        -0.713588             -0.094252   \n",
            "1             1.224559        -0.542147             -0.040044   \n",
            "2             0.692781         0.079437             -0.421930   \n",
            "3             1.704675        -1.676956              2.159515   \n",
            "4             0.785932        -0.140048              0.086626   \n",
            "...                ...              ...                   ...   \n",
            "2420         -0.443082         0.720153             -0.611928   \n",
            "2421         -0.453659         0.645414             -0.614842   \n",
            "2422         -0.422004         0.627261             -0.612344   \n",
            "2423         -0.513857         0.688364             -0.596764   \n",
            "2424         -0.418204         0.602553             -0.612677   \n",
            "\n",
            "      charge_CC_skew_V  charge_CC_time_V  charge_CC_charge  charge_CC_slope_V  \\\n",
            "0            -0.723178         -2.570478         -2.557431           7.281268   \n",
            "1            -0.797171         -2.611115         -2.598165           8.519176   \n",
            "2            -0.471074         -2.599677         -2.586782           8.297809   \n",
            "3            -2.014463         -0.169714         -0.144630          -0.227694   \n",
            "4            -0.736379         -0.690659         -0.667485           0.054417   \n",
            "...                ...               ...               ...                ...   \n",
            "2420          0.417345         -0.381994         -0.356347          -0.103665   \n",
            "2421          0.423965         -0.398756         -0.373971          -0.101562   \n",
            "2422          0.427483         -0.422011         -0.396601          -0.092649   \n",
            "2423          0.499367         -0.168751         -0.142701          -0.206949   \n",
            "2424          0.405564         -0.445883         -0.420552          -0.081591   \n",
            "\n",
            "      charge_CC_entropy_V  charge_CV_mean_I  charge_CV_std_dev_I  \\\n",
            "0               -3.434897         -3.434933            -3.419796   \n",
            "1               -3.615999         -2.054186            -1.588874   \n",
            "2               -3.681913         -0.509941             0.287333   \n",
            "3               -0.517590         -1.430043            -0.720811   \n",
            "4               -0.822937         -0.144988             0.375235   \n",
            "...                   ...               ...                  ...   \n",
            "2420            -0.719246          0.431733            -1.386463   \n",
            "2421            -0.736011          0.417935            -1.654983   \n",
            "2422            -0.752977          0.522672            -1.230410   \n",
            "2423            -0.622616          0.321845            -1.698875   \n",
            "2424            -0.778813          0.738648            -1.126685   \n",
            "\n",
            "      charge_CV_kurtosis_I  charge_CV_skew_I  charge_CV_time_I  \\\n",
            "0                 3.165489          3.146038          1.305891   \n",
            "1                 2.716360          2.960147          1.264916   \n",
            "2                -0.067089          0.816123          0.149106   \n",
            "3                 1.469249          1.189845          1.432231   \n",
            "4                 0.212768          0.748297          1.322877   \n",
            "...                    ...               ...               ...   \n",
            "2420              0.047583         -0.033412         -0.581324   \n",
            "2421             -0.034865         -0.175776         -0.614544   \n",
            "2422              0.033223         -0.014649         -0.585731   \n",
            "2423              0.301483          0.234447         -0.397120   \n",
            "2424              0.042512          0.021309         -0.553738   \n",
            "\n",
            "      charge_CV_charge_I  charge_CV_slope_I  charge_CV_entropy_I  cycle_index  \\\n",
            "0               0.934968           1.224660             0.135281    -1.224588   \n",
            "1               1.084453           1.194460             0.096569    -1.204177   \n",
            "2               0.124126           0.454231            -0.968551    -0.836783   \n",
            "3               1.342167           1.251640             0.196704    -1.224588   \n",
            "4               1.413682           1.239426             0.128908    -1.204177   \n",
            "...                  ...                ...                  ...          ...   \n",
            "2420           -0.568314          -0.375006            -1.572750     1.347170   \n",
            "2421           -0.606129          -0.416189            -1.618723     1.367581   \n",
            "2422           -0.564282          -0.364575            -1.602036     1.387992   \n",
            "2423           -0.377817          -0.099872            -1.411697     1.408402   \n",
            "2424           -0.514577          -0.325104            -1.586194     1.428813   \n",
            "\n",
            "      capacity  \n",
            "0     0.532271  \n",
            "1     0.521953  \n",
            "2     0.350731  \n",
            "3     0.782358  \n",
            "4     0.779403  \n",
            "...        ...  \n",
            "2420  0.698253  \n",
            "2421  0.697567  \n",
            "2422  0.689922  \n",
            "2423  0.689690  \n",
            "2424  0.691855  \n",
            "\n",
            "[2425 rows x 18 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pinn = PINN(arg)"
      ],
      "metadata": {
        "id": "IyRhSLY7NDKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pinn"
      ],
      "metadata": {
        "id": "no_zWIlv7GHh",
        "outputId": "bdd53928-993a-472e-fd79-c60bcd9791d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PINN(\n",
              "  (solv_u): Sol_u(\n",
              "    (encoder): MLPBLock(\n",
              "      (net): Sequential(\n",
              "        (0): Linear(in_features=17, out_features=60, bias=True)\n",
              "        (1): Sin()\n",
              "        (2): Linear(in_features=60, out_features=60, bias=True)\n",
              "        (3): Sin()\n",
              "        (4): Dropout(p=0.2, inplace=False)\n",
              "        (5): Linear(in_features=60, out_features=32, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (predictor): Predict(\n",
              "      (layer): Sequential(\n",
              "        (0): Dropout(p=0.2, inplace=False)\n",
              "        (1): Linear(in_features=32, out_features=32, bias=True)\n",
              "        (2): Sin()\n",
              "        (3): Linear(in_features=32, out_features=1, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dynamic_F): MLPBLock(\n",
              "    (net): Sequential(\n",
              "      (0): Linear(in_features=35, out_features=60, bias=True)\n",
              "      (1): Sin()\n",
              "      (2): Linear(in_features=60, out_features=60, bias=True)\n",
              "      (3): Sin()\n",
              "      (4): Dropout(p=0.2, inplace=False)\n",
              "      (5): Linear(in_features=60, out_features=1, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (loss_func): MSELoss()\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "losses_regular = pinn.Train(trainloader=loader['train'],\n",
        "           validloader=loader['valid'],\n",
        "           testloader=loader['test'], debug=True)\n",
        "if overwrite:\n",
        "  np.save('/content/drive/MyDrive/Machine Learning Project (ECE 228)/Battery Dataset/loss_regular.npy', losses_regular)"
      ],
      "metadata": {
        "id": "fx_z5gMR-p_i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3614695b-0cc5-4842-972b-01d7b8e49ce2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Train] epoch:1, lr:0.000500, total loss:1.341440\n",
            "[Train] epoch:2, lr:0.000556, total loss:0.709628\n",
            "[Train] epoch:3, lr:0.000611, total loss:0.567336\n",
            "[Train] epoch:4, lr:0.000667, total loss:0.441640\n",
            "[Train] epoch:5, lr:0.000722, total loss:0.341767\n",
            "[Train] epoch:6, lr:0.000778, total loss:0.307083\n",
            "[Train] epoch:7, lr:0.000833, total loss:0.239965\n",
            "[Train] epoch:8, lr:0.000889, total loss:0.229554\n",
            "[Train] epoch:9, lr:0.000944, total loss:0.210635\n",
            "[Train] epoch:10, lr:0.001000, total loss:0.160996\n",
            "[Train] epoch:11, lr:0.001000, total loss:0.155837\n",
            "[Train] epoch:12, lr:0.001000, total loss:0.139721\n",
            "[Train] epoch:13, lr:0.001000, total loss:0.120757\n",
            "[Train] epoch:14, lr:0.001000, total loss:0.108301\n",
            "[Train] epoch:15, lr:0.001000, total loss:0.101541\n",
            "[Train] epoch:16, lr:0.001000, total loss:0.100483\n",
            "[Train] epoch:17, lr:0.001000, total loss:0.099477\n",
            "[Train] epoch:18, lr:0.001000, total loss:0.082730\n",
            "[Train] epoch:19, lr:0.001000, total loss:0.073741\n",
            "[Train] epoch:20, lr:0.001000, total loss:0.073740\n",
            "[Train] epoch:21, lr:0.001000, total loss:0.063876\n",
            "[Train] epoch:22, lr:0.001000, total loss:0.058719\n",
            "[Train] epoch:23, lr:0.001000, total loss:0.056718\n",
            "[Train] epoch:24, lr:0.001000, total loss:0.054773\n",
            "[Train] epoch:25, lr:0.001000, total loss:0.058050\n",
            "[Train] epoch:26, lr:0.001000, total loss:0.049234\n",
            "[Train] epoch:27, lr:0.001000, total loss:0.045919\n",
            "[Train] epoch:28, lr:0.001000, total loss:0.048209\n",
            "[Train] epoch:29, lr:0.001000, total loss:0.042874\n",
            "[Train] epoch:30, lr:0.001000, total loss:0.040365\n",
            "[Train] epoch:31, lr:0.001000, total loss:0.036810\n",
            "[Train] epoch:32, lr:0.001000, total loss:0.036822\n",
            "[Train] epoch:33, lr:0.001000, total loss:0.035748\n",
            "[Train] epoch:34, lr:0.001000, total loss:0.033521\n",
            "[Train] epoch:35, lr:0.001000, total loss:0.033496\n",
            "[Train] epoch:36, lr:0.001000, total loss:0.030948\n",
            "[Train] epoch:37, lr:0.001000, total loss:0.029375\n",
            "[Train] epoch:38, lr:0.001000, total loss:0.027292\n",
            "[Train] epoch:39, lr:0.001000, total loss:0.025856\n",
            "[Train] epoch:40, lr:0.001000, total loss:0.024131\n",
            "[Train] epoch:41, lr:0.001000, total loss:0.024931\n",
            "[Train] epoch:42, lr:0.001000, total loss:0.020957\n",
            "[Train] epoch:43, lr:0.001000, total loss:0.022279\n",
            "[Train] epoch:44, lr:0.001000, total loss:0.022566\n",
            "[Train] epoch:45, lr:0.001000, total loss:0.024000\n",
            "[Train] epoch:46, lr:0.001000, total loss:0.020277\n",
            "[Train] epoch:47, lr:0.001000, total loss:0.018857\n",
            "[Train] epoch:48, lr:0.001000, total loss:0.017964\n",
            "[Train] epoch:49, lr:0.001000, total loss:0.017649\n",
            "[Train] epoch:50, lr:0.001000, total loss:0.017493\n",
            "[Train] epoch:51, lr:0.001000, total loss:0.017838\n",
            "[Train] epoch:52, lr:0.001000, total loss:0.015476\n",
            "[Train] epoch:53, lr:0.001000, total loss:0.015445\n",
            "[Train] epoch:54, lr:0.001000, total loss:0.015545\n",
            "[Train] epoch:55, lr:0.001000, total loss:0.014983\n",
            "[Train] epoch:56, lr:0.001000, total loss:0.013952\n",
            "[Train] epoch:57, lr:0.001000, total loss:0.014188\n",
            "[Train] epoch:58, lr:0.001000, total loss:0.012428\n",
            "[Train] epoch:59, lr:0.001000, total loss:0.013368\n",
            "[Train] epoch:60, lr:0.001000, total loss:0.011318\n",
            "[Train] epoch:61, lr:0.001000, total loss:0.013310\n",
            "[Train] epoch:62, lr:0.001000, total loss:0.012559\n",
            "[Train] epoch:63, lr:0.001000, total loss:0.012156\n",
            "[Train] epoch:64, lr:0.001000, total loss:0.011516\n",
            "[Train] epoch:65, lr:0.001000, total loss:0.012068\n",
            "[Train] epoch:66, lr:0.001000, total loss:0.011513\n",
            "[Train] epoch:67, lr:0.001000, total loss:0.011141\n",
            "[Train] epoch:68, lr:0.001000, total loss:0.010097\n",
            "[Train] epoch:69, lr:0.001000, total loss:0.009789\n",
            "[Train] epoch:70, lr:0.001000, total loss:0.010056\n",
            "[Train] epoch:71, lr:0.001000, total loss:0.010809\n",
            "[Train] epoch:72, lr:0.001000, total loss:0.012372\n",
            "[Train] epoch:73, lr:0.001000, total loss:0.009764\n",
            "[Train] epoch:74, lr:0.001000, total loss:0.009300\n",
            "[Train] epoch:75, lr:0.001000, total loss:0.009428\n",
            "[Train] epoch:76, lr:0.001000, total loss:0.009060\n",
            "[Train] epoch:77, lr:0.001000, total loss:0.009431\n",
            "[Train] epoch:78, lr:0.001000, total loss:0.008357\n",
            "[Train] epoch:79, lr:0.001000, total loss:0.008498\n",
            "[Train] epoch:80, lr:0.001000, total loss:0.008579\n",
            "[Train] epoch:81, lr:0.001000, total loss:0.009090\n",
            "[Train] epoch:82, lr:0.001000, total loss:0.007841\n",
            "[Train] epoch:83, lr:0.001000, total loss:0.008197\n",
            "[Train] epoch:84, lr:0.001000, total loss:0.007968\n",
            "[Train] epoch:85, lr:0.001000, total loss:0.008060\n",
            "[Train] epoch:86, lr:0.001000, total loss:0.007469\n",
            "[Train] epoch:87, lr:0.001000, total loss:0.007651\n",
            "[Train] epoch:88, lr:0.001000, total loss:0.007627\n",
            "[Train] epoch:89, lr:0.001000, total loss:0.007798\n",
            "[Train] epoch:90, lr:0.001000, total loss:0.007120\n",
            "[Train] epoch:91, lr:0.001000, total loss:0.007403\n",
            "[Train] epoch:92, lr:0.001000, total loss:0.007115\n",
            "[Train] epoch:93, lr:0.001000, total loss:0.006624\n",
            "[Train] epoch:94, lr:0.001000, total loss:0.006597\n",
            "[Train] epoch:95, lr:0.001000, total loss:0.007804\n",
            "[Train] epoch:96, lr:0.001000, total loss:0.006311\n",
            "[Train] epoch:97, lr:0.001000, total loss:0.007319\n",
            "[Train] epoch:98, lr:0.001000, total loss:0.006545\n",
            "[Train] epoch:99, lr:0.001000, total loss:0.007424\n",
            "[Train] epoch:100, lr:0.001000, total loss:0.006483\n",
            "[Train] epoch:101, lr:0.001000, total loss:0.007029\n",
            "[Train] epoch:102, lr:0.001000, total loss:0.006439\n",
            "[Train] epoch:103, lr:0.001000, total loss:0.006924\n",
            "[Train] epoch:104, lr:0.001000, total loss:0.006312\n",
            "[Train] epoch:105, lr:0.001000, total loss:0.006031\n",
            "[Train] epoch:106, lr:0.001000, total loss:0.006614\n",
            "[Train] epoch:107, lr:0.001000, total loss:0.006426\n",
            "[Train] epoch:108, lr:0.001000, total loss:0.006052\n",
            "[Train] epoch:109, lr:0.001000, total loss:0.006808\n",
            "[Train] epoch:110, lr:0.001000, total loss:0.006424\n",
            "[Train] epoch:111, lr:0.001000, total loss:0.005615\n",
            "[Train] epoch:112, lr:0.001000, total loss:0.006674\n",
            "[Train] epoch:113, lr:0.001000, total loss:0.005783\n",
            "[Train] epoch:114, lr:0.001000, total loss:0.005703\n",
            "[Train] epoch:115, lr:0.001000, total loss:0.006212\n",
            "[Train] epoch:116, lr:0.001000, total loss:0.005755\n",
            "[Train] epoch:117, lr:0.001000, total loss:0.005575\n",
            "[Train] epoch:118, lr:0.001000, total loss:0.005509\n",
            "[Train] epoch:119, lr:0.001000, total loss:0.005206\n",
            "[Train] epoch:120, lr:0.001000, total loss:0.005253\n",
            "[Train] epoch:121, lr:0.001000, total loss:0.004956\n",
            "[Train] epoch:122, lr:0.001000, total loss:0.005842\n",
            "[Train] epoch:123, lr:0.001000, total loss:0.005686\n",
            "[Train] epoch:124, lr:0.001000, total loss:0.004929\n",
            "[Train] epoch:125, lr:0.001000, total loss:0.005298\n",
            "[Train] epoch:126, lr:0.001000, total loss:0.005808\n",
            "[Train] epoch:127, lr:0.001000, total loss:0.005336\n",
            "[Train] epoch:128, lr:0.001000, total loss:0.005519\n",
            "[Train] epoch:129, lr:0.001000, total loss:0.005405\n",
            "[Train] epoch:130, lr:0.001000, total loss:0.004542\n",
            "[Train] epoch:131, lr:0.001000, total loss:0.005109\n",
            "[Train] epoch:132, lr:0.001000, total loss:0.004714\n",
            "[Train] epoch:133, lr:0.001000, total loss:0.004480\n",
            "[Train] epoch:134, lr:0.001000, total loss:0.004744\n",
            "[Train] epoch:135, lr:0.001000, total loss:0.004877\n",
            "[Train] epoch:136, lr:0.001000, total loss:0.005039\n",
            "[Train] epoch:137, lr:0.001000, total loss:0.005516\n",
            "[Train] epoch:138, lr:0.001000, total loss:0.005219\n",
            "[Train] epoch:139, lr:0.001000, total loss:0.004448\n",
            "[Train] epoch:140, lr:0.001000, total loss:0.004453\n",
            "[Train] epoch:141, lr:0.001000, total loss:0.005485\n",
            "[Train] epoch:142, lr:0.001000, total loss:0.004461\n",
            "[Train] epoch:143, lr:0.001000, total loss:0.004545\n",
            "[Train] epoch:144, lr:0.001000, total loss:0.004394\n",
            "[Train] epoch:145, lr:0.001000, total loss:0.004206\n",
            "[Train] epoch:146, lr:0.001000, total loss:0.004462\n",
            "[Train] epoch:147, lr:0.001000, total loss:0.004028\n",
            "[Train] epoch:148, lr:0.001000, total loss:0.004020\n",
            "[Train] epoch:149, lr:0.001000, total loss:0.004095\n",
            "[Train] epoch:150, lr:0.001000, total loss:0.004122\n",
            "[Train] epoch:151, lr:0.001000, total loss:0.004191\n",
            "[Train] epoch:152, lr:0.001000, total loss:0.003690\n",
            "[Train] epoch:153, lr:0.001000, total loss:0.003926\n",
            "[Train] epoch:154, lr:0.001000, total loss:0.004111\n",
            "[Train] epoch:155, lr:0.001000, total loss:0.004141\n",
            "[Train] epoch:156, lr:0.001000, total loss:0.004071\n",
            "[Train] epoch:157, lr:0.001000, total loss:0.004721\n",
            "[Train] epoch:158, lr:0.001000, total loss:0.003598\n",
            "[Train] epoch:159, lr:0.001000, total loss:0.004257\n",
            "[Train] epoch:160, lr:0.001000, total loss:0.004283\n",
            "[Train] epoch:161, lr:0.000999, total loss:0.004420\n",
            "[Train] epoch:162, lr:0.000999, total loss:0.004155\n",
            "[Train] epoch:163, lr:0.000999, total loss:0.003881\n",
            "[Train] epoch:164, lr:0.000999, total loss:0.003835\n",
            "[Train] epoch:165, lr:0.000999, total loss:0.003969\n",
            "[Train] epoch:166, lr:0.000999, total loss:0.003751\n",
            "[Train] epoch:167, lr:0.000999, total loss:0.003762\n",
            "[Train] epoch:168, lr:0.000999, total loss:0.003697\n",
            "[Train] epoch:169, lr:0.000999, total loss:0.003690\n",
            "[Train] epoch:170, lr:0.000999, total loss:0.003784\n",
            "[Train] epoch:171, lr:0.000999, total loss:0.003455\n",
            "[Train] epoch:172, lr:0.000999, total loss:0.003919\n",
            "[Train] epoch:173, lr:0.000999, total loss:0.003425\n",
            "[Train] epoch:174, lr:0.000999, total loss:0.003531\n",
            "[Train] epoch:175, lr:0.000999, total loss:0.004132\n",
            "[Train] epoch:176, lr:0.000999, total loss:0.003973\n",
            "[Train] epoch:177, lr:0.000999, total loss:0.003661\n",
            "[Train] epoch:178, lr:0.000999, total loss:0.003639\n",
            "[Train] epoch:179, lr:0.000999, total loss:0.003690\n",
            "[Train] epoch:180, lr:0.000999, total loss:0.003519\n",
            "[Train] epoch:181, lr:0.000999, total loss:0.003392\n",
            "[Train] epoch:182, lr:0.000999, total loss:0.003200\n",
            "[Train] epoch:183, lr:0.000999, total loss:0.003474\n",
            "[Train] epoch:184, lr:0.000999, total loss:0.003494\n",
            "[Train] epoch:185, lr:0.000999, total loss:0.003765\n",
            "[Train] epoch:186, lr:0.000999, total loss:0.003249\n",
            "[Train] epoch:187, lr:0.000999, total loss:0.003213\n",
            "[Train] epoch:188, lr:0.000999, total loss:0.003114\n",
            "[Train] epoch:189, lr:0.000999, total loss:0.003250\n",
            "[Train] epoch:190, lr:0.000999, total loss:0.003364\n",
            "[Train] epoch:191, lr:0.000999, total loss:0.003001\n",
            "[Train] epoch:192, lr:0.000999, total loss:0.003018\n",
            "[Train] epoch:193, lr:0.000999, total loss:0.003349\n",
            "[Train] epoch:194, lr:0.000999, total loss:0.003412\n",
            "[Train] epoch:195, lr:0.000999, total loss:0.003202\n",
            "[Train] epoch:196, lr:0.000999, total loss:0.003783\n",
            "[Train] epoch:197, lr:0.000999, total loss:0.003398\n",
            "[Train] epoch:198, lr:0.000999, total loss:0.003479\n",
            "[Train] epoch:199, lr:0.000999, total loss:0.002966\n",
            "[Train] epoch:200, lr:0.000999, total loss:0.003014\n",
            "[Train] epoch:201, lr:0.000999, total loss:0.003222\n",
            "[Train] epoch:202, lr:0.000999, total loss:0.003036\n",
            "[Train] epoch:203, lr:0.000999, total loss:0.002779\n",
            "[Train] epoch:204, lr:0.000999, total loss:0.003120\n",
            "[Train] epoch:205, lr:0.000999, total loss:0.003002\n",
            "[Train] epoch:206, lr:0.000999, total loss:0.003973\n",
            "[Train] epoch:207, lr:0.000999, total loss:0.002957\n",
            "[Train] epoch:208, lr:0.000999, total loss:0.002835\n",
            "[Train] epoch:209, lr:0.000999, total loss:0.003002\n",
            "[Train] epoch:210, lr:0.000999, total loss:0.002708\n",
            "[Train] epoch:211, lr:0.000999, total loss:0.002965\n",
            "[Train] epoch:212, lr:0.000999, total loss:0.003105\n",
            "[Train] epoch:213, lr:0.000999, total loss:0.002725\n",
            "[Train] epoch:214, lr:0.000999, total loss:0.002507\n",
            "[Train] epoch:215, lr:0.000999, total loss:0.002769\n",
            "[Train] epoch:216, lr:0.000999, total loss:0.002641\n",
            "[Train] epoch:217, lr:0.000999, total loss:0.002869\n",
            "[Train] epoch:218, lr:0.000999, total loss:0.002752\n",
            "[Train] epoch:219, lr:0.000999, total loss:0.002879\n",
            "[Train] epoch:220, lr:0.000999, total loss:0.002670\n",
            "[Train] epoch:221, lr:0.000999, total loss:0.003055\n",
            "[Train] epoch:222, lr:0.000999, total loss:0.002578\n",
            "[Train] epoch:223, lr:0.000999, total loss:0.003125\n",
            "[Train] epoch:224, lr:0.000999, total loss:0.002742\n",
            "[Train] epoch:225, lr:0.000999, total loss:0.002517\n",
            "[Train] epoch:226, lr:0.000999, total loss:0.003046\n",
            "[Train] epoch:227, lr:0.000999, total loss:0.002633\n",
            "[Train] epoch:228, lr:0.000999, total loss:0.002676\n",
            "[Train] epoch:229, lr:0.000999, total loss:0.002821\n",
            "[Train] epoch:230, lr:0.000999, total loss:0.002692\n",
            "[Train] epoch:231, lr:0.000999, total loss:0.002384\n",
            "[Train] epoch:232, lr:0.000999, total loss:0.002753\n",
            "[Train] epoch:233, lr:0.000999, total loss:0.002503\n",
            "[Train] epoch:234, lr:0.000999, total loss:0.002745\n",
            "[Train] epoch:235, lr:0.000999, total loss:0.002477\n",
            "[Train] epoch:236, lr:0.000999, total loss:0.002680\n",
            "[Train] epoch:237, lr:0.000999, total loss:0.003135\n",
            "[Train] epoch:238, lr:0.000999, total loss:0.003148\n",
            "[Train] epoch:239, lr:0.000999, total loss:0.002591\n",
            "[Train] epoch:240, lr:0.000999, total loss:0.002546\n",
            "[Train] epoch:241, lr:0.000999, total loss:0.002504\n",
            "[Train] epoch:242, lr:0.000999, total loss:0.002755\n",
            "[Train] epoch:243, lr:0.000999, total loss:0.002482\n",
            "[Train] epoch:244, lr:0.000999, total loss:0.002505\n",
            "[Train] epoch:245, lr:0.000999, total loss:0.002553\n",
            "[Train] epoch:246, lr:0.000999, total loss:0.002908\n",
            "[Train] epoch:247, lr:0.000999, total loss:0.002770\n",
            "[Train] epoch:248, lr:0.000999, total loss:0.002547\n",
            "[Train] epoch:249, lr:0.000999, total loss:0.002504\n",
            "[Train] epoch:250, lr:0.000999, total loss:0.002332\n",
            "[Train] epoch:251, lr:0.000999, total loss:0.002418\n",
            "[Train] epoch:252, lr:0.000999, total loss:0.002695\n",
            "[Train] epoch:253, lr:0.000999, total loss:0.002410\n",
            "[Train] epoch:254, lr:0.000999, total loss:0.002356\n",
            "[Train] epoch:255, lr:0.000999, total loss:0.002240\n",
            "[Train] epoch:256, lr:0.000999, total loss:0.002322\n",
            "[Train] epoch:257, lr:0.000999, total loss:0.002293\n",
            "[Train] epoch:258, lr:0.000999, total loss:0.002410\n",
            "[Train] epoch:259, lr:0.000999, total loss:0.002508\n",
            "[Train] epoch:260, lr:0.000999, total loss:0.002241\n",
            "[Train] epoch:261, lr:0.000999, total loss:0.002705\n",
            "[Train] epoch:262, lr:0.000999, total loss:0.002421\n",
            "[Train] epoch:263, lr:0.000999, total loss:0.002273\n",
            "[Train] epoch:264, lr:0.000999, total loss:0.002382\n",
            "[Train] epoch:265, lr:0.000999, total loss:0.002252\n",
            "[Train] epoch:266, lr:0.000999, total loss:0.002344\n",
            "[Train] epoch:267, lr:0.000999, total loss:0.002260\n",
            "[Train] epoch:268, lr:0.000999, total loss:0.002132\n",
            "[Train] epoch:269, lr:0.000999, total loss:0.002223\n",
            "[Train] epoch:270, lr:0.000999, total loss:0.002299\n",
            "[Train] epoch:271, lr:0.000998, total loss:0.002311\n",
            "[Train] epoch:272, lr:0.000998, total loss:0.002370\n",
            "[Train] epoch:273, lr:0.000998, total loss:0.002161\n",
            "[Train] epoch:274, lr:0.000998, total loss:0.002441\n",
            "[Train] epoch:275, lr:0.000998, total loss:0.002339\n",
            "[Train] epoch:276, lr:0.000998, total loss:0.002153\n",
            "[Train] epoch:277, lr:0.000998, total loss:0.002301\n",
            "[Train] epoch:278, lr:0.000998, total loss:0.002246\n",
            "[Train] epoch:279, lr:0.000998, total loss:0.002258\n",
            "[Train] epoch:280, lr:0.000998, total loss:0.002166\n",
            "[Train] epoch:281, lr:0.000998, total loss:0.002471\n",
            "[Train] epoch:282, lr:0.000998, total loss:0.002148\n",
            "[Train] epoch:283, lr:0.000998, total loss:0.002048\n",
            "[Train] epoch:284, lr:0.000998, total loss:0.002003\n",
            "[Train] epoch:285, lr:0.000998, total loss:0.002236\n",
            "[Train] epoch:286, lr:0.000998, total loss:0.002211\n",
            "[Train] epoch:287, lr:0.000998, total loss:0.002177\n",
            "[Train] epoch:288, lr:0.000998, total loss:0.002374\n",
            "[Train] epoch:289, lr:0.000998, total loss:0.002078\n",
            "[Train] epoch:290, lr:0.000998, total loss:0.002295\n",
            "[Train] epoch:291, lr:0.000998, total loss:0.002323\n",
            "[Train] epoch:292, lr:0.000998, total loss:0.002236\n",
            "[Train] epoch:293, lr:0.000998, total loss:0.002298\n",
            "[Train] epoch:294, lr:0.000998, total loss:0.002289\n",
            "[Train] epoch:295, lr:0.000998, total loss:0.002079\n",
            "[Train] epoch:296, lr:0.000998, total loss:0.002200\n",
            "[Train] epoch:297, lr:0.000998, total loss:0.002029\n",
            "[Train] epoch:298, lr:0.000998, total loss:0.002063\n",
            "[Train] epoch:299, lr:0.000998, total loss:0.002092\n",
            "[Train] epoch:300, lr:0.000998, total loss:0.002120\n",
            "[Train] epoch:301, lr:0.000998, total loss:0.002043\n",
            "[Train] epoch:302, lr:0.000998, total loss:0.002103\n",
            "[Train] epoch:303, lr:0.000998, total loss:0.002274\n",
            "[Train] epoch:304, lr:0.000998, total loss:0.002134\n",
            "[Train] epoch:305, lr:0.000998, total loss:0.001948\n",
            "[Train] epoch:306, lr:0.000998, total loss:0.002439\n",
            "[Train] epoch:307, lr:0.000998, total loss:0.001986\n",
            "[Train] epoch:308, lr:0.000998, total loss:0.002243\n",
            "[Train] epoch:309, lr:0.000998, total loss:0.002238\n",
            "[Train] epoch:310, lr:0.000998, total loss:0.002367\n",
            "[Train] epoch:311, lr:0.000998, total loss:0.002302\n",
            "[Train] epoch:312, lr:0.000998, total loss:0.002154\n",
            "[Train] epoch:313, lr:0.000998, total loss:0.001969\n",
            "[Train] epoch:314, lr:0.000998, total loss:0.002036\n",
            "[Train] epoch:315, lr:0.000998, total loss:0.001823\n",
            "[Train] epoch:316, lr:0.000998, total loss:0.001935\n",
            "[Train] epoch:317, lr:0.000998, total loss:0.001990\n",
            "[Train] epoch:318, lr:0.000998, total loss:0.002007\n",
            "[Train] epoch:319, lr:0.000998, total loss:0.002196\n",
            "[Train] epoch:320, lr:0.000998, total loss:0.001997\n",
            "[Train] epoch:321, lr:0.000998, total loss:0.001964\n",
            "[Train] epoch:322, lr:0.000998, total loss:0.002221\n",
            "[Train] epoch:323, lr:0.000998, total loss:0.001990\n",
            "[Train] epoch:324, lr:0.000998, total loss:0.002280\n",
            "[Train] epoch:325, lr:0.000998, total loss:0.001920\n",
            "[Train] epoch:326, lr:0.000998, total loss:0.001997\n",
            "[Train] epoch:327, lr:0.000998, total loss:0.001832\n",
            "[Train] epoch:328, lr:0.000998, total loss:0.001842\n",
            "[Train] epoch:329, lr:0.000998, total loss:0.001789\n",
            "[Train] epoch:330, lr:0.000998, total loss:0.001889\n",
            "[Train] epoch:331, lr:0.000998, total loss:0.002274\n",
            "[Train] epoch:332, lr:0.000998, total loss:0.001876\n",
            "[Train] epoch:333, lr:0.000998, total loss:0.001882\n",
            "[Train] epoch:334, lr:0.000998, total loss:0.001877\n",
            "[Train] epoch:335, lr:0.000998, total loss:0.001749\n",
            "[Train] epoch:336, lr:0.000998, total loss:0.001874\n",
            "[Train] epoch:337, lr:0.000998, total loss:0.001916\n",
            "[Train] epoch:338, lr:0.000998, total loss:0.001695\n",
            "[Train] epoch:339, lr:0.000998, total loss:0.001790\n",
            "[Train] epoch:340, lr:0.000998, total loss:0.001960\n",
            "[Train] epoch:341, lr:0.000998, total loss:0.001777\n",
            "[Train] epoch:342, lr:0.000998, total loss:0.001709\n",
            "[Train] epoch:343, lr:0.000998, total loss:0.001808\n",
            "[Train] epoch:344, lr:0.000998, total loss:0.001820\n",
            "[Train] epoch:345, lr:0.000998, total loss:0.001724\n",
            "[Train] epoch:346, lr:0.000998, total loss:0.001992\n",
            "[Train] epoch:347, lr:0.000997, total loss:0.002046\n",
            "[Train] epoch:348, lr:0.000997, total loss:0.002034\n",
            "[Train] epoch:349, lr:0.000997, total loss:0.002231\n",
            "[Train] epoch:350, lr:0.000997, total loss:0.002056\n",
            "[Train] epoch:351, lr:0.000997, total loss:0.001844\n",
            "[Train] epoch:352, lr:0.000997, total loss:0.002030\n",
            "[Train] epoch:353, lr:0.000997, total loss:0.001857\n",
            "[Train] epoch:354, lr:0.000997, total loss:0.001876\n",
            "[Train] epoch:355, lr:0.000997, total loss:0.001814\n",
            "[Train] epoch:356, lr:0.000997, total loss:0.001871\n",
            "[Train] epoch:357, lr:0.000997, total loss:0.002023\n",
            "[Train] epoch:358, lr:0.000997, total loss:0.001901\n",
            "[Train] epoch:359, lr:0.000997, total loss:0.001845\n",
            "[Train] epoch:360, lr:0.000997, total loss:0.001600\n",
            "[Train] epoch:361, lr:0.000997, total loss:0.001794\n",
            "[Train] epoch:362, lr:0.000997, total loss:0.001728\n",
            "[Train] epoch:363, lr:0.000997, total loss:0.001629\n",
            "[Train] epoch:364, lr:0.000997, total loss:0.001662\n",
            "[Train] epoch:365, lr:0.000997, total loss:0.001883\n",
            "[Train] epoch:366, lr:0.000997, total loss:0.001788\n",
            "[Train] epoch:367, lr:0.000997, total loss:0.001851\n",
            "[Train] epoch:368, lr:0.000997, total loss:0.001824\n",
            "[Train] epoch:369, lr:0.000997, total loss:0.001742\n",
            "[Train] epoch:370, lr:0.000997, total loss:0.001939\n",
            "[Train] epoch:371, lr:0.000997, total loss:0.001678\n",
            "[Train] epoch:372, lr:0.000997, total loss:0.001673\n",
            "[Train] epoch:373, lr:0.000997, total loss:0.001612\n",
            "[Train] epoch:374, lr:0.000997, total loss:0.001807\n",
            "[Train] epoch:375, lr:0.000997, total loss:0.001548\n",
            "[Train] epoch:376, lr:0.000997, total loss:0.001979\n",
            "[Train] epoch:377, lr:0.000997, total loss:0.001631\n",
            "[Train] epoch:378, lr:0.000997, total loss:0.001607\n",
            "[Train] epoch:379, lr:0.000997, total loss:0.002233\n",
            "[Train] epoch:380, lr:0.000997, total loss:0.001810\n",
            "[Train] epoch:381, lr:0.000997, total loss:0.001679\n",
            "[Train] epoch:382, lr:0.000997, total loss:0.001652\n",
            "[Train] epoch:383, lr:0.000997, total loss:0.001669\n",
            "[Train] epoch:384, lr:0.000997, total loss:0.001875\n",
            "[Train] epoch:385, lr:0.000997, total loss:0.001819\n",
            "[Train] epoch:386, lr:0.000997, total loss:0.001708\n",
            "[Train] epoch:387, lr:0.000997, total loss:0.001715\n",
            "[Train] epoch:388, lr:0.000997, total loss:0.001830\n",
            "[Train] epoch:389, lr:0.000997, total loss:0.001672\n",
            "[Train] epoch:390, lr:0.000997, total loss:0.001628\n",
            "[Train] epoch:391, lr:0.000997, total loss:0.001626\n",
            "[Train] epoch:392, lr:0.000997, total loss:0.001642\n",
            "[Train] epoch:393, lr:0.000997, total loss:0.001703\n",
            "[Train] epoch:394, lr:0.000997, total loss:0.001622\n",
            "[Train] epoch:395, lr:0.000997, total loss:0.001614\n",
            "[Train] epoch:396, lr:0.000997, total loss:0.001738\n",
            "[Train] epoch:397, lr:0.000997, total loss:0.001628\n",
            "[Train] epoch:398, lr:0.000997, total loss:0.001956\n",
            "[Train] epoch:399, lr:0.000997, total loss:0.001663\n",
            "[Train] epoch:400, lr:0.000997, total loss:0.001823\n",
            "[Train] epoch:401, lr:0.000997, total loss:0.001592\n",
            "[Train] epoch:402, lr:0.000997, total loss:0.001543\n",
            "[Train] epoch:403, lr:0.000997, total loss:0.001610\n",
            "[Train] epoch:404, lr:0.000997, total loss:0.001760\n",
            "[Train] epoch:405, lr:0.000997, total loss:0.001599\n",
            "[Train] epoch:406, lr:0.000997, total loss:0.001549\n",
            "[Train] epoch:407, lr:0.000997, total loss:0.001865\n",
            "[Train] epoch:408, lr:0.000996, total loss:0.001683\n",
            "[Train] epoch:409, lr:0.000996, total loss:0.001646\n",
            "[Train] epoch:410, lr:0.000996, total loss:0.001599\n",
            "[Train] epoch:411, lr:0.000996, total loss:0.001681\n",
            "[Train] epoch:412, lr:0.000996, total loss:0.001651\n",
            "[Train] epoch:413, lr:0.000996, total loss:0.001646\n",
            "[Train] epoch:414, lr:0.000996, total loss:0.001387\n",
            "[Train] epoch:415, lr:0.000996, total loss:0.001437\n",
            "[Train] epoch:416, lr:0.000996, total loss:0.001586\n",
            "[Train] epoch:417, lr:0.000996, total loss:0.001946\n",
            "[Train] epoch:418, lr:0.000996, total loss:0.001502\n",
            "[Train] epoch:419, lr:0.000996, total loss:0.001616\n",
            "[Train] epoch:420, lr:0.000996, total loss:0.001832\n",
            "[Train] epoch:421, lr:0.000996, total loss:0.001688\n",
            "[Train] epoch:422, lr:0.000996, total loss:0.001557\n",
            "[Train] epoch:423, lr:0.000996, total loss:0.001611\n",
            "[Train] epoch:424, lr:0.000996, total loss:0.001517\n",
            "[Train] epoch:425, lr:0.000996, total loss:0.001506\n",
            "[Train] epoch:426, lr:0.000996, total loss:0.001646\n",
            "[Train] epoch:427, lr:0.000996, total loss:0.001585\n",
            "[Train] epoch:428, lr:0.000996, total loss:0.001439\n",
            "[Train] epoch:429, lr:0.000996, total loss:0.001557\n",
            "[Train] epoch:430, lr:0.000996, total loss:0.001527\n",
            "[Train] epoch:431, lr:0.000996, total loss:0.001454\n",
            "[Train] epoch:432, lr:0.000996, total loss:0.001652\n",
            "[Train] epoch:433, lr:0.000996, total loss:0.001446\n",
            "[Train] epoch:434, lr:0.000996, total loss:0.001465\n",
            "[Train] epoch:435, lr:0.000996, total loss:0.001589\n",
            "[Train] epoch:436, lr:0.000996, total loss:0.001590\n",
            "[Train] epoch:437, lr:0.000996, total loss:0.001515\n",
            "[Train] epoch:438, lr:0.000996, total loss:0.001426\n",
            "[Train] epoch:439, lr:0.000996, total loss:0.001425\n",
            "[Train] epoch:440, lr:0.000996, total loss:0.001520\n",
            "[Train] epoch:441, lr:0.000996, total loss:0.001619\n",
            "[Train] epoch:442, lr:0.000996, total loss:0.001583\n",
            "[Train] epoch:443, lr:0.000996, total loss:0.001453\n",
            "[Train] epoch:444, lr:0.000996, total loss:0.001478\n",
            "[Train] epoch:445, lr:0.000996, total loss:0.001450\n",
            "[Train] epoch:446, lr:0.000996, total loss:0.001776\n",
            "[Train] epoch:447, lr:0.000996, total loss:0.001775\n",
            "[Train] epoch:448, lr:0.000996, total loss:0.001613\n",
            "[Train] epoch:449, lr:0.000996, total loss:0.002193\n",
            "[Train] epoch:450, lr:0.000996, total loss:0.001502\n",
            "[Train] epoch:451, lr:0.000996, total loss:0.001631\n",
            "[Train] epoch:452, lr:0.000996, total loss:0.001714\n",
            "[Train] epoch:453, lr:0.000996, total loss:0.001414\n",
            "[Train] epoch:454, lr:0.000996, total loss:0.001471\n",
            "[Train] epoch:455, lr:0.000996, total loss:0.001489\n",
            "[Train] epoch:456, lr:0.000996, total loss:0.001683\n",
            "[Train] epoch:457, lr:0.000996, total loss:0.001680\n",
            "[Train] epoch:458, lr:0.000996, total loss:0.001494\n",
            "[Train] epoch:459, lr:0.000996, total loss:0.001561\n",
            "[Train] epoch:460, lr:0.000996, total loss:0.001660\n",
            "[Train] epoch:461, lr:0.000996, total loss:0.001496\n",
            "[Train] epoch:462, lr:0.000995, total loss:0.001540\n",
            "[Train] epoch:463, lr:0.000995, total loss:0.001424\n",
            "[Train] epoch:464, lr:0.000995, total loss:0.001588\n",
            "[Train] epoch:465, lr:0.000995, total loss:0.001499\n",
            "[Train] epoch:466, lr:0.000995, total loss:0.001452\n",
            "[Train] epoch:467, lr:0.000995, total loss:0.001574\n",
            "[Train] epoch:468, lr:0.000995, total loss:0.001324\n",
            "[Train] epoch:469, lr:0.000995, total loss:0.001478\n",
            "[Train] epoch:470, lr:0.000995, total loss:0.001340\n",
            "[Train] epoch:471, lr:0.000995, total loss:0.001288\n",
            "[Train] epoch:472, lr:0.000995, total loss:0.001854\n",
            "[Train] epoch:473, lr:0.000995, total loss:0.001454\n",
            "[Train] epoch:474, lr:0.000995, total loss:0.001417\n",
            "[Train] epoch:475, lr:0.000995, total loss:0.001273\n",
            "[Train] epoch:476, lr:0.000995, total loss:0.001398\n",
            "[Train] epoch:477, lr:0.000995, total loss:0.001490\n",
            "[Train] epoch:478, lr:0.000995, total loss:0.001389\n",
            "[Train] epoch:479, lr:0.000995, total loss:0.001434\n",
            "[Train] epoch:480, lr:0.000995, total loss:0.001291\n",
            "[Train] epoch:481, lr:0.000995, total loss:0.001336\n",
            "[Train] epoch:482, lr:0.000995, total loss:0.001427\n",
            "[Train] epoch:483, lr:0.000995, total loss:0.001316\n",
            "[Train] epoch:484, lr:0.000995, total loss:0.001326\n",
            "[Train] epoch:485, lr:0.000995, total loss:0.001371\n",
            "[Train] epoch:486, lr:0.000995, total loss:0.001371\n",
            "[Train] epoch:487, lr:0.000995, total loss:0.001350\n",
            "[Train] epoch:488, lr:0.000995, total loss:0.001380\n",
            "[Train] epoch:489, lr:0.000995, total loss:0.001407\n",
            "[Train] epoch:490, lr:0.000995, total loss:0.001507\n",
            "[Train] epoch:491, lr:0.000995, total loss:0.001699\n",
            "[Train] epoch:492, lr:0.000995, total loss:0.001454\n",
            "[Train] epoch:493, lr:0.000995, total loss:0.001386\n",
            "[Train] epoch:494, lr:0.000995, total loss:0.001399\n",
            "[Train] epoch:495, lr:0.000995, total loss:0.001535\n",
            "[Train] epoch:496, lr:0.000995, total loss:0.001355\n",
            "[Train] epoch:497, lr:0.000995, total loss:0.001304\n",
            "[Train] epoch:498, lr:0.000995, total loss:0.001438\n",
            "[Train] epoch:499, lr:0.000995, total loss:0.001480\n",
            "[Train] epoch:500, lr:0.000995, total loss:0.001387\n",
            "[Train] epoch:501, lr:0.000995, total loss:0.001431\n",
            "[Train] epoch:502, lr:0.000995, total loss:0.001434\n",
            "[Train] epoch:503, lr:0.000995, total loss:0.001370\n",
            "[Train] epoch:504, lr:0.000995, total loss:0.001445\n",
            "[Train] epoch:505, lr:0.000995, total loss:0.001442\n",
            "[Train] epoch:506, lr:0.000995, total loss:0.001316\n",
            "[Train] epoch:507, lr:0.000995, total loss:0.001392\n",
            "[Train] epoch:508, lr:0.000995, total loss:0.001460\n",
            "[Train] epoch:509, lr:0.000994, total loss:0.001358\n",
            "[Train] epoch:510, lr:0.000994, total loss:0.001410\n",
            "[Train] epoch:511, lr:0.000994, total loss:0.001311\n",
            "[Train] epoch:512, lr:0.000994, total loss:0.001242\n",
            "[Train] epoch:513, lr:0.000994, total loss:0.001217\n",
            "[Train] epoch:514, lr:0.000994, total loss:0.001282\n",
            "[Train] epoch:515, lr:0.000994, total loss:0.001369\n",
            "[Train] epoch:516, lr:0.000994, total loss:0.001225\n",
            "[Train] epoch:517, lr:0.000994, total loss:0.001363\n",
            "[Train] epoch:518, lr:0.000994, total loss:0.001380\n",
            "[Train] epoch:519, lr:0.000994, total loss:0.001516\n",
            "[Train] epoch:520, lr:0.000994, total loss:0.001402\n",
            "[Train] epoch:521, lr:0.000994, total loss:0.001311\n",
            "[Train] epoch:522, lr:0.000994, total loss:0.001326\n",
            "[Train] epoch:523, lr:0.000994, total loss:0.001468\n",
            "[Train] epoch:524, lr:0.000994, total loss:0.001256\n",
            "[Train] epoch:525, lr:0.000994, total loss:0.001424\n",
            "[Train] epoch:526, lr:0.000994, total loss:0.001274\n",
            "[Train] epoch:527, lr:0.000994, total loss:0.001292\n",
            "[Train] epoch:528, lr:0.000994, total loss:0.001198\n",
            "[Train] epoch:529, lr:0.000994, total loss:0.001223\n",
            "[Train] epoch:530, lr:0.000994, total loss:0.001193\n",
            "[Train] epoch:531, lr:0.000994, total loss:0.001261\n",
            "[Train] epoch:532, lr:0.000994, total loss:0.001481\n",
            "[Train] epoch:533, lr:0.000994, total loss:0.001358\n",
            "[Train] epoch:534, lr:0.000994, total loss:0.001242\n",
            "[Train] epoch:535, lr:0.000994, total loss:0.001308\n",
            "[Train] epoch:536, lr:0.000994, total loss:0.001214\n",
            "[Train] epoch:537, lr:0.000994, total loss:0.001351\n",
            "[Train] epoch:538, lr:0.000994, total loss:0.001350\n",
            "[Train] epoch:539, lr:0.000994, total loss:0.001371\n",
            "[Train] epoch:540, lr:0.000994, total loss:0.001336\n",
            "[Train] epoch:541, lr:0.000994, total loss:0.001411\n",
            "[Train] epoch:542, lr:0.000994, total loss:0.001162\n",
            "[Train] epoch:543, lr:0.000994, total loss:0.001296\n",
            "[Train] epoch:544, lr:0.000994, total loss:0.001309\n",
            "[Train] epoch:545, lr:0.000994, total loss:0.001210\n",
            "[Train] epoch:546, lr:0.000994, total loss:0.001248\n",
            "[Train] epoch:547, lr:0.000994, total loss:0.001204\n",
            "[Train] epoch:548, lr:0.000994, total loss:0.001405\n",
            "[Train] epoch:549, lr:0.000994, total loss:0.001321\n",
            "[Train] epoch:550, lr:0.000994, total loss:0.001232\n",
            "[Train] epoch:551, lr:0.000994, total loss:0.001195\n",
            "[Train] epoch:552, lr:0.000994, total loss:0.001355\n",
            "[Train] epoch:553, lr:0.000993, total loss:0.001244\n",
            "[Train] epoch:554, lr:0.000993, total loss:0.001201\n",
            "[Train] epoch:555, lr:0.000993, total loss:0.001225\n",
            "[Train] epoch:556, lr:0.000993, total loss:0.001236\n",
            "[Train] epoch:557, lr:0.000993, total loss:0.001187\n",
            "[Train] epoch:558, lr:0.000993, total loss:0.001271\n",
            "[Train] epoch:559, lr:0.000993, total loss:0.001284\n",
            "[Train] epoch:560, lr:0.000993, total loss:0.001116\n",
            "[Train] epoch:561, lr:0.000993, total loss:0.001261\n",
            "[Train] epoch:562, lr:0.000993, total loss:0.001169\n",
            "[Train] epoch:563, lr:0.000993, total loss:0.001189\n",
            "[Train] epoch:564, lr:0.000993, total loss:0.001183\n",
            "[Train] epoch:565, lr:0.000993, total loss:0.001292\n",
            "[Train] epoch:566, lr:0.000993, total loss:0.001248\n",
            "[Train] epoch:567, lr:0.000993, total loss:0.001276\n",
            "[Train] epoch:568, lr:0.000993, total loss:0.001398\n",
            "[Train] epoch:569, lr:0.000993, total loss:0.001322\n",
            "[Train] epoch:570, lr:0.000993, total loss:0.001160\n",
            "[Train] epoch:571, lr:0.000993, total loss:0.001158\n",
            "[Train] epoch:572, lr:0.000993, total loss:0.001309\n",
            "[Train] epoch:573, lr:0.000993, total loss:0.001225\n",
            "[Train] epoch:574, lr:0.000993, total loss:0.001160\n",
            "[Train] epoch:575, lr:0.000993, total loss:0.001119\n",
            "[Train] epoch:576, lr:0.000993, total loss:0.001110\n",
            "[Train] epoch:577, lr:0.000993, total loss:0.001340\n",
            "[Train] epoch:578, lr:0.000993, total loss:0.001115\n",
            "[Train] epoch:579, lr:0.000993, total loss:0.001322\n",
            "[Train] epoch:580, lr:0.000993, total loss:0.001334\n",
            "[Train] epoch:581, lr:0.000993, total loss:0.001247\n",
            "[Train] epoch:582, lr:0.000993, total loss:0.001269\n",
            "[Train] epoch:583, lr:0.000993, total loss:0.001241\n",
            "[Train] epoch:584, lr:0.000993, total loss:0.001184\n",
            "[Train] epoch:585, lr:0.000993, total loss:0.001239\n",
            "[Train] epoch:586, lr:0.000993, total loss:0.001117\n",
            "[Train] epoch:587, lr:0.000993, total loss:0.001312\n",
            "[Train] epoch:588, lr:0.000993, total loss:0.001191\n",
            "[Train] epoch:589, lr:0.000993, total loss:0.001318\n",
            "[Train] epoch:590, lr:0.000993, total loss:0.001425\n",
            "[Train] epoch:591, lr:0.000993, total loss:0.001223\n",
            "[Train] epoch:592, lr:0.000993, total loss:0.001276\n",
            "[Train] epoch:593, lr:0.000992, total loss:0.001230\n",
            "[Train] epoch:594, lr:0.000992, total loss:0.001329\n",
            "[Train] epoch:595, lr:0.000992, total loss:0.001143\n",
            "[Train] epoch:596, lr:0.000992, total loss:0.001226\n",
            "[Train] epoch:597, lr:0.000992, total loss:0.001126\n",
            "[Train] epoch:598, lr:0.000992, total loss:0.001254\n",
            "[Train] epoch:599, lr:0.000992, total loss:0.001093\n",
            "[Train] epoch:600, lr:0.000992, total loss:0.001273\n",
            "[Train] epoch:601, lr:0.000992, total loss:0.001348\n",
            "[Train] epoch:602, lr:0.000992, total loss:0.001306\n",
            "[Train] epoch:603, lr:0.000992, total loss:0.001174\n",
            "[Train] epoch:604, lr:0.000992, total loss:0.001170\n",
            "[Train] epoch:605, lr:0.000992, total loss:0.001333\n",
            "[Train] epoch:606, lr:0.000992, total loss:0.001198\n",
            "[Train] epoch:607, lr:0.000992, total loss:0.001108\n",
            "[Train] epoch:608, lr:0.000992, total loss:0.001276\n",
            "[Train] epoch:609, lr:0.000992, total loss:0.001044\n",
            "[Train] epoch:610, lr:0.000992, total loss:0.001056\n",
            "[Train] epoch:611, lr:0.000992, total loss:0.001089\n",
            "[Train] epoch:612, lr:0.000992, total loss:0.001153\n",
            "[Train] epoch:613, lr:0.000992, total loss:0.001280\n",
            "[Train] epoch:614, lr:0.000992, total loss:0.001201\n",
            "[Train] epoch:615, lr:0.000992, total loss:0.001404\n",
            "[Train] epoch:616, lr:0.000992, total loss:0.001303\n",
            "[Train] epoch:617, lr:0.000992, total loss:0.001331\n",
            "[Train] epoch:618, lr:0.000992, total loss:0.001294\n",
            "[Train] epoch:619, lr:0.000992, total loss:0.001231\n",
            "[Train] epoch:620, lr:0.000992, total loss:0.001134\n",
            "[Train] epoch:621, lr:0.000992, total loss:0.001117\n",
            "[Train] epoch:622, lr:0.000992, total loss:0.001138\n",
            "[Train] epoch:623, lr:0.000992, total loss:0.001217\n",
            "[Train] epoch:624, lr:0.000992, total loss:0.001133\n",
            "[Train] epoch:625, lr:0.000992, total loss:0.001228\n",
            "[Train] epoch:626, lr:0.000992, total loss:0.001209\n",
            "[Train] epoch:627, lr:0.000992, total loss:0.001250\n",
            "[Train] epoch:628, lr:0.000992, total loss:0.001498\n",
            "[Train] epoch:629, lr:0.000992, total loss:0.001167\n",
            "[Train] epoch:630, lr:0.000992, total loss:0.001162\n",
            "[Train] epoch:631, lr:0.000991, total loss:0.001214\n",
            "[Train] epoch:632, lr:0.000991, total loss:0.001200\n",
            "[Train] epoch:633, lr:0.000991, total loss:0.001086\n",
            "[Train] epoch:634, lr:0.000991, total loss:0.001193\n",
            "[Train] epoch:635, lr:0.000991, total loss:0.001114\n",
            "[Train] epoch:636, lr:0.000991, total loss:0.001229\n",
            "[Train] epoch:637, lr:0.000991, total loss:0.001117\n",
            "[Train] epoch:638, lr:0.000991, total loss:0.001147\n",
            "[Train] epoch:639, lr:0.000991, total loss:0.001094\n",
            "[Train] epoch:640, lr:0.000991, total loss:0.001220\n",
            "[Train] epoch:641, lr:0.000991, total loss:0.001264\n",
            "[Train] epoch:642, lr:0.000991, total loss:0.001126\n",
            "[Train] epoch:643, lr:0.000991, total loss:0.001125\n",
            "[Train] epoch:644, lr:0.000991, total loss:0.001136\n",
            "[Train] epoch:645, lr:0.000991, total loss:0.001045\n",
            "[Train] epoch:646, lr:0.000991, total loss:0.001062\n",
            "[Train] epoch:647, lr:0.000991, total loss:0.001079\n",
            "[Train] epoch:648, lr:0.000991, total loss:0.001120\n",
            "[Train] epoch:649, lr:0.000991, total loss:0.001085\n",
            "[Train] epoch:650, lr:0.000991, total loss:0.001172\n",
            "[Train] epoch:651, lr:0.000991, total loss:0.001035\n",
            "[Train] epoch:652, lr:0.000991, total loss:0.001169\n",
            "[Train] epoch:653, lr:0.000991, total loss:0.001371\n",
            "[Train] epoch:654, lr:0.000991, total loss:0.001392\n",
            "[Train] epoch:655, lr:0.000991, total loss:0.001164\n",
            "[Train] epoch:656, lr:0.000991, total loss:0.001386\n",
            "[Train] epoch:657, lr:0.000991, total loss:0.001170\n",
            "[Train] epoch:658, lr:0.000991, total loss:0.001115\n",
            "[Train] epoch:659, lr:0.000991, total loss:0.001073\n",
            "[Train] epoch:660, lr:0.000991, total loss:0.001118\n",
            "[Train] epoch:661, lr:0.000991, total loss:0.001059\n",
            "[Train] epoch:662, lr:0.000991, total loss:0.001131\n",
            "[Train] epoch:663, lr:0.000991, total loss:0.001351\n",
            "[Train] epoch:664, lr:0.000991, total loss:0.001156\n",
            "[Train] epoch:665, lr:0.000991, total loss:0.001123\n",
            "[Train] epoch:666, lr:0.000990, total loss:0.001126\n",
            "[Train] epoch:667, lr:0.000990, total loss:0.000967\n",
            "[Train] epoch:668, lr:0.000990, total loss:0.000971\n",
            "[Train] epoch:669, lr:0.000990, total loss:0.001068\n",
            "[Train] epoch:670, lr:0.000990, total loss:0.001147\n",
            "[Train] epoch:671, lr:0.000990, total loss:0.001014\n",
            "[Train] epoch:672, lr:0.000990, total loss:0.001080\n",
            "[Train] epoch:673, lr:0.000990, total loss:0.000966\n",
            "[Train] epoch:674, lr:0.000990, total loss:0.001212\n",
            "[Train] epoch:675, lr:0.000990, total loss:0.001066\n",
            "[Train] epoch:676, lr:0.000990, total loss:0.001047\n",
            "[Train] epoch:677, lr:0.000990, total loss:0.001197\n",
            "[Train] epoch:678, lr:0.000990, total loss:0.001073\n",
            "[Train] epoch:679, lr:0.000990, total loss:0.001149\n",
            "[Train] epoch:680, lr:0.000990, total loss:0.001028\n",
            "[Train] epoch:681, lr:0.000990, total loss:0.001096\n",
            "[Train] epoch:682, lr:0.000990, total loss:0.001078\n",
            "[Train] epoch:683, lr:0.000990, total loss:0.000991\n",
            "[Train] epoch:684, lr:0.000990, total loss:0.001074\n",
            "[Train] epoch:685, lr:0.000990, total loss:0.001145\n",
            "[Train] epoch:686, lr:0.000990, total loss:0.001176\n",
            "[Train] epoch:687, lr:0.000990, total loss:0.001131\n",
            "[Train] epoch:688, lr:0.000990, total loss:0.001173\n",
            "[Train] epoch:689, lr:0.000990, total loss:0.001141\n",
            "[Train] epoch:690, lr:0.000990, total loss:0.001097\n",
            "[Train] epoch:691, lr:0.000990, total loss:0.001063\n",
            "[Train] epoch:692, lr:0.000990, total loss:0.001020\n",
            "[Train] epoch:693, lr:0.000990, total loss:0.001116\n",
            "[Train] epoch:694, lr:0.000990, total loss:0.001102\n",
            "[Train] epoch:695, lr:0.000990, total loss:0.001192\n",
            "[Train] epoch:696, lr:0.000990, total loss:0.001148\n",
            "[Train] epoch:697, lr:0.000990, total loss:0.001074\n",
            "[Train] epoch:698, lr:0.000990, total loss:0.001077\n",
            "[Train] epoch:699, lr:0.000990, total loss:0.001030\n",
            "[Train] epoch:700, lr:0.000989, total loss:0.001040\n",
            "[Train] epoch:701, lr:0.000989, total loss:0.001030\n",
            "[Train] epoch:702, lr:0.000989, total loss:0.001046\n",
            "[Train] epoch:703, lr:0.000989, total loss:0.001085\n",
            "[Train] epoch:704, lr:0.000989, total loss:0.001002\n",
            "[Train] epoch:705, lr:0.000989, total loss:0.001104\n",
            "[Train] epoch:706, lr:0.000989, total loss:0.001098\n",
            "[Train] epoch:707, lr:0.000989, total loss:0.001099\n",
            "[Train] epoch:708, lr:0.000989, total loss:0.001168\n",
            "[Train] epoch:709, lr:0.000989, total loss:0.001030\n",
            "[Train] epoch:710, lr:0.000989, total loss:0.000973\n",
            "[Train] epoch:711, lr:0.000989, total loss:0.000902\n",
            "[Train] epoch:712, lr:0.000989, total loss:0.001169\n",
            "[Train] epoch:713, lr:0.000989, total loss:0.001087\n",
            "[Train] epoch:714, lr:0.000989, total loss:0.001070\n",
            "[Train] epoch:715, lr:0.000989, total loss:0.001074\n",
            "[Train] epoch:716, lr:0.000989, total loss:0.001226\n",
            "[Train] epoch:717, lr:0.000989, total loss:0.001055\n",
            "[Train] epoch:718, lr:0.000989, total loss:0.000944\n",
            "[Train] epoch:719, lr:0.000989, total loss:0.001049\n",
            "[Train] epoch:720, lr:0.000989, total loss:0.000934\n",
            "[Train] epoch:721, lr:0.000989, total loss:0.001032\n",
            "[Train] epoch:722, lr:0.000989, total loss:0.001025\n",
            "[Train] epoch:723, lr:0.000989, total loss:0.001230\n",
            "[Train] epoch:724, lr:0.000989, total loss:0.001116\n",
            "[Train] epoch:725, lr:0.000989, total loss:0.001001\n",
            "[Train] epoch:726, lr:0.000989, total loss:0.001104\n",
            "[Train] epoch:727, lr:0.000989, total loss:0.001096\n",
            "[Train] epoch:728, lr:0.000989, total loss:0.001034\n",
            "[Train] epoch:729, lr:0.000989, total loss:0.001091\n",
            "[Train] epoch:730, lr:0.000989, total loss:0.000991\n",
            "[Train] epoch:731, lr:0.000989, total loss:0.000941\n",
            "[Train] epoch:732, lr:0.000988, total loss:0.000976\n",
            "[Train] epoch:733, lr:0.000988, total loss:0.000942\n",
            "[Train] epoch:734, lr:0.000988, total loss:0.001050\n",
            "[Train] epoch:735, lr:0.000988, total loss:0.000952\n",
            "[Train] epoch:736, lr:0.000988, total loss:0.001114\n",
            "[Train] epoch:737, lr:0.000988, total loss:0.000989\n",
            "[Train] epoch:738, lr:0.000988, total loss:0.001095\n",
            "[Train] epoch:739, lr:0.000988, total loss:0.000928\n",
            "[Train] epoch:740, lr:0.000988, total loss:0.000933\n",
            "[Train] epoch:741, lr:0.000988, total loss:0.001035\n",
            "[Train] epoch:742, lr:0.000988, total loss:0.000956\n",
            "[Train] epoch:743, lr:0.000988, total loss:0.000962\n",
            "[Train] epoch:744, lr:0.000988, total loss:0.000991\n",
            "[Train] epoch:745, lr:0.000988, total loss:0.001120\n",
            "[Train] epoch:746, lr:0.000988, total loss:0.001006\n",
            "[Train] epoch:747, lr:0.000988, total loss:0.001066\n",
            "[Train] epoch:748, lr:0.000988, total loss:0.000959\n",
            "[Train] epoch:749, lr:0.000988, total loss:0.001040\n",
            "[Train] epoch:750, lr:0.000988, total loss:0.001011\n",
            "[Train] epoch:751, lr:0.000988, total loss:0.001011\n",
            "[Train] epoch:752, lr:0.000988, total loss:0.001137\n",
            "[Train] epoch:753, lr:0.000988, total loss:0.001275\n",
            "[Train] epoch:754, lr:0.000988, total loss:0.001058\n",
            "[Train] epoch:755, lr:0.000988, total loss:0.001166\n",
            "[Train] epoch:756, lr:0.000988, total loss:0.000940\n",
            "[Train] epoch:757, lr:0.000988, total loss:0.000916\n",
            "[Train] epoch:758, lr:0.000988, total loss:0.000933\n",
            "[Train] epoch:759, lr:0.000988, total loss:0.001092\n",
            "[Train] epoch:760, lr:0.000988, total loss:0.001021\n",
            "[Train] epoch:761, lr:0.000988, total loss:0.000962\n",
            "[Train] epoch:762, lr:0.000988, total loss:0.000922\n",
            "[Train] epoch:763, lr:0.000987, total loss:0.001057\n",
            "[Train] epoch:764, lr:0.000987, total loss:0.000965\n",
            "[Train] epoch:765, lr:0.000987, total loss:0.000911\n",
            "[Train] epoch:766, lr:0.000987, total loss:0.000920\n",
            "[Train] epoch:767, lr:0.000987, total loss:0.001043\n",
            "[Train] epoch:768, lr:0.000987, total loss:0.000955\n",
            "[Train] epoch:769, lr:0.000987, total loss:0.000940\n",
            "[Train] epoch:770, lr:0.000987, total loss:0.001024\n",
            "[Train] epoch:771, lr:0.000987, total loss:0.000877\n",
            "[Train] epoch:772, lr:0.000987, total loss:0.000952\n",
            "[Train] epoch:773, lr:0.000987, total loss:0.001128\n",
            "[Train] epoch:774, lr:0.000987, total loss:0.001085\n",
            "[Train] epoch:775, lr:0.000987, total loss:0.001046\n",
            "[Train] epoch:776, lr:0.000987, total loss:0.000988\n",
            "[Train] epoch:777, lr:0.000987, total loss:0.000981\n",
            "[Train] epoch:778, lr:0.000987, total loss:0.001408\n",
            "[Train] epoch:779, lr:0.000987, total loss:0.001154\n",
            "[Train] epoch:780, lr:0.000987, total loss:0.001163\n",
            "[Train] epoch:781, lr:0.000987, total loss:0.001085\n",
            "[Train] epoch:782, lr:0.000987, total loss:0.001074\n",
            "[Train] epoch:783, lr:0.000987, total loss:0.001141\n",
            "[Train] epoch:784, lr:0.000987, total loss:0.000954\n",
            "[Train] epoch:785, lr:0.000987, total loss:0.000942\n",
            "[Train] epoch:786, lr:0.000987, total loss:0.000950\n",
            "[Train] epoch:787, lr:0.000987, total loss:0.000985\n",
            "[Train] epoch:788, lr:0.000987, total loss:0.001022\n",
            "[Train] epoch:789, lr:0.000987, total loss:0.000874\n",
            "[Train] epoch:790, lr:0.000987, total loss:0.000922\n",
            "[Train] epoch:791, lr:0.000987, total loss:0.000918\n",
            "[Train] epoch:792, lr:0.000986, total loss:0.000844\n",
            "[Train] epoch:793, lr:0.000986, total loss:0.000908\n",
            "[Train] epoch:794, lr:0.000986, total loss:0.000882\n",
            "[Train] epoch:795, lr:0.000986, total loss:0.001007\n",
            "[Train] epoch:796, lr:0.000986, total loss:0.001098\n",
            "[Train] epoch:797, lr:0.000986, total loss:0.001034\n",
            "[Train] epoch:798, lr:0.000986, total loss:0.001048\n",
            "[Train] epoch:799, lr:0.000986, total loss:0.000988\n",
            "[Train] epoch:800, lr:0.000986, total loss:0.000990\n",
            "[Train] epoch:801, lr:0.000986, total loss:0.001100\n",
            "[Train] epoch:802, lr:0.000986, total loss:0.000938\n",
            "[Train] epoch:803, lr:0.000986, total loss:0.001112\n",
            "[Train] epoch:804, lr:0.000986, total loss:0.000983\n",
            "[Train] epoch:805, lr:0.000986, total loss:0.001034\n",
            "[Train] epoch:806, lr:0.000986, total loss:0.001077\n",
            "[Train] epoch:807, lr:0.000986, total loss:0.001166\n",
            "[Train] epoch:808, lr:0.000986, total loss:0.001025\n",
            "[Train] epoch:809, lr:0.000986, total loss:0.000910\n",
            "[Train] epoch:810, lr:0.000986, total loss:0.000928\n",
            "[Train] epoch:811, lr:0.000986, total loss:0.000854\n",
            "[Train] epoch:812, lr:0.000986, total loss:0.000874\n",
            "[Train] epoch:813, lr:0.000986, total loss:0.000903\n",
            "[Train] epoch:814, lr:0.000986, total loss:0.000933\n",
            "[Train] epoch:815, lr:0.000986, total loss:0.000807\n",
            "[Train] epoch:816, lr:0.000986, total loss:0.000927\n",
            "[Train] epoch:817, lr:0.000986, total loss:0.001001\n",
            "[Train] epoch:818, lr:0.000986, total loss:0.000938\n",
            "[Train] epoch:819, lr:0.000986, total loss:0.001039\n",
            "[Train] epoch:820, lr:0.000986, total loss:0.000943\n",
            "[Train] epoch:821, lr:0.000985, total loss:0.001078\n",
            "[Train] epoch:822, lr:0.000985, total loss:0.000948\n",
            "[Train] epoch:823, lr:0.000985, total loss:0.000934\n",
            "[Train] epoch:824, lr:0.000985, total loss:0.001041\n",
            "[Train] epoch:825, lr:0.000985, total loss:0.001070\n",
            "[Train] epoch:826, lr:0.000985, total loss:0.000829\n",
            "[Train] epoch:827, lr:0.000985, total loss:0.000820\n",
            "[Train] epoch:828, lr:0.000985, total loss:0.000869\n",
            "[Train] epoch:829, lr:0.000985, total loss:0.000792\n",
            "[Train] epoch:830, lr:0.000985, total loss:0.000803\n",
            "[Train] epoch:831, lr:0.000985, total loss:0.000976\n",
            "[Train] epoch:832, lr:0.000985, total loss:0.000929\n",
            "[Train] epoch:833, lr:0.000985, total loss:0.000937\n",
            "[Train] epoch:834, lr:0.000985, total loss:0.001006\n",
            "[Train] epoch:835, lr:0.000985, total loss:0.000846\n",
            "[Train] epoch:836, lr:0.000985, total loss:0.000922\n",
            "[Train] epoch:837, lr:0.000985, total loss:0.001037\n",
            "[Train] epoch:838, lr:0.000985, total loss:0.000847\n",
            "[Train] epoch:839, lr:0.000985, total loss:0.000908\n",
            "[Train] epoch:840, lr:0.000985, total loss:0.000933\n",
            "[Train] epoch:841, lr:0.000985, total loss:0.000910\n",
            "[Train] epoch:842, lr:0.000985, total loss:0.000880\n",
            "[Train] epoch:843, lr:0.000985, total loss:0.000858\n",
            "[Train] epoch:844, lr:0.000985, total loss:0.000947\n",
            "[Train] epoch:845, lr:0.000985, total loss:0.000880\n",
            "[Train] epoch:846, lr:0.000985, total loss:0.000842\n",
            "[Train] epoch:847, lr:0.000985, total loss:0.000803\n",
            "[Train] epoch:848, lr:0.000985, total loss:0.000858\n",
            "[Train] epoch:849, lr:0.000984, total loss:0.000821\n",
            "[Train] epoch:850, lr:0.000984, total loss:0.000810\n",
            "[Train] epoch:851, lr:0.000984, total loss:0.000897\n",
            "[Train] epoch:852, lr:0.000984, total loss:0.000858\n",
            "[Train] epoch:853, lr:0.000984, total loss:0.000779\n",
            "[Train] epoch:854, lr:0.000984, total loss:0.000868\n",
            "[Train] epoch:855, lr:0.000984, total loss:0.000905\n",
            "[Train] epoch:856, lr:0.000984, total loss:0.000853\n",
            "[Train] epoch:857, lr:0.000984, total loss:0.000872\n",
            "[Train] epoch:858, lr:0.000984, total loss:0.000855\n",
            "[Train] epoch:859, lr:0.000984, total loss:0.000932\n",
            "[Train] epoch:860, lr:0.000984, total loss:0.000844\n",
            "[Train] epoch:861, lr:0.000984, total loss:0.000970\n",
            "[Train] epoch:862, lr:0.000984, total loss:0.000887\n",
            "[Train] epoch:863, lr:0.000984, total loss:0.000812\n",
            "[Train] epoch:864, lr:0.000984, total loss:0.000898\n",
            "[Train] epoch:865, lr:0.000984, total loss:0.000760\n",
            "[Train] epoch:866, lr:0.000984, total loss:0.000962\n",
            "[Train] epoch:867, lr:0.000984, total loss:0.000925\n",
            "[Train] epoch:868, lr:0.000984, total loss:0.000940\n",
            "[Train] epoch:869, lr:0.000984, total loss:0.000853\n",
            "[Train] epoch:870, lr:0.000984, total loss:0.000999\n",
            "[Train] epoch:871, lr:0.000984, total loss:0.001023\n",
            "[Train] epoch:872, lr:0.000984, total loss:0.000834\n",
            "[Train] epoch:873, lr:0.000984, total loss:0.000944\n",
            "[Train] epoch:874, lr:0.000984, total loss:0.001018\n",
            "[Train] epoch:875, lr:0.000983, total loss:0.000994\n",
            "[Train] epoch:876, lr:0.000983, total loss:0.001055\n",
            "[Train] epoch:877, lr:0.000983, total loss:0.001104\n",
            "[Train] epoch:878, lr:0.000983, total loss:0.001167\n",
            "[Train] epoch:879, lr:0.000983, total loss:0.001056\n",
            "[Train] epoch:880, lr:0.000983, total loss:0.001125\n",
            "[Train] epoch:881, lr:0.000983, total loss:0.000940\n",
            "[Train] epoch:882, lr:0.000983, total loss:0.000872\n",
            "[Train] epoch:883, lr:0.000983, total loss:0.000894\n",
            "[Train] epoch:884, lr:0.000983, total loss:0.000888\n",
            "[Train] epoch:885, lr:0.000983, total loss:0.000812\n",
            "[Train] epoch:886, lr:0.000983, total loss:0.000815\n",
            "[Train] epoch:887, lr:0.000983, total loss:0.001008\n",
            "[Train] epoch:888, lr:0.000983, total loss:0.000862\n",
            "[Train] epoch:889, lr:0.000983, total loss:0.000906\n",
            "[Train] epoch:890, lr:0.000983, total loss:0.000923\n",
            "[Train] epoch:891, lr:0.000983, total loss:0.000870\n",
            "[Train] epoch:892, lr:0.000983, total loss:0.001077\n",
            "[Train] epoch:893, lr:0.000983, total loss:0.000922\n",
            "[Train] epoch:894, lr:0.000983, total loss:0.000905\n",
            "[Train] epoch:895, lr:0.000983, total loss:0.000943\n",
            "[Train] epoch:896, lr:0.000983, total loss:0.000806\n",
            "[Train] epoch:897, lr:0.000983, total loss:0.000861\n",
            "[Train] epoch:898, lr:0.000983, total loss:0.000746\n",
            "[Train] epoch:899, lr:0.000983, total loss:0.000882\n",
            "[Train] epoch:900, lr:0.000983, total loss:0.000896\n",
            "[Train] epoch:901, lr:0.000982, total loss:0.000939\n",
            "[Train] epoch:902, lr:0.000982, total loss:0.000883\n",
            "[Train] epoch:903, lr:0.000982, total loss:0.000898\n",
            "[Train] epoch:904, lr:0.000982, total loss:0.000930\n",
            "[Train] epoch:905, lr:0.000982, total loss:0.000880\n",
            "[Train] epoch:906, lr:0.000982, total loss:0.001122\n",
            "[Train] epoch:907, lr:0.000982, total loss:0.001036\n",
            "[Train] epoch:908, lr:0.000982, total loss:0.000995\n",
            "[Train] epoch:909, lr:0.000982, total loss:0.000792\n",
            "[Train] epoch:910, lr:0.000982, total loss:0.000858\n",
            "[Train] epoch:911, lr:0.000982, total loss:0.001005\n",
            "[Train] epoch:912, lr:0.000982, total loss:0.000935\n",
            "[Train] epoch:913, lr:0.000982, total loss:0.001039\n",
            "[Train] epoch:914, lr:0.000982, total loss:0.000874\n",
            "[Train] epoch:915, lr:0.000982, total loss:0.000797\n",
            "[Train] epoch:916, lr:0.000982, total loss:0.000971\n",
            "[Train] epoch:917, lr:0.000982, total loss:0.000831\n",
            "[Train] epoch:918, lr:0.000982, total loss:0.000803\n",
            "[Train] epoch:919, lr:0.000982, total loss:0.000877\n",
            "[Train] epoch:920, lr:0.000982, total loss:0.000933\n",
            "[Train] epoch:921, lr:0.000982, total loss:0.000969\n",
            "[Train] epoch:922, lr:0.000982, total loss:0.001048\n",
            "[Train] epoch:923, lr:0.000982, total loss:0.000892\n",
            "[Train] epoch:924, lr:0.000982, total loss:0.000794\n",
            "[Train] epoch:925, lr:0.000982, total loss:0.000840\n",
            "[Train] epoch:926, lr:0.000981, total loss:0.000958\n",
            "[Train] epoch:927, lr:0.000981, total loss:0.000792\n",
            "[Train] epoch:928, lr:0.000981, total loss:0.000815\n",
            "[Train] epoch:929, lr:0.000981, total loss:0.000878\n",
            "[Train] epoch:930, lr:0.000981, total loss:0.000838\n",
            "[Train] epoch:931, lr:0.000981, total loss:0.000815\n",
            "[Train] epoch:932, lr:0.000981, total loss:0.000795\n",
            "[Train] epoch:933, lr:0.000981, total loss:0.000786\n",
            "[Train] epoch:934, lr:0.000981, total loss:0.000775\n",
            "[Train] epoch:935, lr:0.000981, total loss:0.000717\n",
            "[Train] epoch:936, lr:0.000981, total loss:0.000764\n",
            "[Train] epoch:937, lr:0.000981, total loss:0.000734\n",
            "[Train] epoch:938, lr:0.000981, total loss:0.000776\n",
            "[Train] epoch:939, lr:0.000981, total loss:0.000737\n",
            "[Train] epoch:940, lr:0.000981, total loss:0.001096\n",
            "[Train] epoch:941, lr:0.000981, total loss:0.000964\n",
            "[Train] epoch:942, lr:0.000981, total loss:0.001041\n",
            "[Train] epoch:943, lr:0.000981, total loss:0.000993\n",
            "[Train] epoch:944, lr:0.000981, total loss:0.000858\n",
            "[Train] epoch:945, lr:0.000981, total loss:0.000803\n",
            "[Train] epoch:946, lr:0.000981, total loss:0.000721\n",
            "[Train] epoch:947, lr:0.000981, total loss:0.000878\n",
            "[Train] epoch:948, lr:0.000981, total loss:0.000840\n",
            "[Train] epoch:949, lr:0.000981, total loss:0.000845\n",
            "[Train] epoch:950, lr:0.000981, total loss:0.000936\n",
            "[Train] epoch:951, lr:0.000980, total loss:0.000833\n",
            "[Train] epoch:952, lr:0.000980, total loss:0.000896\n",
            "[Train] epoch:953, lr:0.000980, total loss:0.000740\n",
            "[Train] epoch:954, lr:0.000980, total loss:0.000778\n",
            "[Train] epoch:955, lr:0.000980, total loss:0.000757\n",
            "[Train] epoch:956, lr:0.000980, total loss:0.000755\n",
            "[Train] epoch:957, lr:0.000980, total loss:0.000804\n",
            "[Train] epoch:958, lr:0.000980, total loss:0.000777\n",
            "[Train] epoch:959, lr:0.000980, total loss:0.000866\n",
            "[Train] epoch:960, lr:0.000980, total loss:0.000801\n",
            "[Train] epoch:961, lr:0.000980, total loss:0.000940\n",
            "[Train] epoch:962, lr:0.000980, total loss:0.000935\n",
            "[Train] epoch:963, lr:0.000980, total loss:0.000751\n",
            "[Train] epoch:964, lr:0.000980, total loss:0.000905\n",
            "[Train] epoch:965, lr:0.000980, total loss:0.000811\n",
            "[Train] epoch:966, lr:0.000980, total loss:0.000816\n",
            "[Train] epoch:967, lr:0.000980, total loss:0.000721\n",
            "[Train] epoch:968, lr:0.000980, total loss:0.000795\n",
            "[Train] epoch:969, lr:0.000980, total loss:0.000754\n",
            "[Train] epoch:970, lr:0.000980, total loss:0.000829\n",
            "[Train] epoch:971, lr:0.000980, total loss:0.000797\n",
            "[Train] epoch:972, lr:0.000980, total loss:0.000785\n",
            "[Train] epoch:973, lr:0.000980, total loss:0.000823\n",
            "[Train] epoch:974, lr:0.000980, total loss:0.000871\n",
            "[Train] epoch:975, lr:0.000979, total loss:0.000824\n",
            "[Train] epoch:976, lr:0.000979, total loss:0.000811\n",
            "[Train] epoch:977, lr:0.000979, total loss:0.000708\n",
            "[Train] epoch:978, lr:0.000979, total loss:0.000792\n",
            "[Train] epoch:979, lr:0.000979, total loss:0.000747\n",
            "[Train] epoch:980, lr:0.000979, total loss:0.000889\n",
            "[Train] epoch:981, lr:0.000979, total loss:0.000818\n",
            "[Train] epoch:982, lr:0.000979, total loss:0.000814\n",
            "[Train] epoch:983, lr:0.000979, total loss:0.000859\n",
            "[Train] epoch:984, lr:0.000979, total loss:0.000858\n",
            "[Train] epoch:985, lr:0.000979, total loss:0.000810\n",
            "[Train] epoch:986, lr:0.000979, total loss:0.000912\n",
            "[Train] epoch:987, lr:0.000979, total loss:0.000767\n",
            "[Train] epoch:988, lr:0.000979, total loss:0.000819\n",
            "[Train] epoch:989, lr:0.000979, total loss:0.000857\n",
            "[Train] epoch:990, lr:0.000979, total loss:0.000776\n",
            "[Train] epoch:991, lr:0.000979, total loss:0.000789\n",
            "[Train] epoch:992, lr:0.000979, total loss:0.000751\n",
            "[Train] epoch:993, lr:0.000979, total loss:0.000829\n",
            "[Train] epoch:994, lr:0.000979, total loss:0.000677\n",
            "[Train] epoch:995, lr:0.000979, total loss:0.000678\n",
            "[Train] epoch:996, lr:0.000979, total loss:0.000725\n",
            "[Train] epoch:997, lr:0.000979, total loss:0.000705\n",
            "[Train] epoch:998, lr:0.000978, total loss:0.000687\n",
            "[Train] epoch:999, lr:0.000978, total loss:0.000715\n",
            "[Train] epoch:1000, lr:0.000978, total loss:0.000687\n",
            "[Train] epoch:1001, lr:0.000978, total loss:0.000713\n",
            "[Train] epoch:1002, lr:0.000978, total loss:0.000785\n",
            "[Train] epoch:1003, lr:0.000978, total loss:0.000760\n",
            "[Train] epoch:1004, lr:0.000978, total loss:0.000683\n",
            "[Train] epoch:1005, lr:0.000978, total loss:0.000749\n",
            "[Train] epoch:1006, lr:0.000978, total loss:0.000876\n",
            "[Train] epoch:1007, lr:0.000978, total loss:0.000864\n",
            "[Train] epoch:1008, lr:0.000978, total loss:0.000793\n",
            "[Train] epoch:1009, lr:0.000978, total loss:0.000835\n",
            "[Train] epoch:1010, lr:0.000978, total loss:0.000716\n",
            "[Train] epoch:1011, lr:0.000978, total loss:0.000809\n",
            "[Train] epoch:1012, lr:0.000978, total loss:0.000698\n",
            "[Train] epoch:1013, lr:0.000978, total loss:0.000869\n",
            "[Train] epoch:1014, lr:0.000978, total loss:0.000795\n",
            "[Train] epoch:1015, lr:0.000978, total loss:0.000796\n",
            "[Train] epoch:1016, lr:0.000978, total loss:0.000802\n",
            "[Train] epoch:1017, lr:0.000978, total loss:0.000725\n",
            "[Train] epoch:1018, lr:0.000978, total loss:0.000726\n",
            "[Train] epoch:1019, lr:0.000978, total loss:0.000708\n",
            "[Train] epoch:1020, lr:0.000978, total loss:0.000767\n",
            "[Train] epoch:1021, lr:0.000977, total loss:0.000904\n",
            "[Train] epoch:1022, lr:0.000977, total loss:0.000829\n",
            "[Train] epoch:1023, lr:0.000977, total loss:0.000858\n",
            "[Train] epoch:1024, lr:0.000977, total loss:0.000789\n",
            "[Train] epoch:1025, lr:0.000977, total loss:0.000763\n",
            "[Train] epoch:1026, lr:0.000977, total loss:0.000713\n",
            "[Train] epoch:1027, lr:0.000977, total loss:0.000775\n",
            "[Train] epoch:1028, lr:0.000977, total loss:0.000723\n",
            "[Train] epoch:1029, lr:0.000977, total loss:0.000720\n",
            "[Train] epoch:1030, lr:0.000977, total loss:0.000799\n",
            "[Train] epoch:1031, lr:0.000977, total loss:0.000756\n",
            "[Train] epoch:1032, lr:0.000977, total loss:0.000656\n",
            "[Train] epoch:1033, lr:0.000977, total loss:0.000624\n",
            "[Train] epoch:1034, lr:0.000977, total loss:0.000670\n",
            "[Train] epoch:1035, lr:0.000977, total loss:0.000731\n",
            "[Train] epoch:1036, lr:0.000977, total loss:0.000693\n",
            "[Train] epoch:1037, lr:0.000977, total loss:0.000773\n",
            "[Train] epoch:1038, lr:0.000977, total loss:0.000739\n",
            "[Train] epoch:1039, lr:0.000977, total loss:0.000802\n",
            "[Train] epoch:1040, lr:0.000977, total loss:0.000847\n",
            "[Train] epoch:1041, lr:0.000977, total loss:0.000846\n",
            "[Train] epoch:1042, lr:0.000977, total loss:0.000849\n",
            "[Train] epoch:1043, lr:0.000977, total loss:0.000780\n",
            "[Train] epoch:1044, lr:0.000976, total loss:0.000764\n",
            "[Train] epoch:1045, lr:0.000976, total loss:0.000685\n",
            "[Train] epoch:1046, lr:0.000976, total loss:0.000743\n",
            "[Train] epoch:1047, lr:0.000976, total loss:0.000751\n",
            "[Train] epoch:1048, lr:0.000976, total loss:0.000823\n",
            "[Train] epoch:1049, lr:0.000976, total loss:0.000838\n",
            "[Train] epoch:1050, lr:0.000976, total loss:0.000880\n",
            "[Train] epoch:1051, lr:0.000976, total loss:0.000995\n",
            "[Train] epoch:1052, lr:0.000976, total loss:0.000977\n",
            "[Train] epoch:1053, lr:0.000976, total loss:0.001025\n",
            "[Train] epoch:1054, lr:0.000976, total loss:0.001134\n",
            "[Train] epoch:1055, lr:0.000976, total loss:0.001159\n",
            "[Train] epoch:1056, lr:0.000976, total loss:0.001040\n",
            "[Train] epoch:1057, lr:0.000976, total loss:0.000925\n",
            "[Train] epoch:1058, lr:0.000976, total loss:0.000855\n",
            "[Train] epoch:1059, lr:0.000976, total loss:0.000798\n",
            "[Train] epoch:1060, lr:0.000976, total loss:0.000758\n",
            "[Train] epoch:1061, lr:0.000976, total loss:0.000777\n",
            "[Train] epoch:1062, lr:0.000976, total loss:0.000683\n",
            "[Train] epoch:1063, lr:0.000976, total loss:0.000713\n",
            "[Train] epoch:1064, lr:0.000976, total loss:0.000705\n",
            "[Train] epoch:1065, lr:0.000976, total loss:0.000932\n",
            "[Train] epoch:1066, lr:0.000975, total loss:0.000898\n",
            "[Train] epoch:1067, lr:0.000975, total loss:0.001036\n",
            "[Train] epoch:1068, lr:0.000975, total loss:0.000819\n",
            "[Train] epoch:1069, lr:0.000975, total loss:0.001046\n",
            "[Train] epoch:1070, lr:0.000975, total loss:0.000787\n",
            "[Train] epoch:1071, lr:0.000975, total loss:0.000823\n",
            "[Train] epoch:1072, lr:0.000975, total loss:0.000762\n",
            "[Train] epoch:1073, lr:0.000975, total loss:0.000692\n",
            "[Train] epoch:1074, lr:0.000975, total loss:0.000832\n",
            "[Train] epoch:1075, lr:0.000975, total loss:0.000743\n",
            "[Train] epoch:1076, lr:0.000975, total loss:0.000712\n",
            "[Train] epoch:1077, lr:0.000975, total loss:0.000737\n",
            "[Train] epoch:1078, lr:0.000975, total loss:0.000705\n",
            "[Train] epoch:1079, lr:0.000975, total loss:0.000702\n",
            "[Train] epoch:1080, lr:0.000975, total loss:0.000697\n",
            "[Train] epoch:1081, lr:0.000975, total loss:0.000810\n",
            "[Train] epoch:1082, lr:0.000975, total loss:0.000978\n",
            "[Train] epoch:1083, lr:0.000975, total loss:0.000781\n",
            "[Train] epoch:1084, lr:0.000975, total loss:0.000854\n",
            "[Train] epoch:1085, lr:0.000975, total loss:0.000788\n",
            "[Train] epoch:1086, lr:0.000975, total loss:0.000689\n",
            "[Train] epoch:1087, lr:0.000974, total loss:0.000779\n",
            "[Train] epoch:1088, lr:0.000974, total loss:0.000757\n",
            "[Train] epoch:1089, lr:0.000974, total loss:0.000782\n",
            "[Train] epoch:1090, lr:0.000974, total loss:0.000762\n",
            "[Train] epoch:1091, lr:0.000974, total loss:0.000739\n",
            "[Train] epoch:1092, lr:0.000974, total loss:0.000785\n",
            "[Train] epoch:1093, lr:0.000974, total loss:0.000787\n",
            "[Train] epoch:1094, lr:0.000974, total loss:0.000819\n",
            "[Train] epoch:1095, lr:0.000974, total loss:0.000914\n",
            "[Train] epoch:1096, lr:0.000974, total loss:0.000733\n",
            "[Train] epoch:1097, lr:0.000974, total loss:0.000694\n",
            "[Train] epoch:1098, lr:0.000974, total loss:0.000687\n",
            "[Train] epoch:1099, lr:0.000974, total loss:0.000768\n",
            "[Train] epoch:1100, lr:0.000974, total loss:0.000767\n",
            "[Train] epoch:1101, lr:0.000974, total loss:0.000781\n",
            "[Train] epoch:1102, lr:0.000974, total loss:0.000711\n",
            "[Train] epoch:1103, lr:0.000974, total loss:0.000694\n",
            "[Train] epoch:1104, lr:0.000974, total loss:0.000765\n",
            "[Train] epoch:1105, lr:0.000974, total loss:0.000728\n",
            "[Train] epoch:1106, lr:0.000974, total loss:0.000672\n",
            "[Train] epoch:1107, lr:0.000974, total loss:0.000752\n",
            "[Train] epoch:1108, lr:0.000973, total loss:0.000765\n",
            "[Train] epoch:1109, lr:0.000973, total loss:0.000863\n",
            "[Train] epoch:1110, lr:0.000973, total loss:0.000855\n",
            "[Train] epoch:1111, lr:0.000973, total loss:0.000798\n",
            "[Train] epoch:1112, lr:0.000973, total loss:0.000791\n",
            "[Train] epoch:1113, lr:0.000973, total loss:0.000722\n",
            "[Train] epoch:1114, lr:0.000973, total loss:0.000756\n",
            "[Train] epoch:1115, lr:0.000973, total loss:0.000752\n",
            "[Train] epoch:1116, lr:0.000973, total loss:0.000818\n",
            "[Train] epoch:1117, lr:0.000973, total loss:0.000721\n",
            "[Train] epoch:1118, lr:0.000973, total loss:0.000720\n",
            "[Train] epoch:1119, lr:0.000973, total loss:0.000765\n",
            "[Train] epoch:1120, lr:0.000973, total loss:0.000718\n",
            "[Train] epoch:1121, lr:0.000973, total loss:0.000755\n",
            "[Train] epoch:1122, lr:0.000973, total loss:0.000764\n",
            "[Train] epoch:1123, lr:0.000973, total loss:0.000742\n",
            "[Train] epoch:1124, lr:0.000973, total loss:0.000755\n",
            "[Train] epoch:1125, lr:0.000973, total loss:0.000641\n",
            "[Train] epoch:1126, lr:0.000973, total loss:0.000633\n",
            "[Train] epoch:1127, lr:0.000973, total loss:0.000714\n",
            "[Train] epoch:1128, lr:0.000973, total loss:0.000706\n",
            "[Train] epoch:1129, lr:0.000972, total loss:0.000777\n",
            "[Train] epoch:1130, lr:0.000972, total loss:0.000694\n",
            "[Train] epoch:1131, lr:0.000972, total loss:0.000794\n",
            "[Train] epoch:1132, lr:0.000972, total loss:0.000758\n",
            "[Train] epoch:1133, lr:0.000972, total loss:0.000824\n",
            "[Train] epoch:1134, lr:0.000972, total loss:0.000759\n",
            "[Train] epoch:1135, lr:0.000972, total loss:0.000787\n",
            "[Train] epoch:1136, lr:0.000972, total loss:0.000736\n",
            "[Train] epoch:1137, lr:0.000972, total loss:0.000758\n",
            "[Train] epoch:1138, lr:0.000972, total loss:0.000763\n",
            "[Train] epoch:1139, lr:0.000972, total loss:0.000687\n",
            "[Train] epoch:1140, lr:0.000972, total loss:0.000684\n",
            "[Train] epoch:1141, lr:0.000972, total loss:0.000721\n",
            "[Train] epoch:1142, lr:0.000972, total loss:0.000676\n",
            "[Train] epoch:1143, lr:0.000972, total loss:0.000814\n",
            "[Train] epoch:1144, lr:0.000972, total loss:0.000950\n",
            "[Train] epoch:1145, lr:0.000972, total loss:0.000861\n",
            "[Train] epoch:1146, lr:0.000972, total loss:0.000908\n",
            "[Train] epoch:1147, lr:0.000972, total loss:0.000868\n",
            "[Train] epoch:1148, lr:0.000972, total loss:0.000751\n",
            "[Train] epoch:1149, lr:0.000971, total loss:0.000710\n",
            "[Train] epoch:1150, lr:0.000971, total loss:0.000678\n",
            "[Train] epoch:1151, lr:0.000971, total loss:0.000689\n",
            "[Train] epoch:1152, lr:0.000971, total loss:0.000710\n",
            "[Train] epoch:1153, lr:0.000971, total loss:0.000701\n",
            "[Train] epoch:1154, lr:0.000971, total loss:0.000681\n",
            "[Train] epoch:1155, lr:0.000971, total loss:0.000685\n",
            "[Train] epoch:1156, lr:0.000971, total loss:0.000748\n",
            "[Train] epoch:1157, lr:0.000971, total loss:0.000747\n",
            "[Train] epoch:1158, lr:0.000971, total loss:0.000697\n",
            "[Train] epoch:1159, lr:0.000971, total loss:0.000719\n",
            "[Train] epoch:1160, lr:0.000971, total loss:0.000750\n",
            "[Train] epoch:1161, lr:0.000971, total loss:0.000588\n",
            "[Train] epoch:1162, lr:0.000971, total loss:0.000658\n",
            "[Train] epoch:1163, lr:0.000971, total loss:0.000694\n",
            "[Train] epoch:1164, lr:0.000971, total loss:0.000700\n",
            "[Train] epoch:1165, lr:0.000971, total loss:0.000627\n",
            "[Train] epoch:1166, lr:0.000971, total loss:0.000678\n",
            "[Train] epoch:1167, lr:0.000971, total loss:0.000706\n",
            "[Train] epoch:1168, lr:0.000971, total loss:0.000730\n",
            "[Train] epoch:1169, lr:0.000970, total loss:0.000764\n",
            "[Train] epoch:1170, lr:0.000970, total loss:0.000739\n",
            "[Train] epoch:1171, lr:0.000970, total loss:0.000701\n",
            "[Train] epoch:1172, lr:0.000970, total loss:0.000729\n",
            "[Train] epoch:1173, lr:0.000970, total loss:0.000675\n",
            "[Train] epoch:1174, lr:0.000970, total loss:0.000726\n",
            "[Train] epoch:1175, lr:0.000970, total loss:0.000677\n",
            "[Train] epoch:1176, lr:0.000970, total loss:0.000692\n",
            "[Train] epoch:1177, lr:0.000970, total loss:0.000722\n",
            "[Train] epoch:1178, lr:0.000970, total loss:0.000704\n",
            "[Train] epoch:1179, lr:0.000970, total loss:0.000690\n",
            "[Train] epoch:1180, lr:0.000970, total loss:0.000781\n",
            "[Train] epoch:1181, lr:0.000970, total loss:0.000750\n",
            "[Train] epoch:1182, lr:0.000970, total loss:0.000852\n",
            "[Train] epoch:1183, lr:0.000970, total loss:0.000702\n",
            "[Train] epoch:1184, lr:0.000970, total loss:0.000713\n",
            "[Train] epoch:1185, lr:0.000970, total loss:0.000786\n",
            "[Train] epoch:1186, lr:0.000970, total loss:0.000631\n",
            "[Train] epoch:1187, lr:0.000970, total loss:0.000695\n",
            "[Train] epoch:1188, lr:0.000970, total loss:0.000702\n",
            "[Train] epoch:1189, lr:0.000969, total loss:0.000738\n",
            "[Train] epoch:1190, lr:0.000969, total loss:0.000674\n",
            "[Train] epoch:1191, lr:0.000969, total loss:0.000691\n",
            "[Train] epoch:1192, lr:0.000969, total loss:0.000773\n",
            "[Train] epoch:1193, lr:0.000969, total loss:0.000684\n",
            "[Train] epoch:1194, lr:0.000969, total loss:0.000676\n",
            "[Train] epoch:1195, lr:0.000969, total loss:0.000739\n",
            "[Train] epoch:1196, lr:0.000969, total loss:0.000728\n",
            "[Train] epoch:1197, lr:0.000969, total loss:0.000812\n",
            "[Train] epoch:1198, lr:0.000969, total loss:0.000647\n",
            "[Train] epoch:1199, lr:0.000969, total loss:0.000727\n",
            "[Train] epoch:1200, lr:0.000969, total loss:0.000703\n",
            "[Train] epoch:1201, lr:0.000969, total loss:0.000678\n",
            "[Train] epoch:1202, lr:0.000969, total loss:0.000648\n",
            "[Train] epoch:1203, lr:0.000969, total loss:0.000802\n",
            "[Train] epoch:1204, lr:0.000969, total loss:0.000889\n",
            "[Train] epoch:1205, lr:0.000969, total loss:0.000831\n",
            "[Train] epoch:1206, lr:0.000969, total loss:0.000788\n",
            "[Train] epoch:1207, lr:0.000969, total loss:0.000747\n",
            "[Train] epoch:1208, lr:0.000968, total loss:0.000703\n",
            "[Train] epoch:1209, lr:0.000968, total loss:0.000701\n",
            "[Train] epoch:1210, lr:0.000968, total loss:0.000709\n",
            "[Train] epoch:1211, lr:0.000968, total loss:0.000799\n",
            "[Train] epoch:1212, lr:0.000968, total loss:0.000709\n",
            "[Train] epoch:1213, lr:0.000968, total loss:0.000705\n",
            "[Train] epoch:1214, lr:0.000968, total loss:0.000670\n",
            "[Train] epoch:1215, lr:0.000968, total loss:0.000695\n",
            "[Train] epoch:1216, lr:0.000968, total loss:0.000684\n",
            "[Train] epoch:1217, lr:0.000968, total loss:0.000647\n",
            "[Train] epoch:1218, lr:0.000968, total loss:0.000735\n",
            "[Train] epoch:1219, lr:0.000968, total loss:0.000726\n",
            "[Train] epoch:1220, lr:0.000968, total loss:0.000832\n",
            "[Train] epoch:1221, lr:0.000968, total loss:0.000665\n",
            "[Train] epoch:1222, lr:0.000968, total loss:0.000657\n",
            "[Train] epoch:1223, lr:0.000968, total loss:0.000666\n",
            "[Train] epoch:1224, lr:0.000968, total loss:0.000639\n",
            "[Train] epoch:1225, lr:0.000968, total loss:0.000642\n",
            "[Train] epoch:1226, lr:0.000968, total loss:0.000682\n",
            "[Train] epoch:1227, lr:0.000967, total loss:0.000733\n",
            "[Train] epoch:1228, lr:0.000967, total loss:0.000610\n",
            "[Train] epoch:1229, lr:0.000967, total loss:0.000681\n",
            "[Train] epoch:1230, lr:0.000967, total loss:0.000647\n",
            "[Train] epoch:1231, lr:0.000967, total loss:0.000726\n",
            "[Train] epoch:1232, lr:0.000967, total loss:0.000681\n",
            "[Train] epoch:1233, lr:0.000967, total loss:0.000627\n",
            "[Train] epoch:1234, lr:0.000967, total loss:0.000666\n",
            "[Train] epoch:1235, lr:0.000967, total loss:0.000634\n",
            "[Train] epoch:1236, lr:0.000967, total loss:0.000645\n",
            "[Train] epoch:1237, lr:0.000967, total loss:0.000639\n",
            "[Train] epoch:1238, lr:0.000967, total loss:0.000785\n",
            "[Train] epoch:1239, lr:0.000967, total loss:0.000585\n",
            "[Train] epoch:1240, lr:0.000967, total loss:0.000663\n",
            "[Train] epoch:1241, lr:0.000967, total loss:0.000606\n",
            "[Train] epoch:1242, lr:0.000967, total loss:0.000627\n",
            "[Train] epoch:1243, lr:0.000967, total loss:0.000646\n",
            "[Train] epoch:1244, lr:0.000967, total loss:0.000629\n",
            "[Train] epoch:1245, lr:0.000967, total loss:0.000630\n",
            "[Train] epoch:1246, lr:0.000966, total loss:0.000767\n",
            "[Train] epoch:1247, lr:0.000966, total loss:0.000659\n",
            "[Train] epoch:1248, lr:0.000966, total loss:0.000606\n",
            "[Train] epoch:1249, lr:0.000966, total loss:0.000698\n",
            "[Train] epoch:1250, lr:0.000966, total loss:0.000638\n",
            "[Train] epoch:1251, lr:0.000966, total loss:0.000635\n",
            "[Train] epoch:1252, lr:0.000966, total loss:0.000604\n",
            "[Train] epoch:1253, lr:0.000966, total loss:0.000565\n",
            "[Train] epoch:1254, lr:0.000966, total loss:0.000580\n",
            "[Train] epoch:1255, lr:0.000966, total loss:0.000636\n",
            "[Train] epoch:1256, lr:0.000966, total loss:0.000607\n",
            "[Train] epoch:1257, lr:0.000966, total loss:0.000591\n",
            "[Train] epoch:1258, lr:0.000966, total loss:0.000594\n",
            "[Train] epoch:1259, lr:0.000966, total loss:0.000637\n",
            "[Train] epoch:1260, lr:0.000966, total loss:0.000606\n",
            "[Train] epoch:1261, lr:0.000966, total loss:0.000695\n",
            "[Train] epoch:1262, lr:0.000966, total loss:0.000885\n",
            "[Train] epoch:1263, lr:0.000966, total loss:0.000716\n",
            "[Train] epoch:1264, lr:0.000966, total loss:0.000629\n",
            "[Train] epoch:1265, lr:0.000965, total loss:0.000620\n",
            "[Train] epoch:1266, lr:0.000965, total loss:0.000585\n",
            "[Train] epoch:1267, lr:0.000965, total loss:0.000623\n",
            "[Train] epoch:1268, lr:0.000965, total loss:0.000576\n",
            "[Train] epoch:1269, lr:0.000965, total loss:0.000696\n",
            "[Train] epoch:1270, lr:0.000965, total loss:0.000564\n",
            "[Train] epoch:1271, lr:0.000965, total loss:0.000713\n",
            "[Train] epoch:1272, lr:0.000965, total loss:0.000732\n",
            "[Train] epoch:1273, lr:0.000965, total loss:0.000610\n",
            "[Train] epoch:1274, lr:0.000965, total loss:0.000637\n",
            "[Train] epoch:1275, lr:0.000965, total loss:0.000622\n",
            "[Train] epoch:1276, lr:0.000965, total loss:0.000568\n",
            "[Train] epoch:1277, lr:0.000965, total loss:0.000602\n",
            "[Train] epoch:1278, lr:0.000965, total loss:0.000570\n",
            "[Train] epoch:1279, lr:0.000965, total loss:0.000627\n",
            "[Train] epoch:1280, lr:0.000965, total loss:0.000578\n",
            "[Train] epoch:1281, lr:0.000965, total loss:0.000596\n",
            "[Train] epoch:1282, lr:0.000965, total loss:0.000587\n",
            "[Train] epoch:1283, lr:0.000964, total loss:0.000576\n",
            "[Train] epoch:1284, lr:0.000964, total loss:0.000617\n",
            "[Train] epoch:1285, lr:0.000964, total loss:0.000800\n",
            "[Train] epoch:1286, lr:0.000964, total loss:0.000765\n",
            "[Train] epoch:1287, lr:0.000964, total loss:0.000711\n",
            "[Train] epoch:1288, lr:0.000964, total loss:0.000632\n",
            "[Train] epoch:1289, lr:0.000964, total loss:0.000643\n",
            "[Train] epoch:1290, lr:0.000964, total loss:0.000675\n",
            "[Train] epoch:1291, lr:0.000964, total loss:0.000646\n",
            "[Train] epoch:1292, lr:0.000964, total loss:0.000661\n",
            "[Train] epoch:1293, lr:0.000964, total loss:0.000733\n",
            "[Train] epoch:1294, lr:0.000964, total loss:0.001001\n",
            "[Train] epoch:1295, lr:0.000964, total loss:0.000637\n",
            "[Train] epoch:1296, lr:0.000964, total loss:0.000602\n",
            "[Train] epoch:1297, lr:0.000964, total loss:0.000611\n",
            "[Train] epoch:1298, lr:0.000964, total loss:0.000591\n",
            "[Train] epoch:1299, lr:0.000964, total loss:0.000585\n",
            "[Train] epoch:1300, lr:0.000964, total loss:0.000694\n",
            "[Train] epoch:1301, lr:0.000963, total loss:0.000607\n",
            "[Train] epoch:1302, lr:0.000963, total loss:0.000645\n",
            "[Train] epoch:1303, lr:0.000963, total loss:0.000562\n",
            "[Train] epoch:1304, lr:0.000963, total loss:0.000595\n",
            "[Train] epoch:1305, lr:0.000963, total loss:0.000587\n",
            "[Train] epoch:1306, lr:0.000963, total loss:0.000570\n",
            "[Train] epoch:1307, lr:0.000963, total loss:0.000686\n",
            "[Train] epoch:1308, lr:0.000963, total loss:0.000531\n",
            "[Train] epoch:1309, lr:0.000963, total loss:0.000693\n",
            "[Train] epoch:1310, lr:0.000963, total loss:0.000734\n",
            "[Train] epoch:1311, lr:0.000963, total loss:0.000691\n",
            "[Train] epoch:1312, lr:0.000963, total loss:0.000644\n",
            "[Train] epoch:1313, lr:0.000963, total loss:0.000678\n",
            "[Train] epoch:1314, lr:0.000963, total loss:0.000753\n",
            "[Train] epoch:1315, lr:0.000963, total loss:0.000846\n",
            "[Train] epoch:1316, lr:0.000963, total loss:0.000885\n",
            "[Train] epoch:1317, lr:0.000963, total loss:0.000877\n",
            "[Train] epoch:1318, lr:0.000963, total loss:0.000817\n",
            "[Train] epoch:1319, lr:0.000962, total loss:0.000697\n",
            "[Train] epoch:1320, lr:0.000962, total loss:0.000802\n",
            "[Train] epoch:1321, lr:0.000962, total loss:0.000685\n",
            "[Train] epoch:1322, lr:0.000962, total loss:0.000673\n",
            "[Train] epoch:1323, lr:0.000962, total loss:0.000659\n",
            "[Train] epoch:1324, lr:0.000962, total loss:0.000631\n",
            "[Train] epoch:1325, lr:0.000962, total loss:0.000627\n",
            "[Train] epoch:1326, lr:0.000962, total loss:0.000617\n",
            "[Train] epoch:1327, lr:0.000962, total loss:0.000649\n",
            "[Train] epoch:1328, lr:0.000962, total loss:0.000615\n",
            "[Train] epoch:1329, lr:0.000962, total loss:0.000550\n",
            "[Train] epoch:1330, lr:0.000962, total loss:0.000581\n",
            "[Train] epoch:1331, lr:0.000962, total loss:0.000612\n",
            "[Train] epoch:1332, lr:0.000962, total loss:0.000688\n",
            "[Train] epoch:1333, lr:0.000962, total loss:0.000591\n",
            "[Train] epoch:1334, lr:0.000962, total loss:0.000565\n",
            "[Train] epoch:1335, lr:0.000962, total loss:0.000738\n",
            "[Train] epoch:1336, lr:0.000961, total loss:0.000639\n",
            "[Train] epoch:1337, lr:0.000961, total loss:0.000748\n",
            "[Train] epoch:1338, lr:0.000961, total loss:0.000687\n",
            "[Train] epoch:1339, lr:0.000961, total loss:0.000687\n",
            "[Train] epoch:1340, lr:0.000961, total loss:0.000748\n",
            "[Train] epoch:1341, lr:0.000961, total loss:0.000716\n",
            "[Train] epoch:1342, lr:0.000961, total loss:0.000764\n",
            "[Train] epoch:1343, lr:0.000961, total loss:0.000704\n",
            "[Train] epoch:1344, lr:0.000961, total loss:0.000658\n",
            "[Train] epoch:1345, lr:0.000961, total loss:0.000581\n",
            "[Train] epoch:1346, lr:0.000961, total loss:0.000660\n",
            "[Train] epoch:1347, lr:0.000961, total loss:0.000642\n",
            "[Train] epoch:1348, lr:0.000961, total loss:0.000653\n",
            "[Train] epoch:1349, lr:0.000961, total loss:0.000621\n",
            "[Train] epoch:1350, lr:0.000961, total loss:0.000768\n",
            "[Train] epoch:1351, lr:0.000961, total loss:0.000736\n",
            "[Train] epoch:1352, lr:0.000961, total loss:0.000652\n",
            "[Train] epoch:1353, lr:0.000961, total loss:0.000597\n",
            "[Train] epoch:1354, lr:0.000960, total loss:0.000600\n",
            "[Train] epoch:1355, lr:0.000960, total loss:0.000561\n",
            "[Train] epoch:1356, lr:0.000960, total loss:0.000560\n",
            "[Train] epoch:1357, lr:0.000960, total loss:0.000678\n",
            "[Train] epoch:1358, lr:0.000960, total loss:0.000546\n",
            "[Train] epoch:1359, lr:0.000960, total loss:0.000659\n",
            "[Train] epoch:1360, lr:0.000960, total loss:0.000680\n",
            "[Train] epoch:1361, lr:0.000960, total loss:0.000611\n",
            "[Train] epoch:1362, lr:0.000960, total loss:0.000657\n",
            "[Train] epoch:1363, lr:0.000960, total loss:0.000552\n",
            "[Train] epoch:1364, lr:0.000960, total loss:0.000565\n",
            "[Train] epoch:1365, lr:0.000960, total loss:0.000575\n",
            "[Train] epoch:1366, lr:0.000960, total loss:0.000526\n",
            "[Train] epoch:1367, lr:0.000960, total loss:0.000618\n",
            "[Train] epoch:1368, lr:0.000960, total loss:0.000819\n",
            "[Train] epoch:1369, lr:0.000960, total loss:0.000661\n",
            "[Train] epoch:1370, lr:0.000960, total loss:0.000675\n",
            "[Train] epoch:1371, lr:0.000959, total loss:0.000776\n",
            "[Train] epoch:1372, lr:0.000959, total loss:0.000673\n",
            "[Train] epoch:1373, lr:0.000959, total loss:0.000746\n",
            "[Train] epoch:1374, lr:0.000959, total loss:0.000731\n",
            "[Train] epoch:1375, lr:0.000959, total loss:0.000669\n",
            "[Train] epoch:1376, lr:0.000959, total loss:0.000727\n",
            "[Train] epoch:1377, lr:0.000959, total loss:0.000684\n",
            "[Train] epoch:1378, lr:0.000959, total loss:0.000690\n",
            "[Train] epoch:1379, lr:0.000959, total loss:0.000647\n",
            "[Train] epoch:1380, lr:0.000959, total loss:0.000655\n",
            "[Train] epoch:1381, lr:0.000959, total loss:0.000719\n",
            "[Train] epoch:1382, lr:0.000959, total loss:0.000653\n",
            "[Train] epoch:1383, lr:0.000959, total loss:0.000659\n",
            "[Train] epoch:1384, lr:0.000959, total loss:0.000602\n",
            "[Train] epoch:1385, lr:0.000959, total loss:0.000586\n",
            "[Train] epoch:1386, lr:0.000959, total loss:0.000656\n",
            "[Train] epoch:1387, lr:0.000959, total loss:0.000657\n",
            "[Train] epoch:1388, lr:0.000958, total loss:0.000606\n",
            "[Train] epoch:1389, lr:0.000958, total loss:0.000691\n",
            "[Train] epoch:1390, lr:0.000958, total loss:0.000610\n",
            "[Train] epoch:1391, lr:0.000958, total loss:0.000702\n",
            "[Train] epoch:1392, lr:0.000958, total loss:0.000661\n",
            "[Train] epoch:1393, lr:0.000958, total loss:0.000640\n",
            "[Train] epoch:1394, lr:0.000958, total loss:0.000683\n",
            "[Train] epoch:1395, lr:0.000958, total loss:0.000609\n",
            "[Train] epoch:1396, lr:0.000958, total loss:0.000594\n",
            "[Train] epoch:1397, lr:0.000958, total loss:0.000583\n",
            "[Train] epoch:1398, lr:0.000958, total loss:0.000618\n",
            "[Train] epoch:1399, lr:0.000958, total loss:0.000623\n",
            "[Train] epoch:1400, lr:0.000958, total loss:0.000596\n",
            "[Train] epoch:1401, lr:0.000958, total loss:0.000555\n",
            "[Train] epoch:1402, lr:0.000958, total loss:0.000577\n",
            "[Train] epoch:1403, lr:0.000958, total loss:0.000553\n",
            "[Train] epoch:1404, lr:0.000958, total loss:0.000611\n",
            "[Train] epoch:1405, lr:0.000957, total loss:0.000724\n",
            "[Train] epoch:1406, lr:0.000957, total loss:0.000680\n",
            "[Train] epoch:1407, lr:0.000957, total loss:0.000689\n",
            "[Train] epoch:1408, lr:0.000957, total loss:0.000771\n",
            "[Train] epoch:1409, lr:0.000957, total loss:0.000641\n",
            "[Train] epoch:1410, lr:0.000957, total loss:0.000771\n",
            "[Train] epoch:1411, lr:0.000957, total loss:0.000635\n",
            "[Train] epoch:1412, lr:0.000957, total loss:0.000597\n",
            "[Train] epoch:1413, lr:0.000957, total loss:0.000648\n",
            "[Train] epoch:1414, lr:0.000957, total loss:0.000555\n",
            "[Train] epoch:1415, lr:0.000957, total loss:0.000576\n",
            "[Train] epoch:1416, lr:0.000957, total loss:0.000617\n",
            "[Train] epoch:1417, lr:0.000957, total loss:0.000556\n",
            "[Train] epoch:1418, lr:0.000957, total loss:0.000628\n",
            "[Train] epoch:1419, lr:0.000957, total loss:0.000694\n",
            "[Train] epoch:1420, lr:0.000957, total loss:0.000659\n",
            "[Train] epoch:1421, lr:0.000956, total loss:0.000623\n",
            "[Train] epoch:1422, lr:0.000956, total loss:0.000614\n",
            "[Train] epoch:1423, lr:0.000956, total loss:0.000609\n",
            "[Train] epoch:1424, lr:0.000956, total loss:0.000565\n",
            "[Train] epoch:1425, lr:0.000956, total loss:0.000564\n",
            "[Train] epoch:1426, lr:0.000956, total loss:0.000674\n",
            "[Train] epoch:1427, lr:0.000956, total loss:0.000546\n",
            "[Train] epoch:1428, lr:0.000956, total loss:0.000586\n",
            "[Train] epoch:1429, lr:0.000956, total loss:0.000512\n",
            "[Train] epoch:1430, lr:0.000956, total loss:0.000543\n",
            "[Train] epoch:1431, lr:0.000956, total loss:0.000602\n",
            "[Train] epoch:1432, lr:0.000956, total loss:0.000596\n",
            "[Train] epoch:1433, lr:0.000956, total loss:0.000565\n",
            "[Train] epoch:1434, lr:0.000956, total loss:0.000557\n",
            "[Train] epoch:1435, lr:0.000956, total loss:0.000566\n",
            "[Train] epoch:1436, lr:0.000956, total loss:0.000498\n",
            "[Train] epoch:1437, lr:0.000956, total loss:0.000561\n",
            "[Train] epoch:1438, lr:0.000955, total loss:0.000570\n",
            "[Train] epoch:1439, lr:0.000955, total loss:0.000593\n",
            "[Train] epoch:1440, lr:0.000955, total loss:0.000568\n",
            "[Train] epoch:1441, lr:0.000955, total loss:0.000631\n",
            "[Train] epoch:1442, lr:0.000955, total loss:0.000518\n",
            "[Train] epoch:1443, lr:0.000955, total loss:0.000567\n",
            "[Train] epoch:1444, lr:0.000955, total loss:0.000564\n",
            "[Train] epoch:1445, lr:0.000955, total loss:0.000559\n",
            "[Train] epoch:1446, lr:0.000955, total loss:0.000535\n",
            "[Train] epoch:1447, lr:0.000955, total loss:0.000558\n",
            "[Train] epoch:1448, lr:0.000955, total loss:0.000546\n",
            "[Train] epoch:1449, lr:0.000955, total loss:0.000565\n",
            "[Train] epoch:1450, lr:0.000955, total loss:0.000566\n",
            "[Train] epoch:1451, lr:0.000955, total loss:0.000544\n",
            "[Train] epoch:1452, lr:0.000955, total loss:0.000568\n",
            "[Train] epoch:1453, lr:0.000955, total loss:0.000615\n",
            "[Train] epoch:1454, lr:0.000954, total loss:0.000620\n",
            "[Train] epoch:1455, lr:0.000954, total loss:0.000595\n",
            "[Train] epoch:1456, lr:0.000954, total loss:0.000556\n",
            "[Train] epoch:1457, lr:0.000954, total loss:0.000674\n",
            "[Train] epoch:1458, lr:0.000954, total loss:0.000663\n",
            "[Train] epoch:1459, lr:0.000954, total loss:0.000636\n",
            "[Train] epoch:1460, lr:0.000954, total loss:0.000656\n",
            "[Train] epoch:1461, lr:0.000954, total loss:0.000663\n",
            "[Train] epoch:1462, lr:0.000954, total loss:0.000615\n",
            "[Train] epoch:1463, lr:0.000954, total loss:0.000552\n",
            "[Train] epoch:1464, lr:0.000954, total loss:0.000586\n",
            "[Train] epoch:1465, lr:0.000954, total loss:0.000565\n",
            "[Train] epoch:1466, lr:0.000954, total loss:0.000754\n",
            "[Train] epoch:1467, lr:0.000954, total loss:0.000899\n",
            "[Train] epoch:1468, lr:0.000954, total loss:0.001087\n",
            "[Train] epoch:1469, lr:0.000954, total loss:0.000918\n",
            "[Train] epoch:1470, lr:0.000953, total loss:0.000830\n",
            "[Train] epoch:1471, lr:0.000953, total loss:0.000663\n",
            "[Train] epoch:1472, lr:0.000953, total loss:0.000588\n",
            "[Train] epoch:1473, lr:0.000953, total loss:0.000556\n",
            "[Train] epoch:1474, lr:0.000953, total loss:0.000578\n",
            "[Train] epoch:1475, lr:0.000953, total loss:0.000540\n",
            "[Train] epoch:1476, lr:0.000953, total loss:0.000574\n",
            "[Train] epoch:1477, lr:0.000953, total loss:0.000617\n",
            "[Train] epoch:1478, lr:0.000953, total loss:0.000677\n",
            "[Train] epoch:1479, lr:0.000953, total loss:0.000601\n",
            "[Train] epoch:1480, lr:0.000953, total loss:0.000566\n",
            "[Train] epoch:1481, lr:0.000953, total loss:0.000536\n",
            "[Train] epoch:1482, lr:0.000953, total loss:0.000527\n",
            "[Train] epoch:1483, lr:0.000953, total loss:0.000555\n",
            "[Train] epoch:1484, lr:0.000953, total loss:0.000555\n",
            "[Train] epoch:1485, lr:0.000953, total loss:0.000696\n",
            "[Train] epoch:1486, lr:0.000952, total loss:0.000545\n",
            "[Train] epoch:1487, lr:0.000952, total loss:0.000561\n",
            "[Train] epoch:1488, lr:0.000952, total loss:0.000552\n",
            "[Train] epoch:1489, lr:0.000952, total loss:0.000501\n",
            "[Train] epoch:1490, lr:0.000952, total loss:0.000536\n",
            "[Train] epoch:1491, lr:0.000952, total loss:0.000554\n",
            "[Train] epoch:1492, lr:0.000952, total loss:0.000500\n",
            "[Train] epoch:1493, lr:0.000952, total loss:0.000564\n",
            "[Train] epoch:1494, lr:0.000952, total loss:0.000763\n",
            "[Train] epoch:1495, lr:0.000952, total loss:0.000606\n",
            "[Train] epoch:1496, lr:0.000952, total loss:0.000620\n",
            "[Train] epoch:1497, lr:0.000952, total loss:0.000580\n",
            "[Train] epoch:1498, lr:0.000952, total loss:0.000594\n",
            "[Train] epoch:1499, lr:0.000952, total loss:0.000562\n",
            "[Train] epoch:1500, lr:0.000952, total loss:0.000597\n",
            "[Train] epoch:1501, lr:0.000951, total loss:0.000731\n",
            "[Train] epoch:1502, lr:0.000951, total loss:0.000662\n",
            "[Train] epoch:1503, lr:0.000951, total loss:0.000609\n",
            "[Train] epoch:1504, lr:0.000951, total loss:0.000577\n",
            "[Train] epoch:1505, lr:0.000951, total loss:0.000563\n",
            "[Train] epoch:1506, lr:0.000951, total loss:0.000576\n",
            "[Train] epoch:1507, lr:0.000951, total loss:0.000570\n",
            "[Train] epoch:1508, lr:0.000951, total loss:0.000787\n",
            "[Train] epoch:1509, lr:0.000951, total loss:0.000603\n",
            "[Train] epoch:1510, lr:0.000951, total loss:0.000624\n",
            "[Train] epoch:1511, lr:0.000951, total loss:0.000597\n",
            "[Train] epoch:1512, lr:0.000951, total loss:0.000666\n",
            "[Train] epoch:1513, lr:0.000951, total loss:0.000651\n",
            "[Train] epoch:1514, lr:0.000951, total loss:0.000585\n",
            "[Train] epoch:1515, lr:0.000951, total loss:0.000626\n",
            "[Train] epoch:1516, lr:0.000951, total loss:0.000543\n",
            "[Train] epoch:1517, lr:0.000950, total loss:0.000517\n",
            "[Train] epoch:1518, lr:0.000950, total loss:0.000524\n",
            "[Train] epoch:1519, lr:0.000950, total loss:0.000603\n",
            "[Train] epoch:1520, lr:0.000950, total loss:0.000597\n",
            "[Train] epoch:1521, lr:0.000950, total loss:0.000550\n",
            "[Train] epoch:1522, lr:0.000950, total loss:0.000553\n",
            "[Train] epoch:1523, lr:0.000950, total loss:0.000537\n",
            "[Train] epoch:1524, lr:0.000950, total loss:0.000540\n",
            "[Train] epoch:1525, lr:0.000950, total loss:0.000588\n",
            "[Train] epoch:1526, lr:0.000950, total loss:0.000585\n",
            "[Train] epoch:1527, lr:0.000950, total loss:0.000506\n",
            "[Train] epoch:1528, lr:0.000950, total loss:0.000566\n",
            "[Train] epoch:1529, lr:0.000950, total loss:0.000551\n",
            "[Train] epoch:1530, lr:0.000950, total loss:0.000524\n",
            "[Train] epoch:1531, lr:0.000950, total loss:0.000583\n",
            "[Train] epoch:1532, lr:0.000949, total loss:0.000583\n",
            "[Train] epoch:1533, lr:0.000949, total loss:0.000735\n",
            "[Train] epoch:1534, lr:0.000949, total loss:0.000556\n",
            "[Train] epoch:1535, lr:0.000949, total loss:0.000542\n",
            "[Train] epoch:1536, lr:0.000949, total loss:0.000523\n",
            "[Train] epoch:1537, lr:0.000949, total loss:0.000552\n",
            "[Train] epoch:1538, lr:0.000949, total loss:0.000560\n",
            "[Train] epoch:1539, lr:0.000949, total loss:0.000525\n",
            "[Train] epoch:1540, lr:0.000949, total loss:0.000479\n",
            "[Train] epoch:1541, lr:0.000949, total loss:0.000469\n",
            "[Train] epoch:1542, lr:0.000949, total loss:0.000493\n",
            "[Train] epoch:1543, lr:0.000949, total loss:0.000510\n",
            "[Train] epoch:1544, lr:0.000949, total loss:0.000520\n",
            "[Train] epoch:1545, lr:0.000949, total loss:0.000457\n",
            "[Train] epoch:1546, lr:0.000949, total loss:0.000471\n",
            "[Train] epoch:1547, lr:0.000949, total loss:0.000491\n",
            "[Train] epoch:1548, lr:0.000948, total loss:0.000540\n",
            "[Train] epoch:1549, lr:0.000948, total loss:0.000547\n",
            "[Train] epoch:1550, lr:0.000948, total loss:0.000588\n",
            "[Train] epoch:1551, lr:0.000948, total loss:0.000604\n",
            "[Train] epoch:1552, lr:0.000948, total loss:0.000598\n",
            "[Train] epoch:1553, lr:0.000948, total loss:0.000537\n",
            "[Train] epoch:1554, lr:0.000948, total loss:0.000553\n",
            "[Train] epoch:1555, lr:0.000948, total loss:0.000509\n",
            "[Train] epoch:1556, lr:0.000948, total loss:0.000522\n",
            "[Train] epoch:1557, lr:0.000948, total loss:0.000591\n",
            "[Train] epoch:1558, lr:0.000948, total loss:0.000583\n",
            "[Train] epoch:1559, lr:0.000948, total loss:0.000586\n",
            "[Train] epoch:1560, lr:0.000948, total loss:0.000682\n",
            "[Train] epoch:1561, lr:0.000948, total loss:0.000562\n",
            "[Train] epoch:1562, lr:0.000948, total loss:0.000524\n",
            "[Train] epoch:1563, lr:0.000947, total loss:0.000581\n",
            "[Train] epoch:1564, lr:0.000947, total loss:0.000556\n",
            "[Train] epoch:1565, lr:0.000947, total loss:0.000584\n",
            "[Train] epoch:1566, lr:0.000947, total loss:0.000555\n",
            "[Train] epoch:1567, lr:0.000947, total loss:0.000493\n",
            "[Train] epoch:1568, lr:0.000947, total loss:0.000479\n",
            "[Train] epoch:1569, lr:0.000947, total loss:0.000489\n",
            "[Train] epoch:1570, lr:0.000947, total loss:0.000448\n",
            "[Train] epoch:1571, lr:0.000947, total loss:0.000483\n",
            "[Train] epoch:1572, lr:0.000947, total loss:0.000520\n",
            "[Train] epoch:1573, lr:0.000947, total loss:0.000510\n",
            "[Train] epoch:1574, lr:0.000947, total loss:0.000494\n",
            "[Train] epoch:1575, lr:0.000947, total loss:0.000512\n",
            "[Train] epoch:1576, lr:0.000947, total loss:0.000489\n",
            "[Train] epoch:1577, lr:0.000947, total loss:0.000468\n",
            "[Train] epoch:1578, lr:0.000946, total loss:0.000511\n",
            "[Train] epoch:1579, lr:0.000946, total loss:0.000555\n",
            "[Train] epoch:1580, lr:0.000946, total loss:0.000523\n",
            "[Train] epoch:1581, lr:0.000946, total loss:0.000534\n",
            "[Train] epoch:1582, lr:0.000946, total loss:0.000568\n",
            "[Train] epoch:1583, lr:0.000946, total loss:0.000528\n",
            "[Train] epoch:1584, lr:0.000946, total loss:0.000572\n",
            "[Train] epoch:1585, lr:0.000946, total loss:0.000554\n",
            "[Train] epoch:1586, lr:0.000946, total loss:0.000601\n",
            "[Train] epoch:1587, lr:0.000946, total loss:0.000645\n",
            "[Train] epoch:1588, lr:0.000946, total loss:0.000563\n",
            "[Train] epoch:1589, lr:0.000946, total loss:0.000538\n",
            "[Train] epoch:1590, lr:0.000946, total loss:0.000559\n",
            "[Train] epoch:1591, lr:0.000946, total loss:0.000604\n",
            "[Train] epoch:1592, lr:0.000946, total loss:0.000576\n",
            "[Train] epoch:1593, lr:0.000945, total loss:0.000540\n",
            "[Train] epoch:1594, lr:0.000945, total loss:0.000542\n",
            "[Train] epoch:1595, lr:0.000945, total loss:0.000493\n",
            "[Train] epoch:1596, lr:0.000945, total loss:0.000557\n",
            "[Train] epoch:1597, lr:0.000945, total loss:0.000591\n",
            "[Train] epoch:1598, lr:0.000945, total loss:0.000570\n",
            "[Train] epoch:1599, lr:0.000945, total loss:0.000588\n",
            "[Train] epoch:1600, lr:0.000945, total loss:0.000557\n",
            "[Train] epoch:1601, lr:0.000945, total loss:0.000541\n",
            "[Train] epoch:1602, lr:0.000945, total loss:0.000534\n",
            "[Train] epoch:1603, lr:0.000945, total loss:0.000556\n",
            "[Train] epoch:1604, lr:0.000945, total loss:0.000523\n",
            "[Train] epoch:1605, lr:0.000945, total loss:0.000713\n",
            "[Train] epoch:1606, lr:0.000945, total loss:0.000742\n",
            "[Train] epoch:1607, lr:0.000945, total loss:0.000646\n",
            "[Train] epoch:1608, lr:0.000944, total loss:0.000585\n",
            "[Train] epoch:1609, lr:0.000944, total loss:0.000621\n",
            "[Train] epoch:1610, lr:0.000944, total loss:0.000547\n",
            "[Train] epoch:1611, lr:0.000944, total loss:0.000593\n",
            "[Train] epoch:1612, lr:0.000944, total loss:0.000594\n",
            "[Train] epoch:1613, lr:0.000944, total loss:0.000634\n",
            "[Train] epoch:1614, lr:0.000944, total loss:0.000534\n",
            "[Train] epoch:1615, lr:0.000944, total loss:0.000615\n",
            "[Train] epoch:1616, lr:0.000944, total loss:0.000540\n",
            "[Train] epoch:1617, lr:0.000944, total loss:0.000596\n",
            "[Train] epoch:1618, lr:0.000944, total loss:0.000627\n",
            "[Train] epoch:1619, lr:0.000944, total loss:0.000571\n",
            "[Train] epoch:1620, lr:0.000944, total loss:0.000543\n",
            "[Train] epoch:1621, lr:0.000944, total loss:0.000563\n",
            "[Train] epoch:1622, lr:0.000943, total loss:0.000491\n",
            "[Train] epoch:1623, lr:0.000943, total loss:0.000557\n",
            "[Train] epoch:1624, lr:0.000943, total loss:0.000601\n",
            "[Train] epoch:1625, lr:0.000943, total loss:0.000646\n",
            "[Train] epoch:1626, lr:0.000943, total loss:0.000620\n",
            "[Train] epoch:1627, lr:0.000943, total loss:0.000540\n",
            "[Train] epoch:1628, lr:0.000943, total loss:0.000518\n",
            "[Train] epoch:1629, lr:0.000943, total loss:0.000507\n",
            "[Train] epoch:1630, lr:0.000943, total loss:0.000555\n",
            "[Train] epoch:1631, lr:0.000943, total loss:0.000554\n",
            "[Train] epoch:1632, lr:0.000943, total loss:0.000611\n",
            "[Train] epoch:1633, lr:0.000943, total loss:0.000633\n",
            "[Train] epoch:1634, lr:0.000943, total loss:0.000569\n",
            "[Train] epoch:1635, lr:0.000943, total loss:0.000551\n",
            "[Train] epoch:1636, lr:0.000943, total loss:0.000510\n",
            "[Train] epoch:1637, lr:0.000942, total loss:0.000546\n",
            "[Train] epoch:1638, lr:0.000942, total loss:0.000509\n",
            "[Train] epoch:1639, lr:0.000942, total loss:0.000482\n",
            "[Train] epoch:1640, lr:0.000942, total loss:0.000483\n",
            "[Train] epoch:1641, lr:0.000942, total loss:0.000536\n",
            "[Train] epoch:1642, lr:0.000942, total loss:0.000532\n",
            "[Train] epoch:1643, lr:0.000942, total loss:0.000513\n",
            "[Train] epoch:1644, lr:0.000942, total loss:0.000502\n",
            "[Train] epoch:1645, lr:0.000942, total loss:0.000536\n",
            "[Train] epoch:1646, lr:0.000942, total loss:0.000532\n",
            "[Train] epoch:1647, lr:0.000942, total loss:0.000528\n",
            "[Train] epoch:1648, lr:0.000942, total loss:0.000559\n",
            "[Train] epoch:1649, lr:0.000942, total loss:0.000524\n",
            "[Train] epoch:1650, lr:0.000942, total loss:0.000536\n",
            "[Train] epoch:1651, lr:0.000941, total loss:0.000533\n",
            "[Train] epoch:1652, lr:0.000941, total loss:0.000523\n",
            "[Train] epoch:1653, lr:0.000941, total loss:0.000506\n",
            "[Train] epoch:1654, lr:0.000941, total loss:0.000539\n",
            "[Train] epoch:1655, lr:0.000941, total loss:0.000531\n",
            "[Train] epoch:1656, lr:0.000941, total loss:0.000536\n",
            "[Train] epoch:1657, lr:0.000941, total loss:0.000534\n",
            "[Train] epoch:1658, lr:0.000941, total loss:0.000507\n",
            "[Train] epoch:1659, lr:0.000941, total loss:0.000581\n",
            "[Train] epoch:1660, lr:0.000941, total loss:0.000592\n",
            "[Train] epoch:1661, lr:0.000941, total loss:0.000538\n",
            "[Train] epoch:1662, lr:0.000941, total loss:0.000561\n",
            "[Train] epoch:1663, lr:0.000941, total loss:0.000736\n",
            "[Train] epoch:1664, lr:0.000941, total loss:0.000634\n",
            "[Train] epoch:1665, lr:0.000940, total loss:0.000513\n",
            "[Train] epoch:1666, lr:0.000940, total loss:0.000494\n",
            "[Train] epoch:1667, lr:0.000940, total loss:0.000439\n",
            "[Train] epoch:1668, lr:0.000940, total loss:0.000454\n",
            "[Train] epoch:1669, lr:0.000940, total loss:0.000445\n",
            "[Train] epoch:1670, lr:0.000940, total loss:0.000499\n",
            "[Train] epoch:1671, lr:0.000940, total loss:0.000527\n",
            "[Train] epoch:1672, lr:0.000940, total loss:0.000483\n",
            "[Train] epoch:1673, lr:0.000940, total loss:0.000625\n",
            "[Train] epoch:1674, lr:0.000940, total loss:0.000568\n",
            "[Train] epoch:1675, lr:0.000940, total loss:0.000545\n",
            "[Train] epoch:1676, lr:0.000940, total loss:0.000588\n",
            "[Train] epoch:1677, lr:0.000940, total loss:0.000517\n",
            "[Train] epoch:1678, lr:0.000940, total loss:0.000491\n",
            "[Train] epoch:1679, lr:0.000939, total loss:0.000475\n",
            "[Train] epoch:1680, lr:0.000939, total loss:0.000568\n",
            "[Train] epoch:1681, lr:0.000939, total loss:0.000555\n",
            "[Train] epoch:1682, lr:0.000939, total loss:0.000570\n",
            "[Train] epoch:1683, lr:0.000939, total loss:0.000508\n",
            "[Train] epoch:1684, lr:0.000939, total loss:0.000489\n",
            "[Train] epoch:1685, lr:0.000939, total loss:0.000588\n",
            "[Train] epoch:1686, lr:0.000939, total loss:0.000493\n",
            "[Train] epoch:1687, lr:0.000939, total loss:0.000499\n",
            "[Train] epoch:1688, lr:0.000939, total loss:0.000537\n",
            "[Train] epoch:1689, lr:0.000939, total loss:0.000424\n",
            "[Train] epoch:1690, lr:0.000939, total loss:0.000470\n",
            "[Train] epoch:1691, lr:0.000939, total loss:0.000473\n",
            "[Train] epoch:1692, lr:0.000939, total loss:0.000493\n",
            "[Train] epoch:1693, lr:0.000939, total loss:0.000476\n",
            "[Train] epoch:1694, lr:0.000938, total loss:0.000465\n",
            "[Train] epoch:1695, lr:0.000938, total loss:0.000439\n",
            "[Train] epoch:1696, lr:0.000938, total loss:0.000472\n",
            "[Train] epoch:1697, lr:0.000938, total loss:0.000484\n",
            "[Train] epoch:1698, lr:0.000938, total loss:0.000450\n",
            "[Train] epoch:1699, lr:0.000938, total loss:0.000467\n",
            "[Train] epoch:1700, lr:0.000938, total loss:0.000438\n",
            "[Train] epoch:1701, lr:0.000938, total loss:0.000523\n",
            "[Train] epoch:1702, lr:0.000938, total loss:0.000516\n",
            "[Train] epoch:1703, lr:0.000938, total loss:0.000501\n",
            "[Train] epoch:1704, lr:0.000938, total loss:0.000427\n",
            "[Train] epoch:1705, lr:0.000938, total loss:0.000529\n",
            "[Train] epoch:1706, lr:0.000938, total loss:0.000487\n",
            "[Train] epoch:1707, lr:0.000937, total loss:0.000549\n",
            "[Train] epoch:1708, lr:0.000937, total loss:0.000524\n",
            "[Train] epoch:1709, lr:0.000937, total loss:0.000495\n",
            "[Train] epoch:1710, lr:0.000937, total loss:0.000472\n",
            "[Train] epoch:1711, lr:0.000937, total loss:0.000460\n",
            "[Train] epoch:1712, lr:0.000937, total loss:0.000581\n",
            "[Train] epoch:1713, lr:0.000937, total loss:0.000587\n",
            "[Train] epoch:1714, lr:0.000937, total loss:0.000563\n",
            "[Train] epoch:1715, lr:0.000937, total loss:0.000533\n",
            "[Train] epoch:1716, lr:0.000937, total loss:0.000523\n",
            "[Train] epoch:1717, lr:0.000937, total loss:0.000518\n",
            "[Train] epoch:1718, lr:0.000937, total loss:0.000480\n",
            "[Train] epoch:1719, lr:0.000937, total loss:0.000539\n",
            "[Train] epoch:1720, lr:0.000937, total loss:0.000574\n",
            "[Train] epoch:1721, lr:0.000936, total loss:0.000555\n",
            "[Train] epoch:1722, lr:0.000936, total loss:0.000553\n",
            "[Train] epoch:1723, lr:0.000936, total loss:0.000551\n",
            "[Train] epoch:1724, lr:0.000936, total loss:0.000591\n",
            "[Train] epoch:1725, lr:0.000936, total loss:0.000492\n",
            "[Train] epoch:1726, lr:0.000936, total loss:0.000483\n",
            "[Train] epoch:1727, lr:0.000936, total loss:0.000439\n",
            "[Train] epoch:1728, lr:0.000936, total loss:0.000577\n",
            "[Train] epoch:1729, lr:0.000936, total loss:0.000575\n",
            "[Train] epoch:1730, lr:0.000936, total loss:0.000628\n",
            "[Train] epoch:1731, lr:0.000936, total loss:0.000565\n",
            "[Train] epoch:1732, lr:0.000936, total loss:0.000511\n",
            "[Train] epoch:1733, lr:0.000936, total loss:0.000536\n",
            "[Train] epoch:1734, lr:0.000936, total loss:0.000500\n",
            "[Train] epoch:1735, lr:0.000935, total loss:0.000498\n",
            "[Train] epoch:1736, lr:0.000935, total loss:0.000535\n",
            "[Train] epoch:1737, lr:0.000935, total loss:0.000477\n",
            "[Train] epoch:1738, lr:0.000935, total loss:0.000500\n",
            "[Train] epoch:1739, lr:0.000935, total loss:0.000478\n",
            "[Train] epoch:1740, lr:0.000935, total loss:0.000567\n",
            "[Train] epoch:1741, lr:0.000935, total loss:0.000496\n",
            "[Train] epoch:1742, lr:0.000935, total loss:0.000493\n",
            "[Train] epoch:1743, lr:0.000935, total loss:0.000512\n",
            "[Train] epoch:1744, lr:0.000935, total loss:0.000500\n",
            "[Train] epoch:1745, lr:0.000935, total loss:0.000499\n",
            "[Train] epoch:1746, lr:0.000935, total loss:0.000483\n",
            "[Train] epoch:1747, lr:0.000935, total loss:0.000503\n",
            "[Train] epoch:1748, lr:0.000935, total loss:0.000516\n",
            "[Train] epoch:1749, lr:0.000934, total loss:0.000418\n",
            "[Train] epoch:1750, lr:0.000934, total loss:0.000552\n",
            "[Train] epoch:1751, lr:0.000934, total loss:0.000589\n",
            "[Train] epoch:1752, lr:0.000934, total loss:0.000629\n",
            "[Train] epoch:1753, lr:0.000934, total loss:0.000459\n",
            "[Train] epoch:1754, lr:0.000934, total loss:0.000478\n",
            "[Train] epoch:1755, lr:0.000934, total loss:0.000567\n",
            "[Train] epoch:1756, lr:0.000934, total loss:0.000673\n",
            "[Train] epoch:1757, lr:0.000934, total loss:0.000626\n",
            "[Train] epoch:1758, lr:0.000934, total loss:0.000629\n",
            "[Train] epoch:1759, lr:0.000934, total loss:0.000590\n",
            "[Train] epoch:1760, lr:0.000934, total loss:0.000561\n",
            "[Train] epoch:1761, lr:0.000934, total loss:0.000494\n",
            "[Train] epoch:1762, lr:0.000933, total loss:0.000486\n",
            "[Train] epoch:1763, lr:0.000933, total loss:0.000531\n",
            "[Train] epoch:1764, lr:0.000933, total loss:0.000521\n",
            "[Train] epoch:1765, lr:0.000933, total loss:0.000463\n",
            "[Train] epoch:1766, lr:0.000933, total loss:0.000536\n",
            "[Train] epoch:1767, lr:0.000933, total loss:0.000507\n",
            "[Train] epoch:1768, lr:0.000933, total loss:0.000468\n",
            "[Train] epoch:1769, lr:0.000933, total loss:0.000532\n",
            "[Train] epoch:1770, lr:0.000933, total loss:0.000542\n",
            "[Train] epoch:1771, lr:0.000933, total loss:0.000507\n",
            "[Train] epoch:1772, lr:0.000933, total loss:0.000468\n",
            "[Train] epoch:1773, lr:0.000933, total loss:0.000542\n",
            "[Train] epoch:1774, lr:0.000933, total loss:0.000498\n",
            "[Train] epoch:1775, lr:0.000933, total loss:0.000445\n",
            "[Train] epoch:1776, lr:0.000932, total loss:0.000452\n",
            "[Train] epoch:1777, lr:0.000932, total loss:0.000448\n",
            "[Train] epoch:1778, lr:0.000932, total loss:0.000482\n",
            "[Train] epoch:1779, lr:0.000932, total loss:0.000557\n",
            "[Train] epoch:1780, lr:0.000932, total loss:0.000534\n",
            "[Train] epoch:1781, lr:0.000932, total loss:0.000509\n",
            "[Train] epoch:1782, lr:0.000932, total loss:0.000485\n",
            "[Train] epoch:1783, lr:0.000932, total loss:0.000479\n",
            "[Train] epoch:1784, lr:0.000932, total loss:0.000514\n",
            "[Train] epoch:1785, lr:0.000932, total loss:0.000614\n",
            "[Train] epoch:1786, lr:0.000932, total loss:0.000538\n",
            "[Train] epoch:1787, lr:0.000932, total loss:0.000517\n",
            "[Train] epoch:1788, lr:0.000932, total loss:0.000549\n",
            "[Train] epoch:1789, lr:0.000931, total loss:0.000508\n",
            "[Train] epoch:1790, lr:0.000931, total loss:0.000482\n",
            "[Train] epoch:1791, lr:0.000931, total loss:0.000485\n",
            "[Train] epoch:1792, lr:0.000931, total loss:0.000588\n",
            "[Train] epoch:1793, lr:0.000931, total loss:0.000552\n",
            "[Train] epoch:1794, lr:0.000931, total loss:0.000501\n",
            "[Train] epoch:1795, lr:0.000931, total loss:0.000460\n",
            "[Train] epoch:1796, lr:0.000931, total loss:0.000487\n",
            "[Train] epoch:1797, lr:0.000931, total loss:0.000500\n",
            "[Train] epoch:1798, lr:0.000931, total loss:0.000450\n",
            "[Train] epoch:1799, lr:0.000931, total loss:0.000481\n",
            "[Train] epoch:1800, lr:0.000931, total loss:0.000456\n",
            "[Train] epoch:1801, lr:0.000931, total loss:0.000510\n",
            "[Train] epoch:1802, lr:0.000930, total loss:0.000504\n",
            "[Train] epoch:1803, lr:0.000930, total loss:0.000442\n",
            "[Train] epoch:1804, lr:0.000930, total loss:0.000439\n",
            "[Train] epoch:1805, lr:0.000930, total loss:0.000518\n",
            "[Train] epoch:1806, lr:0.000930, total loss:0.000526\n",
            "[Train] epoch:1807, lr:0.000930, total loss:0.000571\n",
            "[Train] epoch:1808, lr:0.000930, total loss:0.000474\n",
            "[Train] epoch:1809, lr:0.000930, total loss:0.000445\n",
            "[Train] epoch:1810, lr:0.000930, total loss:0.000502\n",
            "[Train] epoch:1811, lr:0.000930, total loss:0.000577\n",
            "[Train] epoch:1812, lr:0.000930, total loss:0.000544\n",
            "[Train] epoch:1813, lr:0.000930, total loss:0.000551\n",
            "[Train] epoch:1814, lr:0.000930, total loss:0.000468\n",
            "[Train] epoch:1815, lr:0.000930, total loss:0.000461\n",
            "[Train] epoch:1816, lr:0.000929, total loss:0.000446\n",
            "[Train] epoch:1817, lr:0.000929, total loss:0.000510\n",
            "[Train] epoch:1818, lr:0.000929, total loss:0.000495\n",
            "[Train] epoch:1819, lr:0.000929, total loss:0.000494\n",
            "[Train] epoch:1820, lr:0.000929, total loss:0.000575\n",
            "[Train] epoch:1821, lr:0.000929, total loss:0.000521\n",
            "[Train] epoch:1822, lr:0.000929, total loss:0.000518\n",
            "[Train] epoch:1823, lr:0.000929, total loss:0.000496\n",
            "[Train] epoch:1824, lr:0.000929, total loss:0.000700\n",
            "[Train] epoch:1825, lr:0.000929, total loss:0.000657\n",
            "[Train] epoch:1826, lr:0.000929, total loss:0.000593\n",
            "[Train] epoch:1827, lr:0.000929, total loss:0.000558\n",
            "[Train] epoch:1828, lr:0.000929, total loss:0.000619\n",
            "[Train] epoch:1829, lr:0.000928, total loss:0.000567\n",
            "[Train] epoch:1830, lr:0.000928, total loss:0.000562\n",
            "[Train] epoch:1831, lr:0.000928, total loss:0.000592\n",
            "[Train] epoch:1832, lr:0.000928, total loss:0.000626\n",
            "[Train] epoch:1833, lr:0.000928, total loss:0.000505\n",
            "[Train] epoch:1834, lr:0.000928, total loss:0.000513\n",
            "[Train] epoch:1835, lr:0.000928, total loss:0.000470\n",
            "[Train] epoch:1836, lr:0.000928, total loss:0.000514\n",
            "[Train] epoch:1837, lr:0.000928, total loss:0.000574\n",
            "[Train] epoch:1838, lr:0.000928, total loss:0.000550\n",
            "[Train] epoch:1839, lr:0.000928, total loss:0.000549\n",
            "[Train] epoch:1840, lr:0.000928, total loss:0.000529\n",
            "[Train] epoch:1841, lr:0.000928, total loss:0.000563\n",
            "[Train] epoch:1842, lr:0.000927, total loss:0.000595\n",
            "[Train] epoch:1843, lr:0.000927, total loss:0.000641\n",
            "[Train] epoch:1844, lr:0.000927, total loss:0.000563\n",
            "[Train] epoch:1845, lr:0.000927, total loss:0.000532\n",
            "[Train] epoch:1846, lr:0.000927, total loss:0.000707\n",
            "[Train] epoch:1847, lr:0.000927, total loss:0.000772\n",
            "[Train] epoch:1848, lr:0.000927, total loss:0.000721\n",
            "[Train] epoch:1849, lr:0.000927, total loss:0.000555\n",
            "[Train] epoch:1850, lr:0.000927, total loss:0.000536\n",
            "[Train] epoch:1851, lr:0.000927, total loss:0.000586\n",
            "[Train] epoch:1852, lr:0.000927, total loss:0.000591\n",
            "[Train] epoch:1853, lr:0.000927, total loss:0.000607\n",
            "[Train] epoch:1854, lr:0.000927, total loss:0.000572\n",
            "[Train] epoch:1855, lr:0.000926, total loss:0.000516\n",
            "[Train] epoch:1856, lr:0.000926, total loss:0.000542\n",
            "[Train] epoch:1857, lr:0.000926, total loss:0.000541\n",
            "[Train] epoch:1858, lr:0.000926, total loss:0.000546\n",
            "[Train] epoch:1859, lr:0.000926, total loss:0.000510\n",
            "[Train] epoch:1860, lr:0.000926, total loss:0.000493\n",
            "[Train] epoch:1861, lr:0.000926, total loss:0.000492\n",
            "[Train] epoch:1862, lr:0.000926, total loss:0.000527\n",
            "[Train] epoch:1863, lr:0.000926, total loss:0.000603\n",
            "[Train] epoch:1864, lr:0.000926, total loss:0.000545\n",
            "[Train] epoch:1865, lr:0.000926, total loss:0.000474\n",
            "[Train] epoch:1866, lr:0.000926, total loss:0.000422\n",
            "[Train] epoch:1867, lr:0.000926, total loss:0.000507\n",
            "[Train] epoch:1868, lr:0.000925, total loss:0.000539\n",
            "[Train] epoch:1869, lr:0.000925, total loss:0.000483\n",
            "[Train] epoch:1870, lr:0.000925, total loss:0.000501\n",
            "[Train] epoch:1871, lr:0.000925, total loss:0.000495\n",
            "[Train] epoch:1872, lr:0.000925, total loss:0.000463\n",
            "[Train] epoch:1873, lr:0.000925, total loss:0.000400\n",
            "[Train] epoch:1874, lr:0.000925, total loss:0.000487\n",
            "[Train] epoch:1875, lr:0.000925, total loss:0.000493\n",
            "[Train] epoch:1876, lr:0.000925, total loss:0.000492\n",
            "[Train] epoch:1877, lr:0.000925, total loss:0.000471\n",
            "[Train] epoch:1878, lr:0.000925, total loss:0.000505\n",
            "[Train] epoch:1879, lr:0.000925, total loss:0.000460\n",
            "[Train] epoch:1880, lr:0.000924, total loss:0.000455\n",
            "[Train] epoch:1881, lr:0.000924, total loss:0.000521\n",
            "[Train] epoch:1882, lr:0.000924, total loss:0.000519\n",
            "[Train] epoch:1883, lr:0.000924, total loss:0.000560\n",
            "[Train] epoch:1884, lr:0.000924, total loss:0.000584\n",
            "[Train] epoch:1885, lr:0.000924, total loss:0.000560\n",
            "[Train] epoch:1886, lr:0.000924, total loss:0.000592\n",
            "[Train] epoch:1887, lr:0.000924, total loss:0.000547\n",
            "[Train] epoch:1888, lr:0.000924, total loss:0.000558\n",
            "[Train] epoch:1889, lr:0.000924, total loss:0.000549\n",
            "[Train] epoch:1890, lr:0.000924, total loss:0.000558\n",
            "[Train] epoch:1891, lr:0.000924, total loss:0.000489\n",
            "[Train] epoch:1892, lr:0.000924, total loss:0.000503\n",
            "[Train] epoch:1893, lr:0.000923, total loss:0.000508\n",
            "[Train] epoch:1894, lr:0.000923, total loss:0.000595\n",
            "[Train] epoch:1895, lr:0.000923, total loss:0.000474\n",
            "[Train] epoch:1896, lr:0.000923, total loss:0.000477\n",
            "[Train] epoch:1897, lr:0.000923, total loss:0.000469\n",
            "[Train] epoch:1898, lr:0.000923, total loss:0.000495\n",
            "[Train] epoch:1899, lr:0.000923, total loss:0.000471\n",
            "[Train] epoch:1900, lr:0.000923, total loss:0.000470\n",
            "[Train] epoch:1901, lr:0.000923, total loss:0.000533\n",
            "[Train] epoch:1902, lr:0.000923, total loss:0.000578\n",
            "[Train] epoch:1903, lr:0.000923, total loss:0.000492\n",
            "[Train] epoch:1904, lr:0.000923, total loss:0.000457\n",
            "[Train] epoch:1905, lr:0.000923, total loss:0.000458\n",
            "[Train] epoch:1906, lr:0.000922, total loss:0.000523\n",
            "[Train] epoch:1907, lr:0.000922, total loss:0.000437\n",
            "[Train] epoch:1908, lr:0.000922, total loss:0.000412\n",
            "[Train] epoch:1909, lr:0.000922, total loss:0.000448\n",
            "[Train] epoch:1910, lr:0.000922, total loss:0.000448\n",
            "[Train] epoch:1911, lr:0.000922, total loss:0.000471\n",
            "[Train] epoch:1912, lr:0.000922, total loss:0.000457\n",
            "[Train] epoch:1913, lr:0.000922, total loss:0.000423\n",
            "[Train] epoch:1914, lr:0.000922, total loss:0.000473\n",
            "[Train] epoch:1915, lr:0.000922, total loss:0.000548\n",
            "[Train] epoch:1916, lr:0.000922, total loss:0.000538\n",
            "[Train] epoch:1917, lr:0.000922, total loss:0.000537\n",
            "[Train] epoch:1918, lr:0.000921, total loss:0.000600\n",
            "[Train] epoch:1919, lr:0.000921, total loss:0.000490\n",
            "[Train] epoch:1920, lr:0.000921, total loss:0.000574\n",
            "[Train] epoch:1921, lr:0.000921, total loss:0.000530\n",
            "[Train] epoch:1922, lr:0.000921, total loss:0.000571\n",
            "[Train] epoch:1923, lr:0.000921, total loss:0.000610\n",
            "[Train] epoch:1924, lr:0.000921, total loss:0.000560\n",
            "[Train] epoch:1925, lr:0.000921, total loss:0.000501\n",
            "[Train] epoch:1926, lr:0.000921, total loss:0.000495\n",
            "[Train] epoch:1927, lr:0.000921, total loss:0.000483\n",
            "[Train] epoch:1928, lr:0.000921, total loss:0.000463\n",
            "[Train] epoch:1929, lr:0.000921, total loss:0.000494\n",
            "[Train] epoch:1930, lr:0.000921, total loss:0.000535\n",
            "[Train] epoch:1931, lr:0.000920, total loss:0.000442\n",
            "[Train] epoch:1932, lr:0.000920, total loss:0.000512\n",
            "[Train] epoch:1933, lr:0.000920, total loss:0.000441\n",
            "[Train] epoch:1934, lr:0.000920, total loss:0.000448\n",
            "[Train] epoch:1935, lr:0.000920, total loss:0.000440\n",
            "[Train] epoch:1936, lr:0.000920, total loss:0.000479\n",
            "[Train] epoch:1937, lr:0.000920, total loss:0.000445\n",
            "[Train] epoch:1938, lr:0.000920, total loss:0.000430\n",
            "[Train] epoch:1939, lr:0.000920, total loss:0.000461\n",
            "[Train] epoch:1940, lr:0.000920, total loss:0.000415\n",
            "[Train] epoch:1941, lr:0.000920, total loss:0.000420\n",
            "[Train] epoch:1942, lr:0.000920, total loss:0.000420\n",
            "[Train] epoch:1943, lr:0.000919, total loss:0.000406\n",
            "[Train] epoch:1944, lr:0.000919, total loss:0.000456\n",
            "[Train] epoch:1945, lr:0.000919, total loss:0.000401\n",
            "[Train] epoch:1946, lr:0.000919, total loss:0.000441\n",
            "[Train] epoch:1947, lr:0.000919, total loss:0.000426\n",
            "[Train] epoch:1948, lr:0.000919, total loss:0.000469\n",
            "[Train] epoch:1949, lr:0.000919, total loss:0.000574\n",
            "[Train] epoch:1950, lr:0.000919, total loss:0.000690\n",
            "[Train] epoch:1951, lr:0.000919, total loss:0.000589\n",
            "[Train] epoch:1952, lr:0.000919, total loss:0.000521\n",
            "[Train] epoch:1953, lr:0.000919, total loss:0.000463\n",
            "[Train] epoch:1954, lr:0.000919, total loss:0.000518\n",
            "[Train] epoch:1955, lr:0.000918, total loss:0.000566\n",
            "[Train] epoch:1956, lr:0.000918, total loss:0.000558\n",
            "[Train] epoch:1957, lr:0.000918, total loss:0.000560\n",
            "[Train] epoch:1958, lr:0.000918, total loss:0.000524\n",
            "[Train] epoch:1959, lr:0.000918, total loss:0.000479\n",
            "[Train] epoch:1960, lr:0.000918, total loss:0.000428\n",
            "[Train] epoch:1961, lr:0.000918, total loss:0.000470\n",
            "[Train] epoch:1962, lr:0.000918, total loss:0.000515\n",
            "[Train] epoch:1963, lr:0.000918, total loss:0.000508\n",
            "[Train] epoch:1964, lr:0.000918, total loss:0.000452\n",
            "[Train] epoch:1965, lr:0.000918, total loss:0.000436\n",
            "[Train] epoch:1966, lr:0.000918, total loss:0.000446\n",
            "[Train] epoch:1967, lr:0.000918, total loss:0.000469\n",
            "[Train] epoch:1968, lr:0.000917, total loss:0.000539\n",
            "[Train] epoch:1969, lr:0.000917, total loss:0.000541\n",
            "[Train] epoch:1970, lr:0.000917, total loss:0.000436\n",
            "[Train] epoch:1971, lr:0.000917, total loss:0.000494\n",
            "[Train] epoch:1972, lr:0.000917, total loss:0.000519\n",
            "[Train] epoch:1973, lr:0.000917, total loss:0.000458\n",
            "[Train] epoch:1974, lr:0.000917, total loss:0.000474\n",
            "[Train] epoch:1975, lr:0.000917, total loss:0.000497\n",
            "[Train] epoch:1976, lr:0.000917, total loss:0.000439\n",
            "[Train] epoch:1977, lr:0.000917, total loss:0.000428\n",
            "[Train] epoch:1978, lr:0.000917, total loss:0.000438\n",
            "[Train] epoch:1979, lr:0.000917, total loss:0.000452\n",
            "[Train] epoch:1980, lr:0.000916, total loss:0.000439\n",
            "[Train] epoch:1981, lr:0.000916, total loss:0.000433\n",
            "[Train] epoch:1982, lr:0.000916, total loss:0.000401\n",
            "[Train] epoch:1983, lr:0.000916, total loss:0.000608\n",
            "[Train] epoch:1984, lr:0.000916, total loss:0.000473\n",
            "[Train] epoch:1985, lr:0.000916, total loss:0.000455\n",
            "[Train] epoch:1986, lr:0.000916, total loss:0.000424\n",
            "[Train] epoch:1987, lr:0.000916, total loss:0.000518\n",
            "[Train] epoch:1988, lr:0.000916, total loss:0.000553\n",
            "[Train] epoch:1989, lr:0.000916, total loss:0.000672\n",
            "[Train] epoch:1990, lr:0.000916, total loss:0.000648\n",
            "[Train] epoch:1991, lr:0.000916, total loss:0.000588\n",
            "[Train] epoch:1992, lr:0.000915, total loss:0.000533\n",
            "[Train] epoch:1993, lr:0.000915, total loss:0.000481\n",
            "[Train] epoch:1994, lr:0.000915, total loss:0.000472\n",
            "[Train] epoch:1995, lr:0.000915, total loss:0.000501\n",
            "[Train] epoch:1996, lr:0.000915, total loss:0.000478\n",
            "[Train] epoch:1997, lr:0.000915, total loss:0.000572\n",
            "[Train] epoch:1998, lr:0.000915, total loss:0.000548\n",
            "[Train] epoch:1999, lr:0.000915, total loss:0.000560\n",
            "[Train] epoch:2000, lr:0.000915, total loss:0.000508\n",
            "[Train] epoch:2001, lr:0.000915, total loss:0.000626\n",
            "[Train] epoch:2002, lr:0.000915, total loss:0.000538\n",
            "[Train] epoch:2003, lr:0.000915, total loss:0.000392\n",
            "[Train] epoch:2004, lr:0.000914, total loss:0.000493\n",
            "[Train] epoch:2005, lr:0.000914, total loss:0.000465\n",
            "[Train] epoch:2006, lr:0.000914, total loss:0.000524\n",
            "[Train] epoch:2007, lr:0.000914, total loss:0.000476\n",
            "[Train] epoch:2008, lr:0.000914, total loss:0.000462\n",
            "[Train] epoch:2009, lr:0.000914, total loss:0.000446\n",
            "[Train] epoch:2010, lr:0.000914, total loss:0.000442\n",
            "[Train] epoch:2011, lr:0.000914, total loss:0.000477\n",
            "[Train] epoch:2012, lr:0.000914, total loss:0.000446\n",
            "[Train] epoch:2013, lr:0.000914, total loss:0.000408\n",
            "[Train] epoch:2014, lr:0.000914, total loss:0.000437\n",
            "[Train] epoch:2015, lr:0.000914, total loss:0.000462\n",
            "[Train] epoch:2016, lr:0.000913, total loss:0.000424\n",
            "[Train] epoch:2017, lr:0.000913, total loss:0.000477\n",
            "[Train] epoch:2018, lr:0.000913, total loss:0.000537\n",
            "[Train] epoch:2019, lr:0.000913, total loss:0.000554\n",
            "[Train] epoch:2020, lr:0.000913, total loss:0.000583\n",
            "[Train] epoch:2021, lr:0.000913, total loss:0.000727\n",
            "[Train] epoch:2022, lr:0.000913, total loss:0.000600\n",
            "[Train] epoch:2023, lr:0.000913, total loss:0.000600\n",
            "[Train] epoch:2024, lr:0.000913, total loss:0.000548\n",
            "[Train] epoch:2025, lr:0.000913, total loss:0.000534\n",
            "[Train] epoch:2026, lr:0.000913, total loss:0.000568\n",
            "[Train] epoch:2027, lr:0.000913, total loss:0.000466\n",
            "[Train] epoch:2028, lr:0.000912, total loss:0.000435\n",
            "[Train] epoch:2029, lr:0.000912, total loss:0.000474\n",
            "[Train] epoch:2030, lr:0.000912, total loss:0.000471\n",
            "[Train] epoch:2031, lr:0.000912, total loss:0.000518\n",
            "[Train] epoch:2032, lr:0.000912, total loss:0.000445\n",
            "[Train] epoch:2033, lr:0.000912, total loss:0.000406\n",
            "[Train] epoch:2034, lr:0.000912, total loss:0.000500\n",
            "[Train] epoch:2035, lr:0.000912, total loss:0.000401\n",
            "[Train] epoch:2036, lr:0.000912, total loss:0.000430\n",
            "[Train] epoch:2037, lr:0.000912, total loss:0.000500\n",
            "[Train] epoch:2038, lr:0.000912, total loss:0.000416\n",
            "[Train] epoch:2039, lr:0.000912, total loss:0.000449\n",
            "[Train] epoch:2040, lr:0.000911, total loss:0.000408\n",
            "[Train] epoch:2041, lr:0.000911, total loss:0.000394\n",
            "[Train] epoch:2042, lr:0.000911, total loss:0.000438\n",
            "[Train] epoch:2043, lr:0.000911, total loss:0.000461\n",
            "[Train] epoch:2044, lr:0.000911, total loss:0.000455\n",
            "[Train] epoch:2045, lr:0.000911, total loss:0.000510\n",
            "[Train] epoch:2046, lr:0.000911, total loss:0.000435\n",
            "[Train] epoch:2047, lr:0.000911, total loss:0.000468\n",
            "[Train] epoch:2048, lr:0.000911, total loss:0.000504\n",
            "[Train] epoch:2049, lr:0.000911, total loss:0.000418\n",
            "[Train] epoch:2050, lr:0.000911, total loss:0.000393\n",
            "[Train] epoch:2051, lr:0.000911, total loss:0.000426\n",
            "[Train] epoch:2052, lr:0.000910, total loss:0.000410\n",
            "[Train] epoch:2053, lr:0.000910, total loss:0.000367\n",
            "[Train] epoch:2054, lr:0.000910, total loss:0.000376\n",
            "[Train] epoch:2055, lr:0.000910, total loss:0.000531\n",
            "[Train] epoch:2056, lr:0.000910, total loss:0.000456\n",
            "[Train] epoch:2057, lr:0.000910, total loss:0.000431\n",
            "[Train] epoch:2058, lr:0.000910, total loss:0.000506\n",
            "[Train] epoch:2059, lr:0.000910, total loss:0.000470\n",
            "[Train] epoch:2060, lr:0.000910, total loss:0.000586\n",
            "[Train] epoch:2061, lr:0.000910, total loss:0.000454\n",
            "[Train] epoch:2062, lr:0.000910, total loss:0.000423\n",
            "[Train] epoch:2063, lr:0.000910, total loss:0.000462\n",
            "[Train] epoch:2064, lr:0.000909, total loss:0.000445\n",
            "[Train] epoch:2065, lr:0.000909, total loss:0.000423\n",
            "[Train] epoch:2066, lr:0.000909, total loss:0.000436\n",
            "[Train] epoch:2067, lr:0.000909, total loss:0.000447\n",
            "[Train] epoch:2068, lr:0.000909, total loss:0.000455\n",
            "[Train] epoch:2069, lr:0.000909, total loss:0.000521\n",
            "[Train] epoch:2070, lr:0.000909, total loss:0.000457\n",
            "[Train] epoch:2071, lr:0.000909, total loss:0.000479\n",
            "[Train] epoch:2072, lr:0.000909, total loss:0.000482\n",
            "[Train] epoch:2073, lr:0.000909, total loss:0.000462\n",
            "[Train] epoch:2074, lr:0.000909, total loss:0.000408\n",
            "[Train] epoch:2075, lr:0.000908, total loss:0.000398\n",
            "[Train] epoch:2076, lr:0.000908, total loss:0.000406\n",
            "[Train] epoch:2077, lr:0.000908, total loss:0.000387\n",
            "[Train] epoch:2078, lr:0.000908, total loss:0.000415\n",
            "[Train] epoch:2079, lr:0.000908, total loss:0.000443\n",
            "[Train] epoch:2080, lr:0.000908, total loss:0.000512\n",
            "[Train] epoch:2081, lr:0.000908, total loss:0.000434\n",
            "[Train] epoch:2082, lr:0.000908, total loss:0.000443\n",
            "[Train] epoch:2083, lr:0.000908, total loss:0.000497\n",
            "[Train] epoch:2084, lr:0.000908, total loss:0.000463\n",
            "[Train] epoch:2085, lr:0.000908, total loss:0.000485\n",
            "[Train] epoch:2086, lr:0.000908, total loss:0.000447\n",
            "[Train] epoch:2087, lr:0.000907, total loss:0.000461\n",
            "[Train] epoch:2088, lr:0.000907, total loss:0.000507\n",
            "[Train] epoch:2089, lr:0.000907, total loss:0.000527\n",
            "[Train] epoch:2090, lr:0.000907, total loss:0.000545\n",
            "[Train] epoch:2091, lr:0.000907, total loss:0.000585\n",
            "[Train] epoch:2092, lr:0.000907, total loss:0.000509\n",
            "[Train] epoch:2093, lr:0.000907, total loss:0.000456\n",
            "[Train] epoch:2094, lr:0.000907, total loss:0.000556\n",
            "[Train] epoch:2095, lr:0.000907, total loss:0.000602\n",
            "[Train] epoch:2096, lr:0.000907, total loss:0.000517\n",
            "[Train] epoch:2097, lr:0.000907, total loss:0.000425\n",
            "[Train] epoch:2098, lr:0.000907, total loss:0.000404\n",
            "[Train] epoch:2099, lr:0.000906, total loss:0.000439\n",
            "[Train] epoch:2100, lr:0.000906, total loss:0.000410\n",
            "[Train] epoch:2101, lr:0.000906, total loss:0.000405\n",
            "[Train] epoch:2102, lr:0.000906, total loss:0.000398\n",
            "[Train] epoch:2103, lr:0.000906, total loss:0.000429\n",
            "[Train] epoch:2104, lr:0.000906, total loss:0.000371\n",
            "[Train] epoch:2105, lr:0.000906, total loss:0.000462\n",
            "[Train] epoch:2106, lr:0.000906, total loss:0.000426\n",
            "[Train] epoch:2107, lr:0.000906, total loss:0.000450\n",
            "[Train] epoch:2108, lr:0.000906, total loss:0.000478\n",
            "[Train] epoch:2109, lr:0.000906, total loss:0.000420\n",
            "[Train] epoch:2110, lr:0.000905, total loss:0.000384\n",
            "[Train] epoch:2111, lr:0.000905, total loss:0.000408\n",
            "[Train] epoch:2112, lr:0.000905, total loss:0.000396\n",
            "[Train] epoch:2113, lr:0.000905, total loss:0.000420\n",
            "[Train] epoch:2114, lr:0.000905, total loss:0.000377\n",
            "[Train] epoch:2115, lr:0.000905, total loss:0.000430\n",
            "[Train] epoch:2116, lr:0.000905, total loss:0.000416\n",
            "[Train] epoch:2117, lr:0.000905, total loss:0.000426\n",
            "[Train] epoch:2118, lr:0.000905, total loss:0.000409\n",
            "[Train] epoch:2119, lr:0.000905, total loss:0.000387\n",
            "[Train] epoch:2120, lr:0.000905, total loss:0.000402\n",
            "[Train] epoch:2121, lr:0.000905, total loss:0.000431\n",
            "[Train] epoch:2122, lr:0.000904, total loss:0.000390\n",
            "[Train] epoch:2123, lr:0.000904, total loss:0.000401\n",
            "[Train] epoch:2124, lr:0.000904, total loss:0.000439\n",
            "[Train] epoch:2125, lr:0.000904, total loss:0.000453\n",
            "[Train] epoch:2126, lr:0.000904, total loss:0.000508\n",
            "[Train] epoch:2127, lr:0.000904, total loss:0.000457\n",
            "[Train] epoch:2128, lr:0.000904, total loss:0.000419\n",
            "[Train] epoch:2129, lr:0.000904, total loss:0.000424\n",
            "[Train] epoch:2130, lr:0.000904, total loss:0.000447\n",
            "[Train] epoch:2131, lr:0.000904, total loss:0.000419\n",
            "[Train] epoch:2132, lr:0.000904, total loss:0.000481\n",
            "[Train] epoch:2133, lr:0.000903, total loss:0.000467\n",
            "[Train] epoch:2134, lr:0.000903, total loss:0.000472\n",
            "[Train] epoch:2135, lr:0.000903, total loss:0.000413\n",
            "[Train] epoch:2136, lr:0.000903, total loss:0.000406\n",
            "[Train] epoch:2137, lr:0.000903, total loss:0.000459\n",
            "[Train] epoch:2138, lr:0.000903, total loss:0.000413\n",
            "[Train] epoch:2139, lr:0.000903, total loss:0.000447\n",
            "[Train] epoch:2140, lr:0.000903, total loss:0.000431\n",
            "[Train] epoch:2141, lr:0.000903, total loss:0.000490\n",
            "[Train] epoch:2142, lr:0.000903, total loss:0.000435\n",
            "[Train] epoch:2143, lr:0.000903, total loss:0.000429\n",
            "[Train] epoch:2144, lr:0.000903, total loss:0.000425\n",
            "[Train] epoch:2145, lr:0.000902, total loss:0.000426\n",
            "[Train] epoch:2146, lr:0.000902, total loss:0.000424\n",
            "[Train] epoch:2147, lr:0.000902, total loss:0.000436\n",
            "[Train] epoch:2148, lr:0.000902, total loss:0.000429\n",
            "[Train] epoch:2149, lr:0.000902, total loss:0.000380\n",
            "[Train] epoch:2150, lr:0.000902, total loss:0.000462\n",
            "[Train] epoch:2151, lr:0.000902, total loss:0.000488\n",
            "[Train] epoch:2152, lr:0.000902, total loss:0.000472\n",
            "[Train] epoch:2153, lr:0.000902, total loss:0.000480\n",
            "[Train] epoch:2154, lr:0.000902, total loss:0.000490\n",
            "[Train] epoch:2155, lr:0.000902, total loss:0.000439\n",
            "[Train] epoch:2156, lr:0.000901, total loss:0.000487\n",
            "[Train] epoch:2157, lr:0.000901, total loss:0.000421\n",
            "[Train] epoch:2158, lr:0.000901, total loss:0.000430\n",
            "[Train] epoch:2159, lr:0.000901, total loss:0.000439\n",
            "[Train] epoch:2160, lr:0.000901, total loss:0.000580\n",
            "[Train] epoch:2161, lr:0.000901, total loss:0.000430\n",
            "[Train] epoch:2162, lr:0.000901, total loss:0.000498\n",
            "[Train] epoch:2163, lr:0.000901, total loss:0.000478\n",
            "[Train] epoch:2164, lr:0.000901, total loss:0.000401\n",
            "[Train] epoch:2165, lr:0.000901, total loss:0.000442\n",
            "[Train] epoch:2166, lr:0.000901, total loss:0.000486\n",
            "[Train] epoch:2167, lr:0.000900, total loss:0.000473\n",
            "[Train] epoch:2168, lr:0.000900, total loss:0.000401\n",
            "[Train] epoch:2169, lr:0.000900, total loss:0.000484\n",
            "[Train] epoch:2170, lr:0.000900, total loss:0.000505\n",
            "[Train] epoch:2171, lr:0.000900, total loss:0.000544\n",
            "[Train] epoch:2172, lr:0.000900, total loss:0.000568\n",
            "[Train] epoch:2173, lr:0.000900, total loss:0.000503\n",
            "[Train] epoch:2174, lr:0.000900, total loss:0.000528\n",
            "[Train] epoch:2175, lr:0.000900, total loss:0.000513\n",
            "[Train] epoch:2176, lr:0.000900, total loss:0.000440\n",
            "[Train] epoch:2177, lr:0.000900, total loss:0.000492\n",
            "[Train] epoch:2178, lr:0.000899, total loss:0.000414\n",
            "[Train] epoch:2179, lr:0.000899, total loss:0.000437\n",
            "[Train] epoch:2180, lr:0.000899, total loss:0.000459\n",
            "[Train] epoch:2181, lr:0.000899, total loss:0.000398\n",
            "[Train] epoch:2182, lr:0.000899, total loss:0.000453\n",
            "[Train] epoch:2183, lr:0.000899, total loss:0.000435\n",
            "[Train] epoch:2184, lr:0.000899, total loss:0.000424\n",
            "[Train] epoch:2185, lr:0.000899, total loss:0.000433\n",
            "[Train] epoch:2186, lr:0.000899, total loss:0.000596\n",
            "[Train] epoch:2187, lr:0.000899, total loss:0.000456\n",
            "[Train] epoch:2188, lr:0.000899, total loss:0.000463\n",
            "[Train] epoch:2189, lr:0.000899, total loss:0.000485\n",
            "[Train] epoch:2190, lr:0.000898, total loss:0.000526\n",
            "[Train] epoch:2191, lr:0.000898, total loss:0.000462\n",
            "[Train] epoch:2192, lr:0.000898, total loss:0.000466\n",
            "[Train] epoch:2193, lr:0.000898, total loss:0.000497\n",
            "[Train] epoch:2194, lr:0.000898, total loss:0.000420\n",
            "[Train] epoch:2195, lr:0.000898, total loss:0.000407\n",
            "[Train] epoch:2196, lr:0.000898, total loss:0.000540\n",
            "[Train] epoch:2197, lr:0.000898, total loss:0.000426\n",
            "[Train] epoch:2198, lr:0.000898, total loss:0.000511\n",
            "[Train] epoch:2199, lr:0.000898, total loss:0.000404\n",
            "[Train] epoch:2200, lr:0.000898, total loss:0.000401\n",
            "[Train] epoch:2201, lr:0.000897, total loss:0.000397\n",
            "[Train] epoch:2202, lr:0.000897, total loss:0.000477\n",
            "[Train] epoch:2203, lr:0.000897, total loss:0.000424\n",
            "[Train] epoch:2204, lr:0.000897, total loss:0.000498\n",
            "[Train] epoch:2205, lr:0.000897, total loss:0.000455\n",
            "[Train] epoch:2206, lr:0.000897, total loss:0.000443\n",
            "[Train] epoch:2207, lr:0.000897, total loss:0.000452\n",
            "[Train] epoch:2208, lr:0.000897, total loss:0.000457\n",
            "[Train] epoch:2209, lr:0.000897, total loss:0.000430\n",
            "[Train] epoch:2210, lr:0.000897, total loss:0.000458\n",
            "[Train] epoch:2211, lr:0.000897, total loss:0.000421\n",
            "[Train] epoch:2212, lr:0.000896, total loss:0.000403\n",
            "[Train] epoch:2213, lr:0.000896, total loss:0.000417\n",
            "[Train] epoch:2214, lr:0.000896, total loss:0.000503\n",
            "[Train] epoch:2215, lr:0.000896, total loss:0.000506\n",
            "[Train] epoch:2216, lr:0.000896, total loss:0.000445\n",
            "[Train] epoch:2217, lr:0.000896, total loss:0.000406\n",
            "[Train] epoch:2218, lr:0.000896, total loss:0.000406\n",
            "[Train] epoch:2219, lr:0.000896, total loss:0.000431\n",
            "[Train] epoch:2220, lr:0.000896, total loss:0.000446\n",
            "[Train] epoch:2221, lr:0.000896, total loss:0.000514\n",
            "[Train] epoch:2222, lr:0.000896, total loss:0.000561\n",
            "[Train] epoch:2223, lr:0.000895, total loss:0.000608\n",
            "[Train] epoch:2224, lr:0.000895, total loss:0.000505\n",
            "[Train] epoch:2225, lr:0.000895, total loss:0.000483\n",
            "[Train] epoch:2226, lr:0.000895, total loss:0.000532\n",
            "[Train] epoch:2227, lr:0.000895, total loss:0.000531\n",
            "[Train] epoch:2228, lr:0.000895, total loss:0.000427\n",
            "[Train] epoch:2229, lr:0.000895, total loss:0.000418\n",
            "[Train] epoch:2230, lr:0.000895, total loss:0.000448\n",
            "[Train] epoch:2231, lr:0.000895, total loss:0.000456\n",
            "[Train] epoch:2232, lr:0.000895, total loss:0.000407\n",
            "[Train] epoch:2233, lr:0.000895, total loss:0.000416\n",
            "[Train] epoch:2234, lr:0.000894, total loss:0.000435\n",
            "[Train] epoch:2235, lr:0.000894, total loss:0.000430\n",
            "[Train] epoch:2236, lr:0.000894, total loss:0.000448\n",
            "[Train] epoch:2237, lr:0.000894, total loss:0.000466\n",
            "[Train] epoch:2238, lr:0.000894, total loss:0.000464\n",
            "[Train] epoch:2239, lr:0.000894, total loss:0.000672\n",
            "[Train] epoch:2240, lr:0.000894, total loss:0.000519\n",
            "[Train] epoch:2241, lr:0.000894, total loss:0.000541\n",
            "[Train] epoch:2242, lr:0.000894, total loss:0.000519\n",
            "[Train] epoch:2243, lr:0.000894, total loss:0.000495\n",
            "[Train] epoch:2244, lr:0.000894, total loss:0.000456\n",
            "[Train] epoch:2245, lr:0.000893, total loss:0.000393\n",
            "[Train] epoch:2246, lr:0.000893, total loss:0.000399\n",
            "[Train] epoch:2247, lr:0.000893, total loss:0.000410\n",
            "[Train] epoch:2248, lr:0.000893, total loss:0.000657\n",
            "[Train] epoch:2249, lr:0.000893, total loss:0.000472\n",
            "[Train] epoch:2250, lr:0.000893, total loss:0.000500\n",
            "[Train] epoch:2251, lr:0.000893, total loss:0.000458\n",
            "[Train] epoch:2252, lr:0.000893, total loss:0.000442\n",
            "[Train] epoch:2253, lr:0.000893, total loss:0.000456\n",
            "[Train] epoch:2254, lr:0.000893, total loss:0.000694\n",
            "[Train] epoch:2255, lr:0.000893, total loss:0.001032\n",
            "[Train] epoch:2256, lr:0.000892, total loss:0.000963\n",
            "[Train] epoch:2257, lr:0.000892, total loss:0.001040\n",
            "[Train] epoch:2258, lr:0.000892, total loss:0.000866\n",
            "[Train] epoch:2259, lr:0.000892, total loss:0.000754\n",
            "[Train] epoch:2260, lr:0.000892, total loss:0.000641\n",
            "[Train] epoch:2261, lr:0.000892, total loss:0.000704\n",
            "[Train] epoch:2262, lr:0.000892, total loss:0.000674\n",
            "[Train] epoch:2263, lr:0.000892, total loss:0.000682\n",
            "[Train] epoch:2264, lr:0.000892, total loss:0.000634\n",
            "[Train] epoch:2265, lr:0.000892, total loss:0.000592\n",
            "[Train] epoch:2266, lr:0.000892, total loss:0.000576\n",
            "[Train] epoch:2267, lr:0.000891, total loss:0.000488\n",
            "[Train] epoch:2268, lr:0.000891, total loss:0.000547\n",
            "[Train] epoch:2269, lr:0.000891, total loss:0.000508\n",
            "[Train] epoch:2270, lr:0.000891, total loss:0.000484\n",
            "[Train] epoch:2271, lr:0.000891, total loss:0.000431\n",
            "[Train] epoch:2272, lr:0.000891, total loss:0.000457\n",
            "[Train] epoch:2273, lr:0.000891, total loss:0.000460\n",
            "[Train] epoch:2274, lr:0.000891, total loss:0.000432\n",
            "[Train] epoch:2275, lr:0.000891, total loss:0.000442\n",
            "[Train] epoch:2276, lr:0.000891, total loss:0.000446\n",
            "[Train] epoch:2277, lr:0.000890, total loss:0.000479\n",
            "[Train] epoch:2278, lr:0.000890, total loss:0.000468\n",
            "[Train] epoch:2279, lr:0.000890, total loss:0.000452\n",
            "[Train] epoch:2280, lr:0.000890, total loss:0.000445\n",
            "[Train] epoch:2281, lr:0.000890, total loss:0.000471\n",
            "[Train] epoch:2282, lr:0.000890, total loss:0.000413\n",
            "[Train] epoch:2283, lr:0.000890, total loss:0.000506\n",
            "[Train] epoch:2284, lr:0.000890, total loss:0.000496\n",
            "[Train] epoch:2285, lr:0.000890, total loss:0.000473\n",
            "[Train] epoch:2286, lr:0.000890, total loss:0.000453\n",
            "[Train] epoch:2287, lr:0.000890, total loss:0.000421\n",
            "[Train] epoch:2288, lr:0.000889, total loss:0.000465\n",
            "[Train] epoch:2289, lr:0.000889, total loss:0.000402\n",
            "[Train] epoch:2290, lr:0.000889, total loss:0.000440\n",
            "[Train] epoch:2291, lr:0.000889, total loss:0.000449\n",
            "[Train] epoch:2292, lr:0.000889, total loss:0.000487\n",
            "[Train] epoch:2293, lr:0.000889, total loss:0.000522\n",
            "[Train] epoch:2294, lr:0.000889, total loss:0.000425\n",
            "[Train] epoch:2295, lr:0.000889, total loss:0.000459\n",
            "[Train] epoch:2296, lr:0.000889, total loss:0.000564\n",
            "[Train] epoch:2297, lr:0.000889, total loss:0.000470\n",
            "[Train] epoch:2298, lr:0.000889, total loss:0.000422\n",
            "[Train] epoch:2299, lr:0.000888, total loss:0.000415\n",
            "[Train] epoch:2300, lr:0.000888, total loss:0.000414\n",
            "[Train] epoch:2301, lr:0.000888, total loss:0.000396\n",
            "[Train] epoch:2302, lr:0.000888, total loss:0.000430\n",
            "[Train] epoch:2303, lr:0.000888, total loss:0.000421\n",
            "[Train] epoch:2304, lr:0.000888, total loss:0.000450\n",
            "[Train] epoch:2305, lr:0.000888, total loss:0.000390\n",
            "[Train] epoch:2306, lr:0.000888, total loss:0.000384\n",
            "[Train] epoch:2307, lr:0.000888, total loss:0.000410\n",
            "[Train] epoch:2308, lr:0.000888, total loss:0.000410\n",
            "[Train] epoch:2309, lr:0.000888, total loss:0.000422\n",
            "[Train] epoch:2310, lr:0.000887, total loss:0.000407\n",
            "[Train] epoch:2311, lr:0.000887, total loss:0.000424\n",
            "[Train] epoch:2312, lr:0.000887, total loss:0.000394\n",
            "[Train] epoch:2313, lr:0.000887, total loss:0.000393\n",
            "[Train] epoch:2314, lr:0.000887, total loss:0.000405\n",
            "[Train] epoch:2315, lr:0.000887, total loss:0.000413\n",
            "[Train] epoch:2316, lr:0.000887, total loss:0.000386\n",
            "[Train] epoch:2317, lr:0.000887, total loss:0.000385\n",
            "[Train] epoch:2318, lr:0.000887, total loss:0.000360\n",
            "[Train] epoch:2319, lr:0.000887, total loss:0.000423\n",
            "[Train] epoch:2320, lr:0.000886, total loss:0.000389\n",
            "[Train] epoch:2321, lr:0.000886, total loss:0.000469\n",
            "[Train] epoch:2322, lr:0.000886, total loss:0.000440\n",
            "[Train] epoch:2323, lr:0.000886, total loss:0.000517\n",
            "[Train] epoch:2324, lr:0.000886, total loss:0.000467\n",
            "[Train] epoch:2325, lr:0.000886, total loss:0.000478\n",
            "[Train] epoch:2326, lr:0.000886, total loss:0.000429\n",
            "[Train] epoch:2327, lr:0.000886, total loss:0.000445\n",
            "[Train] epoch:2328, lr:0.000886, total loss:0.000415\n",
            "[Train] epoch:2329, lr:0.000886, total loss:0.000378\n",
            "[Train] epoch:2330, lr:0.000886, total loss:0.000418\n",
            "[Train] epoch:2331, lr:0.000885, total loss:0.000392\n",
            "[Train] epoch:2332, lr:0.000885, total loss:0.000446\n",
            "[Train] epoch:2333, lr:0.000885, total loss:0.000409\n",
            "[Train] epoch:2334, lr:0.000885, total loss:0.000410\n",
            "[Train] epoch:2335, lr:0.000885, total loss:0.000454\n",
            "[Train] epoch:2336, lr:0.000885, total loss:0.000457\n",
            "[Train] epoch:2337, lr:0.000885, total loss:0.000467\n",
            "[Train] epoch:2338, lr:0.000885, total loss:0.000432\n",
            "[Train] epoch:2339, lr:0.000885, total loss:0.000493\n",
            "[Train] epoch:2340, lr:0.000885, total loss:0.000456\n",
            "[Train] epoch:2341, lr:0.000885, total loss:0.000553\n",
            "[Train] epoch:2342, lr:0.000884, total loss:0.000453\n",
            "[Train] epoch:2343, lr:0.000884, total loss:0.000390\n",
            "[Train] epoch:2344, lr:0.000884, total loss:0.000456\n",
            "[Train] epoch:2345, lr:0.000884, total loss:0.000425\n",
            "[Train] epoch:2346, lr:0.000884, total loss:0.000407\n",
            "[Train] epoch:2347, lr:0.000884, total loss:0.000399\n",
            "[Train] epoch:2348, lr:0.000884, total loss:0.000424\n",
            "[Train] epoch:2349, lr:0.000884, total loss:0.000397\n",
            "[Train] epoch:2350, lr:0.000884, total loss:0.000471\n",
            "[Train] epoch:2351, lr:0.000884, total loss:0.000497\n",
            "[Train] epoch:2352, lr:0.000883, total loss:0.000466\n",
            "[Train] epoch:2353, lr:0.000883, total loss:0.000429\n",
            "[Train] epoch:2354, lr:0.000883, total loss:0.000437\n",
            "[Train] epoch:2355, lr:0.000883, total loss:0.000399\n",
            "[Train] epoch:2356, lr:0.000883, total loss:0.000492\n",
            "[Train] epoch:2357, lr:0.000883, total loss:0.000450\n",
            "[Train] epoch:2358, lr:0.000883, total loss:0.000575\n",
            "[Train] epoch:2359, lr:0.000883, total loss:0.000535\n",
            "[Train] epoch:2360, lr:0.000883, total loss:0.000463\n",
            "[Train] epoch:2361, lr:0.000883, total loss:0.000475\n",
            "[Train] epoch:2362, lr:0.000883, total loss:0.000463\n",
            "[Train] epoch:2363, lr:0.000882, total loss:0.000574\n",
            "[Train] epoch:2364, lr:0.000882, total loss:0.000601\n",
            "[Train] epoch:2365, lr:0.000882, total loss:0.000513\n",
            "[Train] epoch:2366, lr:0.000882, total loss:0.000547\n",
            "[Train] epoch:2367, lr:0.000882, total loss:0.000470\n",
            "[Train] epoch:2368, lr:0.000882, total loss:0.000582\n",
            "[Train] epoch:2369, lr:0.000882, total loss:0.000709\n",
            "[Train] epoch:2370, lr:0.000882, total loss:0.000531\n",
            "[Train] epoch:2371, lr:0.000882, total loss:0.000455\n",
            "[Train] epoch:2372, lr:0.000882, total loss:0.000475\n",
            "[Train] epoch:2373, lr:0.000881, total loss:0.000475\n",
            "[Train] epoch:2374, lr:0.000881, total loss:0.000412\n",
            "[Train] epoch:2375, lr:0.000881, total loss:0.000384\n",
            "[Train] epoch:2376, lr:0.000881, total loss:0.000441\n",
            "[Train] epoch:2377, lr:0.000881, total loss:0.000470\n",
            "[Train] epoch:2378, lr:0.000881, total loss:0.000480\n",
            "[Train] epoch:2379, lr:0.000881, total loss:0.000402\n",
            "[Train] epoch:2380, lr:0.000881, total loss:0.000420\n",
            "[Train] epoch:2381, lr:0.000881, total loss:0.000377\n",
            "[Train] epoch:2382, lr:0.000881, total loss:0.000383\n",
            "[Train] epoch:2383, lr:0.000881, total loss:0.000377\n",
            "[Train] epoch:2384, lr:0.000880, total loss:0.000353\n",
            "[Train] epoch:2385, lr:0.000880, total loss:0.000384\n",
            "[Train] epoch:2386, lr:0.000880, total loss:0.000384\n",
            "[Train] epoch:2387, lr:0.000880, total loss:0.000374\n",
            "[Train] epoch:2388, lr:0.000880, total loss:0.000369\n",
            "[Train] epoch:2389, lr:0.000880, total loss:0.000404\n",
            "[Train] epoch:2390, lr:0.000880, total loss:0.000418\n",
            "[Train] epoch:2391, lr:0.000880, total loss:0.000448\n",
            "[Train] epoch:2392, lr:0.000880, total loss:0.000420\n",
            "[Train] epoch:2393, lr:0.000880, total loss:0.000455\n",
            "[Train] epoch:2394, lr:0.000879, total loss:0.000527\n",
            "[Train] epoch:2395, lr:0.000879, total loss:0.000525\n",
            "[Train] epoch:2396, lr:0.000879, total loss:0.000455\n",
            "[Train] epoch:2397, lr:0.000879, total loss:0.000506\n",
            "[Train] epoch:2398, lr:0.000879, total loss:0.000531\n",
            "[Train] epoch:2399, lr:0.000879, total loss:0.000514\n",
            "[Train] epoch:2400, lr:0.000879, total loss:0.000565\n",
            "[Train] epoch:2401, lr:0.000879, total loss:0.000593\n",
            "[Train] epoch:2402, lr:0.000879, total loss:0.000564\n",
            "[Train] epoch:2403, lr:0.000879, total loss:0.000585\n",
            "[Train] epoch:2404, lr:0.000878, total loss:0.000568\n",
            "[Train] epoch:2405, lr:0.000878, total loss:0.000556\n",
            "[Train] epoch:2406, lr:0.000878, total loss:0.000518\n",
            "[Train] epoch:2407, lr:0.000878, total loss:0.000470\n",
            "[Train] epoch:2408, lr:0.000878, total loss:0.000416\n",
            "[Train] epoch:2409, lr:0.000878, total loss:0.000419\n",
            "[Train] epoch:2410, lr:0.000878, total loss:0.000490\n",
            "[Train] epoch:2411, lr:0.000878, total loss:0.000415\n",
            "[Train] epoch:2412, lr:0.000878, total loss:0.000462\n",
            "[Train] epoch:2413, lr:0.000878, total loss:0.000359\n",
            "[Train] epoch:2414, lr:0.000878, total loss:0.000398\n",
            "[Train] epoch:2415, lr:0.000877, total loss:0.000436\n",
            "[Train] epoch:2416, lr:0.000877, total loss:0.000394\n",
            "[Train] epoch:2417, lr:0.000877, total loss:0.000351\n",
            "[Train] epoch:2418, lr:0.000877, total loss:0.000362\n",
            "[Train] epoch:2419, lr:0.000877, total loss:0.000407\n",
            "[Train] epoch:2420, lr:0.000877, total loss:0.000365\n",
            "[Train] epoch:2421, lr:0.000877, total loss:0.000400\n",
            "[Train] epoch:2422, lr:0.000877, total loss:0.000471\n",
            "[Train] epoch:2423, lr:0.000877, total loss:0.000455\n",
            "[Train] epoch:2424, lr:0.000877, total loss:0.000402\n",
            "[Train] epoch:2425, lr:0.000876, total loss:0.000380\n",
            "[Train] epoch:2426, lr:0.000876, total loss:0.000401\n",
            "[Train] epoch:2427, lr:0.000876, total loss:0.000405\n",
            "[Train] epoch:2428, lr:0.000876, total loss:0.000404\n",
            "[Train] epoch:2429, lr:0.000876, total loss:0.000372\n",
            "[Train] epoch:2430, lr:0.000876, total loss:0.000403\n",
            "[Train] epoch:2431, lr:0.000876, total loss:0.000368\n",
            "[Train] epoch:2432, lr:0.000876, total loss:0.000416\n",
            "[Train] epoch:2433, lr:0.000876, total loss:0.000453\n",
            "[Train] epoch:2434, lr:0.000876, total loss:0.000446\n",
            "[Train] epoch:2435, lr:0.000875, total loss:0.000403\n",
            "[Train] epoch:2436, lr:0.000875, total loss:0.000435\n",
            "[Train] epoch:2437, lr:0.000875, total loss:0.000451\n",
            "[Train] epoch:2438, lr:0.000875, total loss:0.000414\n",
            "[Train] epoch:2439, lr:0.000875, total loss:0.000422\n",
            "[Train] epoch:2440, lr:0.000875, total loss:0.000423\n",
            "[Train] epoch:2441, lr:0.000875, total loss:0.000428\n",
            "[Train] epoch:2442, lr:0.000875, total loss:0.000372\n",
            "[Train] epoch:2443, lr:0.000875, total loss:0.000365\n",
            "[Train] epoch:2444, lr:0.000875, total loss:0.000393\n",
            "[Train] epoch:2445, lr:0.000874, total loss:0.000394\n",
            "[Train] epoch:2446, lr:0.000874, total loss:0.000384\n",
            "[Train] epoch:2447, lr:0.000874, total loss:0.000384\n",
            "[Train] epoch:2448, lr:0.000874, total loss:0.000388\n",
            "[Train] epoch:2449, lr:0.000874, total loss:0.000597\n",
            "[Train] epoch:2450, lr:0.000874, total loss:0.000498\n",
            "[Train] epoch:2451, lr:0.000874, total loss:0.000465\n",
            "[Train] epoch:2452, lr:0.000874, total loss:0.000416\n",
            "[Train] epoch:2453, lr:0.000874, total loss:0.000413\n",
            "[Train] epoch:2454, lr:0.000874, total loss:0.000381\n",
            "[Train] epoch:2455, lr:0.000874, total loss:0.000338\n",
            "[Train] epoch:2456, lr:0.000873, total loss:0.000367\n",
            "[Train] epoch:2457, lr:0.000873, total loss:0.000381\n",
            "[Train] epoch:2458, lr:0.000873, total loss:0.000406\n",
            "[Train] epoch:2459, lr:0.000873, total loss:0.000402\n",
            "[Train] epoch:2460, lr:0.000873, total loss:0.000391\n",
            "[Train] epoch:2461, lr:0.000873, total loss:0.000405\n",
            "[Train] epoch:2462, lr:0.000873, total loss:0.000409\n",
            "[Train] epoch:2463, lr:0.000873, total loss:0.000402\n",
            "[Train] epoch:2464, lr:0.000873, total loss:0.000415\n",
            "[Train] epoch:2465, lr:0.000873, total loss:0.000384\n",
            "[Train] epoch:2466, lr:0.000872, total loss:0.000411\n",
            "[Train] epoch:2467, lr:0.000872, total loss:0.000391\n",
            "[Train] epoch:2468, lr:0.000872, total loss:0.000494\n",
            "[Train] epoch:2469, lr:0.000872, total loss:0.000528\n",
            "[Train] epoch:2470, lr:0.000872, total loss:0.000469\n",
            "[Train] epoch:2471, lr:0.000872, total loss:0.000421\n",
            "[Train] epoch:2472, lr:0.000872, total loss:0.000388\n",
            "[Train] epoch:2473, lr:0.000872, total loss:0.000367\n",
            "[Train] epoch:2474, lr:0.000872, total loss:0.000377\n",
            "[Train] epoch:2475, lr:0.000872, total loss:0.000414\n",
            "[Train] epoch:2476, lr:0.000871, total loss:0.000363\n",
            "[Train] epoch:2477, lr:0.000871, total loss:0.000472\n",
            "[Train] epoch:2478, lr:0.000871, total loss:0.000403\n",
            "[Train] epoch:2479, lr:0.000871, total loss:0.000437\n",
            "[Train] epoch:2480, lr:0.000871, total loss:0.000419\n",
            "[Train] epoch:2481, lr:0.000871, total loss:0.000361\n",
            "[Train] epoch:2482, lr:0.000871, total loss:0.000435\n",
            "[Train] epoch:2483, lr:0.000871, total loss:0.000389\n",
            "[Train] epoch:2484, lr:0.000871, total loss:0.000389\n",
            "[Train] epoch:2485, lr:0.000871, total loss:0.000375\n",
            "[Train] epoch:2486, lr:0.000870, total loss:0.000379\n",
            "[Train] epoch:2487, lr:0.000870, total loss:0.000370\n",
            "[Train] epoch:2488, lr:0.000870, total loss:0.000390\n",
            "[Train] epoch:2489, lr:0.000870, total loss:0.000372\n",
            "[Train] epoch:2490, lr:0.000870, total loss:0.000417\n",
            "[Train] epoch:2491, lr:0.000870, total loss:0.000449\n",
            "[Train] epoch:2492, lr:0.000870, total loss:0.000392\n",
            "[Train] epoch:2493, lr:0.000870, total loss:0.000399\n",
            "[Train] epoch:2494, lr:0.000870, total loss:0.000411\n",
            "[Train] epoch:2495, lr:0.000870, total loss:0.000422\n",
            "[Train] epoch:2496, lr:0.000869, total loss:0.000388\n",
            "[Train] epoch:2497, lr:0.000869, total loss:0.000409\n",
            "[Train] epoch:2498, lr:0.000869, total loss:0.000451\n",
            "[Train] epoch:2499, lr:0.000869, total loss:0.000447\n",
            "[Train] epoch:2500, lr:0.000869, total loss:0.000415\n",
            "[Train] epoch:2501, lr:0.000869, total loss:0.000370\n",
            "[Train] epoch:2502, lr:0.000869, total loss:0.000416\n",
            "[Train] epoch:2503, lr:0.000869, total loss:0.000373\n",
            "[Train] epoch:2504, lr:0.000869, total loss:0.000379\n",
            "[Train] epoch:2505, lr:0.000869, total loss:0.000355\n",
            "[Train] epoch:2506, lr:0.000868, total loss:0.000356\n",
            "[Train] epoch:2507, lr:0.000868, total loss:0.000387\n",
            "[Train] epoch:2508, lr:0.000868, total loss:0.000344\n",
            "[Train] epoch:2509, lr:0.000868, total loss:0.000480\n",
            "[Train] epoch:2510, lr:0.000868, total loss:0.000473\n",
            "[Train] epoch:2511, lr:0.000868, total loss:0.000481\n",
            "[Train] epoch:2512, lr:0.000868, total loss:0.000489\n",
            "[Train] epoch:2513, lr:0.000868, total loss:0.000495\n",
            "[Train] epoch:2514, lr:0.000868, total loss:0.000490\n",
            "[Train] epoch:2515, lr:0.000868, total loss:0.000394\n",
            "[Train] epoch:2516, lr:0.000867, total loss:0.000385\n",
            "[Train] epoch:2517, lr:0.000867, total loss:0.000373\n",
            "[Train] epoch:2518, lr:0.000867, total loss:0.000395\n",
            "[Train] epoch:2519, lr:0.000867, total loss:0.000448\n",
            "[Train] epoch:2520, lr:0.000867, total loss:0.000431\n",
            "[Train] epoch:2521, lr:0.000867, total loss:0.000384\n",
            "[Train] epoch:2522, lr:0.000867, total loss:0.000356\n",
            "[Train] epoch:2523, lr:0.000867, total loss:0.000378\n",
            "[Train] epoch:2524, lr:0.000867, total loss:0.000414\n",
            "[Train] epoch:2525, lr:0.000867, total loss:0.000374\n",
            "[Train] epoch:2526, lr:0.000866, total loss:0.000381\n",
            "[Train] epoch:2527, lr:0.000866, total loss:0.000421\n",
            "[Train] epoch:2528, lr:0.000866, total loss:0.000412\n",
            "[Train] epoch:2529, lr:0.000866, total loss:0.000380\n",
            "[Train] epoch:2530, lr:0.000866, total loss:0.000393\n",
            "[Train] epoch:2531, lr:0.000866, total loss:0.000401\n",
            "[Train] epoch:2532, lr:0.000866, total loss:0.000515\n",
            "[Train] epoch:2533, lr:0.000866, total loss:0.000444\n",
            "[Train] epoch:2534, lr:0.000866, total loss:0.000472\n",
            "[Train] epoch:2535, lr:0.000866, total loss:0.000419\n",
            "[Train] epoch:2536, lr:0.000865, total loss:0.000417\n",
            "[Train] epoch:2537, lr:0.000865, total loss:0.000376\n",
            "[Train] epoch:2538, lr:0.000865, total loss:0.000380\n",
            "[Train] epoch:2539, lr:0.000865, total loss:0.000401\n",
            "[Train] epoch:2540, lr:0.000865, total loss:0.000382\n",
            "[Train] epoch:2541, lr:0.000865, total loss:0.000360\n",
            "[Train] epoch:2542, lr:0.000865, total loss:0.000404\n",
            "[Train] epoch:2543, lr:0.000865, total loss:0.000400\n",
            "[Train] epoch:2544, lr:0.000865, total loss:0.000401\n",
            "[Train] epoch:2545, lr:0.000865, total loss:0.000347\n",
            "[Train] epoch:2546, lr:0.000864, total loss:0.000389\n",
            "[Train] epoch:2547, lr:0.000864, total loss:0.000371\n",
            "[Train] epoch:2548, lr:0.000864, total loss:0.000326\n",
            "[Train] epoch:2549, lr:0.000864, total loss:0.000432\n",
            "[Train] epoch:2550, lr:0.000864, total loss:0.000426\n",
            "[Train] epoch:2551, lr:0.000864, total loss:0.000400\n",
            "[Train] epoch:2552, lr:0.000864, total loss:0.000367\n",
            "[Train] epoch:2553, lr:0.000864, total loss:0.000389\n",
            "[Train] epoch:2554, lr:0.000864, total loss:0.000394\n",
            "[Train] epoch:2555, lr:0.000864, total loss:0.000364\n",
            "[Train] epoch:2556, lr:0.000863, total loss:0.000357\n",
            "[Train] epoch:2557, lr:0.000863, total loss:0.000362\n",
            "[Train] epoch:2558, lr:0.000863, total loss:0.000390\n",
            "[Train] epoch:2559, lr:0.000863, total loss:0.000408\n",
            "[Train] epoch:2560, lr:0.000863, total loss:0.000423\n",
            "[Train] epoch:2561, lr:0.000863, total loss:0.000431\n",
            "[Train] epoch:2562, lr:0.000863, total loss:0.000464\n",
            "[Train] epoch:2563, lr:0.000863, total loss:0.000393\n",
            "[Train] epoch:2564, lr:0.000863, total loss:0.000410\n",
            "[Train] epoch:2565, lr:0.000862, total loss:0.000380\n",
            "[Train] epoch:2566, lr:0.000862, total loss:0.000344\n",
            "[Train] epoch:2567, lr:0.000862, total loss:0.000497\n",
            "[Train] epoch:2568, lr:0.000862, total loss:0.000404\n",
            "[Train] epoch:2569, lr:0.000862, total loss:0.000402\n",
            "[Train] epoch:2570, lr:0.000862, total loss:0.000466\n",
            "[Train] epoch:2571, lr:0.000862, total loss:0.000474\n",
            "[Train] epoch:2572, lr:0.000862, total loss:0.000408\n",
            "[Train] epoch:2573, lr:0.000862, total loss:0.000448\n",
            "[Train] epoch:2574, lr:0.000862, total loss:0.000377\n",
            "[Train] epoch:2575, lr:0.000861, total loss:0.000393\n",
            "[Train] epoch:2576, lr:0.000861, total loss:0.000403\n",
            "[Train] epoch:2577, lr:0.000861, total loss:0.000403\n",
            "[Train] epoch:2578, lr:0.000861, total loss:0.000381\n",
            "[Train] epoch:2579, lr:0.000861, total loss:0.000401\n",
            "[Train] epoch:2580, lr:0.000861, total loss:0.000357\n",
            "[Train] epoch:2581, lr:0.000861, total loss:0.000361\n",
            "[Train] epoch:2582, lr:0.000861, total loss:0.000463\n",
            "[Train] epoch:2583, lr:0.000861, total loss:0.000517\n",
            "[Train] epoch:2584, lr:0.000861, total loss:0.000526\n",
            "[Train] epoch:2585, lr:0.000860, total loss:0.000566\n",
            "[Train] epoch:2586, lr:0.000860, total loss:0.000469\n",
            "[Train] epoch:2587, lr:0.000860, total loss:0.000480\n",
            "[Train] epoch:2588, lr:0.000860, total loss:0.000499\n",
            "[Train] epoch:2589, lr:0.000860, total loss:0.000476\n",
            "[Train] epoch:2590, lr:0.000860, total loss:0.000460\n",
            "[Train] epoch:2591, lr:0.000860, total loss:0.000508\n",
            "[Train] epoch:2592, lr:0.000860, total loss:0.000446\n",
            "[Train] epoch:2593, lr:0.000860, total loss:0.000502\n",
            "[Train] epoch:2594, lr:0.000860, total loss:0.000475\n",
            "[Train] epoch:2595, lr:0.000859, total loss:0.000415\n",
            "[Train] epoch:2596, lr:0.000859, total loss:0.000444\n",
            "[Train] epoch:2597, lr:0.000859, total loss:0.000387\n",
            "[Train] epoch:2598, lr:0.000859, total loss:0.000385\n",
            "[Train] epoch:2599, lr:0.000859, total loss:0.000390\n",
            "[Train] epoch:2600, lr:0.000859, total loss:0.000408\n",
            "[Train] epoch:2601, lr:0.000859, total loss:0.000420\n",
            "[Train] epoch:2602, lr:0.000859, total loss:0.000416\n",
            "[Train] epoch:2603, lr:0.000859, total loss:0.000384\n",
            "[Train] epoch:2604, lr:0.000858, total loss:0.000418\n",
            "[Train] epoch:2605, lr:0.000858, total loss:0.000369\n",
            "[Train] epoch:2606, lr:0.000858, total loss:0.000393\n",
            "[Train] epoch:2607, lr:0.000858, total loss:0.000372\n",
            "[Train] epoch:2608, lr:0.000858, total loss:0.000350\n",
            "[Train] epoch:2609, lr:0.000858, total loss:0.000413\n",
            "[Train] epoch:2610, lr:0.000858, total loss:0.000357\n",
            "[Train] epoch:2611, lr:0.000858, total loss:0.000415\n",
            "[Train] epoch:2612, lr:0.000858, total loss:0.000365\n",
            "[Train] epoch:2613, lr:0.000858, total loss:0.000353\n",
            "[Train] epoch:2614, lr:0.000857, total loss:0.000372\n",
            "[Train] epoch:2615, lr:0.000857, total loss:0.000358\n",
            "[Train] epoch:2616, lr:0.000857, total loss:0.000351\n",
            "[Train] epoch:2617, lr:0.000857, total loss:0.000354\n",
            "[Train] epoch:2618, lr:0.000857, total loss:0.000356\n",
            "[Train] epoch:2619, lr:0.000857, total loss:0.000381\n",
            "[Train] epoch:2620, lr:0.000857, total loss:0.000426\n",
            "[Train] epoch:2621, lr:0.000857, total loss:0.000438\n",
            "[Train] epoch:2622, lr:0.000857, total loss:0.000428\n",
            "[Train] epoch:2623, lr:0.000857, total loss:0.000370\n",
            "[Train] epoch:2624, lr:0.000856, total loss:0.000416\n",
            "[Train] epoch:2625, lr:0.000856, total loss:0.000399\n",
            "[Train] epoch:2626, lr:0.000856, total loss:0.000369\n",
            "[Train] epoch:2627, lr:0.000856, total loss:0.000426\n",
            "[Train] epoch:2628, lr:0.000856, total loss:0.000352\n",
            "[Train] epoch:2629, lr:0.000856, total loss:0.000372\n",
            "[Train] epoch:2630, lr:0.000856, total loss:0.000353\n",
            "[Train] epoch:2631, lr:0.000856, total loss:0.000318\n",
            "[Train] epoch:2632, lr:0.000856, total loss:0.000354\n",
            "[Train] epoch:2633, lr:0.000855, total loss:0.000326\n",
            "[Train] epoch:2634, lr:0.000855, total loss:0.000373\n",
            "[Train] epoch:2635, lr:0.000855, total loss:0.000397\n",
            "[Train] epoch:2636, lr:0.000855, total loss:0.000411\n",
            "[Train] epoch:2637, lr:0.000855, total loss:0.000411\n",
            "[Train] epoch:2638, lr:0.000855, total loss:0.000410\n",
            "[Train] epoch:2639, lr:0.000855, total loss:0.000368\n",
            "[Train] epoch:2640, lr:0.000855, total loss:0.000367\n",
            "[Train] epoch:2641, lr:0.000855, total loss:0.000386\n",
            "[Train] epoch:2642, lr:0.000855, total loss:0.000352\n",
            "[Train] epoch:2643, lr:0.000854, total loss:0.000363\n",
            "[Train] epoch:2644, lr:0.000854, total loss:0.000343\n",
            "[Train] epoch:2645, lr:0.000854, total loss:0.000343\n",
            "[Train] epoch:2646, lr:0.000854, total loss:0.000359\n",
            "[Train] epoch:2647, lr:0.000854, total loss:0.000403\n",
            "[Train] epoch:2648, lr:0.000854, total loss:0.000372\n",
            "[Train] epoch:2649, lr:0.000854, total loss:0.000360\n",
            "[Train] epoch:2650, lr:0.000854, total loss:0.000434\n",
            "[Train] epoch:2651, lr:0.000854, total loss:0.000436\n",
            "[Train] epoch:2652, lr:0.000854, total loss:0.000482\n",
            "[Train] epoch:2653, lr:0.000853, total loss:0.000486\n",
            "[Train] epoch:2654, lr:0.000853, total loss:0.000572\n",
            "[Train] epoch:2655, lr:0.000853, total loss:0.000705\n",
            "[Train] epoch:2656, lr:0.000853, total loss:0.000532\n",
            "[Train] epoch:2657, lr:0.000853, total loss:0.000438\n",
            "[Train] epoch:2658, lr:0.000853, total loss:0.000400\n",
            "[Train] epoch:2659, lr:0.000853, total loss:0.000433\n",
            "[Train] epoch:2660, lr:0.000853, total loss:0.000475\n",
            "[Train] epoch:2661, lr:0.000853, total loss:0.000528\n",
            "[Train] epoch:2662, lr:0.000852, total loss:0.000529\n",
            "[Train] epoch:2663, lr:0.000852, total loss:0.000506\n",
            "[Train] epoch:2664, lr:0.000852, total loss:0.000523\n",
            "[Train] epoch:2665, lr:0.000852, total loss:0.000442\n",
            "[Train] epoch:2666, lr:0.000852, total loss:0.000419\n",
            "[Train] epoch:2667, lr:0.000852, total loss:0.000426\n",
            "[Train] epoch:2668, lr:0.000852, total loss:0.000384\n",
            "[Train] epoch:2669, lr:0.000852, total loss:0.000533\n",
            "[Train] epoch:2670, lr:0.000852, total loss:0.000404\n",
            "[Train] epoch:2671, lr:0.000852, total loss:0.000413\n",
            "[Train] epoch:2672, lr:0.000851, total loss:0.000467\n",
            "[Train] epoch:2673, lr:0.000851, total loss:0.000527\n",
            "[Train] epoch:2674, lr:0.000851, total loss:0.000375\n",
            "[Train] epoch:2675, lr:0.000851, total loss:0.000392\n",
            "[Train] epoch:2676, lr:0.000851, total loss:0.000407\n",
            "[Train] epoch:2677, lr:0.000851, total loss:0.000392\n",
            "[Train] epoch:2678, lr:0.000851, total loss:0.000374\n",
            "[Train] epoch:2679, lr:0.000851, total loss:0.000361\n",
            "[Train] epoch:2680, lr:0.000851, total loss:0.000386\n",
            "[Train] epoch:2681, lr:0.000850, total loss:0.000393\n",
            "[Train] epoch:2682, lr:0.000850, total loss:0.000405\n",
            "[Train] epoch:2683, lr:0.000850, total loss:0.000345\n",
            "[Train] epoch:2684, lr:0.000850, total loss:0.000414\n",
            "[Train] epoch:2685, lr:0.000850, total loss:0.000438\n",
            "[Train] epoch:2686, lr:0.000850, total loss:0.000428\n",
            "[Train] epoch:2687, lr:0.000850, total loss:0.000473\n",
            "[Train] epoch:2688, lr:0.000850, total loss:0.000436\n",
            "[Train] epoch:2689, lr:0.000850, total loss:0.000431\n",
            "[Train] epoch:2690, lr:0.000850, total loss:0.000403\n",
            "[Train] epoch:2691, lr:0.000849, total loss:0.000403\n",
            "[Train] epoch:2692, lr:0.000849, total loss:0.000390\n",
            "[Train] epoch:2693, lr:0.000849, total loss:0.000407\n",
            "[Train] epoch:2694, lr:0.000849, total loss:0.000429\n",
            "[Train] epoch:2695, lr:0.000849, total loss:0.000408\n",
            "[Train] epoch:2696, lr:0.000849, total loss:0.000360\n",
            "[Train] epoch:2697, lr:0.000849, total loss:0.000370\n",
            "[Train] epoch:2698, lr:0.000849, total loss:0.000372\n",
            "[Train] epoch:2699, lr:0.000849, total loss:0.000349\n",
            "[Train] epoch:2700, lr:0.000848, total loss:0.000342\n",
            "[Train] epoch:2701, lr:0.000848, total loss:0.000351\n",
            "[Train] epoch:2702, lr:0.000848, total loss:0.000391\n",
            "[Train] epoch:2703, lr:0.000848, total loss:0.000362\n",
            "[Train] epoch:2704, lr:0.000848, total loss:0.000359\n",
            "[Train] epoch:2705, lr:0.000848, total loss:0.000391\n",
            "[Train] epoch:2706, lr:0.000848, total loss:0.000386\n",
            "[Train] epoch:2707, lr:0.000848, total loss:0.000355\n",
            "[Train] epoch:2708, lr:0.000848, total loss:0.000354\n",
            "[Train] epoch:2709, lr:0.000848, total loss:0.000365\n",
            "[Train] epoch:2710, lr:0.000847, total loss:0.000366\n",
            "[Train] epoch:2711, lr:0.000847, total loss:0.000350\n",
            "[Train] epoch:2712, lr:0.000847, total loss:0.000338\n",
            "[Train] epoch:2713, lr:0.000847, total loss:0.000312\n",
            "[Train] epoch:2714, lr:0.000847, total loss:0.000356\n",
            "[Train] epoch:2715, lr:0.000847, total loss:0.000359\n",
            "[Train] epoch:2716, lr:0.000847, total loss:0.000394\n",
            "[Train] epoch:2717, lr:0.000847, total loss:0.000403\n",
            "[Train] epoch:2718, lr:0.000847, total loss:0.000385\n",
            "[Train] epoch:2719, lr:0.000846, total loss:0.000437\n",
            "[Train] epoch:2720, lr:0.000846, total loss:0.000464\n",
            "[Train] epoch:2721, lr:0.000846, total loss:0.000370\n",
            "[Train] epoch:2722, lr:0.000846, total loss:0.000474\n",
            "[Train] epoch:2723, lr:0.000846, total loss:0.000428\n",
            "[Train] epoch:2724, lr:0.000846, total loss:0.000435\n",
            "[Train] epoch:2725, lr:0.000846, total loss:0.000363\n",
            "[Train] epoch:2726, lr:0.000846, total loss:0.000336\n",
            "[Train] epoch:2727, lr:0.000846, total loss:0.000394\n",
            "[Train] epoch:2728, lr:0.000845, total loss:0.000416\n",
            "[Train] epoch:2729, lr:0.000845, total loss:0.000407\n",
            "[Train] epoch:2730, lr:0.000845, total loss:0.000392\n",
            "[Train] epoch:2731, lr:0.000845, total loss:0.000347\n",
            "[Train] epoch:2732, lr:0.000845, total loss:0.000371\n",
            "[Train] epoch:2733, lr:0.000845, total loss:0.000353\n",
            "[Train] epoch:2734, lr:0.000845, total loss:0.000342\n",
            "[Train] epoch:2735, lr:0.000845, total loss:0.000355\n",
            "[Train] epoch:2736, lr:0.000845, total loss:0.000394\n",
            "[Train] epoch:2737, lr:0.000845, total loss:0.000497\n",
            "[Train] epoch:2738, lr:0.000844, total loss:0.000374\n",
            "[Train] epoch:2739, lr:0.000844, total loss:0.000373\n",
            "[Train] epoch:2740, lr:0.000844, total loss:0.000429\n",
            "[Train] epoch:2741, lr:0.000844, total loss:0.000387\n",
            "[Train] epoch:2742, lr:0.000844, total loss:0.000344\n",
            "[Train] epoch:2743, lr:0.000844, total loss:0.000419\n",
            "[Train] epoch:2744, lr:0.000844, total loss:0.000359\n",
            "[Train] epoch:2745, lr:0.000844, total loss:0.000382\n",
            "[Train] epoch:2746, lr:0.000844, total loss:0.000414\n",
            "[Train] epoch:2747, lr:0.000843, total loss:0.000395\n",
            "[Train] epoch:2748, lr:0.000843, total loss:0.000445\n",
            "[Train] epoch:2749, lr:0.000843, total loss:0.000396\n",
            "[Train] epoch:2750, lr:0.000843, total loss:0.000406\n",
            "[Train] epoch:2751, lr:0.000843, total loss:0.000451\n",
            "[Train] epoch:2752, lr:0.000843, total loss:0.000322\n",
            "[Train] epoch:2753, lr:0.000843, total loss:0.000341\n",
            "[Train] epoch:2754, lr:0.000843, total loss:0.000369\n",
            "[Train] epoch:2755, lr:0.000843, total loss:0.000383\n",
            "[Train] epoch:2756, lr:0.000842, total loss:0.000439\n",
            "[Train] epoch:2757, lr:0.000842, total loss:0.000476\n",
            "[Train] epoch:2758, lr:0.000842, total loss:0.000384\n",
            "[Train] epoch:2759, lr:0.000842, total loss:0.000372\n",
            "[Train] epoch:2760, lr:0.000842, total loss:0.000337\n",
            "[Train] epoch:2761, lr:0.000842, total loss:0.000420\n",
            "[Train] epoch:2762, lr:0.000842, total loss:0.000398\n",
            "[Train] epoch:2763, lr:0.000842, total loss:0.000371\n",
            "[Train] epoch:2764, lr:0.000842, total loss:0.000364\n",
            "[Train] epoch:2765, lr:0.000842, total loss:0.000363\n",
            "[Train] epoch:2766, lr:0.000841, total loss:0.000380\n",
            "[Train] epoch:2767, lr:0.000841, total loss:0.000374\n",
            "[Train] epoch:2768, lr:0.000841, total loss:0.000398\n",
            "[Train] epoch:2769, lr:0.000841, total loss:0.000345\n",
            "[Train] epoch:2770, lr:0.000841, total loss:0.000403\n",
            "[Train] epoch:2771, lr:0.000841, total loss:0.000363\n",
            "[Train] epoch:2772, lr:0.000841, total loss:0.000388\n",
            "[Train] epoch:2773, lr:0.000841, total loss:0.000355\n",
            "[Train] epoch:2774, lr:0.000841, total loss:0.000355\n",
            "[Train] epoch:2775, lr:0.000840, total loss:0.000339\n",
            "[Train] epoch:2776, lr:0.000840, total loss:0.000334\n",
            "[Train] epoch:2777, lr:0.000840, total loss:0.000331\n",
            "[Train] epoch:2778, lr:0.000840, total loss:0.000484\n",
            "[Train] epoch:2779, lr:0.000840, total loss:0.000346\n",
            "[Train] epoch:2780, lr:0.000840, total loss:0.000345\n",
            "[Train] epoch:2781, lr:0.000840, total loss:0.000354\n",
            "[Train] epoch:2782, lr:0.000840, total loss:0.000371\n",
            "[Train] epoch:2783, lr:0.000840, total loss:0.000410\n",
            "[Train] epoch:2784, lr:0.000839, total loss:0.000405\n",
            "[Train] epoch:2785, lr:0.000839, total loss:0.000432\n",
            "[Train] epoch:2786, lr:0.000839, total loss:0.000437\n",
            "[Train] epoch:2787, lr:0.000839, total loss:0.000374\n",
            "[Train] epoch:2788, lr:0.000839, total loss:0.000400\n",
            "[Train] epoch:2789, lr:0.000839, total loss:0.000384\n",
            "[Train] epoch:2790, lr:0.000839, total loss:0.000382\n",
            "[Train] epoch:2791, lr:0.000839, total loss:0.000330\n",
            "[Train] epoch:2792, lr:0.000839, total loss:0.000335\n",
            "[Train] epoch:2793, lr:0.000838, total loss:0.000415\n",
            "[Train] epoch:2794, lr:0.000838, total loss:0.000493\n",
            "[Train] epoch:2795, lr:0.000838, total loss:0.000420\n",
            "[Train] epoch:2796, lr:0.000838, total loss:0.000401\n",
            "[Train] epoch:2797, lr:0.000838, total loss:0.000403\n",
            "[Train] epoch:2798, lr:0.000838, total loss:0.000462\n",
            "[Train] epoch:2799, lr:0.000838, total loss:0.000391\n",
            "[Train] epoch:2800, lr:0.000838, total loss:0.000376\n",
            "[Train] epoch:2801, lr:0.000838, total loss:0.000358\n",
            "[Train] epoch:2802, lr:0.000838, total loss:0.000370\n",
            "[Train] epoch:2803, lr:0.000837, total loss:0.000402\n",
            "[Train] epoch:2804, lr:0.000837, total loss:0.000417\n",
            "[Train] epoch:2805, lr:0.000837, total loss:0.000452\n",
            "[Train] epoch:2806, lr:0.000837, total loss:0.000382\n",
            "[Train] epoch:2807, lr:0.000837, total loss:0.000394\n",
            "[Train] epoch:2808, lr:0.000837, total loss:0.000359\n",
            "[Train] epoch:2809, lr:0.000837, total loss:0.000377\n",
            "Early stopping at:  2809\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compare metrics, plot loss curve and save best model"
      ],
      "metadata": {
        "id": "XNcYGnux_IbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "if overwrite:\n",
        "  torch.save(pinn.best_model, \"/content/drive/MyDrive/Machine Learning Project (ECE 228)/Battery Dataset/pinn_regular.pth\")\n",
        "  pinn.load_model(\"/content/drive/MyDrive/Machine Learning Project (ECE 228)/Battery Dataset/pinn_regular.pth\")\n",
        "\n",
        "true_label, pred_label = pinn.Test(loader['test_3'])\n",
        "mse = metrics.mean_squared_error(torch.tensor(true_label), torch.tensor(pred_label))\n",
        "mae = metrics.mean_absolute_error(torch.tensor(true_label), torch.tensor(pred_label))\n",
        "r2 = metrics.r2_score(torch.tensor(true_label), torch.tensor(pred_label))\n",
        "mape = metrics.mean_absolute_percentage_error(torch.tensor(true_label), torch.tensor(pred_label))\n",
        "\n",
        "print(mse, mae, r2, mape)\n",
        "\n",
        "print(count_parameters(pinn))\n",
        "\n",
        "result_dict = [{\n",
        "    'run name': 'regular features',\n",
        "    'mse': mse,\n",
        "    'mae': mae,\n",
        "    'r2': r2,\n",
        "    'mape': mape,\n",
        "    'param count': count_parameters(pinn)\n",
        "}]\n",
        "df = pd.DataFrame(result_dict)\n",
        "try:\n",
        "  results = pd.read_csv('/content/drive/MyDrive/Machine Learning Project (ECE 228)/Battery Dataset/results.csv')\n",
        "  results = pd.concat([results, df], ignore_index=True)\n",
        "except:\n",
        "  results = df\n",
        "\n",
        "if overwrite:\n",
        "  results.to_csv('/content/drive/MyDrive/Machine Learning Project (ECE 228)/Battery Dataset/results.csv', index=False)"
      ],
      "metadata": {
        "id": "7Sz6unzeG0cM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66e4e3cc-5871-43bf-eed4-0c4452ec3dbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0010096465438147063 0.01810958200801324 0.9279748825107829 0.024584463291555857\n",
            "13662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the loss curve\n",
        "import matplotlib.pyplot as plt\n",
        "x = list(range(1,len(losses_regular) + 1))\n",
        "plt.yscale('log')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Log-Scale Loss plot for vanilla model')\n",
        "plt.plot(x, losses_regular)\n"
      ],
      "metadata": {
        "id": "0D8NG6oD36Fi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "outputId": "525f1f4c-7300-48f1-e84d-57f854475ccd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7d36484ff090>]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extended Features"
      ],
      "metadata": {
        "id": "4haPzGcIARKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# little bit of hee hee haha data manipulation\n",
        "df = pd.read_csv('/content/drive/MyDrive/Machine Learning Project (ECE 228)/Battery Dataset/features_extended.csv')\n",
        "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "df.dropna(inplace=True)\n",
        "df = df.drop(columns='fft_dominant_freq_i') # normalizing constants leads to div by zero. it's all the same because duh you can't do the FT on a linearly increasing value  and then pick the dominant without just basically getting the linear term back (i.e. index 1)\n",
        "# makes no sense to run the FT on a function that only behaves linearly in the window but it seems the spectral entropy helped so I'm not goint to throw a fit\n",
        "df = df.drop(columns='fft_dominant_freq_v') # normalizing constants leads to div by zero\n",
        "df.to_csv(\"clean_features_extended.csv\", index=False)"
      ],
      "metadata": {
        "id": "zs3isJqOAY6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class args:\n",
        "  pass\n",
        "\n",
        "# hehe tricks\n",
        "arg = args()\n",
        "\n",
        "arg.data = 'our_stuff'\n",
        "arg.batch = 10\n",
        "arg.batch_size = 256\n",
        "arg.normalization_method = 'z-score'\n",
        "arg.epochs = 10000\n",
        "arg.lr = 1e-3\n",
        "arg.warmup_epochs = 10\n",
        "arg.warmup_lr = 5e-4\n",
        "arg.final_lr = 1e-4\n",
        "arg.lr_F = 1e-3\n",
        "arg.iter_per_epoch = 1\n",
        "arg.F_layers_num = 3\n",
        "arg.F_hidden_dim = 60\n",
        "arg.alpha = 1\n",
        "arg.beta = 1\n",
        "arg.early_stop = 500\n",
        "arg.feature_length = 23 # use 23 for extended feature set - 25 minues the fft dominant freq stuff\n",
        "arg.second_derivatives = False # use to not include second derivatives"
      ],
      "metadata": {
        "id": "An4k2o6WAZ4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ok let's load a dataset\n",
        "reader = DF(arg)"
      ],
      "metadata": {
        "id": "snD_XO_UAiO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's get a loader back\n",
        "loader = reader.load_all([\"./clean_features_extended.csv\"], nominal_capacity=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNTkgo4tAoWY",
        "outputId": "d81c4db7-b304-4c44-e9a3-5a1ed3e19161"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      charge_CC_mean_V  charge_CC_std_V  charge_CC_kurtosis_V  \\\n",
            "0             0.719302         0.060387             -0.416205   \n",
            "1             1.744366        -1.770584              2.239328   \n",
            "2             0.813665        -0.168417              0.106946   \n",
            "3             0.746640        -1.103780              0.819037   \n",
            "4             0.615206        -0.662110              0.482629   \n",
            "...                ...              ...                   ...   \n",
            "2396         -0.431346         0.728308             -0.611656   \n",
            "2397         -0.442060         0.650396             -0.614654   \n",
            "2398         -0.409993         0.631472             -0.612084   \n",
            "2399         -0.503042         0.695169             -0.596058   \n",
            "2400         -0.406144         0.605715             -0.612426   \n",
            "\n",
            "      charge_CC_skew_V  charge_CC_time_V  charge_CC_charge  charge_CC_slope_V  \\\n",
            "0            -0.490447         -2.652343         -2.639229           9.289918   \n",
            "1            -2.047867         -0.189973         -0.164574          -0.220123   \n",
            "2            -0.758163         -0.717865         -0.694388           0.094567   \n",
            "3            -1.162773         -0.754520         -0.731544           0.129686   \n",
            "4            -0.731586         -0.945440         -0.923880           0.269518   \n",
            "...                ...               ...               ...                ...   \n",
            "2396          0.406049         -0.405084         -0.379109          -0.081770   \n",
            "2397          0.412729         -0.422070         -0.396967          -0.079425   \n",
            "2398          0.416279         -0.445634         -0.419898          -0.069482   \n",
            "2399          0.488817         -0.188997         -0.162619          -0.196982   \n",
            "2400          0.394161         -0.469825         -0.444168          -0.057148   \n",
            "\n",
            "      charge_CC_entropy_V  charge_CV_mean_I  charge_CV_std_dev_I  ...  \\\n",
            "0               -3.767662         -0.533985             0.280544  ...   \n",
            "1               -0.544218         -1.468485            -0.737614  ...   \n",
            "2               -0.855270         -0.163320             0.369320  ...   \n",
            "3               -0.882921         -1.905650            -1.639340  ...   \n",
            "4               -1.019545         -1.985914            -1.116553  ...   \n",
            "...                   ...               ...                  ...  ...   \n",
            "2396            -0.749642          0.422425            -1.409878  ...   \n",
            "2397            -0.766719          0.408411            -1.681065  ...   \n",
            "2398            -0.784003          0.514787            -1.252275  ...   \n",
            "2399            -0.651206          0.310818            -1.725393  ...   \n",
            "2400            -0.810322          0.734143            -1.147519  ...   \n",
            "\n",
            "      charge_CV_slope_I  charge_CV_entropy_I  voltage_range_CC  \\\n",
            "0              0.465429            -0.965529         -4.062558   \n",
            "1              1.264524             0.204064         -1.228839   \n",
            "2              1.252285             0.136015         -0.012242   \n",
            "3              1.323510             0.211991          0.447969   \n",
            "4              1.383567             0.303588         -0.008673   \n",
            "...                 ...                  ...               ...   \n",
            "2396          -0.365561            -1.571977          0.733176   \n",
            "2397          -0.406831            -1.618121          0.347949   \n",
            "2398          -0.355108            -1.601372          0.184281   \n",
            "2399          -0.089845            -1.410325          0.222597   \n",
            "2400          -0.315553            -1.585471          0.123136   \n",
            "\n",
            "      current_range_CV  voltage_rms_CC  current_rms_CV  \\\n",
            "0            -0.600508        0.720590       -0.441106   \n",
            "1             0.548698        1.742806       -1.505971   \n",
            "2            -0.691349        0.814510       -0.081481   \n",
            "3            -0.751668        0.744993       -2.088463   \n",
            "4             0.169692        0.614443       -2.058659   \n",
            "...                ...             ...             ...   \n",
            "2396         -0.862895       -0.430183        0.119307   \n",
            "2397         -0.993078       -0.441131        0.055572   \n",
            "2398         -1.117055       -0.409062        0.235115   \n",
            "2399         -1.185204       -0.502091       -0.043943   \n",
            "2400         -1.004965       -0.405277        0.459621   \n",
            "\n",
            "      fft_spectral_entropy_v  fft_spectral_entropy_i  cycle_index  capacity  \n",
            "0                  -3.136304               -0.893938    -0.841376  0.350731  \n",
            "1                   2.002703                1.520689    -1.228564  0.782358  \n",
            "2                   0.320334               -0.247006    -1.208186  0.779403  \n",
            "3                   1.530118                1.452607    -1.187807  0.772999  \n",
            "4                   1.146405                0.749716    -1.167429  0.767229  \n",
            "...                      ...                     ...          ...       ...  \n",
            "2396               -0.625367                0.165608     1.339104  0.698253  \n",
            "2397               -0.633874                0.230246     1.359483  0.697567  \n",
            "2398               -0.642112                0.013432     1.379861  0.689922  \n",
            "2399               -0.642174                0.490014     1.400239  0.689690  \n",
            "2400               -0.642954                0.105679     1.420618  0.691855  \n",
            "\n",
            "[2401 rows x 24 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pinn = PINN(arg)"
      ],
      "metadata": {
        "id": "CPNO617DB_TP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses_extended = pinn.Train(trainloader=loader['train'],\n",
        "           validloader=loader['valid'],\n",
        "           testloader=loader['test'], debug=True)\n",
        "if overwrite:\n",
        "  np.save('/content/drive/MyDrive/Machine Learning Project (ECE 228)/Battery Dataset/loss_extended.npy', losses_extended)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEF-GlicCDU3",
        "outputId": "d7b56100-b25f-45c3-a2a8-1f3be2492a69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Train] epoch:1, lr:0.000500, total loss:1.242024\n",
            "[Train] epoch:2, lr:0.000556, total loss:0.770519\n",
            "[Train] epoch:3, lr:0.000611, total loss:0.584880\n",
            "[Train] epoch:4, lr:0.000667, total loss:0.412832\n",
            "[Train] epoch:5, lr:0.000722, total loss:0.347549\n",
            "[Train] epoch:6, lr:0.000778, total loss:0.280631\n",
            "[Train] epoch:7, lr:0.000833, total loss:0.241740\n",
            "[Train] epoch:8, lr:0.000889, total loss:0.214243\n",
            "[Train] epoch:9, lr:0.000944, total loss:0.189203\n",
            "[Train] epoch:10, lr:0.001000, total loss:0.161240\n",
            "[Train] epoch:11, lr:0.001000, total loss:0.140082\n",
            "[Train] epoch:12, lr:0.001000, total loss:0.130589\n",
            "[Train] epoch:13, lr:0.001000, total loss:0.118422\n",
            "[Train] epoch:14, lr:0.001000, total loss:0.110414\n",
            "[Train] epoch:15, lr:0.001000, total loss:0.099445\n",
            "[Train] epoch:16, lr:0.001000, total loss:0.087565\n",
            "[Train] epoch:17, lr:0.001000, total loss:0.083944\n",
            "[Train] epoch:18, lr:0.001000, total loss:0.079620\n",
            "[Train] epoch:19, lr:0.001000, total loss:0.069629\n",
            "[Train] epoch:20, lr:0.001000, total loss:0.071528\n",
            "[Train] epoch:21, lr:0.001000, total loss:0.062925\n",
            "[Train] epoch:22, lr:0.001000, total loss:0.058647\n",
            "[Train] epoch:23, lr:0.001000, total loss:0.055900\n",
            "[Train] epoch:24, lr:0.001000, total loss:0.049463\n",
            "[Train] epoch:25, lr:0.001000, total loss:0.048375\n",
            "[Train] epoch:26, lr:0.001000, total loss:0.044497\n",
            "[Train] epoch:27, lr:0.001000, total loss:0.044396\n",
            "[Train] epoch:28, lr:0.001000, total loss:0.040885\n",
            "[Train] epoch:29, lr:0.001000, total loss:0.038639\n",
            "[Train] epoch:30, lr:0.001000, total loss:0.036790\n",
            "[Train] epoch:31, lr:0.001000, total loss:0.033820\n",
            "[Train] epoch:32, lr:0.001000, total loss:0.032691\n",
            "[Train] epoch:33, lr:0.001000, total loss:0.029360\n",
            "[Train] epoch:34, lr:0.001000, total loss:0.030641\n",
            "[Train] epoch:35, lr:0.001000, total loss:0.027391\n",
            "[Train] epoch:36, lr:0.001000, total loss:0.027637\n",
            "[Train] epoch:37, lr:0.001000, total loss:0.026671\n",
            "[Train] epoch:38, lr:0.001000, total loss:0.024764\n",
            "[Train] epoch:39, lr:0.001000, total loss:0.023424\n",
            "[Train] epoch:40, lr:0.001000, total loss:0.022975\n",
            "[Train] epoch:41, lr:0.001000, total loss:0.021759\n",
            "[Train] epoch:42, lr:0.001000, total loss:0.021237\n",
            "[Train] epoch:43, lr:0.001000, total loss:0.020194\n",
            "[Train] epoch:44, lr:0.001000, total loss:0.017965\n",
            "[Train] epoch:45, lr:0.001000, total loss:0.018441\n",
            "[Train] epoch:46, lr:0.001000, total loss:0.017380\n",
            "[Train] epoch:47, lr:0.001000, total loss:0.017631\n",
            "[Train] epoch:48, lr:0.001000, total loss:0.017609\n",
            "[Train] epoch:49, lr:0.001000, total loss:0.016286\n",
            "[Train] epoch:50, lr:0.001000, total loss:0.015994\n",
            "[Train] epoch:51, lr:0.001000, total loss:0.014762\n",
            "[Train] epoch:52, lr:0.001000, total loss:0.014511\n",
            "[Train] epoch:53, lr:0.001000, total loss:0.014419\n",
            "[Train] epoch:54, lr:0.001000, total loss:0.013743\n",
            "[Train] epoch:55, lr:0.001000, total loss:0.012756\n",
            "[Train] epoch:56, lr:0.001000, total loss:0.012936\n",
            "[Train] epoch:57, lr:0.001000, total loss:0.012892\n",
            "[Train] epoch:58, lr:0.001000, total loss:0.012078\n",
            "[Train] epoch:59, lr:0.001000, total loss:0.012016\n",
            "[Train] epoch:60, lr:0.001000, total loss:0.011080\n",
            "[Train] epoch:61, lr:0.001000, total loss:0.011373\n",
            "[Train] epoch:62, lr:0.001000, total loss:0.011210\n",
            "[Train] epoch:63, lr:0.001000, total loss:0.010503\n",
            "[Train] epoch:64, lr:0.001000, total loss:0.010791\n",
            "[Train] epoch:65, lr:0.001000, total loss:0.009219\n",
            "[Train] epoch:66, lr:0.001000, total loss:0.010301\n",
            "[Train] epoch:67, lr:0.001000, total loss:0.009909\n",
            "[Train] epoch:68, lr:0.001000, total loss:0.009427\n",
            "[Train] epoch:69, lr:0.001000, total loss:0.008983\n",
            "[Train] epoch:70, lr:0.001000, total loss:0.008782\n",
            "[Train] epoch:71, lr:0.001000, total loss:0.008816\n",
            "[Train] epoch:72, lr:0.001000, total loss:0.008563\n",
            "[Train] epoch:73, lr:0.001000, total loss:0.008616\n",
            "[Train] epoch:74, lr:0.001000, total loss:0.008711\n",
            "[Train] epoch:75, lr:0.001000, total loss:0.008398\n",
            "[Train] epoch:76, lr:0.001000, total loss:0.007712\n",
            "[Train] epoch:77, lr:0.001000, total loss:0.007712\n",
            "[Train] epoch:78, lr:0.001000, total loss:0.007164\n",
            "[Train] epoch:79, lr:0.001000, total loss:0.007438\n",
            "[Train] epoch:80, lr:0.001000, total loss:0.007836\n",
            "[Train] epoch:81, lr:0.001000, total loss:0.007605\n",
            "[Train] epoch:82, lr:0.001000, total loss:0.007563\n",
            "[Train] epoch:83, lr:0.001000, total loss:0.007473\n",
            "[Train] epoch:84, lr:0.001000, total loss:0.007278\n",
            "[Train] epoch:85, lr:0.001000, total loss:0.007040\n",
            "[Train] epoch:86, lr:0.001000, total loss:0.007164\n",
            "[Train] epoch:87, lr:0.001000, total loss:0.006989\n",
            "[Train] epoch:88, lr:0.001000, total loss:0.006807\n",
            "[Train] epoch:89, lr:0.001000, total loss:0.006429\n",
            "[Train] epoch:90, lr:0.001000, total loss:0.006398\n",
            "[Train] epoch:91, lr:0.001000, total loss:0.006637\n",
            "[Train] epoch:92, lr:0.001000, total loss:0.005863\n",
            "[Train] epoch:93, lr:0.001000, total loss:0.005926\n",
            "[Train] epoch:94, lr:0.001000, total loss:0.006450\n",
            "[Train] epoch:95, lr:0.001000, total loss:0.006181\n",
            "[Train] epoch:96, lr:0.001000, total loss:0.006035\n",
            "[Train] epoch:97, lr:0.001000, total loss:0.006255\n",
            "[Train] epoch:98, lr:0.001000, total loss:0.005956\n",
            "[Train] epoch:99, lr:0.001000, total loss:0.005557\n",
            "[Train] epoch:100, lr:0.001000, total loss:0.005632\n",
            "[Train] epoch:101, lr:0.001000, total loss:0.005810\n",
            "[Train] epoch:102, lr:0.001000, total loss:0.005597\n",
            "[Train] epoch:103, lr:0.001000, total loss:0.005582\n",
            "[Train] epoch:104, lr:0.001000, total loss:0.005653\n",
            "[Train] epoch:105, lr:0.001000, total loss:0.005787\n",
            "[Train] epoch:106, lr:0.001000, total loss:0.005362\n",
            "[Train] epoch:107, lr:0.001000, total loss:0.005446\n",
            "[Train] epoch:108, lr:0.001000, total loss:0.005472\n",
            "[Train] epoch:109, lr:0.001000, total loss:0.005283\n",
            "[Train] epoch:110, lr:0.001000, total loss:0.005376\n",
            "[Train] epoch:111, lr:0.001000, total loss:0.005346\n",
            "[Train] epoch:112, lr:0.001000, total loss:0.005197\n",
            "[Train] epoch:113, lr:0.001000, total loss:0.005011\n",
            "[Train] epoch:114, lr:0.001000, total loss:0.004889\n",
            "[Train] epoch:115, lr:0.001000, total loss:0.004916\n",
            "[Train] epoch:116, lr:0.001000, total loss:0.004758\n",
            "[Train] epoch:117, lr:0.001000, total loss:0.004938\n",
            "[Train] epoch:118, lr:0.001000, total loss:0.004790\n",
            "[Train] epoch:119, lr:0.001000, total loss:0.004926\n",
            "[Train] epoch:120, lr:0.001000, total loss:0.004692\n",
            "[Train] epoch:121, lr:0.001000, total loss:0.004688\n",
            "[Train] epoch:122, lr:0.001000, total loss:0.004859\n",
            "[Train] epoch:123, lr:0.001000, total loss:0.004595\n",
            "[Train] epoch:124, lr:0.001000, total loss:0.004561\n",
            "[Train] epoch:125, lr:0.001000, total loss:0.004467\n",
            "[Train] epoch:126, lr:0.001000, total loss:0.004511\n",
            "[Train] epoch:127, lr:0.001000, total loss:0.004758\n",
            "[Train] epoch:128, lr:0.001000, total loss:0.004288\n",
            "[Train] epoch:129, lr:0.001000, total loss:0.004307\n",
            "[Train] epoch:130, lr:0.001000, total loss:0.004549\n",
            "[Train] epoch:131, lr:0.001000, total loss:0.004399\n",
            "[Train] epoch:132, lr:0.001000, total loss:0.004528\n",
            "[Train] epoch:133, lr:0.001000, total loss:0.004397\n",
            "[Train] epoch:134, lr:0.001000, total loss:0.004307\n",
            "[Train] epoch:135, lr:0.001000, total loss:0.004293\n",
            "[Train] epoch:136, lr:0.001000, total loss:0.004023\n",
            "[Train] epoch:137, lr:0.001000, total loss:0.004117\n",
            "[Train] epoch:138, lr:0.001000, total loss:0.004125\n",
            "[Train] epoch:139, lr:0.001000, total loss:0.003799\n",
            "[Train] epoch:140, lr:0.001000, total loss:0.004002\n",
            "[Train] epoch:141, lr:0.001000, total loss:0.004216\n",
            "[Train] epoch:142, lr:0.001000, total loss:0.003992\n",
            "[Train] epoch:143, lr:0.001000, total loss:0.003858\n",
            "[Train] epoch:144, lr:0.001000, total loss:0.003823\n",
            "[Train] epoch:145, lr:0.001000, total loss:0.003939\n",
            "[Train] epoch:146, lr:0.001000, total loss:0.003837\n",
            "[Train] epoch:147, lr:0.001000, total loss:0.003998\n",
            "[Train] epoch:148, lr:0.001000, total loss:0.003741\n",
            "[Train] epoch:149, lr:0.001000, total loss:0.004007\n",
            "[Train] epoch:150, lr:0.001000, total loss:0.003740\n",
            "[Train] epoch:151, lr:0.001000, total loss:0.003728\n",
            "[Train] epoch:152, lr:0.001000, total loss:0.003667\n",
            "[Train] epoch:153, lr:0.001000, total loss:0.003699\n",
            "[Train] epoch:154, lr:0.001000, total loss:0.003639\n",
            "[Train] epoch:155, lr:0.001000, total loss:0.003631\n",
            "[Train] epoch:156, lr:0.001000, total loss:0.003766\n",
            "[Train] epoch:157, lr:0.001000, total loss:0.003647\n",
            "[Train] epoch:158, lr:0.001000, total loss:0.003583\n",
            "[Train] epoch:159, lr:0.001000, total loss:0.003711\n",
            "[Train] epoch:160, lr:0.001000, total loss:0.003850\n",
            "[Train] epoch:161, lr:0.000999, total loss:0.003535\n",
            "[Train] epoch:162, lr:0.000999, total loss:0.003534\n",
            "[Train] epoch:163, lr:0.000999, total loss:0.003489\n",
            "[Train] epoch:164, lr:0.000999, total loss:0.003255\n",
            "[Train] epoch:165, lr:0.000999, total loss:0.003360\n",
            "[Train] epoch:166, lr:0.000999, total loss:0.003232\n",
            "[Train] epoch:167, lr:0.000999, total loss:0.003261\n",
            "[Train] epoch:168, lr:0.000999, total loss:0.003224\n",
            "[Train] epoch:169, lr:0.000999, total loss:0.003413\n",
            "[Train] epoch:170, lr:0.000999, total loss:0.003322\n",
            "[Train] epoch:171, lr:0.000999, total loss:0.003194\n",
            "[Train] epoch:172, lr:0.000999, total loss:0.003295\n",
            "[Train] epoch:173, lr:0.000999, total loss:0.003213\n",
            "[Train] epoch:174, lr:0.000999, total loss:0.003175\n",
            "[Train] epoch:175, lr:0.000999, total loss:0.003206\n",
            "[Train] epoch:176, lr:0.000999, total loss:0.003190\n",
            "[Train] epoch:177, lr:0.000999, total loss:0.003019\n",
            "[Train] epoch:178, lr:0.000999, total loss:0.003249\n",
            "[Train] epoch:179, lr:0.000999, total loss:0.003070\n",
            "[Train] epoch:180, lr:0.000999, total loss:0.003170\n",
            "[Train] epoch:181, lr:0.000999, total loss:0.003064\n",
            "[Train] epoch:182, lr:0.000999, total loss:0.002968\n",
            "[Train] epoch:183, lr:0.000999, total loss:0.003025\n",
            "[Train] epoch:184, lr:0.000999, total loss:0.002969\n",
            "[Train] epoch:185, lr:0.000999, total loss:0.003004\n",
            "[Train] epoch:186, lr:0.000999, total loss:0.002838\n",
            "[Train] epoch:187, lr:0.000999, total loss:0.002965\n",
            "[Train] epoch:188, lr:0.000999, total loss:0.002998\n",
            "[Train] epoch:189, lr:0.000999, total loss:0.002923\n",
            "[Train] epoch:190, lr:0.000999, total loss:0.003089\n",
            "[Train] epoch:191, lr:0.000999, total loss:0.002948\n",
            "[Train] epoch:192, lr:0.000999, total loss:0.002881\n",
            "[Train] epoch:193, lr:0.000999, total loss:0.002968\n",
            "[Train] epoch:194, lr:0.000999, total loss:0.002767\n",
            "[Train] epoch:195, lr:0.000999, total loss:0.003000\n",
            "[Train] epoch:196, lr:0.000999, total loss:0.002853\n",
            "[Train] epoch:197, lr:0.000999, total loss:0.002821\n",
            "[Train] epoch:198, lr:0.000999, total loss:0.002791\n",
            "[Train] epoch:199, lr:0.000999, total loss:0.002839\n",
            "[Train] epoch:200, lr:0.000999, total loss:0.002916\n",
            "[Train] epoch:201, lr:0.000999, total loss:0.002720\n",
            "[Train] epoch:202, lr:0.000999, total loss:0.002803\n",
            "[Train] epoch:203, lr:0.000999, total loss:0.002727\n",
            "[Train] epoch:204, lr:0.000999, total loss:0.002831\n",
            "[Train] epoch:205, lr:0.000999, total loss:0.002690\n",
            "[Train] epoch:206, lr:0.000999, total loss:0.002756\n",
            "[Train] epoch:207, lr:0.000999, total loss:0.002658\n",
            "[Train] epoch:208, lr:0.000999, total loss:0.002628\n",
            "[Train] epoch:209, lr:0.000999, total loss:0.002734\n",
            "[Train] epoch:210, lr:0.000999, total loss:0.002719\n",
            "[Train] epoch:211, lr:0.000999, total loss:0.002662\n",
            "[Train] epoch:212, lr:0.000999, total loss:0.002642\n",
            "[Train] epoch:213, lr:0.000999, total loss:0.002653\n",
            "[Train] epoch:214, lr:0.000999, total loss:0.002522\n",
            "[Train] epoch:215, lr:0.000999, total loss:0.002672\n",
            "[Train] epoch:216, lr:0.000999, total loss:0.002565\n",
            "[Train] epoch:217, lr:0.000999, total loss:0.002522\n",
            "[Train] epoch:218, lr:0.000999, total loss:0.002722\n",
            "[Train] epoch:219, lr:0.000999, total loss:0.002485\n",
            "[Train] epoch:220, lr:0.000999, total loss:0.002606\n",
            "[Train] epoch:221, lr:0.000999, total loss:0.002639\n",
            "[Train] epoch:222, lr:0.000999, total loss:0.002484\n",
            "[Train] epoch:223, lr:0.000999, total loss:0.002702\n",
            "[Train] epoch:224, lr:0.000999, total loss:0.002440\n",
            "[Train] epoch:225, lr:0.000999, total loss:0.002643\n",
            "[Train] epoch:226, lr:0.000999, total loss:0.002565\n",
            "[Train] epoch:227, lr:0.000999, total loss:0.002551\n",
            "[Train] epoch:228, lr:0.000999, total loss:0.002541\n",
            "[Train] epoch:229, lr:0.000999, total loss:0.002447\n",
            "[Train] epoch:230, lr:0.000999, total loss:0.002600\n",
            "[Train] epoch:231, lr:0.000999, total loss:0.002416\n",
            "[Train] epoch:232, lr:0.000999, total loss:0.002416\n",
            "[Train] epoch:233, lr:0.000999, total loss:0.002515\n",
            "[Train] epoch:234, lr:0.000999, total loss:0.002483\n",
            "[Train] epoch:235, lr:0.000999, total loss:0.002527\n",
            "[Train] epoch:236, lr:0.000999, total loss:0.002435\n",
            "[Train] epoch:237, lr:0.000999, total loss:0.002430\n",
            "[Train] epoch:238, lr:0.000999, total loss:0.002377\n",
            "[Train] epoch:239, lr:0.000999, total loss:0.002363\n",
            "[Train] epoch:240, lr:0.000999, total loss:0.002363\n",
            "[Train] epoch:241, lr:0.000999, total loss:0.002340\n",
            "[Train] epoch:242, lr:0.000999, total loss:0.002320\n",
            "[Train] epoch:243, lr:0.000999, total loss:0.002490\n",
            "[Train] epoch:244, lr:0.000999, total loss:0.002325\n",
            "[Train] epoch:245, lr:0.000999, total loss:0.002276\n",
            "[Train] epoch:246, lr:0.000999, total loss:0.002309\n",
            "[Train] epoch:247, lr:0.000999, total loss:0.002236\n",
            "[Train] epoch:248, lr:0.000999, total loss:0.002208\n",
            "[Train] epoch:249, lr:0.000999, total loss:0.002250\n",
            "[Train] epoch:250, lr:0.000999, total loss:0.002336\n",
            "[Train] epoch:251, lr:0.000999, total loss:0.002177\n",
            "[Train] epoch:252, lr:0.000999, total loss:0.002151\n",
            "[Train] epoch:253, lr:0.000999, total loss:0.002197\n",
            "[Train] epoch:254, lr:0.000999, total loss:0.002317\n",
            "[Train] epoch:255, lr:0.000999, total loss:0.002242\n",
            "[Train] epoch:256, lr:0.000999, total loss:0.002302\n",
            "[Train] epoch:257, lr:0.000999, total loss:0.002220\n",
            "[Train] epoch:258, lr:0.000999, total loss:0.002224\n",
            "[Train] epoch:259, lr:0.000999, total loss:0.002486\n",
            "[Train] epoch:260, lr:0.000999, total loss:0.002188\n",
            "[Train] epoch:261, lr:0.000999, total loss:0.002265\n",
            "[Train] epoch:262, lr:0.000999, total loss:0.002172\n",
            "[Train] epoch:263, lr:0.000999, total loss:0.002164\n",
            "[Train] epoch:264, lr:0.000999, total loss:0.002184\n",
            "[Train] epoch:265, lr:0.000999, total loss:0.002092\n",
            "[Train] epoch:266, lr:0.000999, total loss:0.002156\n",
            "[Train] epoch:267, lr:0.000999, total loss:0.002069\n",
            "[Train] epoch:268, lr:0.000999, total loss:0.002123\n",
            "[Train] epoch:269, lr:0.000999, total loss:0.002061\n",
            "[Train] epoch:270, lr:0.000999, total loss:0.002105\n",
            "[Train] epoch:271, lr:0.000998, total loss:0.002163\n",
            "[Train] epoch:272, lr:0.000998, total loss:0.002224\n",
            "[Train] epoch:273, lr:0.000998, total loss:0.002111\n",
            "[Train] epoch:274, lr:0.000998, total loss:0.002062\n",
            "[Train] epoch:275, lr:0.000998, total loss:0.002064\n",
            "[Train] epoch:276, lr:0.000998, total loss:0.002084\n",
            "[Train] epoch:277, lr:0.000998, total loss:0.002070\n",
            "[Train] epoch:278, lr:0.000998, total loss:0.002083\n",
            "[Train] epoch:279, lr:0.000998, total loss:0.002080\n",
            "[Train] epoch:280, lr:0.000998, total loss:0.002000\n",
            "[Train] epoch:281, lr:0.000998, total loss:0.002182\n",
            "[Train] epoch:282, lr:0.000998, total loss:0.002124\n",
            "[Train] epoch:283, lr:0.000998, total loss:0.002138\n",
            "[Train] epoch:284, lr:0.000998, total loss:0.002103\n",
            "[Train] epoch:285, lr:0.000998, total loss:0.002000\n",
            "[Train] epoch:286, lr:0.000998, total loss:0.002093\n",
            "[Train] epoch:287, lr:0.000998, total loss:0.001975\n",
            "[Train] epoch:288, lr:0.000998, total loss:0.002028\n",
            "[Train] epoch:289, lr:0.000998, total loss:0.002036\n",
            "[Train] epoch:290, lr:0.000998, total loss:0.002086\n",
            "[Train] epoch:291, lr:0.000998, total loss:0.002078\n",
            "[Train] epoch:292, lr:0.000998, total loss:0.002075\n",
            "[Train] epoch:293, lr:0.000998, total loss:0.002032\n",
            "[Train] epoch:294, lr:0.000998, total loss:0.002054\n",
            "[Train] epoch:295, lr:0.000998, total loss:0.002009\n",
            "[Train] epoch:296, lr:0.000998, total loss:0.001930\n",
            "[Train] epoch:297, lr:0.000998, total loss:0.001931\n",
            "[Train] epoch:298, lr:0.000998, total loss:0.001974\n",
            "[Train] epoch:299, lr:0.000998, total loss:0.002010\n",
            "[Train] epoch:300, lr:0.000998, total loss:0.002008\n",
            "[Train] epoch:301, lr:0.000998, total loss:0.002011\n",
            "[Train] epoch:302, lr:0.000998, total loss:0.001942\n",
            "[Train] epoch:303, lr:0.000998, total loss:0.001872\n",
            "[Train] epoch:304, lr:0.000998, total loss:0.001876\n",
            "[Train] epoch:305, lr:0.000998, total loss:0.001902\n",
            "[Train] epoch:306, lr:0.000998, total loss:0.001998\n",
            "[Train] epoch:307, lr:0.000998, total loss:0.001906\n",
            "[Train] epoch:308, lr:0.000998, total loss:0.001923\n",
            "[Train] epoch:309, lr:0.000998, total loss:0.001838\n",
            "[Train] epoch:310, lr:0.000998, total loss:0.001874\n",
            "[Train] epoch:311, lr:0.000998, total loss:0.001923\n",
            "[Train] epoch:312, lr:0.000998, total loss:0.001913\n",
            "[Train] epoch:313, lr:0.000998, total loss:0.001896\n",
            "[Train] epoch:314, lr:0.000998, total loss:0.001861\n",
            "[Train] epoch:315, lr:0.000998, total loss:0.001923\n",
            "[Train] epoch:316, lr:0.000998, total loss:0.001913\n",
            "[Train] epoch:317, lr:0.000998, total loss:0.001879\n",
            "[Train] epoch:318, lr:0.000998, total loss:0.001819\n",
            "[Train] epoch:319, lr:0.000998, total loss:0.001835\n",
            "[Train] epoch:320, lr:0.000998, total loss:0.001741\n",
            "[Train] epoch:321, lr:0.000998, total loss:0.001755\n",
            "[Train] epoch:322, lr:0.000998, total loss:0.001849\n",
            "[Train] epoch:323, lr:0.000998, total loss:0.001853\n",
            "[Train] epoch:324, lr:0.000998, total loss:0.001885\n",
            "[Train] epoch:325, lr:0.000998, total loss:0.001771\n",
            "[Train] epoch:326, lr:0.000998, total loss:0.001761\n",
            "[Train] epoch:327, lr:0.000998, total loss:0.001773\n",
            "[Train] epoch:328, lr:0.000998, total loss:0.001800\n",
            "[Train] epoch:329, lr:0.000998, total loss:0.001751\n",
            "[Train] epoch:330, lr:0.000998, total loss:0.001809\n",
            "[Train] epoch:331, lr:0.000998, total loss:0.001808\n",
            "[Train] epoch:332, lr:0.000998, total loss:0.001791\n",
            "[Train] epoch:333, lr:0.000998, total loss:0.001834\n",
            "[Train] epoch:334, lr:0.000998, total loss:0.001701\n",
            "[Train] epoch:335, lr:0.000998, total loss:0.001781\n",
            "[Train] epoch:336, lr:0.000998, total loss:0.001793\n",
            "[Train] epoch:337, lr:0.000998, total loss:0.001784\n",
            "[Train] epoch:338, lr:0.000998, total loss:0.001721\n",
            "[Train] epoch:339, lr:0.000998, total loss:0.001788\n",
            "[Train] epoch:340, lr:0.000998, total loss:0.001730\n",
            "[Train] epoch:341, lr:0.000998, total loss:0.001851\n",
            "[Train] epoch:342, lr:0.000998, total loss:0.001780\n",
            "[Train] epoch:343, lr:0.000998, total loss:0.001689\n",
            "[Train] epoch:344, lr:0.000998, total loss:0.001738\n",
            "[Train] epoch:345, lr:0.000998, total loss:0.001676\n",
            "[Train] epoch:346, lr:0.000998, total loss:0.001807\n",
            "[Train] epoch:347, lr:0.000997, total loss:0.001710\n",
            "[Train] epoch:348, lr:0.000997, total loss:0.001711\n",
            "[Train] epoch:349, lr:0.000997, total loss:0.001690\n",
            "[Train] epoch:350, lr:0.000997, total loss:0.001697\n",
            "[Train] epoch:351, lr:0.000997, total loss:0.001718\n",
            "[Train] epoch:352, lr:0.000997, total loss:0.001711\n",
            "[Train] epoch:353, lr:0.000997, total loss:0.001714\n",
            "[Train] epoch:354, lr:0.000997, total loss:0.001596\n",
            "[Train] epoch:355, lr:0.000997, total loss:0.001669\n",
            "[Train] epoch:356, lr:0.000997, total loss:0.001742\n",
            "[Train] epoch:357, lr:0.000997, total loss:0.001697\n",
            "[Train] epoch:358, lr:0.000997, total loss:0.001671\n",
            "[Train] epoch:359, lr:0.000997, total loss:0.001594\n",
            "[Train] epoch:360, lr:0.000997, total loss:0.001770\n",
            "[Train] epoch:361, lr:0.000997, total loss:0.001648\n",
            "[Train] epoch:362, lr:0.000997, total loss:0.001585\n",
            "[Train] epoch:363, lr:0.000997, total loss:0.001551\n",
            "[Train] epoch:364, lr:0.000997, total loss:0.001655\n",
            "[Train] epoch:365, lr:0.000997, total loss:0.001560\n",
            "[Train] epoch:366, lr:0.000997, total loss:0.001623\n",
            "[Train] epoch:367, lr:0.000997, total loss:0.001627\n",
            "[Train] epoch:368, lr:0.000997, total loss:0.001568\n",
            "[Train] epoch:369, lr:0.000997, total loss:0.001575\n",
            "[Train] epoch:370, lr:0.000997, total loss:0.001656\n",
            "[Train] epoch:371, lr:0.000997, total loss:0.001609\n",
            "[Train] epoch:372, lr:0.000997, total loss:0.001664\n",
            "[Train] epoch:373, lr:0.000997, total loss:0.001508\n",
            "[Train] epoch:374, lr:0.000997, total loss:0.001577\n",
            "[Train] epoch:375, lr:0.000997, total loss:0.001703\n",
            "[Train] epoch:376, lr:0.000997, total loss:0.001705\n",
            "[Train] epoch:377, lr:0.000997, total loss:0.001512\n",
            "[Train] epoch:378, lr:0.000997, total loss:0.001539\n",
            "[Train] epoch:379, lr:0.000997, total loss:0.001570\n",
            "[Train] epoch:380, lr:0.000997, total loss:0.001532\n",
            "[Train] epoch:381, lr:0.000997, total loss:0.001621\n",
            "[Train] epoch:382, lr:0.000997, total loss:0.001598\n",
            "[Train] epoch:383, lr:0.000997, total loss:0.001604\n",
            "[Train] epoch:384, lr:0.000997, total loss:0.001566\n",
            "[Train] epoch:385, lr:0.000997, total loss:0.001532\n",
            "[Train] epoch:386, lr:0.000997, total loss:0.001609\n",
            "[Train] epoch:387, lr:0.000997, total loss:0.001538\n",
            "[Train] epoch:388, lr:0.000997, total loss:0.001625\n",
            "[Train] epoch:389, lr:0.000997, total loss:0.001580\n",
            "[Train] epoch:390, lr:0.000997, total loss:0.001508\n",
            "[Train] epoch:391, lr:0.000997, total loss:0.001525\n",
            "[Train] epoch:392, lr:0.000997, total loss:0.001545\n",
            "[Train] epoch:393, lr:0.000997, total loss:0.001500\n",
            "[Train] epoch:394, lr:0.000997, total loss:0.001538\n",
            "[Train] epoch:395, lr:0.000997, total loss:0.001456\n",
            "[Train] epoch:396, lr:0.000997, total loss:0.001529\n",
            "[Train] epoch:397, lr:0.000997, total loss:0.001513\n",
            "[Train] epoch:398, lr:0.000997, total loss:0.001469\n",
            "[Train] epoch:399, lr:0.000997, total loss:0.001551\n",
            "[Train] epoch:400, lr:0.000997, total loss:0.001521\n",
            "[Train] epoch:401, lr:0.000997, total loss:0.001456\n",
            "[Train] epoch:402, lr:0.000997, total loss:0.001460\n",
            "[Train] epoch:403, lr:0.000997, total loss:0.001493\n",
            "[Train] epoch:404, lr:0.000997, total loss:0.001528\n",
            "[Train] epoch:405, lr:0.000997, total loss:0.001469\n",
            "[Train] epoch:406, lr:0.000997, total loss:0.001460\n",
            "[Train] epoch:407, lr:0.000997, total loss:0.001506\n",
            "[Train] epoch:408, lr:0.000996, total loss:0.001436\n",
            "[Train] epoch:409, lr:0.000996, total loss:0.001496\n",
            "[Train] epoch:410, lr:0.000996, total loss:0.001510\n",
            "[Train] epoch:411, lr:0.000996, total loss:0.001539\n",
            "[Train] epoch:412, lr:0.000996, total loss:0.001410\n",
            "[Train] epoch:413, lr:0.000996, total loss:0.001419\n",
            "[Train] epoch:414, lr:0.000996, total loss:0.001515\n",
            "[Train] epoch:415, lr:0.000996, total loss:0.001467\n",
            "[Train] epoch:416, lr:0.000996, total loss:0.001457\n",
            "[Train] epoch:417, lr:0.000996, total loss:0.001423\n",
            "[Train] epoch:418, lr:0.000996, total loss:0.001435\n",
            "[Train] epoch:419, lr:0.000996, total loss:0.001434\n",
            "[Train] epoch:420, lr:0.000996, total loss:0.001333\n",
            "[Train] epoch:421, lr:0.000996, total loss:0.001425\n",
            "[Train] epoch:422, lr:0.000996, total loss:0.001436\n",
            "[Train] epoch:423, lr:0.000996, total loss:0.001444\n",
            "[Train] epoch:424, lr:0.000996, total loss:0.001444\n",
            "[Train] epoch:425, lr:0.000996, total loss:0.001461\n",
            "[Train] epoch:426, lr:0.000996, total loss:0.001459\n",
            "[Train] epoch:427, lr:0.000996, total loss:0.001401\n",
            "[Train] epoch:428, lr:0.000996, total loss:0.001416\n",
            "[Train] epoch:429, lr:0.000996, total loss:0.001455\n",
            "[Train] epoch:430, lr:0.000996, total loss:0.001417\n",
            "[Train] epoch:431, lr:0.000996, total loss:0.001377\n",
            "[Train] epoch:432, lr:0.000996, total loss:0.001390\n",
            "[Train] epoch:433, lr:0.000996, total loss:0.001455\n",
            "[Train] epoch:434, lr:0.000996, total loss:0.001409\n",
            "[Train] epoch:435, lr:0.000996, total loss:0.001412\n",
            "[Train] epoch:436, lr:0.000996, total loss:0.001439\n",
            "[Train] epoch:437, lr:0.000996, total loss:0.001363\n",
            "[Train] epoch:438, lr:0.000996, total loss:0.001374\n",
            "[Train] epoch:439, lr:0.000996, total loss:0.001421\n",
            "[Train] epoch:440, lr:0.000996, total loss:0.001311\n",
            "[Train] epoch:441, lr:0.000996, total loss:0.001326\n",
            "[Train] epoch:442, lr:0.000996, total loss:0.001456\n",
            "[Train] epoch:443, lr:0.000996, total loss:0.001364\n",
            "[Train] epoch:444, lr:0.000996, total loss:0.001417\n",
            "[Train] epoch:445, lr:0.000996, total loss:0.001401\n",
            "[Train] epoch:446, lr:0.000996, total loss:0.001449\n",
            "[Train] epoch:447, lr:0.000996, total loss:0.001350\n",
            "[Train] epoch:448, lr:0.000996, total loss:0.001382\n",
            "[Train] epoch:449, lr:0.000996, total loss:0.001332\n",
            "[Train] epoch:450, lr:0.000996, total loss:0.001423\n",
            "[Train] epoch:451, lr:0.000996, total loss:0.001359\n",
            "[Train] epoch:452, lr:0.000996, total loss:0.001408\n",
            "[Train] epoch:453, lr:0.000996, total loss:0.001345\n",
            "[Train] epoch:454, lr:0.000996, total loss:0.001401\n",
            "[Train] epoch:455, lr:0.000996, total loss:0.001399\n",
            "[Train] epoch:456, lr:0.000996, total loss:0.001359\n",
            "[Train] epoch:457, lr:0.000996, total loss:0.001386\n",
            "[Train] epoch:458, lr:0.000996, total loss:0.001268\n",
            "[Train] epoch:459, lr:0.000996, total loss:0.001357\n",
            "[Train] epoch:460, lr:0.000996, total loss:0.001286\n",
            "[Train] epoch:461, lr:0.000996, total loss:0.001326\n",
            "[Train] epoch:462, lr:0.000995, total loss:0.001367\n",
            "[Train] epoch:463, lr:0.000995, total loss:0.001323\n",
            "[Train] epoch:464, lr:0.000995, total loss:0.001403\n",
            "[Train] epoch:465, lr:0.000995, total loss:0.001400\n",
            "[Train] epoch:466, lr:0.000995, total loss:0.001339\n",
            "[Train] epoch:467, lr:0.000995, total loss:0.001348\n",
            "[Train] epoch:468, lr:0.000995, total loss:0.001202\n",
            "[Train] epoch:469, lr:0.000995, total loss:0.001281\n",
            "[Train] epoch:470, lr:0.000995, total loss:0.001253\n",
            "[Train] epoch:471, lr:0.000995, total loss:0.001317\n",
            "[Train] epoch:472, lr:0.000995, total loss:0.001273\n",
            "[Train] epoch:473, lr:0.000995, total loss:0.001272\n",
            "[Train] epoch:474, lr:0.000995, total loss:0.001234\n",
            "[Train] epoch:475, lr:0.000995, total loss:0.001262\n",
            "[Train] epoch:476, lr:0.000995, total loss:0.001327\n",
            "[Train] epoch:477, lr:0.000995, total loss:0.001316\n",
            "[Train] epoch:478, lr:0.000995, total loss:0.001319\n",
            "[Train] epoch:479, lr:0.000995, total loss:0.001313\n",
            "[Train] epoch:480, lr:0.000995, total loss:0.001233\n",
            "[Train] epoch:481, lr:0.000995, total loss:0.001359\n",
            "[Train] epoch:482, lr:0.000995, total loss:0.001259\n",
            "[Train] epoch:483, lr:0.000995, total loss:0.001283\n",
            "[Train] epoch:484, lr:0.000995, total loss:0.001365\n",
            "[Train] epoch:485, lr:0.000995, total loss:0.001268\n",
            "[Train] epoch:486, lr:0.000995, total loss:0.001335\n",
            "[Train] epoch:487, lr:0.000995, total loss:0.001294\n",
            "[Train] epoch:488, lr:0.000995, total loss:0.001216\n",
            "[Train] epoch:489, lr:0.000995, total loss:0.001273\n",
            "[Train] epoch:490, lr:0.000995, total loss:0.001287\n",
            "[Train] epoch:491, lr:0.000995, total loss:0.001241\n",
            "[Train] epoch:492, lr:0.000995, total loss:0.001284\n",
            "[Train] epoch:493, lr:0.000995, total loss:0.001295\n",
            "[Train] epoch:494, lr:0.000995, total loss:0.001298\n",
            "[Train] epoch:495, lr:0.000995, total loss:0.001260\n",
            "[Train] epoch:496, lr:0.000995, total loss:0.001272\n",
            "[Train] epoch:497, lr:0.000995, total loss:0.001184\n",
            "[Train] epoch:498, lr:0.000995, total loss:0.001244\n",
            "[Train] epoch:499, lr:0.000995, total loss:0.001175\n",
            "[Train] epoch:500, lr:0.000995, total loss:0.001288\n",
            "[Train] epoch:501, lr:0.000995, total loss:0.001231\n",
            "[Train] epoch:502, lr:0.000995, total loss:0.001292\n",
            "[Train] epoch:503, lr:0.000995, total loss:0.001210\n",
            "[Train] epoch:504, lr:0.000995, total loss:0.001207\n",
            "[Train] epoch:505, lr:0.000995, total loss:0.001194\n",
            "[Train] epoch:506, lr:0.000995, total loss:0.001179\n",
            "[Train] epoch:507, lr:0.000995, total loss:0.001194\n",
            "[Train] epoch:508, lr:0.000995, total loss:0.001157\n",
            "[Train] epoch:509, lr:0.000994, total loss:0.001208\n",
            "[Train] epoch:510, lr:0.000994, total loss:0.001243\n",
            "[Train] epoch:511, lr:0.000994, total loss:0.001228\n",
            "[Train] epoch:512, lr:0.000994, total loss:0.001264\n",
            "[Train] epoch:513, lr:0.000994, total loss:0.001218\n",
            "[Train] epoch:514, lr:0.000994, total loss:0.001179\n",
            "[Train] epoch:515, lr:0.000994, total loss:0.001265\n",
            "[Train] epoch:516, lr:0.000994, total loss:0.001192\n",
            "[Train] epoch:517, lr:0.000994, total loss:0.001224\n",
            "[Train] epoch:518, lr:0.000994, total loss:0.001260\n",
            "[Train] epoch:519, lr:0.000994, total loss:0.001209\n",
            "[Train] epoch:520, lr:0.000994, total loss:0.001210\n",
            "[Train] epoch:521, lr:0.000994, total loss:0.001203\n",
            "[Train] epoch:522, lr:0.000994, total loss:0.001221\n",
            "[Train] epoch:523, lr:0.000994, total loss:0.001228\n",
            "[Train] epoch:524, lr:0.000994, total loss:0.001215\n",
            "[Train] epoch:525, lr:0.000994, total loss:0.001178\n",
            "[Train] epoch:526, lr:0.000994, total loss:0.001203\n",
            "[Train] epoch:527, lr:0.000994, total loss:0.001158\n",
            "[Train] epoch:528, lr:0.000994, total loss:0.001176\n",
            "[Train] epoch:529, lr:0.000994, total loss:0.001231\n",
            "[Train] epoch:530, lr:0.000994, total loss:0.001176\n",
            "[Train] epoch:531, lr:0.000994, total loss:0.001201\n",
            "[Train] epoch:532, lr:0.000994, total loss:0.001178\n",
            "[Train] epoch:533, lr:0.000994, total loss:0.001135\n",
            "[Train] epoch:534, lr:0.000994, total loss:0.001185\n",
            "[Train] epoch:535, lr:0.000994, total loss:0.001132\n",
            "[Train] epoch:536, lr:0.000994, total loss:0.001156\n",
            "[Train] epoch:537, lr:0.000994, total loss:0.001138\n",
            "[Train] epoch:538, lr:0.000994, total loss:0.001182\n",
            "[Train] epoch:539, lr:0.000994, total loss:0.001217\n",
            "[Train] epoch:540, lr:0.000994, total loss:0.001164\n",
            "[Train] epoch:541, lr:0.000994, total loss:0.001109\n",
            "[Train] epoch:542, lr:0.000994, total loss:0.001169\n",
            "[Train] epoch:543, lr:0.000994, total loss:0.001145\n",
            "[Train] epoch:544, lr:0.000994, total loss:0.001165\n",
            "[Train] epoch:545, lr:0.000994, total loss:0.001092\n",
            "[Train] epoch:546, lr:0.000994, total loss:0.001124\n",
            "[Train] epoch:547, lr:0.000994, total loss:0.001168\n",
            "[Train] epoch:548, lr:0.000994, total loss:0.001079\n",
            "[Train] epoch:549, lr:0.000994, total loss:0.001093\n",
            "[Train] epoch:550, lr:0.000994, total loss:0.001123\n",
            "[Train] epoch:551, lr:0.000994, total loss:0.001093\n",
            "[Train] epoch:552, lr:0.000994, total loss:0.001149\n",
            "[Train] epoch:553, lr:0.000993, total loss:0.001066\n",
            "[Train] epoch:554, lr:0.000993, total loss:0.001037\n",
            "[Train] epoch:555, lr:0.000993, total loss:0.001057\n",
            "[Train] epoch:556, lr:0.000993, total loss:0.001198\n",
            "[Train] epoch:557, lr:0.000993, total loss:0.001134\n",
            "[Train] epoch:558, lr:0.000993, total loss:0.001125\n",
            "[Train] epoch:559, lr:0.000993, total loss:0.001185\n",
            "[Train] epoch:560, lr:0.000993, total loss:0.001073\n",
            "[Train] epoch:561, lr:0.000993, total loss:0.001102\n",
            "[Train] epoch:562, lr:0.000993, total loss:0.001114\n",
            "[Train] epoch:563, lr:0.000993, total loss:0.001114\n",
            "[Train] epoch:564, lr:0.000993, total loss:0.001204\n",
            "[Train] epoch:565, lr:0.000993, total loss:0.001100\n",
            "[Train] epoch:566, lr:0.000993, total loss:0.001044\n",
            "[Train] epoch:567, lr:0.000993, total loss:0.001092\n",
            "[Train] epoch:568, lr:0.000993, total loss:0.001125\n",
            "[Train] epoch:569, lr:0.000993, total loss:0.001090\n",
            "[Train] epoch:570, lr:0.000993, total loss:0.000997\n",
            "[Train] epoch:571, lr:0.000993, total loss:0.001083\n",
            "[Train] epoch:572, lr:0.000993, total loss:0.001098\n",
            "[Train] epoch:573, lr:0.000993, total loss:0.001096\n",
            "[Train] epoch:574, lr:0.000993, total loss:0.001075\n",
            "[Train] epoch:575, lr:0.000993, total loss:0.001030\n",
            "[Train] epoch:576, lr:0.000993, total loss:0.001119\n",
            "[Train] epoch:577, lr:0.000993, total loss:0.001021\n",
            "[Train] epoch:578, lr:0.000993, total loss:0.001032\n",
            "[Train] epoch:579, lr:0.000993, total loss:0.001084\n",
            "[Train] epoch:580, lr:0.000993, total loss:0.001083\n",
            "[Train] epoch:581, lr:0.000993, total loss:0.001110\n",
            "[Train] epoch:582, lr:0.000993, total loss:0.001031\n",
            "[Train] epoch:583, lr:0.000993, total loss:0.000997\n",
            "[Train] epoch:584, lr:0.000993, total loss:0.001000\n",
            "[Train] epoch:585, lr:0.000993, total loss:0.001035\n",
            "[Train] epoch:586, lr:0.000993, total loss:0.001062\n",
            "[Train] epoch:587, lr:0.000993, total loss:0.001032\n",
            "[Train] epoch:588, lr:0.000993, total loss:0.001047\n",
            "[Train] epoch:589, lr:0.000993, total loss:0.001044\n",
            "[Train] epoch:590, lr:0.000993, total loss:0.000994\n",
            "[Train] epoch:591, lr:0.000993, total loss:0.001034\n",
            "[Train] epoch:592, lr:0.000993, total loss:0.001047\n",
            "[Train] epoch:593, lr:0.000992, total loss:0.001054\n",
            "[Train] epoch:594, lr:0.000992, total loss:0.001108\n",
            "[Train] epoch:595, lr:0.000992, total loss:0.001070\n",
            "[Train] epoch:596, lr:0.000992, total loss:0.001014\n",
            "[Train] epoch:597, lr:0.000992, total loss:0.001073\n",
            "[Train] epoch:598, lr:0.000992, total loss:0.001054\n",
            "[Train] epoch:599, lr:0.000992, total loss:0.001089\n",
            "[Train] epoch:600, lr:0.000992, total loss:0.001014\n",
            "[Train] epoch:601, lr:0.000992, total loss:0.001031\n",
            "[Train] epoch:602, lr:0.000992, total loss:0.001039\n",
            "[Train] epoch:603, lr:0.000992, total loss:0.001089\n",
            "[Train] epoch:604, lr:0.000992, total loss:0.001040\n",
            "[Train] epoch:605, lr:0.000992, total loss:0.001078\n",
            "[Train] epoch:606, lr:0.000992, total loss:0.001072\n",
            "[Train] epoch:607, lr:0.000992, total loss:0.001027\n",
            "[Train] epoch:608, lr:0.000992, total loss:0.001045\n",
            "[Train] epoch:609, lr:0.000992, total loss:0.001032\n",
            "[Train] epoch:610, lr:0.000992, total loss:0.001001\n",
            "[Train] epoch:611, lr:0.000992, total loss:0.000925\n",
            "[Train] epoch:612, lr:0.000992, total loss:0.001030\n",
            "[Train] epoch:613, lr:0.000992, total loss:0.000963\n",
            "[Train] epoch:614, lr:0.000992, total loss:0.001021\n",
            "[Train] epoch:615, lr:0.000992, total loss:0.000980\n",
            "[Train] epoch:616, lr:0.000992, total loss:0.000971\n",
            "[Train] epoch:617, lr:0.000992, total loss:0.001023\n",
            "[Train] epoch:618, lr:0.000992, total loss:0.001128\n",
            "[Train] epoch:619, lr:0.000992, total loss:0.001031\n",
            "[Train] epoch:620, lr:0.000992, total loss:0.001002\n",
            "[Train] epoch:621, lr:0.000992, total loss:0.001055\n",
            "[Train] epoch:622, lr:0.000992, total loss:0.001030\n",
            "[Train] epoch:623, lr:0.000992, total loss:0.001024\n",
            "[Train] epoch:624, lr:0.000992, total loss:0.001007\n",
            "[Train] epoch:625, lr:0.000992, total loss:0.000937\n",
            "[Train] epoch:626, lr:0.000992, total loss:0.001009\n",
            "[Train] epoch:627, lr:0.000992, total loss:0.001046\n",
            "[Train] epoch:628, lr:0.000992, total loss:0.001058\n",
            "[Train] epoch:629, lr:0.000992, total loss:0.001000\n",
            "[Train] epoch:630, lr:0.000992, total loss:0.001004\n",
            "[Train] epoch:631, lr:0.000991, total loss:0.000997\n",
            "[Train] epoch:632, lr:0.000991, total loss:0.000983\n",
            "[Train] epoch:633, lr:0.000991, total loss:0.000981\n",
            "[Train] epoch:634, lr:0.000991, total loss:0.001030\n",
            "[Train] epoch:635, lr:0.000991, total loss:0.000997\n",
            "[Train] epoch:636, lr:0.000991, total loss:0.001000\n",
            "[Train] epoch:637, lr:0.000991, total loss:0.001019\n",
            "[Train] epoch:638, lr:0.000991, total loss:0.000963\n",
            "[Train] epoch:639, lr:0.000991, total loss:0.000896\n",
            "[Train] epoch:640, lr:0.000991, total loss:0.001078\n",
            "[Train] epoch:641, lr:0.000991, total loss:0.000931\n",
            "[Train] epoch:642, lr:0.000991, total loss:0.000960\n",
            "[Train] epoch:643, lr:0.000991, total loss:0.000992\n",
            "[Train] epoch:644, lr:0.000991, total loss:0.000967\n",
            "[Train] epoch:645, lr:0.000991, total loss:0.000972\n",
            "[Train] epoch:646, lr:0.000991, total loss:0.000894\n",
            "[Train] epoch:647, lr:0.000991, total loss:0.000969\n",
            "[Train] epoch:648, lr:0.000991, total loss:0.000947\n",
            "[Train] epoch:649, lr:0.000991, total loss:0.000961\n",
            "[Train] epoch:650, lr:0.000991, total loss:0.000951\n",
            "[Train] epoch:651, lr:0.000991, total loss:0.000926\n",
            "[Train] epoch:652, lr:0.000991, total loss:0.000928\n",
            "[Train] epoch:653, lr:0.000991, total loss:0.000980\n",
            "[Train] epoch:654, lr:0.000991, total loss:0.000952\n",
            "[Train] epoch:655, lr:0.000991, total loss:0.000956\n",
            "[Train] epoch:656, lr:0.000991, total loss:0.000982\n",
            "[Train] epoch:657, lr:0.000991, total loss:0.000955\n",
            "[Train] epoch:658, lr:0.000991, total loss:0.000905\n",
            "[Train] epoch:659, lr:0.000991, total loss:0.000924\n",
            "[Train] epoch:660, lr:0.000991, total loss:0.000989\n",
            "[Train] epoch:661, lr:0.000991, total loss:0.000937\n",
            "[Train] epoch:662, lr:0.000991, total loss:0.000897\n",
            "[Train] epoch:663, lr:0.000991, total loss:0.000945\n",
            "[Train] epoch:664, lr:0.000991, total loss:0.000940\n",
            "[Train] epoch:665, lr:0.000991, total loss:0.000900\n",
            "[Train] epoch:666, lr:0.000990, total loss:0.000942\n",
            "[Train] epoch:667, lr:0.000990, total loss:0.000946\n",
            "[Train] epoch:668, lr:0.000990, total loss:0.000944\n",
            "[Train] epoch:669, lr:0.000990, total loss:0.000908\n",
            "[Train] epoch:670, lr:0.000990, total loss:0.000923\n",
            "[Train] epoch:671, lr:0.000990, total loss:0.000958\n",
            "[Train] epoch:672, lr:0.000990, total loss:0.000910\n",
            "[Train] epoch:673, lr:0.000990, total loss:0.000909\n",
            "[Train] epoch:674, lr:0.000990, total loss:0.000945\n",
            "[Train] epoch:675, lr:0.000990, total loss:0.000994\n",
            "[Train] epoch:676, lr:0.000990, total loss:0.000935\n",
            "[Train] epoch:677, lr:0.000990, total loss:0.000903\n",
            "[Train] epoch:678, lr:0.000990, total loss:0.000940\n",
            "[Train] epoch:679, lr:0.000990, total loss:0.000898\n",
            "[Train] epoch:680, lr:0.000990, total loss:0.000856\n",
            "[Train] epoch:681, lr:0.000990, total loss:0.000826\n",
            "[Train] epoch:682, lr:0.000990, total loss:0.000912\n",
            "[Train] epoch:683, lr:0.000990, total loss:0.000890\n",
            "[Train] epoch:684, lr:0.000990, total loss:0.000884\n",
            "[Train] epoch:685, lr:0.000990, total loss:0.000950\n",
            "[Train] epoch:686, lr:0.000990, total loss:0.000879\n",
            "[Train] epoch:687, lr:0.000990, total loss:0.000917\n",
            "[Train] epoch:688, lr:0.000990, total loss:0.000867\n",
            "[Train] epoch:689, lr:0.000990, total loss:0.000924\n",
            "[Train] epoch:690, lr:0.000990, total loss:0.000853\n",
            "[Train] epoch:691, lr:0.000990, total loss:0.000892\n",
            "[Train] epoch:692, lr:0.000990, total loss:0.000871\n",
            "[Train] epoch:693, lr:0.000990, total loss:0.000874\n",
            "[Train] epoch:694, lr:0.000990, total loss:0.000833\n",
            "[Train] epoch:695, lr:0.000990, total loss:0.000871\n",
            "[Train] epoch:696, lr:0.000990, total loss:0.000866\n",
            "[Train] epoch:697, lr:0.000990, total loss:0.000864\n",
            "[Train] epoch:698, lr:0.000990, total loss:0.000888\n",
            "[Train] epoch:699, lr:0.000990, total loss:0.000871\n",
            "[Train] epoch:700, lr:0.000989, total loss:0.000850\n",
            "[Train] epoch:701, lr:0.000989, total loss:0.000848\n",
            "[Train] epoch:702, lr:0.000989, total loss:0.000900\n",
            "[Train] epoch:703, lr:0.000989, total loss:0.000872\n",
            "[Train] epoch:704, lr:0.000989, total loss:0.000877\n",
            "[Train] epoch:705, lr:0.000989, total loss:0.000855\n",
            "[Train] epoch:706, lr:0.000989, total loss:0.000864\n",
            "[Train] epoch:707, lr:0.000989, total loss:0.000840\n",
            "[Train] epoch:708, lr:0.000989, total loss:0.000910\n",
            "[Train] epoch:709, lr:0.000989, total loss:0.000829\n",
            "[Train] epoch:710, lr:0.000989, total loss:0.000906\n",
            "[Train] epoch:711, lr:0.000989, total loss:0.000860\n",
            "[Train] epoch:712, lr:0.000989, total loss:0.000880\n",
            "[Train] epoch:713, lr:0.000989, total loss:0.000893\n",
            "[Train] epoch:714, lr:0.000989, total loss:0.000852\n",
            "[Train] epoch:715, lr:0.000989, total loss:0.000908\n",
            "[Train] epoch:716, lr:0.000989, total loss:0.000861\n",
            "[Train] epoch:717, lr:0.000989, total loss:0.000892\n",
            "[Train] epoch:718, lr:0.000989, total loss:0.000820\n",
            "[Train] epoch:719, lr:0.000989, total loss:0.000846\n",
            "[Train] epoch:720, lr:0.000989, total loss:0.000813\n",
            "[Train] epoch:721, lr:0.000989, total loss:0.000867\n",
            "[Train] epoch:722, lr:0.000989, total loss:0.000894\n",
            "[Train] epoch:723, lr:0.000989, total loss:0.000821\n",
            "[Train] epoch:724, lr:0.000989, total loss:0.000855\n",
            "[Train] epoch:725, lr:0.000989, total loss:0.000827\n",
            "[Train] epoch:726, lr:0.000989, total loss:0.000836\n",
            "[Train] epoch:727, lr:0.000989, total loss:0.000859\n",
            "[Train] epoch:728, lr:0.000989, total loss:0.000863\n",
            "[Train] epoch:729, lr:0.000989, total loss:0.000838\n",
            "[Train] epoch:730, lr:0.000989, total loss:0.000887\n",
            "[Train] epoch:731, lr:0.000989, total loss:0.000847\n",
            "[Train] epoch:732, lr:0.000988, total loss:0.000857\n",
            "[Train] epoch:733, lr:0.000988, total loss:0.000830\n",
            "[Train] epoch:734, lr:0.000988, total loss:0.000867\n",
            "[Train] epoch:735, lr:0.000988, total loss:0.000823\n",
            "[Train] epoch:736, lr:0.000988, total loss:0.000807\n",
            "[Train] epoch:737, lr:0.000988, total loss:0.000816\n",
            "[Train] epoch:738, lr:0.000988, total loss:0.000810\n",
            "[Train] epoch:739, lr:0.000988, total loss:0.000802\n",
            "[Train] epoch:740, lr:0.000988, total loss:0.000855\n",
            "[Train] epoch:741, lr:0.000988, total loss:0.000803\n",
            "[Train] epoch:742, lr:0.000988, total loss:0.000815\n",
            "[Train] epoch:743, lr:0.000988, total loss:0.000827\n",
            "[Train] epoch:744, lr:0.000988, total loss:0.000839\n",
            "[Train] epoch:745, lr:0.000988, total loss:0.000763\n",
            "[Train] epoch:746, lr:0.000988, total loss:0.000802\n",
            "[Train] epoch:747, lr:0.000988, total loss:0.000791\n",
            "[Train] epoch:748, lr:0.000988, total loss:0.000779\n",
            "[Train] epoch:749, lr:0.000988, total loss:0.000797\n",
            "[Train] epoch:750, lr:0.000988, total loss:0.000773\n",
            "[Train] epoch:751, lr:0.000988, total loss:0.000793\n",
            "[Train] epoch:752, lr:0.000988, total loss:0.000781\n",
            "[Train] epoch:753, lr:0.000988, total loss:0.000797\n",
            "[Train] epoch:754, lr:0.000988, total loss:0.000805\n",
            "[Train] epoch:755, lr:0.000988, total loss:0.000838\n",
            "[Train] epoch:756, lr:0.000988, total loss:0.000812\n",
            "[Train] epoch:757, lr:0.000988, total loss:0.000805\n",
            "[Train] epoch:758, lr:0.000988, total loss:0.000807\n",
            "[Train] epoch:759, lr:0.000988, total loss:0.000804\n",
            "[Train] epoch:760, lr:0.000988, total loss:0.000862\n",
            "[Train] epoch:761, lr:0.000988, total loss:0.000790\n",
            "[Train] epoch:762, lr:0.000988, total loss:0.000770\n",
            "[Train] epoch:763, lr:0.000987, total loss:0.000770\n",
            "[Train] epoch:764, lr:0.000987, total loss:0.000771\n",
            "[Train] epoch:765, lr:0.000987, total loss:0.000784\n",
            "[Train] epoch:766, lr:0.000987, total loss:0.000730\n",
            "[Train] epoch:767, lr:0.000987, total loss:0.000791\n",
            "[Train] epoch:768, lr:0.000987, total loss:0.000803\n",
            "[Train] epoch:769, lr:0.000987, total loss:0.000788\n",
            "[Train] epoch:770, lr:0.000987, total loss:0.000810\n",
            "[Train] epoch:771, lr:0.000987, total loss:0.000814\n",
            "[Train] epoch:772, lr:0.000987, total loss:0.000784\n",
            "[Train] epoch:773, lr:0.000987, total loss:0.000884\n",
            "[Train] epoch:774, lr:0.000987, total loss:0.000906\n",
            "[Train] epoch:775, lr:0.000987, total loss:0.000826\n",
            "[Train] epoch:776, lr:0.000987, total loss:0.000852\n",
            "[Train] epoch:777, lr:0.000987, total loss:0.000791\n",
            "[Train] epoch:778, lr:0.000987, total loss:0.000790\n",
            "[Train] epoch:779, lr:0.000987, total loss:0.000796\n",
            "[Train] epoch:780, lr:0.000987, total loss:0.000843\n",
            "[Train] epoch:781, lr:0.000987, total loss:0.000811\n",
            "[Train] epoch:782, lr:0.000987, total loss:0.000794\n",
            "[Train] epoch:783, lr:0.000987, total loss:0.000807\n",
            "[Train] epoch:784, lr:0.000987, total loss:0.000763\n",
            "[Train] epoch:785, lr:0.000987, total loss:0.000781\n",
            "[Train] epoch:786, lr:0.000987, total loss:0.000801\n",
            "[Train] epoch:787, lr:0.000987, total loss:0.000810\n",
            "[Train] epoch:788, lr:0.000987, total loss:0.000780\n",
            "[Train] epoch:789, lr:0.000987, total loss:0.000744\n",
            "[Train] epoch:790, lr:0.000987, total loss:0.000800\n",
            "[Train] epoch:791, lr:0.000987, total loss:0.000737\n",
            "[Train] epoch:792, lr:0.000986, total loss:0.000777\n",
            "[Train] epoch:793, lr:0.000986, total loss:0.000793\n",
            "[Train] epoch:794, lr:0.000986, total loss:0.000768\n",
            "[Train] epoch:795, lr:0.000986, total loss:0.000766\n",
            "[Train] epoch:796, lr:0.000986, total loss:0.000796\n",
            "[Train] epoch:797, lr:0.000986, total loss:0.000774\n",
            "[Train] epoch:798, lr:0.000986, total loss:0.000747\n",
            "[Train] epoch:799, lr:0.000986, total loss:0.000763\n",
            "[Train] epoch:800, lr:0.000986, total loss:0.000741\n",
            "[Train] epoch:801, lr:0.000986, total loss:0.000733\n",
            "[Train] epoch:802, lr:0.000986, total loss:0.000778\n",
            "[Train] epoch:803, lr:0.000986, total loss:0.000833\n",
            "[Train] epoch:804, lr:0.000986, total loss:0.000735\n",
            "[Train] epoch:805, lr:0.000986, total loss:0.000708\n",
            "[Train] epoch:806, lr:0.000986, total loss:0.000737\n",
            "[Train] epoch:807, lr:0.000986, total loss:0.000757\n",
            "[Train] epoch:808, lr:0.000986, total loss:0.000765\n",
            "[Train] epoch:809, lr:0.000986, total loss:0.000748\n",
            "[Train] epoch:810, lr:0.000986, total loss:0.000729\n",
            "[Train] epoch:811, lr:0.000986, total loss:0.000736\n",
            "[Train] epoch:812, lr:0.000986, total loss:0.000760\n",
            "[Train] epoch:813, lr:0.000986, total loss:0.000767\n",
            "[Train] epoch:814, lr:0.000986, total loss:0.000741\n",
            "[Train] epoch:815, lr:0.000986, total loss:0.000741\n",
            "[Train] epoch:816, lr:0.000986, total loss:0.000734\n",
            "[Train] epoch:817, lr:0.000986, total loss:0.000743\n",
            "[Train] epoch:818, lr:0.000986, total loss:0.000717\n",
            "[Train] epoch:819, lr:0.000986, total loss:0.000770\n",
            "[Train] epoch:820, lr:0.000986, total loss:0.000700\n",
            "[Train] epoch:821, lr:0.000985, total loss:0.000699\n",
            "[Train] epoch:822, lr:0.000985, total loss:0.000727\n",
            "[Train] epoch:823, lr:0.000985, total loss:0.000788\n",
            "[Train] epoch:824, lr:0.000985, total loss:0.000756\n",
            "[Train] epoch:825, lr:0.000985, total loss:0.000777\n",
            "[Train] epoch:826, lr:0.000985, total loss:0.000821\n",
            "[Train] epoch:827, lr:0.000985, total loss:0.000736\n",
            "[Train] epoch:828, lr:0.000985, total loss:0.000712\n",
            "[Train] epoch:829, lr:0.000985, total loss:0.000750\n",
            "[Train] epoch:830, lr:0.000985, total loss:0.000728\n",
            "[Train] epoch:831, lr:0.000985, total loss:0.000759\n",
            "[Train] epoch:832, lr:0.000985, total loss:0.000737\n",
            "[Train] epoch:833, lr:0.000985, total loss:0.000707\n",
            "[Train] epoch:834, lr:0.000985, total loss:0.000769\n",
            "[Train] epoch:835, lr:0.000985, total loss:0.000739\n",
            "[Train] epoch:836, lr:0.000985, total loss:0.000784\n",
            "[Train] epoch:837, lr:0.000985, total loss:0.000711\n",
            "[Train] epoch:838, lr:0.000985, total loss:0.000679\n",
            "[Train] epoch:839, lr:0.000985, total loss:0.000693\n",
            "[Train] epoch:840, lr:0.000985, total loss:0.000730\n",
            "[Train] epoch:841, lr:0.000985, total loss:0.000724\n",
            "[Train] epoch:842, lr:0.000985, total loss:0.000720\n",
            "[Train] epoch:843, lr:0.000985, total loss:0.000717\n",
            "[Train] epoch:844, lr:0.000985, total loss:0.000743\n",
            "[Train] epoch:845, lr:0.000985, total loss:0.000749\n",
            "[Train] epoch:846, lr:0.000985, total loss:0.000733\n",
            "[Train] epoch:847, lr:0.000985, total loss:0.000689\n",
            "[Train] epoch:848, lr:0.000985, total loss:0.000724\n",
            "[Train] epoch:849, lr:0.000984, total loss:0.000720\n",
            "[Train] epoch:850, lr:0.000984, total loss:0.000745\n",
            "[Train] epoch:851, lr:0.000984, total loss:0.000733\n",
            "[Train] epoch:852, lr:0.000984, total loss:0.000762\n",
            "[Train] epoch:853, lr:0.000984, total loss:0.000800\n",
            "[Train] epoch:854, lr:0.000984, total loss:0.000706\n",
            "[Train] epoch:855, lr:0.000984, total loss:0.000723\n",
            "[Train] epoch:856, lr:0.000984, total loss:0.000736\n",
            "[Train] epoch:857, lr:0.000984, total loss:0.000697\n",
            "[Train] epoch:858, lr:0.000984, total loss:0.000699\n",
            "[Train] epoch:859, lr:0.000984, total loss:0.000679\n",
            "[Train] epoch:860, lr:0.000984, total loss:0.000690\n",
            "[Train] epoch:861, lr:0.000984, total loss:0.000726\n",
            "[Train] epoch:862, lr:0.000984, total loss:0.000741\n",
            "[Train] epoch:863, lr:0.000984, total loss:0.000723\n",
            "[Train] epoch:864, lr:0.000984, total loss:0.000735\n",
            "[Train] epoch:865, lr:0.000984, total loss:0.000644\n",
            "[Train] epoch:866, lr:0.000984, total loss:0.000699\n",
            "[Train] epoch:867, lr:0.000984, total loss:0.000687\n",
            "[Train] epoch:868, lr:0.000984, total loss:0.000700\n",
            "[Train] epoch:869, lr:0.000984, total loss:0.000686\n",
            "[Train] epoch:870, lr:0.000984, total loss:0.000738\n",
            "[Train] epoch:871, lr:0.000984, total loss:0.000669\n",
            "[Train] epoch:872, lr:0.000984, total loss:0.000733\n",
            "[Train] epoch:873, lr:0.000984, total loss:0.000688\n",
            "[Train] epoch:874, lr:0.000984, total loss:0.000734\n",
            "[Train] epoch:875, lr:0.000983, total loss:0.000707\n",
            "[Train] epoch:876, lr:0.000983, total loss:0.000694\n",
            "[Train] epoch:877, lr:0.000983, total loss:0.000691\n",
            "[Train] epoch:878, lr:0.000983, total loss:0.000740\n",
            "[Train] epoch:879, lr:0.000983, total loss:0.000676\n",
            "[Train] epoch:880, lr:0.000983, total loss:0.000718\n",
            "[Train] epoch:881, lr:0.000983, total loss:0.000699\n",
            "[Train] epoch:882, lr:0.000983, total loss:0.000709\n",
            "[Train] epoch:883, lr:0.000983, total loss:0.000662\n",
            "[Train] epoch:884, lr:0.000983, total loss:0.000694\n",
            "[Train] epoch:885, lr:0.000983, total loss:0.000714\n",
            "[Train] epoch:886, lr:0.000983, total loss:0.000706\n",
            "[Train] epoch:887, lr:0.000983, total loss:0.000699\n",
            "[Train] epoch:888, lr:0.000983, total loss:0.000680\n",
            "[Train] epoch:889, lr:0.000983, total loss:0.000638\n",
            "[Train] epoch:890, lr:0.000983, total loss:0.000669\n",
            "[Train] epoch:891, lr:0.000983, total loss:0.000736\n",
            "[Train] epoch:892, lr:0.000983, total loss:0.000761\n",
            "[Train] epoch:893, lr:0.000983, total loss:0.000711\n",
            "[Train] epoch:894, lr:0.000983, total loss:0.000695\n",
            "[Train] epoch:895, lr:0.000983, total loss:0.000748\n",
            "[Train] epoch:896, lr:0.000983, total loss:0.000738\n",
            "[Train] epoch:897, lr:0.000983, total loss:0.000702\n",
            "[Train] epoch:898, lr:0.000983, total loss:0.000713\n",
            "[Train] epoch:899, lr:0.000983, total loss:0.000621\n",
            "[Train] epoch:900, lr:0.000983, total loss:0.000663\n",
            "[Train] epoch:901, lr:0.000982, total loss:0.000693\n",
            "[Train] epoch:902, lr:0.000982, total loss:0.000690\n",
            "[Train] epoch:903, lr:0.000982, total loss:0.000717\n",
            "[Train] epoch:904, lr:0.000982, total loss:0.000651\n",
            "[Train] epoch:905, lr:0.000982, total loss:0.000689\n",
            "[Train] epoch:906, lr:0.000982, total loss:0.000694\n",
            "[Train] epoch:907, lr:0.000982, total loss:0.000667\n",
            "[Train] epoch:908, lr:0.000982, total loss:0.000653\n",
            "[Train] epoch:909, lr:0.000982, total loss:0.000650\n",
            "[Train] epoch:910, lr:0.000982, total loss:0.000633\n",
            "[Train] epoch:911, lr:0.000982, total loss:0.000642\n",
            "[Train] epoch:912, lr:0.000982, total loss:0.000670\n",
            "[Train] epoch:913, lr:0.000982, total loss:0.000709\n",
            "[Train] epoch:914, lr:0.000982, total loss:0.000662\n",
            "[Train] epoch:915, lr:0.000982, total loss:0.000633\n",
            "[Train] epoch:916, lr:0.000982, total loss:0.000622\n",
            "[Train] epoch:917, lr:0.000982, total loss:0.000679\n",
            "[Train] epoch:918, lr:0.000982, total loss:0.000667\n",
            "[Train] epoch:919, lr:0.000982, total loss:0.000656\n",
            "[Train] epoch:920, lr:0.000982, total loss:0.000639\n",
            "[Train] epoch:921, lr:0.000982, total loss:0.000667\n",
            "[Train] epoch:922, lr:0.000982, total loss:0.000607\n",
            "[Train] epoch:923, lr:0.000982, total loss:0.000662\n",
            "[Train] epoch:924, lr:0.000982, total loss:0.000637\n",
            "[Train] epoch:925, lr:0.000982, total loss:0.000667\n",
            "[Train] epoch:926, lr:0.000981, total loss:0.000688\n",
            "[Train] epoch:927, lr:0.000981, total loss:0.000658\n",
            "[Train] epoch:928, lr:0.000981, total loss:0.000680\n",
            "[Train] epoch:929, lr:0.000981, total loss:0.000654\n",
            "[Train] epoch:930, lr:0.000981, total loss:0.000669\n",
            "[Train] epoch:931, lr:0.000981, total loss:0.000609\n",
            "[Train] epoch:932, lr:0.000981, total loss:0.000659\n",
            "[Train] epoch:933, lr:0.000981, total loss:0.000666\n",
            "[Train] epoch:934, lr:0.000981, total loss:0.000664\n",
            "[Train] epoch:935, lr:0.000981, total loss:0.000646\n",
            "[Train] epoch:936, lr:0.000981, total loss:0.000648\n",
            "[Train] epoch:937, lr:0.000981, total loss:0.000702\n",
            "[Train] epoch:938, lr:0.000981, total loss:0.000693\n",
            "[Train] epoch:939, lr:0.000981, total loss:0.000676\n",
            "[Train] epoch:940, lr:0.000981, total loss:0.000648\n",
            "[Train] epoch:941, lr:0.000981, total loss:0.000675\n",
            "[Train] epoch:942, lr:0.000981, total loss:0.000653\n",
            "[Train] epoch:943, lr:0.000981, total loss:0.000624\n",
            "[Train] epoch:944, lr:0.000981, total loss:0.000659\n",
            "[Train] epoch:945, lr:0.000981, total loss:0.000600\n",
            "[Train] epoch:946, lr:0.000981, total loss:0.000632\n",
            "[Train] epoch:947, lr:0.000981, total loss:0.000690\n",
            "[Train] epoch:948, lr:0.000981, total loss:0.000628\n",
            "[Train] epoch:949, lr:0.000981, total loss:0.000668\n",
            "[Train] epoch:950, lr:0.000981, total loss:0.000645\n",
            "[Train] epoch:951, lr:0.000980, total loss:0.000621\n",
            "[Train] epoch:952, lr:0.000980, total loss:0.000680\n",
            "[Train] epoch:953, lr:0.000980, total loss:0.000647\n",
            "[Train] epoch:954, lr:0.000980, total loss:0.000649\n",
            "[Train] epoch:955, lr:0.000980, total loss:0.000667\n",
            "[Train] epoch:956, lr:0.000980, total loss:0.000708\n",
            "[Train] epoch:957, lr:0.000980, total loss:0.000741\n",
            "[Train] epoch:958, lr:0.000980, total loss:0.000664\n",
            "[Train] epoch:959, lr:0.000980, total loss:0.000664\n",
            "[Train] epoch:960, lr:0.000980, total loss:0.000706\n",
            "[Train] epoch:961, lr:0.000980, total loss:0.000681\n",
            "[Train] epoch:962, lr:0.000980, total loss:0.000628\n",
            "[Train] epoch:963, lr:0.000980, total loss:0.000642\n",
            "[Train] epoch:964, lr:0.000980, total loss:0.000645\n",
            "[Train] epoch:965, lr:0.000980, total loss:0.000624\n",
            "[Train] epoch:966, lr:0.000980, total loss:0.000577\n",
            "[Train] epoch:967, lr:0.000980, total loss:0.000634\n",
            "[Train] epoch:968, lr:0.000980, total loss:0.000588\n",
            "[Train] epoch:969, lr:0.000980, total loss:0.000691\n",
            "[Train] epoch:970, lr:0.000980, total loss:0.000663\n",
            "[Train] epoch:971, lr:0.000980, total loss:0.000673\n",
            "[Train] epoch:972, lr:0.000980, total loss:0.000632\n",
            "[Train] epoch:973, lr:0.000980, total loss:0.000671\n",
            "[Train] epoch:974, lr:0.000980, total loss:0.000632\n",
            "[Train] epoch:975, lr:0.000979, total loss:0.000592\n",
            "[Train] epoch:976, lr:0.000979, total loss:0.000624\n",
            "[Train] epoch:977, lr:0.000979, total loss:0.000624\n",
            "[Train] epoch:978, lr:0.000979, total loss:0.000659\n",
            "[Train] epoch:979, lr:0.000979, total loss:0.000632\n",
            "[Train] epoch:980, lr:0.000979, total loss:0.000620\n",
            "[Train] epoch:981, lr:0.000979, total loss:0.000644\n",
            "[Train] epoch:982, lr:0.000979, total loss:0.000630\n",
            "[Train] epoch:983, lr:0.000979, total loss:0.000669\n",
            "[Train] epoch:984, lr:0.000979, total loss:0.000596\n",
            "[Train] epoch:985, lr:0.000979, total loss:0.000644\n",
            "[Train] epoch:986, lr:0.000979, total loss:0.000607\n",
            "[Train] epoch:987, lr:0.000979, total loss:0.000632\n",
            "[Train] epoch:988, lr:0.000979, total loss:0.000632\n",
            "[Train] epoch:989, lr:0.000979, total loss:0.000602\n",
            "[Train] epoch:990, lr:0.000979, total loss:0.000616\n",
            "[Train] epoch:991, lr:0.000979, total loss:0.000638\n",
            "[Train] epoch:992, lr:0.000979, total loss:0.000609\n",
            "[Train] epoch:993, lr:0.000979, total loss:0.000637\n",
            "[Train] epoch:994, lr:0.000979, total loss:0.000635\n",
            "[Train] epoch:995, lr:0.000979, total loss:0.000615\n",
            "[Train] epoch:996, lr:0.000979, total loss:0.000607\n",
            "[Train] epoch:997, lr:0.000979, total loss:0.000573\n",
            "[Train] epoch:998, lr:0.000978, total loss:0.000620\n",
            "[Train] epoch:999, lr:0.000978, total loss:0.000651\n",
            "[Train] epoch:1000, lr:0.000978, total loss:0.000658\n",
            "[Train] epoch:1001, lr:0.000978, total loss:0.000652\n",
            "[Train] epoch:1002, lr:0.000978, total loss:0.000638\n",
            "[Train] epoch:1003, lr:0.000978, total loss:0.000562\n",
            "[Train] epoch:1004, lr:0.000978, total loss:0.000619\n",
            "[Train] epoch:1005, lr:0.000978, total loss:0.000556\n",
            "[Train] epoch:1006, lr:0.000978, total loss:0.000599\n",
            "[Train] epoch:1007, lr:0.000978, total loss:0.000594\n",
            "[Train] epoch:1008, lr:0.000978, total loss:0.000611\n",
            "[Train] epoch:1009, lr:0.000978, total loss:0.000621\n",
            "[Train] epoch:1010, lr:0.000978, total loss:0.000596\n",
            "[Train] epoch:1011, lr:0.000978, total loss:0.000576\n",
            "[Train] epoch:1012, lr:0.000978, total loss:0.000580\n",
            "[Train] epoch:1013, lr:0.000978, total loss:0.000625\n",
            "[Train] epoch:1014, lr:0.000978, total loss:0.000628\n",
            "[Train] epoch:1015, lr:0.000978, total loss:0.000613\n",
            "[Train] epoch:1016, lr:0.000978, total loss:0.000609\n",
            "[Train] epoch:1017, lr:0.000978, total loss:0.000604\n",
            "[Train] epoch:1018, lr:0.000978, total loss:0.000601\n",
            "[Train] epoch:1019, lr:0.000978, total loss:0.000586\n",
            "[Train] epoch:1020, lr:0.000978, total loss:0.000592\n",
            "[Train] epoch:1021, lr:0.000977, total loss:0.000574\n",
            "[Train] epoch:1022, lr:0.000977, total loss:0.000644\n",
            "[Train] epoch:1023, lr:0.000977, total loss:0.000606\n",
            "[Train] epoch:1024, lr:0.000977, total loss:0.000560\n",
            "[Train] epoch:1025, lr:0.000977, total loss:0.000562\n",
            "[Train] epoch:1026, lr:0.000977, total loss:0.000579\n",
            "[Train] epoch:1027, lr:0.000977, total loss:0.000598\n",
            "[Train] epoch:1028, lr:0.000977, total loss:0.000600\n",
            "[Train] epoch:1029, lr:0.000977, total loss:0.000609\n",
            "[Train] epoch:1030, lr:0.000977, total loss:0.000621\n",
            "[Train] epoch:1031, lr:0.000977, total loss:0.000622\n",
            "[Train] epoch:1032, lr:0.000977, total loss:0.000585\n",
            "[Train] epoch:1033, lr:0.000977, total loss:0.000568\n",
            "[Train] epoch:1034, lr:0.000977, total loss:0.000605\n",
            "[Train] epoch:1035, lr:0.000977, total loss:0.000584\n",
            "[Train] epoch:1036, lr:0.000977, total loss:0.000672\n",
            "[Train] epoch:1037, lr:0.000977, total loss:0.000576\n",
            "[Train] epoch:1038, lr:0.000977, total loss:0.000593\n",
            "[Train] epoch:1039, lr:0.000977, total loss:0.000590\n",
            "[Train] epoch:1040, lr:0.000977, total loss:0.000613\n",
            "[Train] epoch:1041, lr:0.000977, total loss:0.000611\n",
            "[Train] epoch:1042, lr:0.000977, total loss:0.000618\n",
            "[Train] epoch:1043, lr:0.000977, total loss:0.000600\n",
            "[Train] epoch:1044, lr:0.000976, total loss:0.000595\n",
            "[Train] epoch:1045, lr:0.000976, total loss:0.000606\n",
            "[Train] epoch:1046, lr:0.000976, total loss:0.000582\n",
            "[Train] epoch:1047, lr:0.000976, total loss:0.000578\n",
            "[Train] epoch:1048, lr:0.000976, total loss:0.000562\n",
            "[Train] epoch:1049, lr:0.000976, total loss:0.000569\n",
            "[Train] epoch:1050, lr:0.000976, total loss:0.000594\n",
            "[Train] epoch:1051, lr:0.000976, total loss:0.000549\n",
            "[Train] epoch:1052, lr:0.000976, total loss:0.000598\n",
            "[Train] epoch:1053, lr:0.000976, total loss:0.000605\n",
            "[Train] epoch:1054, lr:0.000976, total loss:0.000562\n",
            "[Train] epoch:1055, lr:0.000976, total loss:0.000589\n",
            "[Train] epoch:1056, lr:0.000976, total loss:0.000606\n",
            "[Train] epoch:1057, lr:0.000976, total loss:0.000613\n",
            "[Train] epoch:1058, lr:0.000976, total loss:0.000556\n",
            "[Train] epoch:1059, lr:0.000976, total loss:0.000569\n",
            "[Train] epoch:1060, lr:0.000976, total loss:0.000591\n",
            "[Train] epoch:1061, lr:0.000976, total loss:0.000616\n",
            "[Train] epoch:1062, lr:0.000976, total loss:0.000584\n",
            "[Train] epoch:1063, lr:0.000976, total loss:0.000578\n",
            "[Train] epoch:1064, lr:0.000976, total loss:0.000614\n",
            "[Train] epoch:1065, lr:0.000976, total loss:0.000563\n",
            "[Train] epoch:1066, lr:0.000975, total loss:0.000536\n",
            "[Train] epoch:1067, lr:0.000975, total loss:0.000533\n",
            "[Train] epoch:1068, lr:0.000975, total loss:0.000552\n",
            "[Train] epoch:1069, lr:0.000975, total loss:0.000542\n",
            "[Train] epoch:1070, lr:0.000975, total loss:0.000568\n",
            "[Train] epoch:1071, lr:0.000975, total loss:0.000559\n",
            "[Train] epoch:1072, lr:0.000975, total loss:0.000600\n",
            "[Train] epoch:1073, lr:0.000975, total loss:0.000559\n",
            "[Train] epoch:1074, lr:0.000975, total loss:0.000608\n",
            "[Train] epoch:1075, lr:0.000975, total loss:0.000554\n",
            "[Train] epoch:1076, lr:0.000975, total loss:0.000566\n",
            "[Train] epoch:1077, lr:0.000975, total loss:0.000558\n",
            "[Train] epoch:1078, lr:0.000975, total loss:0.000601\n",
            "[Train] epoch:1079, lr:0.000975, total loss:0.000569\n",
            "[Train] epoch:1080, lr:0.000975, total loss:0.000555\n",
            "[Train] epoch:1081, lr:0.000975, total loss:0.000560\n",
            "[Train] epoch:1082, lr:0.000975, total loss:0.000602\n",
            "[Train] epoch:1083, lr:0.000975, total loss:0.000606\n",
            "[Train] epoch:1084, lr:0.000975, total loss:0.000617\n",
            "[Train] epoch:1085, lr:0.000975, total loss:0.000558\n",
            "[Train] epoch:1086, lr:0.000975, total loss:0.000528\n",
            "[Train] epoch:1087, lr:0.000974, total loss:0.000599\n",
            "[Train] epoch:1088, lr:0.000974, total loss:0.000591\n",
            "[Train] epoch:1089, lr:0.000974, total loss:0.000549\n",
            "[Train] epoch:1090, lr:0.000974, total loss:0.000545\n",
            "[Train] epoch:1091, lr:0.000974, total loss:0.000584\n",
            "[Train] epoch:1092, lr:0.000974, total loss:0.000591\n",
            "[Train] epoch:1093, lr:0.000974, total loss:0.000567\n",
            "[Train] epoch:1094, lr:0.000974, total loss:0.000556\n",
            "[Train] epoch:1095, lr:0.000974, total loss:0.000575\n",
            "[Train] epoch:1096, lr:0.000974, total loss:0.000650\n",
            "[Train] epoch:1097, lr:0.000974, total loss:0.000598\n",
            "[Train] epoch:1098, lr:0.000974, total loss:0.000593\n",
            "[Train] epoch:1099, lr:0.000974, total loss:0.000610\n",
            "[Train] epoch:1100, lr:0.000974, total loss:0.000553\n",
            "[Train] epoch:1101, lr:0.000974, total loss:0.000545\n",
            "[Train] epoch:1102, lr:0.000974, total loss:0.000512\n",
            "[Train] epoch:1103, lr:0.000974, total loss:0.000533\n",
            "[Train] epoch:1104, lr:0.000974, total loss:0.000563\n",
            "[Train] epoch:1105, lr:0.000974, total loss:0.000545\n",
            "[Train] epoch:1106, lr:0.000974, total loss:0.000567\n",
            "[Train] epoch:1107, lr:0.000974, total loss:0.000595\n",
            "[Train] epoch:1108, lr:0.000973, total loss:0.000542\n",
            "[Train] epoch:1109, lr:0.000973, total loss:0.000566\n",
            "[Train] epoch:1110, lr:0.000973, total loss:0.000538\n",
            "[Train] epoch:1111, lr:0.000973, total loss:0.000559\n",
            "[Train] epoch:1112, lr:0.000973, total loss:0.000552\n",
            "[Train] epoch:1113, lr:0.000973, total loss:0.000553\n",
            "[Train] epoch:1114, lr:0.000973, total loss:0.000544\n",
            "[Train] epoch:1115, lr:0.000973, total loss:0.000536\n",
            "[Train] epoch:1116, lr:0.000973, total loss:0.000570\n",
            "[Train] epoch:1117, lr:0.000973, total loss:0.000545\n",
            "[Train] epoch:1118, lr:0.000973, total loss:0.000533\n",
            "[Train] epoch:1119, lr:0.000973, total loss:0.000564\n",
            "[Train] epoch:1120, lr:0.000973, total loss:0.000539\n",
            "[Train] epoch:1121, lr:0.000973, total loss:0.000522\n",
            "[Train] epoch:1122, lr:0.000973, total loss:0.000582\n",
            "[Train] epoch:1123, lr:0.000973, total loss:0.000582\n",
            "[Train] epoch:1124, lr:0.000973, total loss:0.000560\n",
            "[Train] epoch:1125, lr:0.000973, total loss:0.000576\n",
            "[Train] epoch:1126, lr:0.000973, total loss:0.000541\n",
            "[Train] epoch:1127, lr:0.000973, total loss:0.000544\n",
            "[Train] epoch:1128, lr:0.000973, total loss:0.000519\n",
            "[Train] epoch:1129, lr:0.000972, total loss:0.000542\n",
            "[Train] epoch:1130, lr:0.000972, total loss:0.000581\n",
            "[Train] epoch:1131, lr:0.000972, total loss:0.000546\n",
            "[Train] epoch:1132, lr:0.000972, total loss:0.000523\n",
            "[Train] epoch:1133, lr:0.000972, total loss:0.000526\n",
            "[Train] epoch:1134, lr:0.000972, total loss:0.000584\n",
            "[Train] epoch:1135, lr:0.000972, total loss:0.000570\n",
            "[Train] epoch:1136, lr:0.000972, total loss:0.000557\n",
            "[Train] epoch:1137, lr:0.000972, total loss:0.000531\n",
            "[Train] epoch:1138, lr:0.000972, total loss:0.000579\n",
            "[Train] epoch:1139, lr:0.000972, total loss:0.000539\n",
            "[Train] epoch:1140, lr:0.000972, total loss:0.000565\n",
            "[Train] epoch:1141, lr:0.000972, total loss:0.000554\n",
            "[Train] epoch:1142, lr:0.000972, total loss:0.000539\n",
            "[Train] epoch:1143, lr:0.000972, total loss:0.000551\n",
            "[Train] epoch:1144, lr:0.000972, total loss:0.000558\n",
            "[Train] epoch:1145, lr:0.000972, total loss:0.000581\n",
            "[Train] epoch:1146, lr:0.000972, total loss:0.000503\n",
            "[Train] epoch:1147, lr:0.000972, total loss:0.000532\n",
            "[Train] epoch:1148, lr:0.000972, total loss:0.000567\n",
            "[Train] epoch:1149, lr:0.000971, total loss:0.000521\n",
            "[Train] epoch:1150, lr:0.000971, total loss:0.000546\n",
            "[Train] epoch:1151, lr:0.000971, total loss:0.000530\n",
            "[Train] epoch:1152, lr:0.000971, total loss:0.000553\n",
            "[Train] epoch:1153, lr:0.000971, total loss:0.000604\n",
            "[Train] epoch:1154, lr:0.000971, total loss:0.000543\n",
            "[Train] epoch:1155, lr:0.000971, total loss:0.000515\n",
            "[Train] epoch:1156, lr:0.000971, total loss:0.000520\n",
            "[Train] epoch:1157, lr:0.000971, total loss:0.000534\n",
            "[Train] epoch:1158, lr:0.000971, total loss:0.000530\n",
            "[Train] epoch:1159, lr:0.000971, total loss:0.000548\n",
            "[Train] epoch:1160, lr:0.000971, total loss:0.000540\n",
            "[Train] epoch:1161, lr:0.000971, total loss:0.000518\n",
            "[Train] epoch:1162, lr:0.000971, total loss:0.000530\n",
            "[Train] epoch:1163, lr:0.000971, total loss:0.000549\n",
            "[Train] epoch:1164, lr:0.000971, total loss:0.000510\n",
            "[Train] epoch:1165, lr:0.000971, total loss:0.000544\n",
            "[Train] epoch:1166, lr:0.000971, total loss:0.000527\n",
            "[Train] epoch:1167, lr:0.000971, total loss:0.000577\n",
            "[Train] epoch:1168, lr:0.000971, total loss:0.000529\n",
            "[Train] epoch:1169, lr:0.000970, total loss:0.000553\n",
            "[Train] epoch:1170, lr:0.000970, total loss:0.000516\n",
            "[Train] epoch:1171, lr:0.000970, total loss:0.000543\n",
            "[Train] epoch:1172, lr:0.000970, total loss:0.000503\n",
            "[Train] epoch:1173, lr:0.000970, total loss:0.000519\n",
            "[Train] epoch:1174, lr:0.000970, total loss:0.000518\n",
            "[Train] epoch:1175, lr:0.000970, total loss:0.000539\n",
            "[Train] epoch:1176, lr:0.000970, total loss:0.000562\n",
            "[Train] epoch:1177, lr:0.000970, total loss:0.000562\n",
            "[Train] epoch:1178, lr:0.000970, total loss:0.000600\n",
            "[Train] epoch:1179, lr:0.000970, total loss:0.000546\n",
            "[Train] epoch:1180, lr:0.000970, total loss:0.000493\n",
            "[Train] epoch:1181, lr:0.000970, total loss:0.000547\n",
            "[Train] epoch:1182, lr:0.000970, total loss:0.000485\n",
            "[Train] epoch:1183, lr:0.000970, total loss:0.000480\n",
            "[Train] epoch:1184, lr:0.000970, total loss:0.000482\n",
            "[Train] epoch:1185, lr:0.000970, total loss:0.000519\n",
            "[Train] epoch:1186, lr:0.000970, total loss:0.000508\n",
            "[Train] epoch:1187, lr:0.000970, total loss:0.000515\n",
            "[Train] epoch:1188, lr:0.000970, total loss:0.000543\n",
            "[Train] epoch:1189, lr:0.000969, total loss:0.000544\n",
            "[Train] epoch:1190, lr:0.000969, total loss:0.000551\n",
            "[Train] epoch:1191, lr:0.000969, total loss:0.000491\n",
            "[Train] epoch:1192, lr:0.000969, total loss:0.000501\n",
            "[Train] epoch:1193, lr:0.000969, total loss:0.000524\n",
            "[Train] epoch:1194, lr:0.000969, total loss:0.000474\n",
            "[Train] epoch:1195, lr:0.000969, total loss:0.000497\n",
            "[Train] epoch:1196, lr:0.000969, total loss:0.000510\n",
            "[Train] epoch:1197, lr:0.000969, total loss:0.000496\n",
            "[Train] epoch:1198, lr:0.000969, total loss:0.000497\n",
            "[Train] epoch:1199, lr:0.000969, total loss:0.000507\n",
            "[Train] epoch:1200, lr:0.000969, total loss:0.000530\n",
            "[Train] epoch:1201, lr:0.000969, total loss:0.000495\n",
            "[Train] epoch:1202, lr:0.000969, total loss:0.000511\n",
            "[Train] epoch:1203, lr:0.000969, total loss:0.000539\n",
            "[Train] epoch:1204, lr:0.000969, total loss:0.000528\n",
            "[Train] epoch:1205, lr:0.000969, total loss:0.000535\n",
            "[Train] epoch:1206, lr:0.000969, total loss:0.000509\n",
            "[Train] epoch:1207, lr:0.000969, total loss:0.000522\n",
            "[Train] epoch:1208, lr:0.000968, total loss:0.000498\n",
            "[Train] epoch:1209, lr:0.000968, total loss:0.000545\n",
            "[Train] epoch:1210, lr:0.000968, total loss:0.000584\n",
            "[Train] epoch:1211, lr:0.000968, total loss:0.000532\n",
            "[Train] epoch:1212, lr:0.000968, total loss:0.000570\n",
            "[Train] epoch:1213, lr:0.000968, total loss:0.000523\n",
            "[Train] epoch:1214, lr:0.000968, total loss:0.000564\n",
            "[Train] epoch:1215, lr:0.000968, total loss:0.000554\n",
            "[Train] epoch:1216, lr:0.000968, total loss:0.000543\n",
            "[Train] epoch:1217, lr:0.000968, total loss:0.000515\n",
            "[Train] epoch:1218, lr:0.000968, total loss:0.000519\n",
            "[Train] epoch:1219, lr:0.000968, total loss:0.000512\n",
            "[Train] epoch:1220, lr:0.000968, total loss:0.000514\n",
            "[Train] epoch:1221, lr:0.000968, total loss:0.000502\n",
            "[Train] epoch:1222, lr:0.000968, total loss:0.000533\n",
            "[Train] epoch:1223, lr:0.000968, total loss:0.000494\n",
            "[Train] epoch:1224, lr:0.000968, total loss:0.000472\n",
            "[Train] epoch:1225, lr:0.000968, total loss:0.000500\n",
            "[Train] epoch:1226, lr:0.000968, total loss:0.000491\n",
            "[Train] epoch:1227, lr:0.000967, total loss:0.000536\n",
            "[Train] epoch:1228, lr:0.000967, total loss:0.000505\n",
            "[Train] epoch:1229, lr:0.000967, total loss:0.000536\n",
            "[Train] epoch:1230, lr:0.000967, total loss:0.000544\n",
            "[Train] epoch:1231, lr:0.000967, total loss:0.000544\n",
            "[Train] epoch:1232, lr:0.000967, total loss:0.000662\n",
            "[Train] epoch:1233, lr:0.000967, total loss:0.000600\n",
            "[Train] epoch:1234, lr:0.000967, total loss:0.000553\n",
            "[Train] epoch:1235, lr:0.000967, total loss:0.000516\n",
            "[Train] epoch:1236, lr:0.000967, total loss:0.000480\n",
            "[Train] epoch:1237, lr:0.000967, total loss:0.000514\n",
            "[Train] epoch:1238, lr:0.000967, total loss:0.000503\n",
            "[Train] epoch:1239, lr:0.000967, total loss:0.000504\n",
            "[Train] epoch:1240, lr:0.000967, total loss:0.000550\n",
            "[Train] epoch:1241, lr:0.000967, total loss:0.000496\n",
            "[Train] epoch:1242, lr:0.000967, total loss:0.000481\n",
            "[Train] epoch:1243, lr:0.000967, total loss:0.000503\n",
            "[Train] epoch:1244, lr:0.000967, total loss:0.000488\n",
            "[Train] epoch:1245, lr:0.000967, total loss:0.000521\n",
            "[Train] epoch:1246, lr:0.000966, total loss:0.000541\n",
            "[Train] epoch:1247, lr:0.000966, total loss:0.000555\n",
            "[Train] epoch:1248, lr:0.000966, total loss:0.000499\n",
            "[Train] epoch:1249, lr:0.000966, total loss:0.000473\n",
            "[Train] epoch:1250, lr:0.000966, total loss:0.000497\n",
            "[Train] epoch:1251, lr:0.000966, total loss:0.000502\n",
            "[Train] epoch:1252, lr:0.000966, total loss:0.000462\n",
            "[Train] epoch:1253, lr:0.000966, total loss:0.000457\n",
            "[Train] epoch:1254, lr:0.000966, total loss:0.000517\n",
            "[Train] epoch:1255, lr:0.000966, total loss:0.000501\n",
            "[Train] epoch:1256, lr:0.000966, total loss:0.000474\n",
            "[Train] epoch:1257, lr:0.000966, total loss:0.000472\n",
            "[Train] epoch:1258, lr:0.000966, total loss:0.000494\n",
            "[Train] epoch:1259, lr:0.000966, total loss:0.000477\n",
            "[Train] epoch:1260, lr:0.000966, total loss:0.000508\n",
            "[Train] epoch:1261, lr:0.000966, total loss:0.000536\n",
            "[Train] epoch:1262, lr:0.000966, total loss:0.000566\n",
            "[Train] epoch:1263, lr:0.000966, total loss:0.000540\n",
            "[Train] epoch:1264, lr:0.000966, total loss:0.000509\n",
            "[Train] epoch:1265, lr:0.000965, total loss:0.000515\n",
            "[Train] epoch:1266, lr:0.000965, total loss:0.000518\n",
            "[Train] epoch:1267, lr:0.000965, total loss:0.000544\n",
            "[Train] epoch:1268, lr:0.000965, total loss:0.000556\n",
            "[Train] epoch:1269, lr:0.000965, total loss:0.000497\n",
            "[Train] epoch:1270, lr:0.000965, total loss:0.000501\n",
            "[Train] epoch:1271, lr:0.000965, total loss:0.000531\n",
            "[Train] epoch:1272, lr:0.000965, total loss:0.000533\n",
            "[Train] epoch:1273, lr:0.000965, total loss:0.000498\n",
            "[Train] epoch:1274, lr:0.000965, total loss:0.000479\n",
            "[Train] epoch:1275, lr:0.000965, total loss:0.000476\n",
            "[Train] epoch:1276, lr:0.000965, total loss:0.000493\n",
            "[Train] epoch:1277, lr:0.000965, total loss:0.000512\n",
            "[Train] epoch:1278, lr:0.000965, total loss:0.000468\n",
            "[Train] epoch:1279, lr:0.000965, total loss:0.000501\n",
            "[Train] epoch:1280, lr:0.000965, total loss:0.000509\n",
            "[Train] epoch:1281, lr:0.000965, total loss:0.000476\n",
            "[Train] epoch:1282, lr:0.000965, total loss:0.000505\n",
            "[Train] epoch:1283, lr:0.000964, total loss:0.000497\n",
            "[Train] epoch:1284, lr:0.000964, total loss:0.000499\n",
            "[Train] epoch:1285, lr:0.000964, total loss:0.000500\n",
            "[Train] epoch:1286, lr:0.000964, total loss:0.000481\n",
            "[Train] epoch:1287, lr:0.000964, total loss:0.000497\n",
            "[Train] epoch:1288, lr:0.000964, total loss:0.000469\n",
            "[Train] epoch:1289, lr:0.000964, total loss:0.000498\n",
            "[Train] epoch:1290, lr:0.000964, total loss:0.000447\n",
            "[Train] epoch:1291, lr:0.000964, total loss:0.000486\n",
            "[Train] epoch:1292, lr:0.000964, total loss:0.000466\n",
            "[Train] epoch:1293, lr:0.000964, total loss:0.000471\n",
            "[Train] epoch:1294, lr:0.000964, total loss:0.000475\n",
            "[Train] epoch:1295, lr:0.000964, total loss:0.000470\n",
            "[Train] epoch:1296, lr:0.000964, total loss:0.000477\n",
            "[Train] epoch:1297, lr:0.000964, total loss:0.000481\n",
            "[Train] epoch:1298, lr:0.000964, total loss:0.000471\n",
            "[Train] epoch:1299, lr:0.000964, total loss:0.000495\n",
            "[Train] epoch:1300, lr:0.000964, total loss:0.000513\n",
            "[Train] epoch:1301, lr:0.000963, total loss:0.000503\n",
            "[Train] epoch:1302, lr:0.000963, total loss:0.000516\n",
            "[Train] epoch:1303, lr:0.000963, total loss:0.000538\n",
            "[Train] epoch:1304, lr:0.000963, total loss:0.000516\n",
            "[Train] epoch:1305, lr:0.000963, total loss:0.000511\n",
            "[Train] epoch:1306, lr:0.000963, total loss:0.000534\n",
            "[Train] epoch:1307, lr:0.000963, total loss:0.000474\n",
            "[Train] epoch:1308, lr:0.000963, total loss:0.000500\n",
            "[Train] epoch:1309, lr:0.000963, total loss:0.000466\n",
            "[Train] epoch:1310, lr:0.000963, total loss:0.000441\n",
            "[Train] epoch:1311, lr:0.000963, total loss:0.000422\n",
            "[Train] epoch:1312, lr:0.000963, total loss:0.000450\n",
            "[Train] epoch:1313, lr:0.000963, total loss:0.000448\n",
            "[Train] epoch:1314, lr:0.000963, total loss:0.000462\n",
            "[Train] epoch:1315, lr:0.000963, total loss:0.000470\n",
            "[Train] epoch:1316, lr:0.000963, total loss:0.000448\n",
            "[Train] epoch:1317, lr:0.000963, total loss:0.000491\n",
            "[Train] epoch:1318, lr:0.000963, total loss:0.000469\n",
            "[Train] epoch:1319, lr:0.000962, total loss:0.000464\n",
            "[Train] epoch:1320, lr:0.000962, total loss:0.000456\n",
            "[Train] epoch:1321, lr:0.000962, total loss:0.000468\n",
            "[Train] epoch:1322, lr:0.000962, total loss:0.000475\n",
            "[Train] epoch:1323, lr:0.000962, total loss:0.000448\n",
            "[Train] epoch:1324, lr:0.000962, total loss:0.000490\n",
            "[Train] epoch:1325, lr:0.000962, total loss:0.000452\n",
            "[Train] epoch:1326, lr:0.000962, total loss:0.000475\n",
            "[Train] epoch:1327, lr:0.000962, total loss:0.000454\n",
            "[Train] epoch:1328, lr:0.000962, total loss:0.000466\n",
            "[Train] epoch:1329, lr:0.000962, total loss:0.000464\n",
            "[Train] epoch:1330, lr:0.000962, total loss:0.000456\n",
            "[Train] epoch:1331, lr:0.000962, total loss:0.000453\n",
            "[Train] epoch:1332, lr:0.000962, total loss:0.000412\n",
            "[Train] epoch:1333, lr:0.000962, total loss:0.000457\n",
            "[Train] epoch:1334, lr:0.000962, total loss:0.000457\n",
            "[Train] epoch:1335, lr:0.000962, total loss:0.000465\n",
            "[Train] epoch:1336, lr:0.000961, total loss:0.000440\n",
            "[Train] epoch:1337, lr:0.000961, total loss:0.000472\n",
            "[Train] epoch:1338, lr:0.000961, total loss:0.000458\n",
            "[Train] epoch:1339, lr:0.000961, total loss:0.000464\n",
            "[Train] epoch:1340, lr:0.000961, total loss:0.000413\n",
            "[Train] epoch:1341, lr:0.000961, total loss:0.000454\n",
            "[Train] epoch:1342, lr:0.000961, total loss:0.000432\n",
            "[Train] epoch:1343, lr:0.000961, total loss:0.000480\n",
            "[Train] epoch:1344, lr:0.000961, total loss:0.000451\n",
            "[Train] epoch:1345, lr:0.000961, total loss:0.000433\n",
            "[Train] epoch:1346, lr:0.000961, total loss:0.000449\n",
            "[Train] epoch:1347, lr:0.000961, total loss:0.000455\n",
            "[Train] epoch:1348, lr:0.000961, total loss:0.000473\n",
            "[Train] epoch:1349, lr:0.000961, total loss:0.000523\n",
            "[Train] epoch:1350, lr:0.000961, total loss:0.000478\n",
            "[Train] epoch:1351, lr:0.000961, total loss:0.000499\n",
            "[Train] epoch:1352, lr:0.000961, total loss:0.000479\n",
            "[Train] epoch:1353, lr:0.000961, total loss:0.000442\n",
            "[Train] epoch:1354, lr:0.000960, total loss:0.000444\n",
            "[Train] epoch:1355, lr:0.000960, total loss:0.000490\n",
            "[Train] epoch:1356, lr:0.000960, total loss:0.000459\n",
            "[Train] epoch:1357, lr:0.000960, total loss:0.000440\n",
            "[Train] epoch:1358, lr:0.000960, total loss:0.000454\n",
            "[Train] epoch:1359, lr:0.000960, total loss:0.000430\n",
            "[Train] epoch:1360, lr:0.000960, total loss:0.000449\n",
            "[Train] epoch:1361, lr:0.000960, total loss:0.000439\n",
            "[Train] epoch:1362, lr:0.000960, total loss:0.000451\n",
            "[Train] epoch:1363, lr:0.000960, total loss:0.000453\n",
            "[Train] epoch:1364, lr:0.000960, total loss:0.000446\n",
            "[Train] epoch:1365, lr:0.000960, total loss:0.000443\n",
            "[Train] epoch:1366, lr:0.000960, total loss:0.000461\n",
            "[Train] epoch:1367, lr:0.000960, total loss:0.000453\n",
            "[Train] epoch:1368, lr:0.000960, total loss:0.000444\n",
            "[Train] epoch:1369, lr:0.000960, total loss:0.000463\n",
            "[Train] epoch:1370, lr:0.000960, total loss:0.000435\n",
            "[Train] epoch:1371, lr:0.000959, total loss:0.000461\n",
            "[Train] epoch:1372, lr:0.000959, total loss:0.000481\n",
            "[Train] epoch:1373, lr:0.000959, total loss:0.000495\n",
            "[Train] epoch:1374, lr:0.000959, total loss:0.000525\n",
            "[Train] epoch:1375, lr:0.000959, total loss:0.000474\n",
            "[Train] epoch:1376, lr:0.000959, total loss:0.000473\n",
            "[Train] epoch:1377, lr:0.000959, total loss:0.000433\n",
            "[Train] epoch:1378, lr:0.000959, total loss:0.000446\n",
            "[Train] epoch:1379, lr:0.000959, total loss:0.000411\n",
            "[Train] epoch:1380, lr:0.000959, total loss:0.000462\n",
            "[Train] epoch:1381, lr:0.000959, total loss:0.000486\n",
            "[Train] epoch:1382, lr:0.000959, total loss:0.000447\n",
            "[Train] epoch:1383, lr:0.000959, total loss:0.000497\n",
            "[Train] epoch:1384, lr:0.000959, total loss:0.000458\n",
            "[Train] epoch:1385, lr:0.000959, total loss:0.000438\n",
            "[Train] epoch:1386, lr:0.000959, total loss:0.000420\n",
            "[Train] epoch:1387, lr:0.000959, total loss:0.000444\n",
            "[Train] epoch:1388, lr:0.000958, total loss:0.000438\n",
            "[Train] epoch:1389, lr:0.000958, total loss:0.000430\n",
            "[Train] epoch:1390, lr:0.000958, total loss:0.000428\n",
            "[Train] epoch:1391, lr:0.000958, total loss:0.000425\n",
            "[Train] epoch:1392, lr:0.000958, total loss:0.000421\n",
            "[Train] epoch:1393, lr:0.000958, total loss:0.000427\n",
            "[Train] epoch:1394, lr:0.000958, total loss:0.000413\n",
            "[Train] epoch:1395, lr:0.000958, total loss:0.000447\n",
            "[Train] epoch:1396, lr:0.000958, total loss:0.000433\n",
            "[Train] epoch:1397, lr:0.000958, total loss:0.000449\n",
            "[Train] epoch:1398, lr:0.000958, total loss:0.000454\n",
            "[Train] epoch:1399, lr:0.000958, total loss:0.000422\n",
            "[Train] epoch:1400, lr:0.000958, total loss:0.000437\n",
            "[Train] epoch:1401, lr:0.000958, total loss:0.000459\n",
            "[Train] epoch:1402, lr:0.000958, total loss:0.000434\n",
            "[Train] epoch:1403, lr:0.000958, total loss:0.000426\n",
            "[Train] epoch:1404, lr:0.000958, total loss:0.000428\n",
            "[Train] epoch:1405, lr:0.000957, total loss:0.000430\n",
            "[Train] epoch:1406, lr:0.000957, total loss:0.000434\n",
            "[Train] epoch:1407, lr:0.000957, total loss:0.000459\n",
            "[Train] epoch:1408, lr:0.000957, total loss:0.000456\n",
            "[Train] epoch:1409, lr:0.000957, total loss:0.000450\n",
            "[Train] epoch:1410, lr:0.000957, total loss:0.000409\n",
            "[Train] epoch:1411, lr:0.000957, total loss:0.000398\n",
            "[Train] epoch:1412, lr:0.000957, total loss:0.000442\n",
            "[Train] epoch:1413, lr:0.000957, total loss:0.000468\n",
            "[Train] epoch:1414, lr:0.000957, total loss:0.000476\n",
            "[Train] epoch:1415, lr:0.000957, total loss:0.000458\n",
            "[Train] epoch:1416, lr:0.000957, total loss:0.000466\n",
            "[Train] epoch:1417, lr:0.000957, total loss:0.000432\n",
            "[Train] epoch:1418, lr:0.000957, total loss:0.000460\n",
            "[Train] epoch:1419, lr:0.000957, total loss:0.000447\n",
            "[Train] epoch:1420, lr:0.000957, total loss:0.000461\n",
            "[Train] epoch:1421, lr:0.000956, total loss:0.000427\n",
            "[Train] epoch:1422, lr:0.000956, total loss:0.000452\n",
            "[Train] epoch:1423, lr:0.000956, total loss:0.000479\n",
            "[Train] epoch:1424, lr:0.000956, total loss:0.000535\n",
            "[Train] epoch:1425, lr:0.000956, total loss:0.000439\n",
            "[Train] epoch:1426, lr:0.000956, total loss:0.000423\n",
            "[Train] epoch:1427, lr:0.000956, total loss:0.000452\n",
            "[Train] epoch:1428, lr:0.000956, total loss:0.000461\n",
            "[Train] epoch:1429, lr:0.000956, total loss:0.000456\n",
            "[Train] epoch:1430, lr:0.000956, total loss:0.000436\n",
            "[Train] epoch:1431, lr:0.000956, total loss:0.000444\n",
            "[Train] epoch:1432, lr:0.000956, total loss:0.000430\n",
            "[Train] epoch:1433, lr:0.000956, total loss:0.000444\n",
            "[Train] epoch:1434, lr:0.000956, total loss:0.000452\n",
            "[Train] epoch:1435, lr:0.000956, total loss:0.000432\n",
            "[Train] epoch:1436, lr:0.000956, total loss:0.000426\n",
            "[Train] epoch:1437, lr:0.000956, total loss:0.000418\n",
            "[Train] epoch:1438, lr:0.000955, total loss:0.000429\n",
            "[Train] epoch:1439, lr:0.000955, total loss:0.000414\n",
            "[Train] epoch:1440, lr:0.000955, total loss:0.000454\n",
            "[Train] epoch:1441, lr:0.000955, total loss:0.000412\n",
            "[Train] epoch:1442, lr:0.000955, total loss:0.000410\n",
            "[Train] epoch:1443, lr:0.000955, total loss:0.000415\n",
            "[Train] epoch:1444, lr:0.000955, total loss:0.000398\n",
            "[Train] epoch:1445, lr:0.000955, total loss:0.000434\n",
            "[Train] epoch:1446, lr:0.000955, total loss:0.000420\n",
            "[Train] epoch:1447, lr:0.000955, total loss:0.000450\n",
            "[Train] epoch:1448, lr:0.000955, total loss:0.000405\n",
            "[Train] epoch:1449, lr:0.000955, total loss:0.000469\n",
            "[Train] epoch:1450, lr:0.000955, total loss:0.000423\n",
            "[Train] epoch:1451, lr:0.000955, total loss:0.000409\n",
            "[Train] epoch:1452, lr:0.000955, total loss:0.000442\n",
            "[Train] epoch:1453, lr:0.000955, total loss:0.000439\n",
            "[Train] epoch:1454, lr:0.000954, total loss:0.000419\n",
            "[Train] epoch:1455, lr:0.000954, total loss:0.000433\n",
            "[Train] epoch:1456, lr:0.000954, total loss:0.000442\n",
            "[Train] epoch:1457, lr:0.000954, total loss:0.000470\n",
            "[Train] epoch:1458, lr:0.000954, total loss:0.000419\n",
            "[Train] epoch:1459, lr:0.000954, total loss:0.000433\n",
            "[Train] epoch:1460, lr:0.000954, total loss:0.000456\n",
            "[Train] epoch:1461, lr:0.000954, total loss:0.000436\n",
            "[Train] epoch:1462, lr:0.000954, total loss:0.000461\n",
            "[Train] epoch:1463, lr:0.000954, total loss:0.000428\n",
            "[Train] epoch:1464, lr:0.000954, total loss:0.000449\n",
            "[Train] epoch:1465, lr:0.000954, total loss:0.000413\n",
            "[Train] epoch:1466, lr:0.000954, total loss:0.000482\n",
            "[Train] epoch:1467, lr:0.000954, total loss:0.000440\n",
            "[Train] epoch:1468, lr:0.000954, total loss:0.000417\n",
            "[Train] epoch:1469, lr:0.000954, total loss:0.000414\n",
            "[Train] epoch:1470, lr:0.000953, total loss:0.000429\n",
            "[Train] epoch:1471, lr:0.000953, total loss:0.000406\n",
            "[Train] epoch:1472, lr:0.000953, total loss:0.000409\n",
            "[Train] epoch:1473, lr:0.000953, total loss:0.000419\n",
            "[Train] epoch:1474, lr:0.000953, total loss:0.000441\n",
            "[Train] epoch:1475, lr:0.000953, total loss:0.000408\n",
            "[Train] epoch:1476, lr:0.000953, total loss:0.000423\n",
            "[Train] epoch:1477, lr:0.000953, total loss:0.000468\n",
            "[Train] epoch:1478, lr:0.000953, total loss:0.000462\n",
            "[Train] epoch:1479, lr:0.000953, total loss:0.000449\n",
            "[Train] epoch:1480, lr:0.000953, total loss:0.000432\n",
            "[Train] epoch:1481, lr:0.000953, total loss:0.000425\n",
            "[Train] epoch:1482, lr:0.000953, total loss:0.000461\n",
            "[Train] epoch:1483, lr:0.000953, total loss:0.000416\n",
            "[Train] epoch:1484, lr:0.000953, total loss:0.000439\n",
            "[Train] epoch:1485, lr:0.000953, total loss:0.000452\n",
            "[Train] epoch:1486, lr:0.000952, total loss:0.000423\n",
            "[Train] epoch:1487, lr:0.000952, total loss:0.000437\n",
            "[Train] epoch:1488, lr:0.000952, total loss:0.000419\n",
            "[Train] epoch:1489, lr:0.000952, total loss:0.000449\n",
            "[Train] epoch:1490, lr:0.000952, total loss:0.000399\n",
            "[Train] epoch:1491, lr:0.000952, total loss:0.000408\n",
            "[Train] epoch:1492, lr:0.000952, total loss:0.000427\n",
            "[Train] epoch:1493, lr:0.000952, total loss:0.000374\n",
            "[Train] epoch:1494, lr:0.000952, total loss:0.000410\n",
            "[Train] epoch:1495, lr:0.000952, total loss:0.000440\n",
            "[Train] epoch:1496, lr:0.000952, total loss:0.000442\n",
            "[Train] epoch:1497, lr:0.000952, total loss:0.000425\n",
            "[Train] epoch:1498, lr:0.000952, total loss:0.000432\n",
            "[Train] epoch:1499, lr:0.000952, total loss:0.000407\n",
            "[Train] epoch:1500, lr:0.000952, total loss:0.000408\n",
            "[Train] epoch:1501, lr:0.000951, total loss:0.000385\n",
            "[Train] epoch:1502, lr:0.000951, total loss:0.000407\n",
            "[Train] epoch:1503, lr:0.000951, total loss:0.000453\n",
            "[Train] epoch:1504, lr:0.000951, total loss:0.000441\n",
            "[Train] epoch:1505, lr:0.000951, total loss:0.000410\n",
            "[Train] epoch:1506, lr:0.000951, total loss:0.000401\n",
            "[Train] epoch:1507, lr:0.000951, total loss:0.000428\n",
            "[Train] epoch:1508, lr:0.000951, total loss:0.000410\n",
            "[Train] epoch:1509, lr:0.000951, total loss:0.000420\n",
            "[Train] epoch:1510, lr:0.000951, total loss:0.000421\n",
            "[Train] epoch:1511, lr:0.000951, total loss:0.000415\n",
            "[Train] epoch:1512, lr:0.000951, total loss:0.000426\n",
            "[Train] epoch:1513, lr:0.000951, total loss:0.000422\n",
            "[Train] epoch:1514, lr:0.000951, total loss:0.000422\n",
            "[Train] epoch:1515, lr:0.000951, total loss:0.000505\n",
            "[Train] epoch:1516, lr:0.000951, total loss:0.000434\n",
            "[Train] epoch:1517, lr:0.000950, total loss:0.000419\n",
            "[Train] epoch:1518, lr:0.000950, total loss:0.000407\n",
            "[Train] epoch:1519, lr:0.000950, total loss:0.000438\n",
            "[Train] epoch:1520, lr:0.000950, total loss:0.000405\n",
            "[Train] epoch:1521, lr:0.000950, total loss:0.000399\n",
            "[Train] epoch:1522, lr:0.000950, total loss:0.000394\n",
            "[Train] epoch:1523, lr:0.000950, total loss:0.000390\n",
            "[Train] epoch:1524, lr:0.000950, total loss:0.000427\n",
            "[Train] epoch:1525, lr:0.000950, total loss:0.000431\n",
            "[Train] epoch:1526, lr:0.000950, total loss:0.000416\n",
            "[Train] epoch:1527, lr:0.000950, total loss:0.000396\n",
            "[Train] epoch:1528, lr:0.000950, total loss:0.000404\n",
            "[Train] epoch:1529, lr:0.000950, total loss:0.000405\n",
            "[Train] epoch:1530, lr:0.000950, total loss:0.000436\n",
            "[Train] epoch:1531, lr:0.000950, total loss:0.000458\n",
            "[Train] epoch:1532, lr:0.000949, total loss:0.000398\n",
            "[Train] epoch:1533, lr:0.000949, total loss:0.000430\n",
            "[Train] epoch:1534, lr:0.000949, total loss:0.000425\n",
            "[Train] epoch:1535, lr:0.000949, total loss:0.000398\n",
            "[Train] epoch:1536, lr:0.000949, total loss:0.000406\n",
            "[Train] epoch:1537, lr:0.000949, total loss:0.000371\n",
            "[Train] epoch:1538, lr:0.000949, total loss:0.000383\n",
            "[Train] epoch:1539, lr:0.000949, total loss:0.000397\n",
            "[Train] epoch:1540, lr:0.000949, total loss:0.000422\n",
            "[Train] epoch:1541, lr:0.000949, total loss:0.000394\n",
            "[Train] epoch:1542, lr:0.000949, total loss:0.000418\n",
            "[Train] epoch:1543, lr:0.000949, total loss:0.000430\n",
            "[Train] epoch:1544, lr:0.000949, total loss:0.000424\n",
            "[Train] epoch:1545, lr:0.000949, total loss:0.000388\n",
            "[Train] epoch:1546, lr:0.000949, total loss:0.000445\n",
            "[Train] epoch:1547, lr:0.000949, total loss:0.000468\n",
            "[Train] epoch:1548, lr:0.000948, total loss:0.000431\n",
            "[Train] epoch:1549, lr:0.000948, total loss:0.000388\n",
            "[Train] epoch:1550, lr:0.000948, total loss:0.000386\n",
            "[Train] epoch:1551, lr:0.000948, total loss:0.000407\n",
            "[Train] epoch:1552, lr:0.000948, total loss:0.000418\n",
            "[Train] epoch:1553, lr:0.000948, total loss:0.000394\n",
            "[Train] epoch:1554, lr:0.000948, total loss:0.000464\n",
            "[Train] epoch:1555, lr:0.000948, total loss:0.000391\n",
            "[Train] epoch:1556, lr:0.000948, total loss:0.000396\n",
            "[Train] epoch:1557, lr:0.000948, total loss:0.000438\n",
            "[Train] epoch:1558, lr:0.000948, total loss:0.000428\n",
            "[Train] epoch:1559, lr:0.000948, total loss:0.000396\n",
            "[Train] epoch:1560, lr:0.000948, total loss:0.000427\n",
            "[Train] epoch:1561, lr:0.000948, total loss:0.000405\n",
            "[Train] epoch:1562, lr:0.000948, total loss:0.000387\n",
            "[Train] epoch:1563, lr:0.000947, total loss:0.000398\n",
            "[Train] epoch:1564, lr:0.000947, total loss:0.000404\n",
            "[Train] epoch:1565, lr:0.000947, total loss:0.000400\n",
            "[Train] epoch:1566, lr:0.000947, total loss:0.000389\n",
            "[Train] epoch:1567, lr:0.000947, total loss:0.000395\n",
            "[Train] epoch:1568, lr:0.000947, total loss:0.000384\n",
            "[Train] epoch:1569, lr:0.000947, total loss:0.000399\n",
            "[Train] epoch:1570, lr:0.000947, total loss:0.000422\n",
            "[Train] epoch:1571, lr:0.000947, total loss:0.000405\n",
            "[Train] epoch:1572, lr:0.000947, total loss:0.000390\n",
            "[Train] epoch:1573, lr:0.000947, total loss:0.000407\n",
            "[Train] epoch:1574, lr:0.000947, total loss:0.000449\n",
            "[Train] epoch:1575, lr:0.000947, total loss:0.000402\n",
            "[Train] epoch:1576, lr:0.000947, total loss:0.000440\n",
            "[Train] epoch:1577, lr:0.000947, total loss:0.000386\n",
            "[Train] epoch:1578, lr:0.000946, total loss:0.000427\n",
            "[Train] epoch:1579, lr:0.000946, total loss:0.000384\n",
            "[Train] epoch:1580, lr:0.000946, total loss:0.000401\n",
            "[Train] epoch:1581, lr:0.000946, total loss:0.000392\n",
            "[Train] epoch:1582, lr:0.000946, total loss:0.000435\n",
            "[Train] epoch:1583, lr:0.000946, total loss:0.000390\n",
            "[Train] epoch:1584, lr:0.000946, total loss:0.000393\n",
            "[Train] epoch:1585, lr:0.000946, total loss:0.000403\n",
            "[Train] epoch:1586, lr:0.000946, total loss:0.000407\n",
            "[Train] epoch:1587, lr:0.000946, total loss:0.000362\n",
            "[Train] epoch:1588, lr:0.000946, total loss:0.000404\n",
            "[Train] epoch:1589, lr:0.000946, total loss:0.000380\n",
            "[Train] epoch:1590, lr:0.000946, total loss:0.000385\n",
            "[Train] epoch:1591, lr:0.000946, total loss:0.000435\n",
            "[Train] epoch:1592, lr:0.000946, total loss:0.000413\n",
            "[Train] epoch:1593, lr:0.000945, total loss:0.000403\n",
            "[Train] epoch:1594, lr:0.000945, total loss:0.000384\n",
            "[Train] epoch:1595, lr:0.000945, total loss:0.000424\n",
            "[Train] epoch:1596, lr:0.000945, total loss:0.000406\n",
            "[Train] epoch:1597, lr:0.000945, total loss:0.000398\n",
            "[Train] epoch:1598, lr:0.000945, total loss:0.000391\n",
            "[Train] epoch:1599, lr:0.000945, total loss:0.000402\n",
            "[Train] epoch:1600, lr:0.000945, total loss:0.000375\n",
            "[Train] epoch:1601, lr:0.000945, total loss:0.000404\n",
            "[Train] epoch:1602, lr:0.000945, total loss:0.000393\n",
            "[Train] epoch:1603, lr:0.000945, total loss:0.000370\n",
            "[Train] epoch:1604, lr:0.000945, total loss:0.000408\n",
            "[Train] epoch:1605, lr:0.000945, total loss:0.000402\n",
            "[Train] epoch:1606, lr:0.000945, total loss:0.000405\n",
            "[Train] epoch:1607, lr:0.000945, total loss:0.000393\n",
            "[Train] epoch:1608, lr:0.000944, total loss:0.000420\n",
            "[Train] epoch:1609, lr:0.000944, total loss:0.000396\n",
            "[Train] epoch:1610, lr:0.000944, total loss:0.000399\n",
            "[Train] epoch:1611, lr:0.000944, total loss:0.000398\n",
            "[Train] epoch:1612, lr:0.000944, total loss:0.000383\n",
            "[Train] epoch:1613, lr:0.000944, total loss:0.000384\n",
            "[Train] epoch:1614, lr:0.000944, total loss:0.000392\n",
            "[Train] epoch:1615, lr:0.000944, total loss:0.000408\n",
            "[Train] epoch:1616, lr:0.000944, total loss:0.000366\n",
            "[Train] epoch:1617, lr:0.000944, total loss:0.000361\n",
            "[Train] epoch:1618, lr:0.000944, total loss:0.000413\n",
            "[Train] epoch:1619, lr:0.000944, total loss:0.000406\n",
            "[Train] epoch:1620, lr:0.000944, total loss:0.000388\n",
            "[Train] epoch:1621, lr:0.000944, total loss:0.000431\n",
            "[Train] epoch:1622, lr:0.000943, total loss:0.000385\n",
            "[Train] epoch:1623, lr:0.000943, total loss:0.000397\n",
            "[Train] epoch:1624, lr:0.000943, total loss:0.000392\n",
            "[Train] epoch:1625, lr:0.000943, total loss:0.000394\n",
            "[Train] epoch:1626, lr:0.000943, total loss:0.000396\n",
            "[Train] epoch:1627, lr:0.000943, total loss:0.000365\n",
            "[Train] epoch:1628, lr:0.000943, total loss:0.000360\n",
            "[Train] epoch:1629, lr:0.000943, total loss:0.000382\n",
            "[Train] epoch:1630, lr:0.000943, total loss:0.000356\n",
            "[Train] epoch:1631, lr:0.000943, total loss:0.000383\n",
            "[Train] epoch:1632, lr:0.000943, total loss:0.000371\n",
            "[Train] epoch:1633, lr:0.000943, total loss:0.000394\n",
            "[Train] epoch:1634, lr:0.000943, total loss:0.000410\n",
            "[Train] epoch:1635, lr:0.000943, total loss:0.000375\n",
            "[Train] epoch:1636, lr:0.000943, total loss:0.000385\n",
            "[Train] epoch:1637, lr:0.000942, total loss:0.000408\n",
            "[Train] epoch:1638, lr:0.000942, total loss:0.000404\n",
            "[Train] epoch:1639, lr:0.000942, total loss:0.000392\n",
            "[Train] epoch:1640, lr:0.000942, total loss:0.000409\n",
            "[Train] epoch:1641, lr:0.000942, total loss:0.000407\n",
            "[Train] epoch:1642, lr:0.000942, total loss:0.000418\n",
            "[Train] epoch:1643, lr:0.000942, total loss:0.000439\n",
            "[Train] epoch:1644, lr:0.000942, total loss:0.000420\n",
            "[Train] epoch:1645, lr:0.000942, total loss:0.000392\n",
            "[Train] epoch:1646, lr:0.000942, total loss:0.000393\n",
            "[Train] epoch:1647, lr:0.000942, total loss:0.000382\n",
            "[Train] epoch:1648, lr:0.000942, total loss:0.000401\n",
            "[Train] epoch:1649, lr:0.000942, total loss:0.000372\n",
            "[Train] epoch:1650, lr:0.000942, total loss:0.000405\n",
            "[Train] epoch:1651, lr:0.000941, total loss:0.000399\n",
            "[Train] epoch:1652, lr:0.000941, total loss:0.000354\n",
            "[Train] epoch:1653, lr:0.000941, total loss:0.000369\n",
            "[Train] epoch:1654, lr:0.000941, total loss:0.000375\n",
            "[Train] epoch:1655, lr:0.000941, total loss:0.000358\n",
            "[Train] epoch:1656, lr:0.000941, total loss:0.000355\n",
            "[Train] epoch:1657, lr:0.000941, total loss:0.000402\n",
            "[Train] epoch:1658, lr:0.000941, total loss:0.000393\n",
            "[Train] epoch:1659, lr:0.000941, total loss:0.000369\n",
            "[Train] epoch:1660, lr:0.000941, total loss:0.000387\n",
            "[Train] epoch:1661, lr:0.000941, total loss:0.000366\n",
            "[Train] epoch:1662, lr:0.000941, total loss:0.000379\n",
            "[Train] epoch:1663, lr:0.000941, total loss:0.000366\n",
            "[Train] epoch:1664, lr:0.000941, total loss:0.000386\n",
            "[Train] epoch:1665, lr:0.000940, total loss:0.000382\n",
            "[Train] epoch:1666, lr:0.000940, total loss:0.000429\n",
            "[Train] epoch:1667, lr:0.000940, total loss:0.000424\n",
            "[Train] epoch:1668, lr:0.000940, total loss:0.000387\n",
            "[Train] epoch:1669, lr:0.000940, total loss:0.000381\n",
            "[Train] epoch:1670, lr:0.000940, total loss:0.000380\n",
            "[Train] epoch:1671, lr:0.000940, total loss:0.000373\n",
            "[Train] epoch:1672, lr:0.000940, total loss:0.000371\n",
            "[Train] epoch:1673, lr:0.000940, total loss:0.000380\n",
            "[Train] epoch:1674, lr:0.000940, total loss:0.000412\n",
            "[Train] epoch:1675, lr:0.000940, total loss:0.000365\n",
            "[Train] epoch:1676, lr:0.000940, total loss:0.000370\n",
            "[Train] epoch:1677, lr:0.000940, total loss:0.000359\n",
            "[Train] epoch:1678, lr:0.000940, total loss:0.000376\n",
            "[Train] epoch:1679, lr:0.000939, total loss:0.000381\n",
            "[Train] epoch:1680, lr:0.000939, total loss:0.000361\n",
            "[Train] epoch:1681, lr:0.000939, total loss:0.000361\n",
            "[Train] epoch:1682, lr:0.000939, total loss:0.000345\n",
            "[Train] epoch:1683, lr:0.000939, total loss:0.000367\n",
            "[Train] epoch:1684, lr:0.000939, total loss:0.000364\n",
            "[Train] epoch:1685, lr:0.000939, total loss:0.000401\n",
            "[Train] epoch:1686, lr:0.000939, total loss:0.000415\n",
            "[Train] epoch:1687, lr:0.000939, total loss:0.000405\n",
            "[Train] epoch:1688, lr:0.000939, total loss:0.000404\n",
            "[Train] epoch:1689, lr:0.000939, total loss:0.000364\n",
            "[Train] epoch:1690, lr:0.000939, total loss:0.000389\n",
            "[Train] epoch:1691, lr:0.000939, total loss:0.000397\n",
            "[Train] epoch:1692, lr:0.000939, total loss:0.000433\n",
            "[Train] epoch:1693, lr:0.000939, total loss:0.000422\n",
            "[Train] epoch:1694, lr:0.000938, total loss:0.000428\n",
            "[Train] epoch:1695, lr:0.000938, total loss:0.000374\n",
            "[Train] epoch:1696, lr:0.000938, total loss:0.000385\n",
            "[Train] epoch:1697, lr:0.000938, total loss:0.000352\n",
            "[Train] epoch:1698, lr:0.000938, total loss:0.000364\n",
            "[Train] epoch:1699, lr:0.000938, total loss:0.000360\n",
            "[Train] epoch:1700, lr:0.000938, total loss:0.000356\n",
            "[Train] epoch:1701, lr:0.000938, total loss:0.000375\n",
            "[Train] epoch:1702, lr:0.000938, total loss:0.000378\n",
            "[Train] epoch:1703, lr:0.000938, total loss:0.000375\n",
            "[Train] epoch:1704, lr:0.000938, total loss:0.000384\n",
            "[Train] epoch:1705, lr:0.000938, total loss:0.000414\n",
            "[Train] epoch:1706, lr:0.000938, total loss:0.000385\n",
            "[Train] epoch:1707, lr:0.000937, total loss:0.000402\n",
            "[Train] epoch:1708, lr:0.000937, total loss:0.000378\n",
            "[Train] epoch:1709, lr:0.000937, total loss:0.000365\n",
            "[Train] epoch:1710, lr:0.000937, total loss:0.000388\n",
            "[Train] epoch:1711, lr:0.000937, total loss:0.000377\n",
            "[Train] epoch:1712, lr:0.000937, total loss:0.000371\n",
            "[Train] epoch:1713, lr:0.000937, total loss:0.000368\n",
            "[Train] epoch:1714, lr:0.000937, total loss:0.000372\n",
            "[Train] epoch:1715, lr:0.000937, total loss:0.000370\n",
            "[Train] epoch:1716, lr:0.000937, total loss:0.000370\n",
            "[Train] epoch:1717, lr:0.000937, total loss:0.000387\n",
            "[Train] epoch:1718, lr:0.000937, total loss:0.000378\n",
            "[Train] epoch:1719, lr:0.000937, total loss:0.000374\n",
            "[Train] epoch:1720, lr:0.000937, total loss:0.000370\n",
            "[Train] epoch:1721, lr:0.000936, total loss:0.000401\n",
            "[Train] epoch:1722, lr:0.000936, total loss:0.000361\n",
            "[Train] epoch:1723, lr:0.000936, total loss:0.000377\n",
            "[Train] epoch:1724, lr:0.000936, total loss:0.000393\n",
            "[Train] epoch:1725, lr:0.000936, total loss:0.000400\n",
            "[Train] epoch:1726, lr:0.000936, total loss:0.000357\n",
            "[Train] epoch:1727, lr:0.000936, total loss:0.000348\n",
            "[Train] epoch:1728, lr:0.000936, total loss:0.000377\n",
            "[Train] epoch:1729, lr:0.000936, total loss:0.000352\n",
            "[Train] epoch:1730, lr:0.000936, total loss:0.000366\n",
            "[Train] epoch:1731, lr:0.000936, total loss:0.000385\n",
            "[Train] epoch:1732, lr:0.000936, total loss:0.000348\n",
            "[Train] epoch:1733, lr:0.000936, total loss:0.000388\n",
            "[Train] epoch:1734, lr:0.000936, total loss:0.000378\n",
            "[Train] epoch:1735, lr:0.000935, total loss:0.000347\n",
            "[Train] epoch:1736, lr:0.000935, total loss:0.000356\n",
            "[Train] epoch:1737, lr:0.000935, total loss:0.000365\n",
            "[Train] epoch:1738, lr:0.000935, total loss:0.000365\n",
            "[Train] epoch:1739, lr:0.000935, total loss:0.000385\n",
            "[Train] epoch:1740, lr:0.000935, total loss:0.000378\n",
            "[Train] epoch:1741, lr:0.000935, total loss:0.000343\n",
            "[Train] epoch:1742, lr:0.000935, total loss:0.000358\n",
            "[Train] epoch:1743, lr:0.000935, total loss:0.000361\n",
            "[Train] epoch:1744, lr:0.000935, total loss:0.000370\n",
            "[Train] epoch:1745, lr:0.000935, total loss:0.000386\n",
            "[Train] epoch:1746, lr:0.000935, total loss:0.000384\n",
            "[Train] epoch:1747, lr:0.000935, total loss:0.000381\n",
            "[Train] epoch:1748, lr:0.000935, total loss:0.000358\n",
            "[Train] epoch:1749, lr:0.000934, total loss:0.000360\n",
            "[Train] epoch:1750, lr:0.000934, total loss:0.000382\n",
            "[Train] epoch:1751, lr:0.000934, total loss:0.000392\n",
            "[Train] epoch:1752, lr:0.000934, total loss:0.000343\n",
            "[Train] epoch:1753, lr:0.000934, total loss:0.000357\n",
            "[Train] epoch:1754, lr:0.000934, total loss:0.000361\n",
            "[Train] epoch:1755, lr:0.000934, total loss:0.000410\n",
            "[Train] epoch:1756, lr:0.000934, total loss:0.000375\n",
            "[Train] epoch:1757, lr:0.000934, total loss:0.000349\n",
            "[Train] epoch:1758, lr:0.000934, total loss:0.000340\n",
            "[Train] epoch:1759, lr:0.000934, total loss:0.000346\n",
            "[Train] epoch:1760, lr:0.000934, total loss:0.000365\n",
            "[Train] epoch:1761, lr:0.000934, total loss:0.000349\n",
            "[Train] epoch:1762, lr:0.000933, total loss:0.000368\n",
            "[Train] epoch:1763, lr:0.000933, total loss:0.000362\n",
            "[Train] epoch:1764, lr:0.000933, total loss:0.000419\n",
            "[Train] epoch:1765, lr:0.000933, total loss:0.000391\n",
            "[Train] epoch:1766, lr:0.000933, total loss:0.000349\n",
            "[Train] epoch:1767, lr:0.000933, total loss:0.000363\n",
            "[Train] epoch:1768, lr:0.000933, total loss:0.000367\n",
            "[Train] epoch:1769, lr:0.000933, total loss:0.000353\n",
            "[Train] epoch:1770, lr:0.000933, total loss:0.000370\n",
            "[Train] epoch:1771, lr:0.000933, total loss:0.000368\n",
            "[Train] epoch:1772, lr:0.000933, total loss:0.000371\n",
            "[Train] epoch:1773, lr:0.000933, total loss:0.000347\n",
            "[Train] epoch:1774, lr:0.000933, total loss:0.000354\n",
            "[Train] epoch:1775, lr:0.000933, total loss:0.000389\n",
            "[Train] epoch:1776, lr:0.000932, total loss:0.000343\n",
            "[Train] epoch:1777, lr:0.000932, total loss:0.000347\n",
            "[Train] epoch:1778, lr:0.000932, total loss:0.000355\n",
            "[Train] epoch:1779, lr:0.000932, total loss:0.000390\n",
            "[Train] epoch:1780, lr:0.000932, total loss:0.000354\n",
            "[Train] epoch:1781, lr:0.000932, total loss:0.000330\n",
            "[Train] epoch:1782, lr:0.000932, total loss:0.000358\n",
            "[Train] epoch:1783, lr:0.000932, total loss:0.000351\n",
            "[Train] epoch:1784, lr:0.000932, total loss:0.000377\n",
            "[Train] epoch:1785, lr:0.000932, total loss:0.000362\n",
            "[Train] epoch:1786, lr:0.000932, total loss:0.000339\n",
            "[Train] epoch:1787, lr:0.000932, total loss:0.000377\n",
            "[Train] epoch:1788, lr:0.000932, total loss:0.000373\n",
            "[Train] epoch:1789, lr:0.000931, total loss:0.000337\n",
            "[Train] epoch:1790, lr:0.000931, total loss:0.000368\n",
            "[Train] epoch:1791, lr:0.000931, total loss:0.000367\n",
            "[Train] epoch:1792, lr:0.000931, total loss:0.000353\n",
            "[Train] epoch:1793, lr:0.000931, total loss:0.000365\n",
            "[Train] epoch:1794, lr:0.000931, total loss:0.000391\n",
            "[Train] epoch:1795, lr:0.000931, total loss:0.000379\n",
            "[Train] epoch:1796, lr:0.000931, total loss:0.000376\n",
            "[Train] epoch:1797, lr:0.000931, total loss:0.000370\n",
            "[Train] epoch:1798, lr:0.000931, total loss:0.000368\n",
            "[Train] epoch:1799, lr:0.000931, total loss:0.000345\n",
            "[Train] epoch:1800, lr:0.000931, total loss:0.000319\n",
            "[Train] epoch:1801, lr:0.000931, total loss:0.000337\n",
            "[Train] epoch:1802, lr:0.000930, total loss:0.000334\n",
            "[Train] epoch:1803, lr:0.000930, total loss:0.000382\n",
            "[Train] epoch:1804, lr:0.000930, total loss:0.000366\n",
            "[Train] epoch:1805, lr:0.000930, total loss:0.000369\n",
            "[Train] epoch:1806, lr:0.000930, total loss:0.000367\n",
            "[Train] epoch:1807, lr:0.000930, total loss:0.000370\n",
            "[Train] epoch:1808, lr:0.000930, total loss:0.000360\n",
            "[Train] epoch:1809, lr:0.000930, total loss:0.000338\n",
            "[Train] epoch:1810, lr:0.000930, total loss:0.000325\n",
            "[Train] epoch:1811, lr:0.000930, total loss:0.000365\n",
            "[Train] epoch:1812, lr:0.000930, total loss:0.000358\n",
            "[Train] epoch:1813, lr:0.000930, total loss:0.000359\n",
            "[Train] epoch:1814, lr:0.000930, total loss:0.000368\n",
            "[Train] epoch:1815, lr:0.000930, total loss:0.000357\n",
            "[Train] epoch:1816, lr:0.000929, total loss:0.000368\n",
            "[Train] epoch:1817, lr:0.000929, total loss:0.000347\n",
            "[Train] epoch:1818, lr:0.000929, total loss:0.000361\n",
            "[Train] epoch:1819, lr:0.000929, total loss:0.000342\n",
            "[Train] epoch:1820, lr:0.000929, total loss:0.000390\n",
            "[Train] epoch:1821, lr:0.000929, total loss:0.000336\n",
            "[Train] epoch:1822, lr:0.000929, total loss:0.000323\n",
            "[Train] epoch:1823, lr:0.000929, total loss:0.000367\n",
            "[Train] epoch:1824, lr:0.000929, total loss:0.000329\n",
            "[Train] epoch:1825, lr:0.000929, total loss:0.000346\n",
            "[Train] epoch:1826, lr:0.000929, total loss:0.000347\n",
            "[Train] epoch:1827, lr:0.000929, total loss:0.000376\n",
            "[Train] epoch:1828, lr:0.000929, total loss:0.000363\n",
            "[Train] epoch:1829, lr:0.000928, total loss:0.000383\n",
            "[Train] epoch:1830, lr:0.000928, total loss:0.000358\n",
            "[Train] epoch:1831, lr:0.000928, total loss:0.000350\n",
            "[Train] epoch:1832, lr:0.000928, total loss:0.000379\n",
            "[Train] epoch:1833, lr:0.000928, total loss:0.000373\n",
            "[Train] epoch:1834, lr:0.000928, total loss:0.000333\n",
            "[Train] epoch:1835, lr:0.000928, total loss:0.000346\n",
            "[Train] epoch:1836, lr:0.000928, total loss:0.000366\n",
            "[Train] epoch:1837, lr:0.000928, total loss:0.000372\n",
            "[Train] epoch:1838, lr:0.000928, total loss:0.000342\n",
            "[Train] epoch:1839, lr:0.000928, total loss:0.000352\n",
            "[Train] epoch:1840, lr:0.000928, total loss:0.000389\n",
            "[Train] epoch:1841, lr:0.000928, total loss:0.000374\n",
            "[Train] epoch:1842, lr:0.000927, total loss:0.000366\n",
            "[Train] epoch:1843, lr:0.000927, total loss:0.000375\n",
            "[Train] epoch:1844, lr:0.000927, total loss:0.000348\n",
            "[Train] epoch:1845, lr:0.000927, total loss:0.000331\n",
            "[Train] epoch:1846, lr:0.000927, total loss:0.000344\n",
            "[Train] epoch:1847, lr:0.000927, total loss:0.000343\n",
            "[Train] epoch:1848, lr:0.000927, total loss:0.000341\n",
            "[Train] epoch:1849, lr:0.000927, total loss:0.000333\n",
            "[Train] epoch:1850, lr:0.000927, total loss:0.000349\n",
            "[Train] epoch:1851, lr:0.000927, total loss:0.000351\n",
            "[Train] epoch:1852, lr:0.000927, total loss:0.000365\n",
            "[Train] epoch:1853, lr:0.000927, total loss:0.000353\n",
            "[Train] epoch:1854, lr:0.000927, total loss:0.000343\n",
            "[Train] epoch:1855, lr:0.000926, total loss:0.000356\n",
            "[Train] epoch:1856, lr:0.000926, total loss:0.000350\n",
            "[Train] epoch:1857, lr:0.000926, total loss:0.000357\n",
            "[Train] epoch:1858, lr:0.000926, total loss:0.000335\n",
            "[Train] epoch:1859, lr:0.000926, total loss:0.000342\n",
            "[Train] epoch:1860, lr:0.000926, total loss:0.000322\n",
            "[Train] epoch:1861, lr:0.000926, total loss:0.000336\n",
            "[Train] epoch:1862, lr:0.000926, total loss:0.000366\n",
            "[Train] epoch:1863, lr:0.000926, total loss:0.000360\n",
            "[Train] epoch:1864, lr:0.000926, total loss:0.000350\n",
            "[Train] epoch:1865, lr:0.000926, total loss:0.000338\n",
            "[Train] epoch:1866, lr:0.000926, total loss:0.000344\n",
            "[Train] epoch:1867, lr:0.000926, total loss:0.000358\n",
            "[Train] epoch:1868, lr:0.000925, total loss:0.000334\n",
            "[Train] epoch:1869, lr:0.000925, total loss:0.000343\n",
            "[Train] epoch:1870, lr:0.000925, total loss:0.000344\n",
            "[Train] epoch:1871, lr:0.000925, total loss:0.000350\n",
            "[Train] epoch:1872, lr:0.000925, total loss:0.000362\n",
            "[Train] epoch:1873, lr:0.000925, total loss:0.000341\n",
            "[Train] epoch:1874, lr:0.000925, total loss:0.000348\n",
            "[Train] epoch:1875, lr:0.000925, total loss:0.000331\n",
            "[Train] epoch:1876, lr:0.000925, total loss:0.000343\n",
            "[Train] epoch:1877, lr:0.000925, total loss:0.000334\n",
            "[Train] epoch:1878, lr:0.000925, total loss:0.000342\n",
            "[Train] epoch:1879, lr:0.000925, total loss:0.000355\n",
            "[Train] epoch:1880, lr:0.000924, total loss:0.000334\n",
            "[Train] epoch:1881, lr:0.000924, total loss:0.000367\n",
            "[Train] epoch:1882, lr:0.000924, total loss:0.000347\n",
            "[Train] epoch:1883, lr:0.000924, total loss:0.000327\n",
            "[Train] epoch:1884, lr:0.000924, total loss:0.000327\n",
            "[Train] epoch:1885, lr:0.000924, total loss:0.000337\n",
            "[Train] epoch:1886, lr:0.000924, total loss:0.000336\n",
            "[Train] epoch:1887, lr:0.000924, total loss:0.000332\n",
            "[Train] epoch:1888, lr:0.000924, total loss:0.000354\n",
            "[Train] epoch:1889, lr:0.000924, total loss:0.000326\n",
            "[Train] epoch:1890, lr:0.000924, total loss:0.000327\n",
            "[Train] epoch:1891, lr:0.000924, total loss:0.000317\n",
            "[Train] epoch:1892, lr:0.000924, total loss:0.000332\n",
            "[Train] epoch:1893, lr:0.000923, total loss:0.000358\n",
            "[Train] epoch:1894, lr:0.000923, total loss:0.000349\n",
            "[Train] epoch:1895, lr:0.000923, total loss:0.000332\n",
            "[Train] epoch:1896, lr:0.000923, total loss:0.000344\n",
            "[Train] epoch:1897, lr:0.000923, total loss:0.000333\n",
            "[Train] epoch:1898, lr:0.000923, total loss:0.000348\n",
            "[Train] epoch:1899, lr:0.000923, total loss:0.000386\n",
            "[Train] epoch:1900, lr:0.000923, total loss:0.000359\n",
            "[Train] epoch:1901, lr:0.000923, total loss:0.000337\n",
            "[Train] epoch:1902, lr:0.000923, total loss:0.000347\n",
            "[Train] epoch:1903, lr:0.000923, total loss:0.000356\n",
            "[Train] epoch:1904, lr:0.000923, total loss:0.000346\n",
            "[Train] epoch:1905, lr:0.000923, total loss:0.000334\n",
            "[Train] epoch:1906, lr:0.000922, total loss:0.000323\n",
            "[Train] epoch:1907, lr:0.000922, total loss:0.000325\n",
            "[Train] epoch:1908, lr:0.000922, total loss:0.000324\n",
            "[Train] epoch:1909, lr:0.000922, total loss:0.000344\n",
            "[Train] epoch:1910, lr:0.000922, total loss:0.000335\n",
            "[Train] epoch:1911, lr:0.000922, total loss:0.000337\n",
            "[Train] epoch:1912, lr:0.000922, total loss:0.000327\n",
            "[Train] epoch:1913, lr:0.000922, total loss:0.000344\n",
            "[Train] epoch:1914, lr:0.000922, total loss:0.000319\n",
            "[Train] epoch:1915, lr:0.000922, total loss:0.000357\n",
            "[Train] epoch:1916, lr:0.000922, total loss:0.000393\n",
            "[Train] epoch:1917, lr:0.000922, total loss:0.000329\n",
            "[Train] epoch:1918, lr:0.000921, total loss:0.000331\n",
            "[Train] epoch:1919, lr:0.000921, total loss:0.000359\n",
            "[Train] epoch:1920, lr:0.000921, total loss:0.000359\n",
            "[Train] epoch:1921, lr:0.000921, total loss:0.000326\n",
            "[Train] epoch:1922, lr:0.000921, total loss:0.000358\n",
            "[Train] epoch:1923, lr:0.000921, total loss:0.000330\n",
            "[Train] epoch:1924, lr:0.000921, total loss:0.000354\n",
            "[Train] epoch:1925, lr:0.000921, total loss:0.000316\n",
            "[Train] epoch:1926, lr:0.000921, total loss:0.000358\n",
            "[Train] epoch:1927, lr:0.000921, total loss:0.000335\n",
            "[Train] epoch:1928, lr:0.000921, total loss:0.000356\n",
            "[Train] epoch:1929, lr:0.000921, total loss:0.000334\n",
            "[Train] epoch:1930, lr:0.000921, total loss:0.000348\n",
            "[Train] epoch:1931, lr:0.000920, total loss:0.000386\n",
            "[Train] epoch:1932, lr:0.000920, total loss:0.000364\n",
            "[Train] epoch:1933, lr:0.000920, total loss:0.000341\n",
            "[Train] epoch:1934, lr:0.000920, total loss:0.000362\n",
            "[Train] epoch:1935, lr:0.000920, total loss:0.000403\n",
            "[Train] epoch:1936, lr:0.000920, total loss:0.000364\n",
            "[Train] epoch:1937, lr:0.000920, total loss:0.000373\n",
            "[Train] epoch:1938, lr:0.000920, total loss:0.000348\n",
            "[Train] epoch:1939, lr:0.000920, total loss:0.000316\n",
            "[Train] epoch:1940, lr:0.000920, total loss:0.000332\n",
            "[Train] epoch:1941, lr:0.000920, total loss:0.000336\n",
            "[Train] epoch:1942, lr:0.000920, total loss:0.000346\n",
            "[Train] epoch:1943, lr:0.000919, total loss:0.000343\n",
            "[Train] epoch:1944, lr:0.000919, total loss:0.000340\n",
            "[Train] epoch:1945, lr:0.000919, total loss:0.000359\n",
            "[Train] epoch:1946, lr:0.000919, total loss:0.000327\n",
            "[Train] epoch:1947, lr:0.000919, total loss:0.000347\n",
            "[Train] epoch:1948, lr:0.000919, total loss:0.000344\n",
            "[Train] epoch:1949, lr:0.000919, total loss:0.000366\n",
            "[Train] epoch:1950, lr:0.000919, total loss:0.000391\n",
            "[Train] epoch:1951, lr:0.000919, total loss:0.000374\n",
            "[Train] epoch:1952, lr:0.000919, total loss:0.000330\n",
            "[Train] epoch:1953, lr:0.000919, total loss:0.000362\n",
            "[Train] epoch:1954, lr:0.000919, total loss:0.000341\n",
            "[Train] epoch:1955, lr:0.000918, total loss:0.000365\n",
            "[Train] epoch:1956, lr:0.000918, total loss:0.000340\n",
            "[Train] epoch:1957, lr:0.000918, total loss:0.000326\n",
            "[Train] epoch:1958, lr:0.000918, total loss:0.000313\n",
            "[Train] epoch:1959, lr:0.000918, total loss:0.000338\n",
            "[Train] epoch:1960, lr:0.000918, total loss:0.000318\n",
            "[Train] epoch:1961, lr:0.000918, total loss:0.000314\n",
            "[Train] epoch:1962, lr:0.000918, total loss:0.000325\n",
            "[Train] epoch:1963, lr:0.000918, total loss:0.000339\n",
            "[Train] epoch:1964, lr:0.000918, total loss:0.000352\n",
            "[Train] epoch:1965, lr:0.000918, total loss:0.000346\n",
            "[Train] epoch:1966, lr:0.000918, total loss:0.000320\n",
            "[Train] epoch:1967, lr:0.000918, total loss:0.000320\n",
            "[Train] epoch:1968, lr:0.000917, total loss:0.000304\n",
            "[Train] epoch:1969, lr:0.000917, total loss:0.000322\n",
            "[Train] epoch:1970, lr:0.000917, total loss:0.000311\n",
            "[Train] epoch:1971, lr:0.000917, total loss:0.000325\n",
            "[Train] epoch:1972, lr:0.000917, total loss:0.000347\n",
            "[Train] epoch:1973, lr:0.000917, total loss:0.000326\n",
            "[Train] epoch:1974, lr:0.000917, total loss:0.000349\n",
            "[Train] epoch:1975, lr:0.000917, total loss:0.000348\n",
            "[Train] epoch:1976, lr:0.000917, total loss:0.000335\n",
            "[Train] epoch:1977, lr:0.000917, total loss:0.000332\n",
            "[Train] epoch:1978, lr:0.000917, total loss:0.000338\n",
            "[Train] epoch:1979, lr:0.000917, total loss:0.000343\n",
            "[Train] epoch:1980, lr:0.000916, total loss:0.000346\n",
            "[Train] epoch:1981, lr:0.000916, total loss:0.000371\n",
            "[Train] epoch:1982, lr:0.000916, total loss:0.000357\n",
            "[Train] epoch:1983, lr:0.000916, total loss:0.000333\n",
            "[Train] epoch:1984, lr:0.000916, total loss:0.000343\n",
            "[Train] epoch:1985, lr:0.000916, total loss:0.000363\n",
            "[Train] epoch:1986, lr:0.000916, total loss:0.000347\n",
            "[Train] epoch:1987, lr:0.000916, total loss:0.000392\n",
            "[Train] epoch:1988, lr:0.000916, total loss:0.000340\n",
            "[Train] epoch:1989, lr:0.000916, total loss:0.000343\n",
            "[Train] epoch:1990, lr:0.000916, total loss:0.000325\n",
            "[Train] epoch:1991, lr:0.000916, total loss:0.000362\n",
            "[Train] epoch:1992, lr:0.000915, total loss:0.000329\n",
            "[Train] epoch:1993, lr:0.000915, total loss:0.000322\n",
            "[Train] epoch:1994, lr:0.000915, total loss:0.000327\n",
            "[Train] epoch:1995, lr:0.000915, total loss:0.000336\n",
            "[Train] epoch:1996, lr:0.000915, total loss:0.000351\n",
            "[Train] epoch:1997, lr:0.000915, total loss:0.000350\n",
            "[Train] epoch:1998, lr:0.000915, total loss:0.000356\n",
            "[Train] epoch:1999, lr:0.000915, total loss:0.000336\n",
            "[Train] epoch:2000, lr:0.000915, total loss:0.000318\n",
            "[Train] epoch:2001, lr:0.000915, total loss:0.000325\n",
            "[Train] epoch:2002, lr:0.000915, total loss:0.000309\n",
            "[Train] epoch:2003, lr:0.000915, total loss:0.000314\n",
            "Early stopping at:  2003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compare metrics, plot loss curve and save best model"
      ],
      "metadata": {
        "id": "IAKBIACZEIK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "if overwrite:\n",
        "  torch.save(pinn.best_model, \"/content/drive/MyDrive/Machine Learning Project (ECE 228)/Battery Dataset/pinn_extended.pth\")\n",
        "  pinn.load_model(\"/content/drive/MyDrive/Machine Learning Project (ECE 228)/Battery Dataset/pinn_extended.pth\")\n",
        "true_label, pred_label = pinn.Test(loader['test_3'])\n",
        "mse = metrics.mean_squared_error(torch.tensor(true_label), torch.tensor(pred_label))\n",
        "mae = metrics.mean_absolute_error(torch.tensor(true_label), torch.tensor(pred_label))\n",
        "r2 = metrics.r2_score(torch.tensor(true_label), torch.tensor(pred_label))\n",
        "mape = metrics.mean_absolute_percentage_error(torch.tensor(true_label), torch.tensor(pred_label))\n",
        "\n",
        "print(mse, mae, r2, mape)\n",
        "\n",
        "print(count_parameters(pinn))\n",
        "result_dict = [{\n",
        "    'run name': 'extended features',\n",
        "    'mse': mse,\n",
        "    'mae': mae,\n",
        "    'r2': r2,\n",
        "    'mape': mape,\n",
        "    'param count': count_parameters(pinn)\n",
        "}]\n",
        "df = pd.DataFrame(result_dict)\n",
        "try:\n",
        "  results = pd.read_csv('/content/drive/MyDrive/Machine Learning Project (ECE 228)/Battery Dataset/results.csv')\n",
        "  results = pd.concat([results, df], ignore_index=True)\n",
        "except:\n",
        "  results = df\n",
        "\n",
        "if overwrite:\n",
        "  results.to_csv('/content/drive/MyDrive/Machine Learning Project (ECE 228)/Battery Dataset/results.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MsXOil-DTbn",
        "outputId": "f12e6295-9641-4051-90f1-7d83c2947e18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0008128000624456007 0.017109501014153164 0.9405050176506593 0.02352942733821024\n",
            "14742\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the loss curve\n",
        "import matplotlib.pyplot as plt\n",
        "x = list(range(1,len(losses_extended) + 1))\n",
        "plt.yscale('log')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Log-Scale Loss plot for extended feature model')\n",
        "plt.plot(x, losses_extended)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "t58vWc9XDazv",
        "outputId": "ff410076-0679-432b-e8fd-34ff1d973668"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7d353c918a90>]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Second Derivatives on Regular Features"
      ],
      "metadata": {
        "id": "ta0EQrE6DusP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# little bit of hee hee haha data manipulation\n",
        "df = pd.read_csv('/content/drive/MyDrive/Machine Learning Project (ECE 228)/Battery Dataset/features.csv')\n",
        "df = df[df['charge_CC_time_V']!=0.0] # these were giving strange results\n",
        "df = df[df['charge_CV_time_I']!=0.0]\n",
        "df.to_csv(\"clean_features.csv\", index=False)"
      ],
      "metadata": {
        "id": "iSf2LalQEKof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class args:\n",
        "  pass\n",
        "\n",
        "# hehe tricks\n",
        "arg = args()\n",
        "\n",
        "arg.data = 'our_stuff'\n",
        "arg.batch = 10\n",
        "arg.batch_size = 256\n",
        "arg.normalization_method = 'z-score'\n",
        "arg.epochs = 10000\n",
        "arg.lr = 1e-3\n",
        "arg.warmup_epochs = 10\n",
        "arg.warmup_lr = 5e-4\n",
        "arg.final_lr = 1e-4\n",
        "arg.lr_F = 1e-3\n",
        "arg.iter_per_epoch = 1\n",
        "arg.F_layers_num = 3\n",
        "arg.F_hidden_dim = 60\n",
        "arg.alpha = 1\n",
        "arg.beta = 1\n",
        "arg.early_stop = 500\n",
        "arg.feature_length = 17 # use 17 for regular feature count\n",
        "arg.second_derivatives = True # use to not include second derivatives\n"
      ],
      "metadata": {
        "id": "zdxgzvl2EVjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ok let's load a dataset\n",
        "reader = DF(arg)"
      ],
      "metadata": {
        "id": "TY4PnQxvEYrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's get a loader back\n",
        "loader = reader.load_all([\"./clean_features.csv\"], nominal_capacity=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvbvlfWzEnm3",
        "outputId": "31fef41f-baa7-413c-bc21-5db8d860f3d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      charge_CC_mean_V  charge_CC_std_V  charge_CC_kurtosis_V  \\\n",
            "0             1.051718        -0.713588             -0.094252   \n",
            "1             1.224559        -0.542147             -0.040044   \n",
            "2             0.692781         0.079437             -0.421930   \n",
            "3             1.704675        -1.676956              2.159515   \n",
            "4             0.785932        -0.140048              0.086626   \n",
            "...                ...              ...                   ...   \n",
            "2420         -0.443082         0.720153             -0.611928   \n",
            "2421         -0.453659         0.645414             -0.614842   \n",
            "2422         -0.422004         0.627261             -0.612344   \n",
            "2423         -0.513857         0.688364             -0.596764   \n",
            "2424         -0.418204         0.602553             -0.612677   \n",
            "\n",
            "      charge_CC_skew_V  charge_CC_time_V  charge_CC_charge  charge_CC_slope_V  \\\n",
            "0            -0.723178         -2.570478         -2.557431           7.281268   \n",
            "1            -0.797171         -2.611115         -2.598165           8.519176   \n",
            "2            -0.471074         -2.599677         -2.586782           8.297809   \n",
            "3            -2.014463         -0.169714         -0.144630          -0.227694   \n",
            "4            -0.736379         -0.690659         -0.667485           0.054417   \n",
            "...                ...               ...               ...                ...   \n",
            "2420          0.417345         -0.381994         -0.356347          -0.103665   \n",
            "2421          0.423965         -0.398756         -0.373971          -0.101562   \n",
            "2422          0.427483         -0.422011         -0.396601          -0.092649   \n",
            "2423          0.499367         -0.168751         -0.142701          -0.206949   \n",
            "2424          0.405564         -0.445883         -0.420552          -0.081591   \n",
            "\n",
            "      charge_CC_entropy_V  charge_CV_mean_I  charge_CV_std_dev_I  \\\n",
            "0               -3.434897         -3.434933            -3.419796   \n",
            "1               -3.615999         -2.054186            -1.588874   \n",
            "2               -3.681913         -0.509941             0.287333   \n",
            "3               -0.517590         -1.430043            -0.720811   \n",
            "4               -0.822937         -0.144988             0.375235   \n",
            "...                   ...               ...                  ...   \n",
            "2420            -0.719246          0.431733            -1.386463   \n",
            "2421            -0.736011          0.417935            -1.654983   \n",
            "2422            -0.752977          0.522672            -1.230410   \n",
            "2423            -0.622616          0.321845            -1.698875   \n",
            "2424            -0.778813          0.738648            -1.126685   \n",
            "\n",
            "      charge_CV_kurtosis_I  charge_CV_skew_I  charge_CV_time_I  \\\n",
            "0                 3.165489          3.146038          1.305891   \n",
            "1                 2.716360          2.960147          1.264916   \n",
            "2                -0.067089          0.816123          0.149106   \n",
            "3                 1.469249          1.189845          1.432231   \n",
            "4                 0.212768          0.748297          1.322877   \n",
            "...                    ...               ...               ...   \n",
            "2420              0.047583         -0.033412         -0.581324   \n",
            "2421             -0.034865         -0.175776         -0.614544   \n",
            "2422              0.033223         -0.014649         -0.585731   \n",
            "2423              0.301483          0.234447         -0.397120   \n",
            "2424              0.042512          0.021309         -0.553738   \n",
            "\n",
            "      charge_CV_charge_I  charge_CV_slope_I  charge_CV_entropy_I  cycle_index  \\\n",
            "0               0.934968           1.224660             0.135281    -1.224588   \n",
            "1               1.084453           1.194460             0.096569    -1.204177   \n",
            "2               0.124126           0.454231            -0.968551    -0.836783   \n",
            "3               1.342167           1.251640             0.196704    -1.224588   \n",
            "4               1.413682           1.239426             0.128908    -1.204177   \n",
            "...                  ...                ...                  ...          ...   \n",
            "2420           -0.568314          -0.375006            -1.572750     1.347170   \n",
            "2421           -0.606129          -0.416189            -1.618723     1.367581   \n",
            "2422           -0.564282          -0.364575            -1.602036     1.387992   \n",
            "2423           -0.377817          -0.099872            -1.411697     1.408402   \n",
            "2424           -0.514577          -0.325104            -1.586194     1.428813   \n",
            "\n",
            "      capacity  \n",
            "0     0.532271  \n",
            "1     0.521953  \n",
            "2     0.350731  \n",
            "3     0.782358  \n",
            "4     0.779403  \n",
            "...        ...  \n",
            "2420  0.698253  \n",
            "2421  0.697567  \n",
            "2422  0.689922  \n",
            "2423  0.689690  \n",
            "2424  0.691855  \n",
            "\n",
            "[2425 rows x 18 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pinn = PINN(arg)"
      ],
      "metadata": {
        "id": "oteVgNZqErqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses_second_derivative = pinn.Train(trainloader=loader['train'],\n",
        "           validloader=loader['valid'],\n",
        "           testloader=loader['test'], debug=True)\n",
        "if overwrite:\n",
        "  np.save('/content/drive/MyDrive/Machine Learning Project (ECE 228)/Battery Dataset/loss_second_derivative.npy', losses_second_derivative)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuA7C_YwE1B-",
        "outputId": "cde2c5ab-c267-4bad-ef64-8aab227f02f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "[Train] epoch:1204, lr:0.000969, total loss:0.000625\n",
            "[Train] epoch:1205, lr:0.000969, total loss:0.000650\n",
            "[Train] epoch:1206, lr:0.000969, total loss:0.000637\n",
            "[Train] epoch:1207, lr:0.000969, total loss:0.000675\n",
            "[Train] epoch:1208, lr:0.000968, total loss:0.000670\n",
            "[Train] epoch:1209, lr:0.000968, total loss:0.000694\n",
            "[Train] epoch:1210, lr:0.000968, total loss:0.000653\n",
            "[Train] epoch:1211, lr:0.000968, total loss:0.000646\n",
            "[Train] epoch:1212, lr:0.000968, total loss:0.000677\n",
            "[Train] epoch:1213, lr:0.000968, total loss:0.000678\n",
            "[Train] epoch:1214, lr:0.000968, total loss:0.000636\n",
            "[Train] epoch:1215, lr:0.000968, total loss:0.000653\n",
            "[Train] epoch:1216, lr:0.000968, total loss:0.000615\n",
            "[Train] epoch:1217, lr:0.000968, total loss:0.000694\n",
            "[Train] epoch:1218, lr:0.000968, total loss:0.000727\n",
            "[Train] epoch:1219, lr:0.000968, total loss:0.000736\n",
            "[Train] epoch:1220, lr:0.000968, total loss:0.000667\n",
            "[Train] epoch:1221, lr:0.000968, total loss:0.000711\n",
            "[Train] epoch:1222, lr:0.000968, total loss:0.000685\n",
            "[Train] epoch:1223, lr:0.000968, total loss:0.000669\n",
            "[Train] epoch:1224, lr:0.000968, total loss:0.000689\n",
            "[Train] epoch:1225, lr:0.000968, total loss:0.000743\n",
            "[Train] epoch:1226, lr:0.000968, total loss:0.000715\n",
            "[Train] epoch:1227, lr:0.000967, total loss:0.000665\n",
            "[Train] epoch:1228, lr:0.000967, total loss:0.000654\n",
            "[Train] epoch:1229, lr:0.000967, total loss:0.000690\n",
            "[Train] epoch:1230, lr:0.000967, total loss:0.000627\n",
            "[Train] epoch:1231, lr:0.000967, total loss:0.000615\n",
            "[Train] epoch:1232, lr:0.000967, total loss:0.000677\n",
            "[Train] epoch:1233, lr:0.000967, total loss:0.000628\n",
            "[Train] epoch:1234, lr:0.000967, total loss:0.000597\n",
            "[Train] epoch:1235, lr:0.000967, total loss:0.000713\n",
            "[Train] epoch:1236, lr:0.000967, total loss:0.000657\n",
            "[Train] epoch:1237, lr:0.000967, total loss:0.000636\n",
            "[Train] epoch:1238, lr:0.000967, total loss:0.000732\n",
            "[Train] epoch:1239, lr:0.000967, total loss:0.000854\n",
            "[Train] epoch:1240, lr:0.000967, total loss:0.000680\n",
            "[Train] epoch:1241, lr:0.000967, total loss:0.000804\n",
            "[Train] epoch:1242, lr:0.000967, total loss:0.000812\n",
            "[Train] epoch:1243, lr:0.000967, total loss:0.000790\n",
            "[Train] epoch:1244, lr:0.000967, total loss:0.000704\n",
            "[Train] epoch:1245, lr:0.000967, total loss:0.000652\n",
            "[Train] epoch:1246, lr:0.000966, total loss:0.000682\n",
            "[Train] epoch:1247, lr:0.000966, total loss:0.000646\n",
            "[Train] epoch:1248, lr:0.000966, total loss:0.000706\n",
            "[Train] epoch:1249, lr:0.000966, total loss:0.000717\n",
            "[Train] epoch:1250, lr:0.000966, total loss:0.000807\n",
            "[Train] epoch:1251, lr:0.000966, total loss:0.000677\n",
            "[Train] epoch:1252, lr:0.000966, total loss:0.000695\n",
            "[Train] epoch:1253, lr:0.000966, total loss:0.000860\n",
            "[Train] epoch:1254, lr:0.000966, total loss:0.000808\n",
            "[Train] epoch:1255, lr:0.000966, total loss:0.000849\n",
            "[Train] epoch:1256, lr:0.000966, total loss:0.000715\n",
            "[Train] epoch:1257, lr:0.000966, total loss:0.000826\n",
            "[Train] epoch:1258, lr:0.000966, total loss:0.000628\n",
            "[Train] epoch:1259, lr:0.000966, total loss:0.000675\n",
            "[Train] epoch:1260, lr:0.000966, total loss:0.000650\n",
            "[Train] epoch:1261, lr:0.000966, total loss:0.000676\n",
            "[Train] epoch:1262, lr:0.000966, total loss:0.000650\n",
            "[Train] epoch:1263, lr:0.000966, total loss:0.000644\n",
            "[Train] epoch:1264, lr:0.000966, total loss:0.000717\n",
            "[Train] epoch:1265, lr:0.000965, total loss:0.000624\n",
            "[Train] epoch:1266, lr:0.000965, total loss:0.000687\n",
            "[Train] epoch:1267, lr:0.000965, total loss:0.000671\n",
            "[Train] epoch:1268, lr:0.000965, total loss:0.000621\n",
            "[Train] epoch:1269, lr:0.000965, total loss:0.000570\n",
            "[Train] epoch:1270, lr:0.000965, total loss:0.000620\n",
            "[Train] epoch:1271, lr:0.000965, total loss:0.000633\n",
            "[Train] epoch:1272, lr:0.000965, total loss:0.000578\n",
            "[Train] epoch:1273, lr:0.000965, total loss:0.000605\n",
            "[Train] epoch:1274, lr:0.000965, total loss:0.000591\n",
            "[Train] epoch:1275, lr:0.000965, total loss:0.000650\n",
            "[Train] epoch:1276, lr:0.000965, total loss:0.000611\n",
            "[Train] epoch:1277, lr:0.000965, total loss:0.000656\n",
            "[Train] epoch:1278, lr:0.000965, total loss:0.000715\n",
            "[Train] epoch:1279, lr:0.000965, total loss:0.000616\n",
            "[Train] epoch:1280, lr:0.000965, total loss:0.000546\n",
            "[Train] epoch:1281, lr:0.000965, total loss:0.000672\n",
            "[Train] epoch:1282, lr:0.000965, total loss:0.000590\n",
            "[Train] epoch:1283, lr:0.000964, total loss:0.000603\n",
            "[Train] epoch:1284, lr:0.000964, total loss:0.000609\n",
            "[Train] epoch:1285, lr:0.000964, total loss:0.000674\n",
            "[Train] epoch:1286, lr:0.000964, total loss:0.000689\n",
            "[Train] epoch:1287, lr:0.000964, total loss:0.000693\n",
            "[Train] epoch:1288, lr:0.000964, total loss:0.000749\n",
            "[Train] epoch:1289, lr:0.000964, total loss:0.000730\n",
            "[Train] epoch:1290, lr:0.000964, total loss:0.000864\n",
            "[Train] epoch:1291, lr:0.000964, total loss:0.000936\n",
            "[Train] epoch:1292, lr:0.000964, total loss:0.000759\n",
            "[Train] epoch:1293, lr:0.000964, total loss:0.000893\n",
            "[Train] epoch:1294, lr:0.000964, total loss:0.000839\n",
            "[Train] epoch:1295, lr:0.000964, total loss:0.000708\n",
            "[Train] epoch:1296, lr:0.000964, total loss:0.000695\n",
            "[Train] epoch:1297, lr:0.000964, total loss:0.000689\n",
            "[Train] epoch:1298, lr:0.000964, total loss:0.000628\n",
            "[Train] epoch:1299, lr:0.000964, total loss:0.000627\n",
            "[Train] epoch:1300, lr:0.000964, total loss:0.000626\n",
            "[Train] epoch:1301, lr:0.000963, total loss:0.000586\n",
            "[Train] epoch:1302, lr:0.000963, total loss:0.000592\n",
            "[Train] epoch:1303, lr:0.000963, total loss:0.000765\n",
            "[Train] epoch:1304, lr:0.000963, total loss:0.000719\n",
            "[Train] epoch:1305, lr:0.000963, total loss:0.000704\n",
            "[Train] epoch:1306, lr:0.000963, total loss:0.000594\n",
            "[Train] epoch:1307, lr:0.000963, total loss:0.000596\n",
            "[Train] epoch:1308, lr:0.000963, total loss:0.000587\n",
            "[Train] epoch:1309, lr:0.000963, total loss:0.000648\n",
            "[Train] epoch:1310, lr:0.000963, total loss:0.000586\n",
            "[Train] epoch:1311, lr:0.000963, total loss:0.000611\n",
            "[Train] epoch:1312, lr:0.000963, total loss:0.000595\n",
            "[Train] epoch:1313, lr:0.000963, total loss:0.000779\n",
            "[Train] epoch:1314, lr:0.000963, total loss:0.000680\n",
            "[Train] epoch:1315, lr:0.000963, total loss:0.000673\n",
            "[Train] epoch:1316, lr:0.000963, total loss:0.000633\n",
            "[Train] epoch:1317, lr:0.000963, total loss:0.000610\n",
            "[Train] epoch:1318, lr:0.000963, total loss:0.000698\n",
            "[Train] epoch:1319, lr:0.000962, total loss:0.000592\n",
            "[Train] epoch:1320, lr:0.000962, total loss:0.000564\n",
            "[Train] epoch:1321, lr:0.000962, total loss:0.000664\n",
            "[Train] epoch:1322, lr:0.000962, total loss:0.000658\n",
            "[Train] epoch:1323, lr:0.000962, total loss:0.000707\n",
            "[Train] epoch:1324, lr:0.000962, total loss:0.000726\n",
            "[Train] epoch:1325, lr:0.000962, total loss:0.000673\n",
            "[Train] epoch:1326, lr:0.000962, total loss:0.000628\n",
            "[Train] epoch:1327, lr:0.000962, total loss:0.000608\n",
            "[Train] epoch:1328, lr:0.000962, total loss:0.000622\n",
            "[Train] epoch:1329, lr:0.000962, total loss:0.000695\n",
            "[Train] epoch:1330, lr:0.000962, total loss:0.000574\n",
            "[Train] epoch:1331, lr:0.000962, total loss:0.000678\n",
            "[Train] epoch:1332, lr:0.000962, total loss:0.000643\n",
            "[Train] epoch:1333, lr:0.000962, total loss:0.000763\n",
            "[Train] epoch:1334, lr:0.000962, total loss:0.000804\n",
            "[Train] epoch:1335, lr:0.000962, total loss:0.000695\n",
            "[Train] epoch:1336, lr:0.000961, total loss:0.000782\n",
            "[Train] epoch:1337, lr:0.000961, total loss:0.000666\n",
            "[Train] epoch:1338, lr:0.000961, total loss:0.000759\n",
            "[Train] epoch:1339, lr:0.000961, total loss:0.000734\n",
            "[Train] epoch:1340, lr:0.000961, total loss:0.000659\n",
            "[Train] epoch:1341, lr:0.000961, total loss:0.000571\n",
            "[Train] epoch:1342, lr:0.000961, total loss:0.000555\n",
            "[Train] epoch:1343, lr:0.000961, total loss:0.000584\n",
            "[Train] epoch:1344, lr:0.000961, total loss:0.000589\n",
            "[Train] epoch:1345, lr:0.000961, total loss:0.000611\n",
            "[Train] epoch:1346, lr:0.000961, total loss:0.000557\n",
            "[Train] epoch:1347, lr:0.000961, total loss:0.000558\n",
            "[Train] epoch:1348, lr:0.000961, total loss:0.000568\n",
            "[Train] epoch:1349, lr:0.000961, total loss:0.000552\n",
            "[Train] epoch:1350, lr:0.000961, total loss:0.000636\n",
            "[Train] epoch:1351, lr:0.000961, total loss:0.000630\n",
            "[Train] epoch:1352, lr:0.000961, total loss:0.000717\n",
            "[Train] epoch:1353, lr:0.000961, total loss:0.000718\n",
            "[Train] epoch:1354, lr:0.000960, total loss:0.000633\n",
            "[Train] epoch:1355, lr:0.000960, total loss:0.000761\n",
            "[Train] epoch:1356, lr:0.000960, total loss:0.000876\n",
            "[Train] epoch:1357, lr:0.000960, total loss:0.000759\n",
            "[Train] epoch:1358, lr:0.000960, total loss:0.000686\n",
            "[Train] epoch:1359, lr:0.000960, total loss:0.000704\n",
            "[Train] epoch:1360, lr:0.000960, total loss:0.000663\n",
            "[Train] epoch:1361, lr:0.000960, total loss:0.000565\n",
            "[Train] epoch:1362, lr:0.000960, total loss:0.000548\n",
            "[Train] epoch:1363, lr:0.000960, total loss:0.000562\n",
            "[Train] epoch:1364, lr:0.000960, total loss:0.000600\n",
            "[Train] epoch:1365, lr:0.000960, total loss:0.000697\n",
            "[Train] epoch:1366, lr:0.000960, total loss:0.000672\n",
            "[Train] epoch:1367, lr:0.000960, total loss:0.000563\n",
            "[Train] epoch:1368, lr:0.000960, total loss:0.000629\n",
            "[Train] epoch:1369, lr:0.000960, total loss:0.000624\n",
            "[Train] epoch:1370, lr:0.000960, total loss:0.000573\n",
            "[Train] epoch:1371, lr:0.000959, total loss:0.000606\n",
            "[Train] epoch:1372, lr:0.000959, total loss:0.000603\n",
            "[Train] epoch:1373, lr:0.000959, total loss:0.000558\n",
            "[Train] epoch:1374, lr:0.000959, total loss:0.000639\n",
            "[Train] epoch:1375, lr:0.000959, total loss:0.000629\n",
            "[Train] epoch:1376, lr:0.000959, total loss:0.000759\n",
            "[Train] epoch:1377, lr:0.000959, total loss:0.000577\n",
            "[Train] epoch:1378, lr:0.000959, total loss:0.000606\n",
            "[Train] epoch:1379, lr:0.000959, total loss:0.000625\n",
            "[Train] epoch:1380, lr:0.000959, total loss:0.000568\n",
            "[Train] epoch:1381, lr:0.000959, total loss:0.000589\n",
            "[Train] epoch:1382, lr:0.000959, total loss:0.000627\n",
            "[Train] epoch:1383, lr:0.000959, total loss:0.000730\n",
            "[Train] epoch:1384, lr:0.000959, total loss:0.000776\n",
            "[Train] epoch:1385, lr:0.000959, total loss:0.000657\n",
            "[Train] epoch:1386, lr:0.000959, total loss:0.000615\n",
            "[Train] epoch:1387, lr:0.000959, total loss:0.000664\n",
            "[Train] epoch:1388, lr:0.000958, total loss:0.000622\n",
            "[Train] epoch:1389, lr:0.000958, total loss:0.000582\n",
            "[Train] epoch:1390, lr:0.000958, total loss:0.000586\n",
            "[Train] epoch:1391, lr:0.000958, total loss:0.000556\n",
            "[Train] epoch:1392, lr:0.000958, total loss:0.000567\n",
            "[Train] epoch:1393, lr:0.000958, total loss:0.000545\n",
            "[Train] epoch:1394, lr:0.000958, total loss:0.000585\n",
            "[Train] epoch:1395, lr:0.000958, total loss:0.000540\n",
            "[Train] epoch:1396, lr:0.000958, total loss:0.000562\n",
            "[Train] epoch:1397, lr:0.000958, total loss:0.000552\n",
            "[Train] epoch:1398, lr:0.000958, total loss:0.000581\n",
            "[Train] epoch:1399, lr:0.000958, total loss:0.000605\n",
            "[Train] epoch:1400, lr:0.000958, total loss:0.000612\n",
            "[Train] epoch:1401, lr:0.000958, total loss:0.000735\n",
            "[Train] epoch:1402, lr:0.000958, total loss:0.000615\n",
            "[Train] epoch:1403, lr:0.000958, total loss:0.000592\n",
            "[Train] epoch:1404, lr:0.000958, total loss:0.000660\n",
            "[Train] epoch:1405, lr:0.000957, total loss:0.000713\n",
            "[Train] epoch:1406, lr:0.000957, total loss:0.000679\n",
            "[Train] epoch:1407, lr:0.000957, total loss:0.000688\n",
            "[Train] epoch:1408, lr:0.000957, total loss:0.000617\n",
            "[Train] epoch:1409, lr:0.000957, total loss:0.000620\n",
            "[Train] epoch:1410, lr:0.000957, total loss:0.000530\n",
            "[Train] epoch:1411, lr:0.000957, total loss:0.000567\n",
            "[Train] epoch:1412, lr:0.000957, total loss:0.000672\n",
            "[Train] epoch:1413, lr:0.000957, total loss:0.000738\n",
            "[Train] epoch:1414, lr:0.000957, total loss:0.000629\n",
            "[Train] epoch:1415, lr:0.000957, total loss:0.000551\n",
            "[Train] epoch:1416, lr:0.000957, total loss:0.000585\n",
            "[Train] epoch:1417, lr:0.000957, total loss:0.000633\n",
            "[Train] epoch:1418, lr:0.000957, total loss:0.000700\n",
            "[Train] epoch:1419, lr:0.000957, total loss:0.000552\n",
            "[Train] epoch:1420, lr:0.000957, total loss:0.000551\n",
            "[Train] epoch:1421, lr:0.000956, total loss:0.000630\n",
            "[Train] epoch:1422, lr:0.000956, total loss:0.000639\n",
            "[Train] epoch:1423, lr:0.000956, total loss:0.000596\n",
            "[Train] epoch:1424, lr:0.000956, total loss:0.000515\n",
            "[Train] epoch:1425, lr:0.000956, total loss:0.000577\n",
            "[Train] epoch:1426, lr:0.000956, total loss:0.000573\n",
            "[Train] epoch:1427, lr:0.000956, total loss:0.000616\n",
            "[Train] epoch:1428, lr:0.000956, total loss:0.000605\n",
            "[Train] epoch:1429, lr:0.000956, total loss:0.000613\n",
            "[Train] epoch:1430, lr:0.000956, total loss:0.000575\n",
            "[Train] epoch:1431, lr:0.000956, total loss:0.000617\n",
            "[Train] epoch:1432, lr:0.000956, total loss:0.000727\n",
            "[Train] epoch:1433, lr:0.000956, total loss:0.000551\n",
            "[Train] epoch:1434, lr:0.000956, total loss:0.000673\n",
            "[Train] epoch:1435, lr:0.000956, total loss:0.000530\n",
            "[Train] epoch:1436, lr:0.000956, total loss:0.000565\n",
            "[Train] epoch:1437, lr:0.000956, total loss:0.000668\n",
            "[Train] epoch:1438, lr:0.000955, total loss:0.000752\n",
            "[Train] epoch:1439, lr:0.000955, total loss:0.000750\n",
            "[Train] epoch:1440, lr:0.000955, total loss:0.000744\n",
            "[Train] epoch:1441, lr:0.000955, total loss:0.000649\n",
            "[Train] epoch:1442, lr:0.000955, total loss:0.000650\n",
            "[Train] epoch:1443, lr:0.000955, total loss:0.000638\n",
            "[Train] epoch:1444, lr:0.000955, total loss:0.000689\n",
            "[Train] epoch:1445, lr:0.000955, total loss:0.000617\n",
            "[Train] epoch:1446, lr:0.000955, total loss:0.000612\n",
            "[Train] epoch:1447, lr:0.000955, total loss:0.000682\n",
            "[Train] epoch:1448, lr:0.000955, total loss:0.000576\n",
            "[Train] epoch:1449, lr:0.000955, total loss:0.000734\n",
            "[Train] epoch:1450, lr:0.000955, total loss:0.000562\n",
            "[Train] epoch:1451, lr:0.000955, total loss:0.000585\n",
            "[Train] epoch:1452, lr:0.000955, total loss:0.000586\n",
            "[Train] epoch:1453, lr:0.000955, total loss:0.000580\n",
            "[Train] epoch:1454, lr:0.000954, total loss:0.000570\n",
            "[Train] epoch:1455, lr:0.000954, total loss:0.000493\n",
            "[Train] epoch:1456, lr:0.000954, total loss:0.000508\n",
            "[Train] epoch:1457, lr:0.000954, total loss:0.000538\n",
            "[Train] epoch:1458, lr:0.000954, total loss:0.000518\n",
            "[Train] epoch:1459, lr:0.000954, total loss:0.000517\n",
            "[Train] epoch:1460, lr:0.000954, total loss:0.000532\n",
            "[Train] epoch:1461, lr:0.000954, total loss:0.000519\n",
            "[Train] epoch:1462, lr:0.000954, total loss:0.000534\n",
            "[Train] epoch:1463, lr:0.000954, total loss:0.000482\n",
            "[Train] epoch:1464, lr:0.000954, total loss:0.000494\n",
            "[Train] epoch:1465, lr:0.000954, total loss:0.000534\n",
            "[Train] epoch:1466, lr:0.000954, total loss:0.000578\n",
            "[Train] epoch:1467, lr:0.000954, total loss:0.000515\n",
            "[Train] epoch:1468, lr:0.000954, total loss:0.000594\n",
            "[Train] epoch:1469, lr:0.000954, total loss:0.000555\n",
            "[Train] epoch:1470, lr:0.000953, total loss:0.000588\n",
            "[Train] epoch:1471, lr:0.000953, total loss:0.000573\n",
            "[Train] epoch:1472, lr:0.000953, total loss:0.000583\n",
            "[Train] epoch:1473, lr:0.000953, total loss:0.000514\n",
            "[Train] epoch:1474, lr:0.000953, total loss:0.000502\n",
            "[Train] epoch:1475, lr:0.000953, total loss:0.000527\n",
            "[Train] epoch:1476, lr:0.000953, total loss:0.000526\n",
            "[Train] epoch:1477, lr:0.000953, total loss:0.000558\n",
            "[Train] epoch:1478, lr:0.000953, total loss:0.000618\n",
            "[Train] epoch:1479, lr:0.000953, total loss:0.000607\n",
            "[Train] epoch:1480, lr:0.000953, total loss:0.000548\n",
            "[Train] epoch:1481, lr:0.000953, total loss:0.000594\n",
            "[Train] epoch:1482, lr:0.000953, total loss:0.000552\n",
            "[Train] epoch:1483, lr:0.000953, total loss:0.000553\n",
            "[Train] epoch:1484, lr:0.000953, total loss:0.000513\n",
            "[Train] epoch:1485, lr:0.000953, total loss:0.000610\n",
            "[Train] epoch:1486, lr:0.000952, total loss:0.000533\n",
            "[Train] epoch:1487, lr:0.000952, total loss:0.000563\n",
            "[Train] epoch:1488, lr:0.000952, total loss:0.000578\n",
            "[Train] epoch:1489, lr:0.000952, total loss:0.000545\n",
            "[Train] epoch:1490, lr:0.000952, total loss:0.000679\n",
            "[Train] epoch:1491, lr:0.000952, total loss:0.000799\n",
            "[Train] epoch:1492, lr:0.000952, total loss:0.000632\n",
            "[Train] epoch:1493, lr:0.000952, total loss:0.000557\n",
            "[Train] epoch:1494, lr:0.000952, total loss:0.000561\n",
            "[Train] epoch:1495, lr:0.000952, total loss:0.000676\n",
            "[Train] epoch:1496, lr:0.000952, total loss:0.000604\n",
            "[Train] epoch:1497, lr:0.000952, total loss:0.000701\n",
            "[Train] epoch:1498, lr:0.000952, total loss:0.000677\n",
            "[Train] epoch:1499, lr:0.000952, total loss:0.000663\n",
            "[Train] epoch:1500, lr:0.000952, total loss:0.000662\n",
            "[Train] epoch:1501, lr:0.000951, total loss:0.000621\n",
            "[Train] epoch:1502, lr:0.000951, total loss:0.000562\n",
            "[Train] epoch:1503, lr:0.000951, total loss:0.000594\n",
            "[Train] epoch:1504, lr:0.000951, total loss:0.000554\n",
            "[Train] epoch:1505, lr:0.000951, total loss:0.000620\n",
            "[Train] epoch:1506, lr:0.000951, total loss:0.000576\n",
            "[Train] epoch:1507, lr:0.000951, total loss:0.000643\n",
            "[Train] epoch:1508, lr:0.000951, total loss:0.000741\n",
            "[Train] epoch:1509, lr:0.000951, total loss:0.000634\n",
            "[Train] epoch:1510, lr:0.000951, total loss:0.000598\n",
            "[Train] epoch:1511, lr:0.000951, total loss:0.000505\n",
            "[Train] epoch:1512, lr:0.000951, total loss:0.000663\n",
            "[Train] epoch:1513, lr:0.000951, total loss:0.000494\n",
            "[Train] epoch:1514, lr:0.000951, total loss:0.000545\n",
            "[Train] epoch:1515, lr:0.000951, total loss:0.000703\n",
            "[Train] epoch:1516, lr:0.000951, total loss:0.000638\n",
            "[Train] epoch:1517, lr:0.000950, total loss:0.000523\n",
            "[Train] epoch:1518, lr:0.000950, total loss:0.000515\n",
            "[Train] epoch:1519, lr:0.000950, total loss:0.000559\n",
            "[Train] epoch:1520, lr:0.000950, total loss:0.000590\n",
            "[Train] epoch:1521, lr:0.000950, total loss:0.000516\n",
            "[Train] epoch:1522, lr:0.000950, total loss:0.000540\n",
            "[Train] epoch:1523, lr:0.000950, total loss:0.000569\n",
            "[Train] epoch:1524, lr:0.000950, total loss:0.000522\n",
            "[Train] epoch:1525, lr:0.000950, total loss:0.000549\n",
            "[Train] epoch:1526, lr:0.000950, total loss:0.000526\n",
            "[Train] epoch:1527, lr:0.000950, total loss:0.000516\n",
            "[Train] epoch:1528, lr:0.000950, total loss:0.000529\n",
            "[Train] epoch:1529, lr:0.000950, total loss:0.000550\n",
            "[Train] epoch:1530, lr:0.000950, total loss:0.000569\n",
            "[Train] epoch:1531, lr:0.000950, total loss:0.000580\n",
            "[Train] epoch:1532, lr:0.000949, total loss:0.000671\n",
            "[Train] epoch:1533, lr:0.000949, total loss:0.000652\n",
            "[Train] epoch:1534, lr:0.000949, total loss:0.000589\n",
            "[Train] epoch:1535, lr:0.000949, total loss:0.000626\n",
            "[Train] epoch:1536, lr:0.000949, total loss:0.000597\n",
            "[Train] epoch:1537, lr:0.000949, total loss:0.000629\n",
            "[Train] epoch:1538, lr:0.000949, total loss:0.000579\n",
            "[Train] epoch:1539, lr:0.000949, total loss:0.000650\n",
            "[Train] epoch:1540, lr:0.000949, total loss:0.000561\n",
            "[Train] epoch:1541, lr:0.000949, total loss:0.000596\n",
            "[Train] epoch:1542, lr:0.000949, total loss:0.000545\n",
            "[Train] epoch:1543, lr:0.000949, total loss:0.000702\n",
            "[Train] epoch:1544, lr:0.000949, total loss:0.000676\n",
            "[Train] epoch:1545, lr:0.000949, total loss:0.000638\n",
            "[Train] epoch:1546, lr:0.000949, total loss:0.000671\n",
            "[Train] epoch:1547, lr:0.000949, total loss:0.000618\n",
            "[Train] epoch:1548, lr:0.000948, total loss:0.000549\n",
            "[Train] epoch:1549, lr:0.000948, total loss:0.000602\n",
            "[Train] epoch:1550, lr:0.000948, total loss:0.000527\n",
            "[Train] epoch:1551, lr:0.000948, total loss:0.000567\n",
            "[Train] epoch:1552, lr:0.000948, total loss:0.000586\n",
            "[Train] epoch:1553, lr:0.000948, total loss:0.000538\n",
            "[Train] epoch:1554, lr:0.000948, total loss:0.000615\n",
            "[Train] epoch:1555, lr:0.000948, total loss:0.000608\n",
            "[Train] epoch:1556, lr:0.000948, total loss:0.000533\n",
            "[Train] epoch:1557, lr:0.000948, total loss:0.000524\n",
            "[Train] epoch:1558, lr:0.000948, total loss:0.000558\n",
            "[Train] epoch:1559, lr:0.000948, total loss:0.000591\n",
            "[Train] epoch:1560, lr:0.000948, total loss:0.000559\n",
            "[Train] epoch:1561, lr:0.000948, total loss:0.000495\n",
            "[Train] epoch:1562, lr:0.000948, total loss:0.000543\n",
            "[Train] epoch:1563, lr:0.000947, total loss:0.000542\n",
            "[Train] epoch:1564, lr:0.000947, total loss:0.000563\n",
            "[Train] epoch:1565, lr:0.000947, total loss:0.000522\n",
            "[Train] epoch:1566, lr:0.000947, total loss:0.000547\n",
            "[Train] epoch:1567, lr:0.000947, total loss:0.000483\n",
            "[Train] epoch:1568, lr:0.000947, total loss:0.000516\n",
            "[Train] epoch:1569, lr:0.000947, total loss:0.000585\n",
            "[Train] epoch:1570, lr:0.000947, total loss:0.000518\n",
            "[Train] epoch:1571, lr:0.000947, total loss:0.000543\n",
            "[Train] epoch:1572, lr:0.000947, total loss:0.000549\n",
            "[Train] epoch:1573, lr:0.000947, total loss:0.000537\n",
            "[Train] epoch:1574, lr:0.000947, total loss:0.000514\n",
            "[Train] epoch:1575, lr:0.000947, total loss:0.000524\n",
            "[Train] epoch:1576, lr:0.000947, total loss:0.000509\n",
            "[Train] epoch:1577, lr:0.000947, total loss:0.000492\n",
            "[Train] epoch:1578, lr:0.000946, total loss:0.000488\n",
            "[Train] epoch:1579, lr:0.000946, total loss:0.000551\n",
            "[Train] epoch:1580, lr:0.000946, total loss:0.000499\n",
            "[Train] epoch:1581, lr:0.000946, total loss:0.000542\n",
            "[Train] epoch:1582, lr:0.000946, total loss:0.000531\n",
            "[Train] epoch:1583, lr:0.000946, total loss:0.000593\n",
            "[Train] epoch:1584, lr:0.000946, total loss:0.000559\n",
            "[Train] epoch:1585, lr:0.000946, total loss:0.000533\n",
            "[Train] epoch:1586, lr:0.000946, total loss:0.000510\n",
            "[Train] epoch:1587, lr:0.000946, total loss:0.000513\n",
            "[Train] epoch:1588, lr:0.000946, total loss:0.000557\n",
            "[Train] epoch:1589, lr:0.000946, total loss:0.000541\n",
            "[Train] epoch:1590, lr:0.000946, total loss:0.000540\n",
            "[Train] epoch:1591, lr:0.000946, total loss:0.000519\n",
            "[Train] epoch:1592, lr:0.000946, total loss:0.000489\n",
            "[Train] epoch:1593, lr:0.000945, total loss:0.000499\n",
            "[Train] epoch:1594, lr:0.000945, total loss:0.000578\n",
            "[Train] epoch:1595, lr:0.000945, total loss:0.000510\n",
            "[Train] epoch:1596, lr:0.000945, total loss:0.000527\n",
            "[Train] epoch:1597, lr:0.000945, total loss:0.000541\n",
            "[Train] epoch:1598, lr:0.000945, total loss:0.000522\n",
            "[Train] epoch:1599, lr:0.000945, total loss:0.000588\n",
            "[Train] epoch:1600, lr:0.000945, total loss:0.000611\n",
            "[Train] epoch:1601, lr:0.000945, total loss:0.000563\n",
            "[Train] epoch:1602, lr:0.000945, total loss:0.000563\n",
            "[Train] epoch:1603, lr:0.000945, total loss:0.000574\n",
            "[Train] epoch:1604, lr:0.000945, total loss:0.000624\n",
            "[Train] epoch:1605, lr:0.000945, total loss:0.000530\n",
            "[Train] epoch:1606, lr:0.000945, total loss:0.000707\n",
            "[Train] epoch:1607, lr:0.000945, total loss:0.000738\n",
            "[Train] epoch:1608, lr:0.000944, total loss:0.000567\n",
            "[Train] epoch:1609, lr:0.000944, total loss:0.000542\n",
            "[Train] epoch:1610, lr:0.000944, total loss:0.000513\n",
            "[Train] epoch:1611, lr:0.000944, total loss:0.000542\n",
            "[Train] epoch:1612, lr:0.000944, total loss:0.000540\n",
            "[Train] epoch:1613, lr:0.000944, total loss:0.000492\n",
            "[Train] epoch:1614, lr:0.000944, total loss:0.000624\n",
            "[Train] epoch:1615, lr:0.000944, total loss:0.000590\n",
            "[Train] epoch:1616, lr:0.000944, total loss:0.000565\n",
            "[Train] epoch:1617, lr:0.000944, total loss:0.000605\n",
            "[Train] epoch:1618, lr:0.000944, total loss:0.000488\n",
            "[Train] epoch:1619, lr:0.000944, total loss:0.000602\n",
            "[Train] epoch:1620, lr:0.000944, total loss:0.000584\n",
            "[Train] epoch:1621, lr:0.000944, total loss:0.000575\n",
            "[Train] epoch:1622, lr:0.000943, total loss:0.000552\n",
            "[Train] epoch:1623, lr:0.000943, total loss:0.000556\n",
            "[Train] epoch:1624, lr:0.000943, total loss:0.000516\n",
            "[Train] epoch:1625, lr:0.000943, total loss:0.000499\n",
            "[Train] epoch:1626, lr:0.000943, total loss:0.000515\n",
            "[Train] epoch:1627, lr:0.000943, total loss:0.000515\n",
            "[Train] epoch:1628, lr:0.000943, total loss:0.000606\n",
            "[Train] epoch:1629, lr:0.000943, total loss:0.000549\n",
            "[Train] epoch:1630, lr:0.000943, total loss:0.000585\n",
            "[Train] epoch:1631, lr:0.000943, total loss:0.000504\n",
            "[Train] epoch:1632, lr:0.000943, total loss:0.000548\n",
            "[Train] epoch:1633, lr:0.000943, total loss:0.000565\n",
            "[Train] epoch:1634, lr:0.000943, total loss:0.000556\n",
            "[Train] epoch:1635, lr:0.000943, total loss:0.000568\n",
            "[Train] epoch:1636, lr:0.000943, total loss:0.000535\n",
            "[Train] epoch:1637, lr:0.000942, total loss:0.000559\n",
            "[Train] epoch:1638, lr:0.000942, total loss:0.000586\n",
            "[Train] epoch:1639, lr:0.000942, total loss:0.000570\n",
            "[Train] epoch:1640, lr:0.000942, total loss:0.000581\n",
            "[Train] epoch:1641, lr:0.000942, total loss:0.000796\n",
            "[Train] epoch:1642, lr:0.000942, total loss:0.000763\n",
            "[Train] epoch:1643, lr:0.000942, total loss:0.000796\n",
            "[Train] epoch:1644, lr:0.000942, total loss:0.000635\n",
            "[Train] epoch:1645, lr:0.000942, total loss:0.000722\n",
            "[Train] epoch:1646, lr:0.000942, total loss:0.000744\n",
            "[Train] epoch:1647, lr:0.000942, total loss:0.000673\n",
            "[Train] epoch:1648, lr:0.000942, total loss:0.000674\n",
            "[Train] epoch:1649, lr:0.000942, total loss:0.000624\n",
            "[Train] epoch:1650, lr:0.000942, total loss:0.000607\n",
            "[Train] epoch:1651, lr:0.000941, total loss:0.000490\n",
            "[Train] epoch:1652, lr:0.000941, total loss:0.000513\n",
            "[Train] epoch:1653, lr:0.000941, total loss:0.000512\n",
            "[Train] epoch:1654, lr:0.000941, total loss:0.000735\n",
            "[Train] epoch:1655, lr:0.000941, total loss:0.000570\n",
            "[Train] epoch:1656, lr:0.000941, total loss:0.000495\n",
            "[Train] epoch:1657, lr:0.000941, total loss:0.000549\n",
            "[Train] epoch:1658, lr:0.000941, total loss:0.000590\n",
            "[Train] epoch:1659, lr:0.000941, total loss:0.000552\n",
            "[Train] epoch:1660, lr:0.000941, total loss:0.000464\n",
            "[Train] epoch:1661, lr:0.000941, total loss:0.000516\n",
            "[Train] epoch:1662, lr:0.000941, total loss:0.000467\n",
            "[Train] epoch:1663, lr:0.000941, total loss:0.000490\n",
            "[Train] epoch:1664, lr:0.000941, total loss:0.000470\n",
            "[Train] epoch:1665, lr:0.000940, total loss:0.000527\n",
            "[Train] epoch:1666, lr:0.000940, total loss:0.000510\n",
            "[Train] epoch:1667, lr:0.000940, total loss:0.000479\n",
            "[Train] epoch:1668, lr:0.000940, total loss:0.000540\n",
            "[Train] epoch:1669, lr:0.000940, total loss:0.000557\n",
            "[Train] epoch:1670, lr:0.000940, total loss:0.000570\n",
            "[Train] epoch:1671, lr:0.000940, total loss:0.000518\n",
            "[Train] epoch:1672, lr:0.000940, total loss:0.000532\n",
            "[Train] epoch:1673, lr:0.000940, total loss:0.000483\n",
            "[Train] epoch:1674, lr:0.000940, total loss:0.000542\n",
            "[Train] epoch:1675, lr:0.000940, total loss:0.000523\n",
            "[Train] epoch:1676, lr:0.000940, total loss:0.000545\n",
            "[Train] epoch:1677, lr:0.000940, total loss:0.000521\n",
            "[Train] epoch:1678, lr:0.000940, total loss:0.000554\n",
            "[Train] epoch:1679, lr:0.000939, total loss:0.000516\n",
            "[Train] epoch:1680, lr:0.000939, total loss:0.000557\n",
            "[Train] epoch:1681, lr:0.000939, total loss:0.000499\n",
            "[Train] epoch:1682, lr:0.000939, total loss:0.000620\n",
            "[Train] epoch:1683, lr:0.000939, total loss:0.000478\n",
            "[Train] epoch:1684, lr:0.000939, total loss:0.000602\n",
            "[Train] epoch:1685, lr:0.000939, total loss:0.000586\n",
            "[Train] epoch:1686, lr:0.000939, total loss:0.000560\n",
            "[Train] epoch:1687, lr:0.000939, total loss:0.000522\n",
            "[Train] epoch:1688, lr:0.000939, total loss:0.000557\n",
            "[Train] epoch:1689, lr:0.000939, total loss:0.000550\n",
            "[Train] epoch:1690, lr:0.000939, total loss:0.000585\n",
            "[Train] epoch:1691, lr:0.000939, total loss:0.000603\n",
            "[Train] epoch:1692, lr:0.000939, total loss:0.000520\n",
            "[Train] epoch:1693, lr:0.000939, total loss:0.000525\n",
            "[Train] epoch:1694, lr:0.000938, total loss:0.000608\n",
            "[Train] epoch:1695, lr:0.000938, total loss:0.000505\n",
            "[Train] epoch:1696, lr:0.000938, total loss:0.000524\n",
            "[Train] epoch:1697, lr:0.000938, total loss:0.000456\n",
            "[Train] epoch:1698, lr:0.000938, total loss:0.000528\n",
            "[Train] epoch:1699, lr:0.000938, total loss:0.000539\n",
            "[Train] epoch:1700, lr:0.000938, total loss:0.000511\n",
            "[Train] epoch:1701, lr:0.000938, total loss:0.000468\n",
            "[Train] epoch:1702, lr:0.000938, total loss:0.000494\n",
            "[Train] epoch:1703, lr:0.000938, total loss:0.000523\n",
            "[Train] epoch:1704, lr:0.000938, total loss:0.000491\n",
            "[Train] epoch:1705, lr:0.000938, total loss:0.000558\n",
            "[Train] epoch:1706, lr:0.000938, total loss:0.000480\n",
            "[Train] epoch:1707, lr:0.000937, total loss:0.000484\n",
            "[Train] epoch:1708, lr:0.000937, total loss:0.000507\n",
            "[Train] epoch:1709, lr:0.000937, total loss:0.000491\n",
            "[Train] epoch:1710, lr:0.000937, total loss:0.000489\n",
            "[Train] epoch:1711, lr:0.000937, total loss:0.000533\n",
            "[Train] epoch:1712, lr:0.000937, total loss:0.000464\n",
            "[Train] epoch:1713, lr:0.000937, total loss:0.000513\n",
            "[Train] epoch:1714, lr:0.000937, total loss:0.000533\n",
            "[Train] epoch:1715, lr:0.000937, total loss:0.000499\n",
            "[Train] epoch:1716, lr:0.000937, total loss:0.000502\n",
            "[Train] epoch:1717, lr:0.000937, total loss:0.000492\n",
            "[Train] epoch:1718, lr:0.000937, total loss:0.000544\n",
            "[Train] epoch:1719, lr:0.000937, total loss:0.000630\n",
            "[Train] epoch:1720, lr:0.000937, total loss:0.000564\n",
            "[Train] epoch:1721, lr:0.000936, total loss:0.000580\n",
            "[Train] epoch:1722, lr:0.000936, total loss:0.000583\n",
            "[Train] epoch:1723, lr:0.000936, total loss:0.000502\n",
            "[Train] epoch:1724, lr:0.000936, total loss:0.000633\n",
            "[Train] epoch:1725, lr:0.000936, total loss:0.000581\n",
            "[Train] epoch:1726, lr:0.000936, total loss:0.000528\n",
            "[Train] epoch:1727, lr:0.000936, total loss:0.000658\n",
            "[Train] epoch:1728, lr:0.000936, total loss:0.000618\n",
            "[Train] epoch:1729, lr:0.000936, total loss:0.000673\n",
            "[Train] epoch:1730, lr:0.000936, total loss:0.000523\n",
            "[Train] epoch:1731, lr:0.000936, total loss:0.000558\n",
            "[Train] epoch:1732, lr:0.000936, total loss:0.000560\n",
            "[Train] epoch:1733, lr:0.000936, total loss:0.000487\n",
            "[Train] epoch:1734, lr:0.000936, total loss:0.000478\n",
            "[Train] epoch:1735, lr:0.000935, total loss:0.000545\n",
            "[Train] epoch:1736, lr:0.000935, total loss:0.000547\n",
            "[Train] epoch:1737, lr:0.000935, total loss:0.000450\n",
            "[Train] epoch:1738, lr:0.000935, total loss:0.000489\n",
            "[Train] epoch:1739, lr:0.000935, total loss:0.000479\n",
            "[Train] epoch:1740, lr:0.000935, total loss:0.000499\n",
            "[Train] epoch:1741, lr:0.000935, total loss:0.000482\n",
            "[Train] epoch:1742, lr:0.000935, total loss:0.000496\n",
            "[Train] epoch:1743, lr:0.000935, total loss:0.000524\n",
            "[Train] epoch:1744, lr:0.000935, total loss:0.000507\n",
            "[Train] epoch:1745, lr:0.000935, total loss:0.000518\n",
            "[Train] epoch:1746, lr:0.000935, total loss:0.000480\n",
            "[Train] epoch:1747, lr:0.000935, total loss:0.000515\n",
            "[Train] epoch:1748, lr:0.000935, total loss:0.000452\n",
            "[Train] epoch:1749, lr:0.000934, total loss:0.000470\n",
            "[Train] epoch:1750, lr:0.000934, total loss:0.000477\n",
            "[Train] epoch:1751, lr:0.000934, total loss:0.000526\n",
            "[Train] epoch:1752, lr:0.000934, total loss:0.000564\n",
            "[Train] epoch:1753, lr:0.000934, total loss:0.000543\n",
            "[Train] epoch:1754, lr:0.000934, total loss:0.000498\n",
            "[Train] epoch:1755, lr:0.000934, total loss:0.000621\n",
            "[Train] epoch:1756, lr:0.000934, total loss:0.000535\n",
            "[Train] epoch:1757, lr:0.000934, total loss:0.000553\n",
            "[Train] epoch:1758, lr:0.000934, total loss:0.000551\n",
            "[Train] epoch:1759, lr:0.000934, total loss:0.000475\n",
            "[Train] epoch:1760, lr:0.000934, total loss:0.000479\n",
            "[Train] epoch:1761, lr:0.000934, total loss:0.000514\n",
            "[Train] epoch:1762, lr:0.000933, total loss:0.000482\n",
            "[Train] epoch:1763, lr:0.000933, total loss:0.000520\n",
            "[Train] epoch:1764, lr:0.000933, total loss:0.000536\n",
            "[Train] epoch:1765, lr:0.000933, total loss:0.000491\n",
            "[Train] epoch:1766, lr:0.000933, total loss:0.000485\n",
            "[Train] epoch:1767, lr:0.000933, total loss:0.000483\n",
            "[Train] epoch:1768, lr:0.000933, total loss:0.000479\n",
            "[Train] epoch:1769, lr:0.000933, total loss:0.000508\n",
            "[Train] epoch:1770, lr:0.000933, total loss:0.000662\n",
            "[Train] epoch:1771, lr:0.000933, total loss:0.000650\n",
            "[Train] epoch:1772, lr:0.000933, total loss:0.000600\n",
            "[Train] epoch:1773, lr:0.000933, total loss:0.000559\n",
            "[Train] epoch:1774, lr:0.000933, total loss:0.000520\n",
            "[Train] epoch:1775, lr:0.000933, total loss:0.000503\n",
            "[Train] epoch:1776, lr:0.000932, total loss:0.000558\n",
            "[Train] epoch:1777, lr:0.000932, total loss:0.000472\n",
            "[Train] epoch:1778, lr:0.000932, total loss:0.000649\n",
            "[Train] epoch:1779, lr:0.000932, total loss:0.000702\n",
            "[Train] epoch:1780, lr:0.000932, total loss:0.000805\n",
            "[Train] epoch:1781, lr:0.000932, total loss:0.000699\n",
            "[Train] epoch:1782, lr:0.000932, total loss:0.000649\n",
            "[Train] epoch:1783, lr:0.000932, total loss:0.000542\n",
            "[Train] epoch:1784, lr:0.000932, total loss:0.000541\n",
            "[Train] epoch:1785, lr:0.000932, total loss:0.000509\n",
            "[Train] epoch:1786, lr:0.000932, total loss:0.000543\n",
            "[Train] epoch:1787, lr:0.000932, total loss:0.000501\n",
            "[Train] epoch:1788, lr:0.000932, total loss:0.000504\n",
            "[Train] epoch:1789, lr:0.000931, total loss:0.000503\n",
            "[Train] epoch:1790, lr:0.000931, total loss:0.000669\n",
            "[Train] epoch:1791, lr:0.000931, total loss:0.000509\n",
            "[Train] epoch:1792, lr:0.000931, total loss:0.000604\n",
            "[Train] epoch:1793, lr:0.000931, total loss:0.000526\n",
            "[Train] epoch:1794, lr:0.000931, total loss:0.000499\n",
            "[Train] epoch:1795, lr:0.000931, total loss:0.000550\n",
            "[Train] epoch:1796, lr:0.000931, total loss:0.000480\n",
            "[Train] epoch:1797, lr:0.000931, total loss:0.000502\n",
            "[Train] epoch:1798, lr:0.000931, total loss:0.000463\n",
            "[Train] epoch:1799, lr:0.000931, total loss:0.000494\n",
            "[Train] epoch:1800, lr:0.000931, total loss:0.000475\n",
            "[Train] epoch:1801, lr:0.000931, total loss:0.000470\n",
            "[Train] epoch:1802, lr:0.000930, total loss:0.000573\n",
            "[Train] epoch:1803, lr:0.000930, total loss:0.000453\n",
            "[Train] epoch:1804, lr:0.000930, total loss:0.000549\n",
            "[Train] epoch:1805, lr:0.000930, total loss:0.000611\n",
            "[Train] epoch:1806, lr:0.000930, total loss:0.000708\n",
            "[Train] epoch:1807, lr:0.000930, total loss:0.000644\n",
            "[Train] epoch:1808, lr:0.000930, total loss:0.000585\n",
            "[Train] epoch:1809, lr:0.000930, total loss:0.000575\n",
            "[Train] epoch:1810, lr:0.000930, total loss:0.000560\n",
            "[Train] epoch:1811, lr:0.000930, total loss:0.000533\n",
            "[Train] epoch:1812, lr:0.000930, total loss:0.000444\n",
            "[Train] epoch:1813, lr:0.000930, total loss:0.000495\n",
            "[Train] epoch:1814, lr:0.000930, total loss:0.000473\n",
            "[Train] epoch:1815, lr:0.000930, total loss:0.000498\n",
            "[Train] epoch:1816, lr:0.000929, total loss:0.000479\n",
            "[Train] epoch:1817, lr:0.000929, total loss:0.000518\n",
            "[Train] epoch:1818, lr:0.000929, total loss:0.000503\n",
            "[Train] epoch:1819, lr:0.000929, total loss:0.000449\n",
            "[Train] epoch:1820, lr:0.000929, total loss:0.000523\n",
            "[Train] epoch:1821, lr:0.000929, total loss:0.000582\n",
            "[Train] epoch:1822, lr:0.000929, total loss:0.000604\n",
            "[Train] epoch:1823, lr:0.000929, total loss:0.000592\n",
            "[Train] epoch:1824, lr:0.000929, total loss:0.000866\n",
            "[Train] epoch:1825, lr:0.000929, total loss:0.000577\n",
            "[Train] epoch:1826, lr:0.000929, total loss:0.000565\n",
            "[Train] epoch:1827, lr:0.000929, total loss:0.000554\n",
            "[Train] epoch:1828, lr:0.000929, total loss:0.000611\n",
            "[Train] epoch:1829, lr:0.000928, total loss:0.000649\n",
            "[Train] epoch:1830, lr:0.000928, total loss:0.000577\n",
            "[Train] epoch:1831, lr:0.000928, total loss:0.000548\n",
            "[Train] epoch:1832, lr:0.000928, total loss:0.000624\n",
            "[Train] epoch:1833, lr:0.000928, total loss:0.000615\n",
            "[Train] epoch:1834, lr:0.000928, total loss:0.000524\n",
            "[Train] epoch:1835, lr:0.000928, total loss:0.000582\n",
            "[Train] epoch:1836, lr:0.000928, total loss:0.000544\n",
            "[Train] epoch:1837, lr:0.000928, total loss:0.000551\n",
            "[Train] epoch:1838, lr:0.000928, total loss:0.000477\n",
            "[Train] epoch:1839, lr:0.000928, total loss:0.000464\n",
            "[Train] epoch:1840, lr:0.000928, total loss:0.000480\n",
            "[Train] epoch:1841, lr:0.000928, total loss:0.000535\n",
            "[Train] epoch:1842, lr:0.000927, total loss:0.000511\n",
            "[Train] epoch:1843, lr:0.000927, total loss:0.000503\n",
            "[Train] epoch:1844, lr:0.000927, total loss:0.000428\n",
            "[Train] epoch:1845, lr:0.000927, total loss:0.000506\n",
            "[Train] epoch:1846, lr:0.000927, total loss:0.000483\n",
            "[Train] epoch:1847, lr:0.000927, total loss:0.000457\n",
            "[Train] epoch:1848, lr:0.000927, total loss:0.000529\n",
            "[Train] epoch:1849, lr:0.000927, total loss:0.000591\n",
            "[Train] epoch:1850, lr:0.000927, total loss:0.000520\n",
            "[Train] epoch:1851, lr:0.000927, total loss:0.000430\n",
            "[Train] epoch:1852, lr:0.000927, total loss:0.000469\n",
            "[Train] epoch:1853, lr:0.000927, total loss:0.000453\n",
            "[Train] epoch:1854, lr:0.000927, total loss:0.000483\n",
            "[Train] epoch:1855, lr:0.000926, total loss:0.000442\n",
            "[Train] epoch:1856, lr:0.000926, total loss:0.000517\n",
            "[Train] epoch:1857, lr:0.000926, total loss:0.000476\n",
            "[Train] epoch:1858, lr:0.000926, total loss:0.000525\n",
            "[Train] epoch:1859, lr:0.000926, total loss:0.000442\n",
            "[Train] epoch:1860, lr:0.000926, total loss:0.000426\n",
            "[Train] epoch:1861, lr:0.000926, total loss:0.000457\n",
            "[Train] epoch:1862, lr:0.000926, total loss:0.000455\n",
            "[Train] epoch:1863, lr:0.000926, total loss:0.000445\n",
            "[Train] epoch:1864, lr:0.000926, total loss:0.000479\n",
            "[Train] epoch:1865, lr:0.000926, total loss:0.000479\n",
            "[Train] epoch:1866, lr:0.000926, total loss:0.000436\n",
            "[Train] epoch:1867, lr:0.000926, total loss:0.000466\n",
            "[Train] epoch:1868, lr:0.000925, total loss:0.000518\n",
            "[Train] epoch:1869, lr:0.000925, total loss:0.000476\n",
            "[Train] epoch:1870, lr:0.000925, total loss:0.000529\n",
            "[Train] epoch:1871, lr:0.000925, total loss:0.000502\n",
            "[Train] epoch:1872, lr:0.000925, total loss:0.000469\n",
            "[Train] epoch:1873, lr:0.000925, total loss:0.000468\n",
            "[Train] epoch:1874, lr:0.000925, total loss:0.000448\n",
            "[Train] epoch:1875, lr:0.000925, total loss:0.000464\n",
            "[Train] epoch:1876, lr:0.000925, total loss:0.000486\n",
            "[Train] epoch:1877, lr:0.000925, total loss:0.000461\n",
            "[Train] epoch:1878, lr:0.000925, total loss:0.000541\n",
            "[Train] epoch:1879, lr:0.000925, total loss:0.000523\n",
            "[Train] epoch:1880, lr:0.000924, total loss:0.000590\n",
            "[Train] epoch:1881, lr:0.000924, total loss:0.000609\n",
            "[Train] epoch:1882, lr:0.000924, total loss:0.000755\n",
            "[Train] epoch:1883, lr:0.000924, total loss:0.000515\n",
            "[Train] epoch:1884, lr:0.000924, total loss:0.000540\n",
            "[Train] epoch:1885, lr:0.000924, total loss:0.000480\n",
            "[Train] epoch:1886, lr:0.000924, total loss:0.000437\n",
            "[Train] epoch:1887, lr:0.000924, total loss:0.000464\n",
            "[Train] epoch:1888, lr:0.000924, total loss:0.000405\n",
            "[Train] epoch:1889, lr:0.000924, total loss:0.000512\n",
            "[Train] epoch:1890, lr:0.000924, total loss:0.000475\n",
            "[Train] epoch:1891, lr:0.000924, total loss:0.000467\n",
            "[Train] epoch:1892, lr:0.000924, total loss:0.000554\n",
            "[Train] epoch:1893, lr:0.000923, total loss:0.000512\n",
            "[Train] epoch:1894, lr:0.000923, total loss:0.000455\n",
            "[Train] epoch:1895, lr:0.000923, total loss:0.000586\n",
            "[Train] epoch:1896, lr:0.000923, total loss:0.000609\n",
            "[Train] epoch:1897, lr:0.000923, total loss:0.000492\n",
            "[Train] epoch:1898, lr:0.000923, total loss:0.000432\n",
            "[Train] epoch:1899, lr:0.000923, total loss:0.000486\n",
            "[Train] epoch:1900, lr:0.000923, total loss:0.000474\n",
            "[Train] epoch:1901, lr:0.000923, total loss:0.000488\n",
            "[Train] epoch:1902, lr:0.000923, total loss:0.000496\n",
            "[Train] epoch:1903, lr:0.000923, total loss:0.000535\n",
            "[Train] epoch:1904, lr:0.000923, total loss:0.000486\n",
            "[Train] epoch:1905, lr:0.000923, total loss:0.000443\n",
            "[Train] epoch:1906, lr:0.000922, total loss:0.000453\n",
            "[Train] epoch:1907, lr:0.000922, total loss:0.000490\n",
            "[Train] epoch:1908, lr:0.000922, total loss:0.000453\n",
            "[Train] epoch:1909, lr:0.000922, total loss:0.000461\n",
            "[Train] epoch:1910, lr:0.000922, total loss:0.000467\n",
            "[Train] epoch:1911, lr:0.000922, total loss:0.000550\n",
            "[Train] epoch:1912, lr:0.000922, total loss:0.000546\n",
            "[Train] epoch:1913, lr:0.000922, total loss:0.000535\n",
            "[Train] epoch:1914, lr:0.000922, total loss:0.000506\n",
            "[Train] epoch:1915, lr:0.000922, total loss:0.000583\n",
            "[Train] epoch:1916, lr:0.000922, total loss:0.000477\n",
            "[Train] epoch:1917, lr:0.000922, total loss:0.000491\n",
            "[Train] epoch:1918, lr:0.000921, total loss:0.000455\n",
            "[Train] epoch:1919, lr:0.000921, total loss:0.000434\n",
            "[Train] epoch:1920, lr:0.000921, total loss:0.000475\n",
            "[Train] epoch:1921, lr:0.000921, total loss:0.000469\n",
            "[Train] epoch:1922, lr:0.000921, total loss:0.000464\n",
            "[Train] epoch:1923, lr:0.000921, total loss:0.000459\n",
            "[Train] epoch:1924, lr:0.000921, total loss:0.000459\n",
            "[Train] epoch:1925, lr:0.000921, total loss:0.000467\n",
            "[Train] epoch:1926, lr:0.000921, total loss:0.000466\n",
            "[Train] epoch:1927, lr:0.000921, total loss:0.000442\n",
            "[Train] epoch:1928, lr:0.000921, total loss:0.000496\n",
            "[Train] epoch:1929, lr:0.000921, total loss:0.000445\n",
            "[Train] epoch:1930, lr:0.000921, total loss:0.000508\n",
            "[Train] epoch:1931, lr:0.000920, total loss:0.000551\n",
            "[Train] epoch:1932, lr:0.000920, total loss:0.000469\n",
            "[Train] epoch:1933, lr:0.000920, total loss:0.000439\n",
            "[Train] epoch:1934, lr:0.000920, total loss:0.000456\n",
            "[Train] epoch:1935, lr:0.000920, total loss:0.000444\n",
            "[Train] epoch:1936, lr:0.000920, total loss:0.000482\n",
            "[Train] epoch:1937, lr:0.000920, total loss:0.000539\n",
            "[Train] epoch:1938, lr:0.000920, total loss:0.000481\n",
            "[Train] epoch:1939, lr:0.000920, total loss:0.000583\n",
            "[Train] epoch:1940, lr:0.000920, total loss:0.000578\n",
            "[Train] epoch:1941, lr:0.000920, total loss:0.000491\n",
            "[Train] epoch:1942, lr:0.000920, total loss:0.000485\n",
            "[Train] epoch:1943, lr:0.000919, total loss:0.000430\n",
            "[Train] epoch:1944, lr:0.000919, total loss:0.000483\n",
            "[Train] epoch:1945, lr:0.000919, total loss:0.000432\n",
            "[Train] epoch:1946, lr:0.000919, total loss:0.000435\n",
            "[Train] epoch:1947, lr:0.000919, total loss:0.000406\n",
            "[Train] epoch:1948, lr:0.000919, total loss:0.000450\n",
            "[Train] epoch:1949, lr:0.000919, total loss:0.000426\n",
            "[Train] epoch:1950, lr:0.000919, total loss:0.000449\n",
            "[Train] epoch:1951, lr:0.000919, total loss:0.000559\n",
            "[Train] epoch:1952, lr:0.000919, total loss:0.000487\n",
            "[Train] epoch:1953, lr:0.000919, total loss:0.000572\n",
            "[Train] epoch:1954, lr:0.000919, total loss:0.000438\n",
            "[Train] epoch:1955, lr:0.000918, total loss:0.000444\n",
            "[Train] epoch:1956, lr:0.000918, total loss:0.000439\n",
            "[Train] epoch:1957, lr:0.000918, total loss:0.000452\n",
            "[Train] epoch:1958, lr:0.000918, total loss:0.000462\n",
            "[Train] epoch:1959, lr:0.000918, total loss:0.000574\n",
            "[Train] epoch:1960, lr:0.000918, total loss:0.000598\n",
            "[Train] epoch:1961, lr:0.000918, total loss:0.000498\n",
            "[Train] epoch:1962, lr:0.000918, total loss:0.000473\n",
            "[Train] epoch:1963, lr:0.000918, total loss:0.000419\n",
            "[Train] epoch:1964, lr:0.000918, total loss:0.000439\n",
            "[Train] epoch:1965, lr:0.000918, total loss:0.000442\n",
            "[Train] epoch:1966, lr:0.000918, total loss:0.000410\n",
            "[Train] epoch:1967, lr:0.000918, total loss:0.000460\n",
            "[Train] epoch:1968, lr:0.000917, total loss:0.000461\n",
            "[Train] epoch:1969, lr:0.000917, total loss:0.000413\n",
            "[Train] epoch:1970, lr:0.000917, total loss:0.000430\n",
            "[Train] epoch:1971, lr:0.000917, total loss:0.000451\n",
            "[Train] epoch:1972, lr:0.000917, total loss:0.000424\n",
            "[Train] epoch:1973, lr:0.000917, total loss:0.000411\n",
            "[Train] epoch:1974, lr:0.000917, total loss:0.000484\n",
            "[Train] epoch:1975, lr:0.000917, total loss:0.000450\n",
            "[Train] epoch:1976, lr:0.000917, total loss:0.000466\n",
            "[Train] epoch:1977, lr:0.000917, total loss:0.000475\n",
            "[Train] epoch:1978, lr:0.000917, total loss:0.000450\n",
            "[Train] epoch:1979, lr:0.000917, total loss:0.000417\n",
            "[Train] epoch:1980, lr:0.000916, total loss:0.000442\n",
            "[Train] epoch:1981, lr:0.000916, total loss:0.000386\n",
            "[Train] epoch:1982, lr:0.000916, total loss:0.000437\n",
            "[Train] epoch:1983, lr:0.000916, total loss:0.000454\n",
            "[Train] epoch:1984, lr:0.000916, total loss:0.000399\n",
            "[Train] epoch:1985, lr:0.000916, total loss:0.000440\n",
            "[Train] epoch:1986, lr:0.000916, total loss:0.000466\n",
            "[Train] epoch:1987, lr:0.000916, total loss:0.000395\n",
            "[Train] epoch:1988, lr:0.000916, total loss:0.000458\n",
            "[Train] epoch:1989, lr:0.000916, total loss:0.000502\n",
            "[Train] epoch:1990, lr:0.000916, total loss:0.000437\n",
            "[Train] epoch:1991, lr:0.000916, total loss:0.000457\n",
            "[Train] epoch:1992, lr:0.000915, total loss:0.000474\n",
            "[Train] epoch:1993, lr:0.000915, total loss:0.000550\n",
            "[Train] epoch:1994, lr:0.000915, total loss:0.000475\n",
            "[Train] epoch:1995, lr:0.000915, total loss:0.000511\n",
            "[Train] epoch:1996, lr:0.000915, total loss:0.000497\n",
            "[Train] epoch:1997, lr:0.000915, total loss:0.000464\n",
            "[Train] epoch:1998, lr:0.000915, total loss:0.000500\n",
            "[Train] epoch:1999, lr:0.000915, total loss:0.000522\n",
            "[Train] epoch:2000, lr:0.000915, total loss:0.000693\n",
            "[Train] epoch:2001, lr:0.000915, total loss:0.000609\n",
            "[Train] epoch:2002, lr:0.000915, total loss:0.000509\n",
            "[Train] epoch:2003, lr:0.000915, total loss:0.000670\n",
            "[Train] epoch:2004, lr:0.000914, total loss:0.000685\n",
            "[Train] epoch:2005, lr:0.000914, total loss:0.000621\n",
            "[Train] epoch:2006, lr:0.000914, total loss:0.000710\n",
            "[Train] epoch:2007, lr:0.000914, total loss:0.000673\n",
            "[Train] epoch:2008, lr:0.000914, total loss:0.000563\n",
            "[Train] epoch:2009, lr:0.000914, total loss:0.000552\n",
            "[Train] epoch:2010, lr:0.000914, total loss:0.000546\n",
            "[Train] epoch:2011, lr:0.000914, total loss:0.000471\n",
            "[Train] epoch:2012, lr:0.000914, total loss:0.000582\n",
            "[Train] epoch:2013, lr:0.000914, total loss:0.000476\n",
            "[Train] epoch:2014, lr:0.000914, total loss:0.000497\n",
            "[Train] epoch:2015, lr:0.000914, total loss:0.000479\n",
            "[Train] epoch:2016, lr:0.000913, total loss:0.000444\n",
            "[Train] epoch:2017, lr:0.000913, total loss:0.000491\n",
            "[Train] epoch:2018, lr:0.000913, total loss:0.000446\n",
            "[Train] epoch:2019, lr:0.000913, total loss:0.000607\n",
            "[Train] epoch:2020, lr:0.000913, total loss:0.000516\n",
            "[Train] epoch:2021, lr:0.000913, total loss:0.000504\n",
            "[Train] epoch:2022, lr:0.000913, total loss:0.000509\n",
            "[Train] epoch:2023, lr:0.000913, total loss:0.000463\n",
            "[Train] epoch:2024, lr:0.000913, total loss:0.000429\n",
            "[Train] epoch:2025, lr:0.000913, total loss:0.000464\n",
            "[Train] epoch:2026, lr:0.000913, total loss:0.000452\n",
            "[Train] epoch:2027, lr:0.000913, total loss:0.000414\n",
            "[Train] epoch:2028, lr:0.000912, total loss:0.000455\n",
            "[Train] epoch:2029, lr:0.000912, total loss:0.000504\n",
            "[Train] epoch:2030, lr:0.000912, total loss:0.000473\n",
            "[Train] epoch:2031, lr:0.000912, total loss:0.000479\n",
            "[Train] epoch:2032, lr:0.000912, total loss:0.000463\n",
            "[Train] epoch:2033, lr:0.000912, total loss:0.000412\n",
            "[Train] epoch:2034, lr:0.000912, total loss:0.000406\n",
            "[Train] epoch:2035, lr:0.000912, total loss:0.000419\n",
            "[Train] epoch:2036, lr:0.000912, total loss:0.000411\n",
            "[Train] epoch:2037, lr:0.000912, total loss:0.000430\n",
            "[Train] epoch:2038, lr:0.000912, total loss:0.000414\n",
            "[Train] epoch:2039, lr:0.000912, total loss:0.000432\n",
            "[Train] epoch:2040, lr:0.000911, total loss:0.000541\n",
            "[Train] epoch:2041, lr:0.000911, total loss:0.000437\n",
            "[Train] epoch:2042, lr:0.000911, total loss:0.000451\n",
            "[Train] epoch:2043, lr:0.000911, total loss:0.000580\n",
            "[Train] epoch:2044, lr:0.000911, total loss:0.000646\n",
            "[Train] epoch:2045, lr:0.000911, total loss:0.000602\n",
            "[Train] epoch:2046, lr:0.000911, total loss:0.000575\n",
            "[Train] epoch:2047, lr:0.000911, total loss:0.000542\n",
            "[Train] epoch:2048, lr:0.000911, total loss:0.000497\n",
            "[Train] epoch:2049, lr:0.000911, total loss:0.000533\n",
            "[Train] epoch:2050, lr:0.000911, total loss:0.000484\n",
            "[Train] epoch:2051, lr:0.000911, total loss:0.000521\n",
            "[Train] epoch:2052, lr:0.000910, total loss:0.000519\n",
            "[Train] epoch:2053, lr:0.000910, total loss:0.000471\n",
            "[Train] epoch:2054, lr:0.000910, total loss:0.000455\n",
            "[Train] epoch:2055, lr:0.000910, total loss:0.000556\n",
            "[Train] epoch:2056, lr:0.000910, total loss:0.000489\n",
            "[Train] epoch:2057, lr:0.000910, total loss:0.000488\n",
            "[Train] epoch:2058, lr:0.000910, total loss:0.000419\n",
            "[Train] epoch:2059, lr:0.000910, total loss:0.000448\n",
            "[Train] epoch:2060, lr:0.000910, total loss:0.000509\n",
            "[Train] epoch:2061, lr:0.000910, total loss:0.000465\n",
            "[Train] epoch:2062, lr:0.000910, total loss:0.000434\n",
            "[Train] epoch:2063, lr:0.000910, total loss:0.000443\n",
            "[Train] epoch:2064, lr:0.000909, total loss:0.000502\n",
            "[Train] epoch:2065, lr:0.000909, total loss:0.000511\n",
            "[Train] epoch:2066, lr:0.000909, total loss:0.000477\n",
            "[Train] epoch:2067, lr:0.000909, total loss:0.000475\n",
            "[Train] epoch:2068, lr:0.000909, total loss:0.000443\n",
            "[Train] epoch:2069, lr:0.000909, total loss:0.000467\n",
            "[Train] epoch:2070, lr:0.000909, total loss:0.000590\n",
            "[Train] epoch:2071, lr:0.000909, total loss:0.000536\n",
            "[Train] epoch:2072, lr:0.000909, total loss:0.000607\n",
            "[Train] epoch:2073, lr:0.000909, total loss:0.000614\n",
            "[Train] epoch:2074, lr:0.000909, total loss:0.000535\n",
            "[Train] epoch:2075, lr:0.000908, total loss:0.000523\n",
            "[Train] epoch:2076, lr:0.000908, total loss:0.000449\n",
            "[Train] epoch:2077, lr:0.000908, total loss:0.000450\n",
            "[Train] epoch:2078, lr:0.000908, total loss:0.000444\n",
            "[Train] epoch:2079, lr:0.000908, total loss:0.000498\n",
            "[Train] epoch:2080, lr:0.000908, total loss:0.000432\n",
            "[Train] epoch:2081, lr:0.000908, total loss:0.000410\n",
            "[Train] epoch:2082, lr:0.000908, total loss:0.000409\n",
            "[Train] epoch:2083, lr:0.000908, total loss:0.000417\n",
            "[Train] epoch:2084, lr:0.000908, total loss:0.000432\n",
            "[Train] epoch:2085, lr:0.000908, total loss:0.000508\n",
            "[Train] epoch:2086, lr:0.000908, total loss:0.000417\n",
            "[Train] epoch:2087, lr:0.000907, total loss:0.000424\n",
            "[Train] epoch:2088, lr:0.000907, total loss:0.000506\n",
            "[Train] epoch:2089, lr:0.000907, total loss:0.000394\n",
            "[Train] epoch:2090, lr:0.000907, total loss:0.000453\n",
            "[Train] epoch:2091, lr:0.000907, total loss:0.000437\n",
            "[Train] epoch:2092, lr:0.000907, total loss:0.000429\n",
            "[Train] epoch:2093, lr:0.000907, total loss:0.000482\n",
            "[Train] epoch:2094, lr:0.000907, total loss:0.000478\n",
            "[Train] epoch:2095, lr:0.000907, total loss:0.000474\n",
            "[Train] epoch:2096, lr:0.000907, total loss:0.000394\n",
            "[Train] epoch:2097, lr:0.000907, total loss:0.000477\n",
            "[Train] epoch:2098, lr:0.000907, total loss:0.000490\n",
            "[Train] epoch:2099, lr:0.000906, total loss:0.000397\n",
            "[Train] epoch:2100, lr:0.000906, total loss:0.000441\n",
            "[Train] epoch:2101, lr:0.000906, total loss:0.000453\n",
            "[Train] epoch:2102, lr:0.000906, total loss:0.000446\n",
            "[Train] epoch:2103, lr:0.000906, total loss:0.000659\n",
            "[Train] epoch:2104, lr:0.000906, total loss:0.000497\n",
            "[Train] epoch:2105, lr:0.000906, total loss:0.000425\n",
            "[Train] epoch:2106, lr:0.000906, total loss:0.000438\n",
            "[Train] epoch:2107, lr:0.000906, total loss:0.000472\n",
            "[Train] epoch:2108, lr:0.000906, total loss:0.000438\n",
            "[Train] epoch:2109, lr:0.000906, total loss:0.000405\n",
            "[Train] epoch:2110, lr:0.000905, total loss:0.000442\n",
            "[Train] epoch:2111, lr:0.000905, total loss:0.000418\n",
            "[Train] epoch:2112, lr:0.000905, total loss:0.000439\n",
            "[Train] epoch:2113, lr:0.000905, total loss:0.000501\n",
            "[Train] epoch:2114, lr:0.000905, total loss:0.000480\n",
            "[Train] epoch:2115, lr:0.000905, total loss:0.000436\n",
            "[Train] epoch:2116, lr:0.000905, total loss:0.000456\n",
            "[Train] epoch:2117, lr:0.000905, total loss:0.000435\n",
            "[Train] epoch:2118, lr:0.000905, total loss:0.000429\n",
            "[Train] epoch:2119, lr:0.000905, total loss:0.000382\n",
            "[Train] epoch:2120, lr:0.000905, total loss:0.000410\n",
            "[Train] epoch:2121, lr:0.000905, total loss:0.000421\n",
            "[Train] epoch:2122, lr:0.000904, total loss:0.000408\n",
            "[Train] epoch:2123, lr:0.000904, total loss:0.000379\n",
            "[Train] epoch:2124, lr:0.000904, total loss:0.000400\n",
            "[Train] epoch:2125, lr:0.000904, total loss:0.000443\n",
            "[Train] epoch:2126, lr:0.000904, total loss:0.000385\n",
            "[Train] epoch:2127, lr:0.000904, total loss:0.000447\n",
            "[Train] epoch:2128, lr:0.000904, total loss:0.000501\n",
            "[Train] epoch:2129, lr:0.000904, total loss:0.000444\n",
            "[Train] epoch:2130, lr:0.000904, total loss:0.000599\n",
            "[Train] epoch:2131, lr:0.000904, total loss:0.000562\n",
            "[Train] epoch:2132, lr:0.000904, total loss:0.000539\n",
            "[Train] epoch:2133, lr:0.000903, total loss:0.000532\n",
            "[Train] epoch:2134, lr:0.000903, total loss:0.000465\n",
            "[Train] epoch:2135, lr:0.000903, total loss:0.000440\n",
            "[Train] epoch:2136, lr:0.000903, total loss:0.000473\n",
            "[Train] epoch:2137, lr:0.000903, total loss:0.000518\n",
            "[Train] epoch:2138, lr:0.000903, total loss:0.000465\n",
            "[Train] epoch:2139, lr:0.000903, total loss:0.000438\n",
            "[Train] epoch:2140, lr:0.000903, total loss:0.000416\n",
            "[Train] epoch:2141, lr:0.000903, total loss:0.000480\n",
            "[Train] epoch:2142, lr:0.000903, total loss:0.000428\n",
            "[Train] epoch:2143, lr:0.000903, total loss:0.000582\n",
            "[Train] epoch:2144, lr:0.000903, total loss:0.000506\n",
            "[Train] epoch:2145, lr:0.000902, total loss:0.000507\n",
            "[Train] epoch:2146, lr:0.000902, total loss:0.000428\n",
            "[Train] epoch:2147, lr:0.000902, total loss:0.000356\n",
            "[Train] epoch:2148, lr:0.000902, total loss:0.000416\n",
            "[Train] epoch:2149, lr:0.000902, total loss:0.000374\n",
            "[Train] epoch:2150, lr:0.000902, total loss:0.000391\n",
            "[Train] epoch:2151, lr:0.000902, total loss:0.000443\n",
            "[Train] epoch:2152, lr:0.000902, total loss:0.000466\n",
            "[Train] epoch:2153, lr:0.000902, total loss:0.000430\n",
            "[Train] epoch:2154, lr:0.000902, total loss:0.000446\n",
            "[Train] epoch:2155, lr:0.000902, total loss:0.000383\n",
            "[Train] epoch:2156, lr:0.000901, total loss:0.000420\n",
            "[Train] epoch:2157, lr:0.000901, total loss:0.000387\n",
            "[Train] epoch:2158, lr:0.000901, total loss:0.000433\n",
            "[Train] epoch:2159, lr:0.000901, total loss:0.000413\n",
            "[Train] epoch:2160, lr:0.000901, total loss:0.000457\n",
            "[Train] epoch:2161, lr:0.000901, total loss:0.000448\n",
            "[Train] epoch:2162, lr:0.000901, total loss:0.000468\n",
            "[Train] epoch:2163, lr:0.000901, total loss:0.000429\n",
            "[Train] epoch:2164, lr:0.000901, total loss:0.000426\n",
            "[Train] epoch:2165, lr:0.000901, total loss:0.000425\n",
            "[Train] epoch:2166, lr:0.000901, total loss:0.000415\n",
            "[Train] epoch:2167, lr:0.000900, total loss:0.000396\n",
            "[Train] epoch:2168, lr:0.000900, total loss:0.000373\n",
            "[Train] epoch:2169, lr:0.000900, total loss:0.000492\n",
            "[Train] epoch:2170, lr:0.000900, total loss:0.000487\n",
            "[Train] epoch:2171, lr:0.000900, total loss:0.000542\n",
            "[Train] epoch:2172, lr:0.000900, total loss:0.000502\n",
            "[Train] epoch:2173, lr:0.000900, total loss:0.000483\n",
            "[Train] epoch:2174, lr:0.000900, total loss:0.000579\n",
            "[Train] epoch:2175, lr:0.000900, total loss:0.000598\n",
            "[Train] epoch:2176, lr:0.000900, total loss:0.000554\n",
            "[Train] epoch:2177, lr:0.000900, total loss:0.000478\n",
            "[Train] epoch:2178, lr:0.000899, total loss:0.000495\n",
            "[Train] epoch:2179, lr:0.000899, total loss:0.000423\n",
            "[Train] epoch:2180, lr:0.000899, total loss:0.000482\n",
            "[Train] epoch:2181, lr:0.000899, total loss:0.000446\n",
            "[Train] epoch:2182, lr:0.000899, total loss:0.000432\n",
            "[Train] epoch:2183, lr:0.000899, total loss:0.000461\n",
            "[Train] epoch:2184, lr:0.000899, total loss:0.000473\n",
            "[Train] epoch:2185, lr:0.000899, total loss:0.000459\n",
            "[Train] epoch:2186, lr:0.000899, total loss:0.000444\n",
            "[Train] epoch:2187, lr:0.000899, total loss:0.000456\n",
            "[Train] epoch:2188, lr:0.000899, total loss:0.000400\n",
            "[Train] epoch:2189, lr:0.000899, total loss:0.000429\n",
            "[Train] epoch:2190, lr:0.000898, total loss:0.000434\n",
            "[Train] epoch:2191, lr:0.000898, total loss:0.000412\n",
            "[Train] epoch:2192, lr:0.000898, total loss:0.000477\n",
            "[Train] epoch:2193, lr:0.000898, total loss:0.000499\n",
            "[Train] epoch:2194, lr:0.000898, total loss:0.000509\n",
            "[Train] epoch:2195, lr:0.000898, total loss:0.000407\n",
            "[Train] epoch:2196, lr:0.000898, total loss:0.000392\n",
            "[Train] epoch:2197, lr:0.000898, total loss:0.000405\n",
            "[Train] epoch:2198, lr:0.000898, total loss:0.000419\n",
            "[Train] epoch:2199, lr:0.000898, total loss:0.000405\n",
            "[Train] epoch:2200, lr:0.000898, total loss:0.000408\n",
            "[Train] epoch:2201, lr:0.000897, total loss:0.000411\n",
            "[Train] epoch:2202, lr:0.000897, total loss:0.000426\n",
            "[Train] epoch:2203, lr:0.000897, total loss:0.000431\n",
            "[Train] epoch:2204, lr:0.000897, total loss:0.000424\n",
            "[Train] epoch:2205, lr:0.000897, total loss:0.000392\n",
            "[Train] epoch:2206, lr:0.000897, total loss:0.000407\n",
            "[Train] epoch:2207, lr:0.000897, total loss:0.000401\n",
            "[Train] epoch:2208, lr:0.000897, total loss:0.000453\n",
            "[Train] epoch:2209, lr:0.000897, total loss:0.000412\n",
            "[Train] epoch:2210, lr:0.000897, total loss:0.000406\n",
            "[Train] epoch:2211, lr:0.000897, total loss:0.000469\n",
            "[Train] epoch:2212, lr:0.000896, total loss:0.000452\n",
            "[Train] epoch:2213, lr:0.000896, total loss:0.000428\n",
            "[Train] epoch:2214, lr:0.000896, total loss:0.000394\n",
            "[Train] epoch:2215, lr:0.000896, total loss:0.000466\n",
            "[Train] epoch:2216, lr:0.000896, total loss:0.000459\n",
            "[Train] epoch:2217, lr:0.000896, total loss:0.000408\n",
            "[Train] epoch:2218, lr:0.000896, total loss:0.000450\n",
            "[Train] epoch:2219, lr:0.000896, total loss:0.000438\n",
            "[Train] epoch:2220, lr:0.000896, total loss:0.000424\n",
            "[Train] epoch:2221, lr:0.000896, total loss:0.000400\n",
            "[Train] epoch:2222, lr:0.000896, total loss:0.000439\n",
            "[Train] epoch:2223, lr:0.000895, total loss:0.000395\n",
            "[Train] epoch:2224, lr:0.000895, total loss:0.000409\n",
            "[Train] epoch:2225, lr:0.000895, total loss:0.000465\n",
            "[Train] epoch:2226, lr:0.000895, total loss:0.000477\n",
            "[Train] epoch:2227, lr:0.000895, total loss:0.000503\n",
            "[Train] epoch:2228, lr:0.000895, total loss:0.000428\n",
            "[Train] epoch:2229, lr:0.000895, total loss:0.000410\n",
            "[Train] epoch:2230, lr:0.000895, total loss:0.000364\n",
            "[Train] epoch:2231, lr:0.000895, total loss:0.000414\n",
            "[Train] epoch:2232, lr:0.000895, total loss:0.000429\n",
            "[Train] epoch:2233, lr:0.000895, total loss:0.000418\n",
            "[Train] epoch:2234, lr:0.000894, total loss:0.000441\n",
            "[Train] epoch:2235, lr:0.000894, total loss:0.000390\n",
            "[Train] epoch:2236, lr:0.000894, total loss:0.000372\n",
            "[Train] epoch:2237, lr:0.000894, total loss:0.000371\n",
            "[Train] epoch:2238, lr:0.000894, total loss:0.000358\n",
            "[Train] epoch:2239, lr:0.000894, total loss:0.000364\n",
            "[Train] epoch:2240, lr:0.000894, total loss:0.000456\n",
            "[Train] epoch:2241, lr:0.000894, total loss:0.000440\n",
            "[Train] epoch:2242, lr:0.000894, total loss:0.000428\n",
            "[Train] epoch:2243, lr:0.000894, total loss:0.000455\n",
            "[Train] epoch:2244, lr:0.000894, total loss:0.000413\n",
            "[Train] epoch:2245, lr:0.000893, total loss:0.000444\n",
            "[Train] epoch:2246, lr:0.000893, total loss:0.000465\n",
            "[Train] epoch:2247, lr:0.000893, total loss:0.000434\n",
            "[Train] epoch:2248, lr:0.000893, total loss:0.000450\n",
            "[Train] epoch:2249, lr:0.000893, total loss:0.000437\n",
            "[Train] epoch:2250, lr:0.000893, total loss:0.000463\n",
            "[Train] epoch:2251, lr:0.000893, total loss:0.000678\n",
            "[Train] epoch:2252, lr:0.000893, total loss:0.000662\n",
            "[Train] epoch:2253, lr:0.000893, total loss:0.000656\n",
            "[Train] epoch:2254, lr:0.000893, total loss:0.000606\n",
            "[Train] epoch:2255, lr:0.000893, total loss:0.000538\n",
            "[Train] epoch:2256, lr:0.000892, total loss:0.000486\n",
            "[Train] epoch:2257, lr:0.000892, total loss:0.000468\n",
            "[Train] epoch:2258, lr:0.000892, total loss:0.000439\n",
            "[Train] epoch:2259, lr:0.000892, total loss:0.000437\n",
            "[Train] epoch:2260, lr:0.000892, total loss:0.000520\n",
            "[Train] epoch:2261, lr:0.000892, total loss:0.000465\n",
            "[Train] epoch:2262, lr:0.000892, total loss:0.000508\n",
            "[Train] epoch:2263, lr:0.000892, total loss:0.000537\n",
            "[Train] epoch:2264, lr:0.000892, total loss:0.000442\n",
            "[Train] epoch:2265, lr:0.000892, total loss:0.000452\n",
            "[Train] epoch:2266, lr:0.000892, total loss:0.000451\n",
            "[Train] epoch:2267, lr:0.000891, total loss:0.000523\n",
            "[Train] epoch:2268, lr:0.000891, total loss:0.000501\n",
            "[Train] epoch:2269, lr:0.000891, total loss:0.000504\n",
            "[Train] epoch:2270, lr:0.000891, total loss:0.000416\n",
            "[Train] epoch:2271, lr:0.000891, total loss:0.000470\n",
            "[Train] epoch:2272, lr:0.000891, total loss:0.000417\n",
            "[Train] epoch:2273, lr:0.000891, total loss:0.000473\n",
            "[Train] epoch:2274, lr:0.000891, total loss:0.000370\n",
            "[Train] epoch:2275, lr:0.000891, total loss:0.000387\n",
            "[Train] epoch:2276, lr:0.000891, total loss:0.000444\n",
            "[Train] epoch:2277, lr:0.000890, total loss:0.000565\n",
            "[Train] epoch:2278, lr:0.000890, total loss:0.000629\n",
            "[Train] epoch:2279, lr:0.000890, total loss:0.000709\n",
            "[Train] epoch:2280, lr:0.000890, total loss:0.000710\n",
            "[Train] epoch:2281, lr:0.000890, total loss:0.000561\n",
            "[Train] epoch:2282, lr:0.000890, total loss:0.000495\n",
            "[Train] epoch:2283, lr:0.000890, total loss:0.000447\n",
            "[Train] epoch:2284, lr:0.000890, total loss:0.000455\n",
            "[Train] epoch:2285, lr:0.000890, total loss:0.000506\n",
            "[Train] epoch:2286, lr:0.000890, total loss:0.000467\n",
            "[Train] epoch:2287, lr:0.000890, total loss:0.000441\n",
            "[Train] epoch:2288, lr:0.000889, total loss:0.000390\n",
            "[Train] epoch:2289, lr:0.000889, total loss:0.000463\n",
            "[Train] epoch:2290, lr:0.000889, total loss:0.000434\n",
            "[Train] epoch:2291, lr:0.000889, total loss:0.000448\n",
            "[Train] epoch:2292, lr:0.000889, total loss:0.000396\n",
            "[Train] epoch:2293, lr:0.000889, total loss:0.000408\n",
            "[Train] epoch:2294, lr:0.000889, total loss:0.000401\n",
            "[Train] epoch:2295, lr:0.000889, total loss:0.000405\n",
            "[Train] epoch:2296, lr:0.000889, total loss:0.000401\n",
            "[Train] epoch:2297, lr:0.000889, total loss:0.000362\n",
            "[Train] epoch:2298, lr:0.000889, total loss:0.000367\n",
            "[Train] epoch:2299, lr:0.000888, total loss:0.000392\n",
            "[Train] epoch:2300, lr:0.000888, total loss:0.000369\n",
            "[Train] epoch:2301, lr:0.000888, total loss:0.000402\n",
            "[Train] epoch:2302, lr:0.000888, total loss:0.000375\n",
            "[Train] epoch:2303, lr:0.000888, total loss:0.000382\n",
            "[Train] epoch:2304, lr:0.000888, total loss:0.000396\n",
            "[Train] epoch:2305, lr:0.000888, total loss:0.000383\n",
            "[Train] epoch:2306, lr:0.000888, total loss:0.000368\n",
            "[Train] epoch:2307, lr:0.000888, total loss:0.000384\n",
            "[Train] epoch:2308, lr:0.000888, total loss:0.000390\n",
            "[Train] epoch:2309, lr:0.000888, total loss:0.000479\n",
            "[Train] epoch:2310, lr:0.000887, total loss:0.000515\n",
            "[Train] epoch:2311, lr:0.000887, total loss:0.000513\n",
            "[Train] epoch:2312, lr:0.000887, total loss:0.000440\n",
            "[Train] epoch:2313, lr:0.000887, total loss:0.000468\n",
            "[Train] epoch:2314, lr:0.000887, total loss:0.000448\n",
            "[Train] epoch:2315, lr:0.000887, total loss:0.000447\n",
            "[Train] epoch:2316, lr:0.000887, total loss:0.000402\n",
            "[Train] epoch:2317, lr:0.000887, total loss:0.000422\n",
            "[Train] epoch:2318, lr:0.000887, total loss:0.000409\n",
            "[Train] epoch:2319, lr:0.000887, total loss:0.000420\n",
            "[Train] epoch:2320, lr:0.000886, total loss:0.000368\n",
            "[Train] epoch:2321, lr:0.000886, total loss:0.000382\n",
            "[Train] epoch:2322, lr:0.000886, total loss:0.000382\n",
            "[Train] epoch:2323, lr:0.000886, total loss:0.000403\n",
            "[Train] epoch:2324, lr:0.000886, total loss:0.000334\n",
            "[Train] epoch:2325, lr:0.000886, total loss:0.000391\n",
            "[Train] epoch:2326, lr:0.000886, total loss:0.000418\n",
            "[Train] epoch:2327, lr:0.000886, total loss:0.000372\n",
            "[Train] epoch:2328, lr:0.000886, total loss:0.000397\n",
            "[Train] epoch:2329, lr:0.000886, total loss:0.000407\n",
            "[Train] epoch:2330, lr:0.000886, total loss:0.000464\n",
            "[Train] epoch:2331, lr:0.000885, total loss:0.000442\n",
            "[Train] epoch:2332, lr:0.000885, total loss:0.000382\n",
            "[Train] epoch:2333, lr:0.000885, total loss:0.000397\n",
            "[Train] epoch:2334, lr:0.000885, total loss:0.000367\n",
            "[Train] epoch:2335, lr:0.000885, total loss:0.000460\n",
            "[Train] epoch:2336, lr:0.000885, total loss:0.000410\n",
            "[Train] epoch:2337, lr:0.000885, total loss:0.000412\n",
            "[Train] epoch:2338, lr:0.000885, total loss:0.000428\n",
            "[Train] epoch:2339, lr:0.000885, total loss:0.000403\n",
            "[Train] epoch:2340, lr:0.000885, total loss:0.000431\n",
            "[Train] epoch:2341, lr:0.000885, total loss:0.000410\n",
            "[Train] epoch:2342, lr:0.000884, total loss:0.000421\n",
            "[Train] epoch:2343, lr:0.000884, total loss:0.000418\n",
            "[Train] epoch:2344, lr:0.000884, total loss:0.000403\n",
            "[Train] epoch:2345, lr:0.000884, total loss:0.000456\n",
            "[Train] epoch:2346, lr:0.000884, total loss:0.000378\n",
            "[Train] epoch:2347, lr:0.000884, total loss:0.000441\n",
            "[Train] epoch:2348, lr:0.000884, total loss:0.000447\n",
            "[Train] epoch:2349, lr:0.000884, total loss:0.000402\n",
            "[Train] epoch:2350, lr:0.000884, total loss:0.000503\n",
            "[Train] epoch:2351, lr:0.000884, total loss:0.000417\n",
            "[Train] epoch:2352, lr:0.000883, total loss:0.000407\n",
            "[Train] epoch:2353, lr:0.000883, total loss:0.000385\n",
            "[Train] epoch:2354, lr:0.000883, total loss:0.000402\n",
            "[Train] epoch:2355, lr:0.000883, total loss:0.000403\n",
            "[Train] epoch:2356, lr:0.000883, total loss:0.000433\n",
            "[Train] epoch:2357, lr:0.000883, total loss:0.000388\n",
            "[Train] epoch:2358, lr:0.000883, total loss:0.000375\n",
            "[Train] epoch:2359, lr:0.000883, total loss:0.000370\n",
            "[Train] epoch:2360, lr:0.000883, total loss:0.000386\n",
            "[Train] epoch:2361, lr:0.000883, total loss:0.000399\n",
            "[Train] epoch:2362, lr:0.000883, total loss:0.000362\n",
            "[Train] epoch:2363, lr:0.000882, total loss:0.000367\n",
            "[Train] epoch:2364, lr:0.000882, total loss:0.000362\n",
            "[Train] epoch:2365, lr:0.000882, total loss:0.000372\n",
            "[Train] epoch:2366, lr:0.000882, total loss:0.000397\n",
            "[Train] epoch:2367, lr:0.000882, total loss:0.000360\n",
            "[Train] epoch:2368, lr:0.000882, total loss:0.000380\n",
            "[Train] epoch:2369, lr:0.000882, total loss:0.000360\n",
            "[Train] epoch:2370, lr:0.000882, total loss:0.000444\n",
            "[Train] epoch:2371, lr:0.000882, total loss:0.000414\n",
            "[Train] epoch:2372, lr:0.000882, total loss:0.000391\n",
            "[Train] epoch:2373, lr:0.000881, total loss:0.000370\n",
            "[Train] epoch:2374, lr:0.000881, total loss:0.000416\n",
            "[Train] epoch:2375, lr:0.000881, total loss:0.000403\n",
            "[Train] epoch:2376, lr:0.000881, total loss:0.000430\n",
            "[Train] epoch:2377, lr:0.000881, total loss:0.000423\n",
            "[Train] epoch:2378, lr:0.000881, total loss:0.000395\n",
            "[Train] epoch:2379, lr:0.000881, total loss:0.000377\n",
            "[Train] epoch:2380, lr:0.000881, total loss:0.000363\n",
            "[Train] epoch:2381, lr:0.000881, total loss:0.000383\n",
            "[Train] epoch:2382, lr:0.000881, total loss:0.000395\n",
            "[Train] epoch:2383, lr:0.000881, total loss:0.000446\n",
            "[Train] epoch:2384, lr:0.000880, total loss:0.000588\n",
            "[Train] epoch:2385, lr:0.000880, total loss:0.000520\n",
            "[Train] epoch:2386, lr:0.000880, total loss:0.000620\n",
            "[Train] epoch:2387, lr:0.000880, total loss:0.000488\n",
            "[Train] epoch:2388, lr:0.000880, total loss:0.000503\n",
            "[Train] epoch:2389, lr:0.000880, total loss:0.000485\n",
            "[Train] epoch:2390, lr:0.000880, total loss:0.000392\n",
            "[Train] epoch:2391, lr:0.000880, total loss:0.000409\n",
            "[Train] epoch:2392, lr:0.000880, total loss:0.000423\n",
            "[Train] epoch:2393, lr:0.000880, total loss:0.000407\n",
            "[Train] epoch:2394, lr:0.000879, total loss:0.000369\n",
            "[Train] epoch:2395, lr:0.000879, total loss:0.000510\n",
            "[Train] epoch:2396, lr:0.000879, total loss:0.000404\n",
            "[Train] epoch:2397, lr:0.000879, total loss:0.000423\n",
            "[Train] epoch:2398, lr:0.000879, total loss:0.000400\n",
            "[Train] epoch:2399, lr:0.000879, total loss:0.000428\n",
            "[Train] epoch:2400, lr:0.000879, total loss:0.000412\n",
            "[Train] epoch:2401, lr:0.000879, total loss:0.000424\n",
            "[Train] epoch:2402, lr:0.000879, total loss:0.000395\n",
            "[Train] epoch:2403, lr:0.000879, total loss:0.000397\n",
            "[Train] epoch:2404, lr:0.000878, total loss:0.000381\n",
            "[Train] epoch:2405, lr:0.000878, total loss:0.000396\n",
            "[Train] epoch:2406, lr:0.000878, total loss:0.000430\n",
            "[Train] epoch:2407, lr:0.000878, total loss:0.000387\n",
            "[Train] epoch:2408, lr:0.000878, total loss:0.000385\n",
            "[Train] epoch:2409, lr:0.000878, total loss:0.000427\n",
            "[Train] epoch:2410, lr:0.000878, total loss:0.000410\n",
            "[Train] epoch:2411, lr:0.000878, total loss:0.000398\n",
            "[Train] epoch:2412, lr:0.000878, total loss:0.000390\n",
            "[Train] epoch:2413, lr:0.000878, total loss:0.000357\n",
            "[Train] epoch:2414, lr:0.000878, total loss:0.000363\n",
            "[Train] epoch:2415, lr:0.000877, total loss:0.000382\n",
            "[Train] epoch:2416, lr:0.000877, total loss:0.000375\n",
            "[Train] epoch:2417, lr:0.000877, total loss:0.000407\n",
            "[Train] epoch:2418, lr:0.000877, total loss:0.000364\n",
            "[Train] epoch:2419, lr:0.000877, total loss:0.000438\n",
            "[Train] epoch:2420, lr:0.000877, total loss:0.000386\n",
            "[Train] epoch:2421, lr:0.000877, total loss:0.000384\n",
            "[Train] epoch:2422, lr:0.000877, total loss:0.000420\n",
            "[Train] epoch:2423, lr:0.000877, total loss:0.000392\n",
            "[Train] epoch:2424, lr:0.000877, total loss:0.000369\n",
            "[Train] epoch:2425, lr:0.000876, total loss:0.000427\n",
            "[Train] epoch:2426, lr:0.000876, total loss:0.000413\n",
            "[Train] epoch:2427, lr:0.000876, total loss:0.000378\n",
            "[Train] epoch:2428, lr:0.000876, total loss:0.000409\n",
            "[Train] epoch:2429, lr:0.000876, total loss:0.000412\n",
            "[Train] epoch:2430, lr:0.000876, total loss:0.000352\n",
            "[Train] epoch:2431, lr:0.000876, total loss:0.000376\n",
            "[Train] epoch:2432, lr:0.000876, total loss:0.000379\n",
            "[Train] epoch:2433, lr:0.000876, total loss:0.000366\n",
            "[Train] epoch:2434, lr:0.000876, total loss:0.000383\n",
            "[Train] epoch:2435, lr:0.000875, total loss:0.000422\n",
            "[Train] epoch:2436, lr:0.000875, total loss:0.000392\n",
            "[Train] epoch:2437, lr:0.000875, total loss:0.000419\n",
            "[Train] epoch:2438, lr:0.000875, total loss:0.000475\n",
            "[Train] epoch:2439, lr:0.000875, total loss:0.000429\n",
            "[Train] epoch:2440, lr:0.000875, total loss:0.000433\n",
            "[Train] epoch:2441, lr:0.000875, total loss:0.000539\n",
            "[Train] epoch:2442, lr:0.000875, total loss:0.000403\n",
            "[Train] epoch:2443, lr:0.000875, total loss:0.000428\n",
            "[Train] epoch:2444, lr:0.000875, total loss:0.000406\n",
            "[Train] epoch:2445, lr:0.000874, total loss:0.000398\n",
            "[Train] epoch:2446, lr:0.000874, total loss:0.000413\n",
            "[Train] epoch:2447, lr:0.000874, total loss:0.000428\n",
            "[Train] epoch:2448, lr:0.000874, total loss:0.000445\n",
            "[Train] epoch:2449, lr:0.000874, total loss:0.000409\n",
            "[Train] epoch:2450, lr:0.000874, total loss:0.000419\n",
            "[Train] epoch:2451, lr:0.000874, total loss:0.000379\n",
            "[Train] epoch:2452, lr:0.000874, total loss:0.000402\n",
            "[Train] epoch:2453, lr:0.000874, total loss:0.000459\n",
            "[Train] epoch:2454, lr:0.000874, total loss:0.000485\n",
            "[Train] epoch:2455, lr:0.000874, total loss:0.000400\n",
            "[Train] epoch:2456, lr:0.000873, total loss:0.000498\n",
            "[Train] epoch:2457, lr:0.000873, total loss:0.000426\n",
            "[Train] epoch:2458, lr:0.000873, total loss:0.000399\n",
            "[Train] epoch:2459, lr:0.000873, total loss:0.000383\n",
            "[Train] epoch:2460, lr:0.000873, total loss:0.000415\n",
            "[Train] epoch:2461, lr:0.000873, total loss:0.000429\n",
            "[Train] epoch:2462, lr:0.000873, total loss:0.000405\n",
            "[Train] epoch:2463, lr:0.000873, total loss:0.000361\n",
            "[Train] epoch:2464, lr:0.000873, total loss:0.000372\n",
            "[Train] epoch:2465, lr:0.000873, total loss:0.000437\n",
            "[Train] epoch:2466, lr:0.000872, total loss:0.000439\n",
            "[Train] epoch:2467, lr:0.000872, total loss:0.000446\n",
            "[Train] epoch:2468, lr:0.000872, total loss:0.000464\n",
            "[Train] epoch:2469, lr:0.000872, total loss:0.000482\n",
            "[Train] epoch:2470, lr:0.000872, total loss:0.000419\n",
            "[Train] epoch:2471, lr:0.000872, total loss:0.000432\n",
            "[Train] epoch:2472, lr:0.000872, total loss:0.000456\n",
            "[Train] epoch:2473, lr:0.000872, total loss:0.000532\n",
            "[Train] epoch:2474, lr:0.000872, total loss:0.000464\n",
            "[Train] epoch:2475, lr:0.000872, total loss:0.000411\n",
            "[Train] epoch:2476, lr:0.000871, total loss:0.000395\n",
            "[Train] epoch:2477, lr:0.000871, total loss:0.000365\n",
            "[Train] epoch:2478, lr:0.000871, total loss:0.000409\n",
            "[Train] epoch:2479, lr:0.000871, total loss:0.000361\n",
            "[Train] epoch:2480, lr:0.000871, total loss:0.000393\n",
            "[Train] epoch:2481, lr:0.000871, total loss:0.000376\n",
            "[Train] epoch:2482, lr:0.000871, total loss:0.000395\n",
            "[Train] epoch:2483, lr:0.000871, total loss:0.000406\n",
            "[Train] epoch:2484, lr:0.000871, total loss:0.000361\n",
            "[Train] epoch:2485, lr:0.000871, total loss:0.000363\n",
            "[Train] epoch:2486, lr:0.000870, total loss:0.000347\n",
            "[Train] epoch:2487, lr:0.000870, total loss:0.000387\n",
            "[Train] epoch:2488, lr:0.000870, total loss:0.000358\n",
            "[Train] epoch:2489, lr:0.000870, total loss:0.000352\n",
            "[Train] epoch:2490, lr:0.000870, total loss:0.000344\n",
            "[Train] epoch:2491, lr:0.000870, total loss:0.000390\n",
            "[Train] epoch:2492, lr:0.000870, total loss:0.000371\n",
            "[Train] epoch:2493, lr:0.000870, total loss:0.000375\n",
            "[Train] epoch:2494, lr:0.000870, total loss:0.000372\n",
            "[Train] epoch:2495, lr:0.000870, total loss:0.000382\n",
            "[Train] epoch:2496, lr:0.000869, total loss:0.000424\n",
            "[Train] epoch:2497, lr:0.000869, total loss:0.000396\n",
            "[Train] epoch:2498, lr:0.000869, total loss:0.000368\n",
            "[Train] epoch:2499, lr:0.000869, total loss:0.000426\n",
            "[Train] epoch:2500, lr:0.000869, total loss:0.000561\n",
            "[Train] epoch:2501, lr:0.000869, total loss:0.000357\n",
            "[Train] epoch:2502, lr:0.000869, total loss:0.000357\n",
            "[Train] epoch:2503, lr:0.000869, total loss:0.000391\n",
            "[Train] epoch:2504, lr:0.000869, total loss:0.000390\n",
            "[Train] epoch:2505, lr:0.000869, total loss:0.000371\n",
            "[Train] epoch:2506, lr:0.000868, total loss:0.000389\n",
            "[Train] epoch:2507, lr:0.000868, total loss:0.000349\n",
            "[Train] epoch:2508, lr:0.000868, total loss:0.000371\n",
            "[Train] epoch:2509, lr:0.000868, total loss:0.000403\n",
            "[Train] epoch:2510, lr:0.000868, total loss:0.000395\n",
            "[Train] epoch:2511, lr:0.000868, total loss:0.000374\n",
            "[Train] epoch:2512, lr:0.000868, total loss:0.000370\n",
            "[Train] epoch:2513, lr:0.000868, total loss:0.000378\n",
            "[Train] epoch:2514, lr:0.000868, total loss:0.000379\n",
            "[Train] epoch:2515, lr:0.000868, total loss:0.000433\n",
            "[Train] epoch:2516, lr:0.000867, total loss:0.000402\n",
            "[Train] epoch:2517, lr:0.000867, total loss:0.000477\n",
            "[Train] epoch:2518, lr:0.000867, total loss:0.000416\n",
            "[Train] epoch:2519, lr:0.000867, total loss:0.000457\n",
            "[Train] epoch:2520, lr:0.000867, total loss:0.000388\n",
            "[Train] epoch:2521, lr:0.000867, total loss:0.000405\n",
            "[Train] epoch:2522, lr:0.000867, total loss:0.000442\n",
            "[Train] epoch:2523, lr:0.000867, total loss:0.000402\n",
            "[Train] epoch:2524, lr:0.000867, total loss:0.000413\n",
            "[Train] epoch:2525, lr:0.000867, total loss:0.000414\n",
            "[Train] epoch:2526, lr:0.000866, total loss:0.000432\n",
            "[Train] epoch:2527, lr:0.000866, total loss:0.000411\n",
            "[Train] epoch:2528, lr:0.000866, total loss:0.000376\n",
            "[Train] epoch:2529, lr:0.000866, total loss:0.000396\n",
            "[Train] epoch:2530, lr:0.000866, total loss:0.000375\n",
            "[Train] epoch:2531, lr:0.000866, total loss:0.000371\n",
            "[Train] epoch:2532, lr:0.000866, total loss:0.000409\n",
            "[Train] epoch:2533, lr:0.000866, total loss:0.000400\n",
            "[Train] epoch:2534, lr:0.000866, total loss:0.000431\n",
            "[Train] epoch:2535, lr:0.000866, total loss:0.000466\n",
            "[Train] epoch:2536, lr:0.000865, total loss:0.000467\n",
            "[Train] epoch:2537, lr:0.000865, total loss:0.000430\n",
            "[Train] epoch:2538, lr:0.000865, total loss:0.000434\n",
            "[Train] epoch:2539, lr:0.000865, total loss:0.000484\n",
            "[Train] epoch:2540, lr:0.000865, total loss:0.000418\n",
            "[Train] epoch:2541, lr:0.000865, total loss:0.000398\n",
            "[Train] epoch:2542, lr:0.000865, total loss:0.000338\n",
            "[Train] epoch:2543, lr:0.000865, total loss:0.000377\n",
            "[Train] epoch:2544, lr:0.000865, total loss:0.000354\n",
            "[Train] epoch:2545, lr:0.000865, total loss:0.000372\n",
            "[Train] epoch:2546, lr:0.000864, total loss:0.000374\n",
            "[Train] epoch:2547, lr:0.000864, total loss:0.000426\n",
            "[Train] epoch:2548, lr:0.000864, total loss:0.000427\n",
            "[Train] epoch:2549, lr:0.000864, total loss:0.000370\n",
            "[Train] epoch:2550, lr:0.000864, total loss:0.000357\n",
            "[Train] epoch:2551, lr:0.000864, total loss:0.000387\n",
            "[Train] epoch:2552, lr:0.000864, total loss:0.000385\n",
            "[Train] epoch:2553, lr:0.000864, total loss:0.000334\n",
            "[Train] epoch:2554, lr:0.000864, total loss:0.000389\n",
            "[Train] epoch:2555, lr:0.000864, total loss:0.000391\n",
            "[Train] epoch:2556, lr:0.000863, total loss:0.000395\n",
            "[Train] epoch:2557, lr:0.000863, total loss:0.000487\n",
            "[Train] epoch:2558, lr:0.000863, total loss:0.000586\n",
            "[Train] epoch:2559, lr:0.000863, total loss:0.000520\n",
            "[Train] epoch:2560, lr:0.000863, total loss:0.000588\n",
            "[Train] epoch:2561, lr:0.000863, total loss:0.000540\n",
            "[Train] epoch:2562, lr:0.000863, total loss:0.000506\n",
            "[Train] epoch:2563, lr:0.000863, total loss:0.000487\n",
            "[Train] epoch:2564, lr:0.000863, total loss:0.000404\n",
            "[Train] epoch:2565, lr:0.000862, total loss:0.000382\n",
            "[Train] epoch:2566, lr:0.000862, total loss:0.000375\n",
            "[Train] epoch:2567, lr:0.000862, total loss:0.000424\n",
            "[Train] epoch:2568, lr:0.000862, total loss:0.000467\n",
            "[Train] epoch:2569, lr:0.000862, total loss:0.000399\n",
            "[Train] epoch:2570, lr:0.000862, total loss:0.000393\n",
            "[Train] epoch:2571, lr:0.000862, total loss:0.000362\n",
            "[Train] epoch:2572, lr:0.000862, total loss:0.000345\n",
            "[Train] epoch:2573, lr:0.000862, total loss:0.000380\n",
            "[Train] epoch:2574, lr:0.000862, total loss:0.000404\n",
            "[Train] epoch:2575, lr:0.000861, total loss:0.000434\n",
            "[Train] epoch:2576, lr:0.000861, total loss:0.000390\n",
            "[Train] epoch:2577, lr:0.000861, total loss:0.000400\n",
            "[Train] epoch:2578, lr:0.000861, total loss:0.000363\n",
            "[Train] epoch:2579, lr:0.000861, total loss:0.000404\n",
            "[Train] epoch:2580, lr:0.000861, total loss:0.000358\n",
            "[Train] epoch:2581, lr:0.000861, total loss:0.000429\n",
            "[Train] epoch:2582, lr:0.000861, total loss:0.000403\n",
            "[Train] epoch:2583, lr:0.000861, total loss:0.000426\n",
            "[Train] epoch:2584, lr:0.000861, total loss:0.000429\n",
            "[Train] epoch:2585, lr:0.000860, total loss:0.000379\n",
            "[Train] epoch:2586, lr:0.000860, total loss:0.000401\n",
            "[Train] epoch:2587, lr:0.000860, total loss:0.000399\n",
            "[Train] epoch:2588, lr:0.000860, total loss:0.000383\n",
            "[Train] epoch:2589, lr:0.000860, total loss:0.000368\n",
            "[Train] epoch:2590, lr:0.000860, total loss:0.000358\n",
            "[Train] epoch:2591, lr:0.000860, total loss:0.000403\n",
            "[Train] epoch:2592, lr:0.000860, total loss:0.000429\n",
            "[Train] epoch:2593, lr:0.000860, total loss:0.000435\n",
            "[Train] epoch:2594, lr:0.000860, total loss:0.000391\n",
            "[Train] epoch:2595, lr:0.000859, total loss:0.000361\n",
            "[Train] epoch:2596, lr:0.000859, total loss:0.000353\n",
            "[Train] epoch:2597, lr:0.000859, total loss:0.000374\n",
            "[Train] epoch:2598, lr:0.000859, total loss:0.000372\n",
            "[Train] epoch:2599, lr:0.000859, total loss:0.000373\n",
            "[Train] epoch:2600, lr:0.000859, total loss:0.000386\n",
            "[Train] epoch:2601, lr:0.000859, total loss:0.000434\n",
            "[Train] epoch:2602, lr:0.000859, total loss:0.000403\n",
            "[Train] epoch:2603, lr:0.000859, total loss:0.000452\n",
            "[Train] epoch:2604, lr:0.000858, total loss:0.000552\n",
            "[Train] epoch:2605, lr:0.000858, total loss:0.000499\n",
            "[Train] epoch:2606, lr:0.000858, total loss:0.000460\n",
            "[Train] epoch:2607, lr:0.000858, total loss:0.000460\n",
            "[Train] epoch:2608, lr:0.000858, total loss:0.000377\n",
            "[Train] epoch:2609, lr:0.000858, total loss:0.000395\n",
            "[Train] epoch:2610, lr:0.000858, total loss:0.000343\n",
            "[Train] epoch:2611, lr:0.000858, total loss:0.000363\n",
            "[Train] epoch:2612, lr:0.000858, total loss:0.000372\n",
            "[Train] epoch:2613, lr:0.000858, total loss:0.000428\n",
            "[Train] epoch:2614, lr:0.000857, total loss:0.000393\n",
            "[Train] epoch:2615, lr:0.000857, total loss:0.000394\n",
            "[Train] epoch:2616, lr:0.000857, total loss:0.000392\n",
            "[Train] epoch:2617, lr:0.000857, total loss:0.000452\n",
            "[Train] epoch:2618, lr:0.000857, total loss:0.000411\n",
            "[Train] epoch:2619, lr:0.000857, total loss:0.000360\n",
            "[Train] epoch:2620, lr:0.000857, total loss:0.000351\n",
            "[Train] epoch:2621, lr:0.000857, total loss:0.000383\n",
            "[Train] epoch:2622, lr:0.000857, total loss:0.000393\n",
            "[Train] epoch:2623, lr:0.000857, total loss:0.000428\n",
            "[Train] epoch:2624, lr:0.000856, total loss:0.000407\n",
            "[Train] epoch:2625, lr:0.000856, total loss:0.000372\n",
            "[Train] epoch:2626, lr:0.000856, total loss:0.000373\n",
            "[Train] epoch:2627, lr:0.000856, total loss:0.000540\n",
            "[Train] epoch:2628, lr:0.000856, total loss:0.000379\n",
            "[Train] epoch:2629, lr:0.000856, total loss:0.000361\n",
            "[Train] epoch:2630, lr:0.000856, total loss:0.000381\n",
            "[Train] epoch:2631, lr:0.000856, total loss:0.000359\n",
            "[Train] epoch:2632, lr:0.000856, total loss:0.000358\n",
            "[Train] epoch:2633, lr:0.000855, total loss:0.000325\n",
            "[Train] epoch:2634, lr:0.000855, total loss:0.000379\n",
            "[Train] epoch:2635, lr:0.000855, total loss:0.000391\n",
            "[Train] epoch:2636, lr:0.000855, total loss:0.000370\n",
            "[Train] epoch:2637, lr:0.000855, total loss:0.000375\n",
            "[Train] epoch:2638, lr:0.000855, total loss:0.000360\n",
            "[Train] epoch:2639, lr:0.000855, total loss:0.000383\n",
            "[Train] epoch:2640, lr:0.000855, total loss:0.000393\n",
            "[Train] epoch:2641, lr:0.000855, total loss:0.000437\n",
            "[Train] epoch:2642, lr:0.000855, total loss:0.000378\n",
            "[Train] epoch:2643, lr:0.000854, total loss:0.000442\n",
            "[Train] epoch:2644, lr:0.000854, total loss:0.000423\n",
            "[Train] epoch:2645, lr:0.000854, total loss:0.000395\n",
            "[Train] epoch:2646, lr:0.000854, total loss:0.000437\n",
            "[Train] epoch:2647, lr:0.000854, total loss:0.000424\n",
            "[Train] epoch:2648, lr:0.000854, total loss:0.000337\n",
            "[Train] epoch:2649, lr:0.000854, total loss:0.000402\n",
            "[Train] epoch:2650, lr:0.000854, total loss:0.000377\n",
            "[Train] epoch:2651, lr:0.000854, total loss:0.000368\n",
            "[Train] epoch:2652, lr:0.000854, total loss:0.000360\n",
            "[Train] epoch:2653, lr:0.000853, total loss:0.000409\n",
            "[Train] epoch:2654, lr:0.000853, total loss:0.000380\n",
            "[Train] epoch:2655, lr:0.000853, total loss:0.000375\n",
            "[Train] epoch:2656, lr:0.000853, total loss:0.000347\n",
            "[Train] epoch:2657, lr:0.000853, total loss:0.000364\n",
            "[Train] epoch:2658, lr:0.000853, total loss:0.000403\n",
            "[Train] epoch:2659, lr:0.000853, total loss:0.000416\n",
            "[Train] epoch:2660, lr:0.000853, total loss:0.000409\n",
            "[Train] epoch:2661, lr:0.000853, total loss:0.000395\n",
            "[Train] epoch:2662, lr:0.000852, total loss:0.000385\n",
            "[Train] epoch:2663, lr:0.000852, total loss:0.000400\n",
            "[Train] epoch:2664, lr:0.000852, total loss:0.000425\n",
            "[Train] epoch:2665, lr:0.000852, total loss:0.000432\n",
            "[Train] epoch:2666, lr:0.000852, total loss:0.000366\n",
            "[Train] epoch:2667, lr:0.000852, total loss:0.000389\n",
            "[Train] epoch:2668, lr:0.000852, total loss:0.000420\n",
            "[Train] epoch:2669, lr:0.000852, total loss:0.000383\n",
            "[Train] epoch:2670, lr:0.000852, total loss:0.000380\n",
            "[Train] epoch:2671, lr:0.000852, total loss:0.000567\n",
            "[Train] epoch:2672, lr:0.000851, total loss:0.000435\n",
            "[Train] epoch:2673, lr:0.000851, total loss:0.000467\n",
            "[Train] epoch:2674, lr:0.000851, total loss:0.000467\n",
            "[Train] epoch:2675, lr:0.000851, total loss:0.000378\n",
            "[Train] epoch:2676, lr:0.000851, total loss:0.000407\n",
            "[Train] epoch:2677, lr:0.000851, total loss:0.000332\n",
            "[Train] epoch:2678, lr:0.000851, total loss:0.000388\n",
            "[Train] epoch:2679, lr:0.000851, total loss:0.000412\n",
            "[Train] epoch:2680, lr:0.000851, total loss:0.000383\n",
            "[Train] epoch:2681, lr:0.000850, total loss:0.000410\n",
            "[Train] epoch:2682, lr:0.000850, total loss:0.000463\n",
            "[Train] epoch:2683, lr:0.000850, total loss:0.000464\n",
            "[Train] epoch:2684, lr:0.000850, total loss:0.000417\n",
            "[Train] epoch:2685, lr:0.000850, total loss:0.000388\n",
            "[Train] epoch:2686, lr:0.000850, total loss:0.000401\n",
            "[Train] epoch:2687, lr:0.000850, total loss:0.000403\n",
            "[Train] epoch:2688, lr:0.000850, total loss:0.000351\n",
            "[Train] epoch:2689, lr:0.000850, total loss:0.000397\n",
            "[Train] epoch:2690, lr:0.000850, total loss:0.000379\n",
            "[Train] epoch:2691, lr:0.000849, total loss:0.000380\n",
            "[Train] epoch:2692, lr:0.000849, total loss:0.000376\n",
            "[Train] epoch:2693, lr:0.000849, total loss:0.000384\n",
            "[Train] epoch:2694, lr:0.000849, total loss:0.000389\n",
            "[Train] epoch:2695, lr:0.000849, total loss:0.000439\n",
            "[Train] epoch:2696, lr:0.000849, total loss:0.000371\n",
            "[Train] epoch:2697, lr:0.000849, total loss:0.000396\n",
            "[Train] epoch:2698, lr:0.000849, total loss:0.000412\n",
            "[Train] epoch:2699, lr:0.000849, total loss:0.000370\n",
            "[Train] epoch:2700, lr:0.000848, total loss:0.000389\n",
            "[Train] epoch:2701, lr:0.000848, total loss:0.000415\n",
            "[Train] epoch:2702, lr:0.000848, total loss:0.000390\n",
            "[Train] epoch:2703, lr:0.000848, total loss:0.000367\n",
            "[Train] epoch:2704, lr:0.000848, total loss:0.000392\n",
            "[Train] epoch:2705, lr:0.000848, total loss:0.000319\n",
            "[Train] epoch:2706, lr:0.000848, total loss:0.000337\n",
            "[Train] epoch:2707, lr:0.000848, total loss:0.000331\n",
            "[Train] epoch:2708, lr:0.000848, total loss:0.000337\n",
            "[Train] epoch:2709, lr:0.000848, total loss:0.000391\n",
            "[Train] epoch:2710, lr:0.000847, total loss:0.000397\n",
            "[Train] epoch:2711, lr:0.000847, total loss:0.000357\n",
            "[Train] epoch:2712, lr:0.000847, total loss:0.000374\n",
            "[Train] epoch:2713, lr:0.000847, total loss:0.000364\n",
            "[Train] epoch:2714, lr:0.000847, total loss:0.000358\n",
            "[Train] epoch:2715, lr:0.000847, total loss:0.000356\n",
            "[Train] epoch:2716, lr:0.000847, total loss:0.000374\n",
            "[Train] epoch:2717, lr:0.000847, total loss:0.000451\n",
            "[Train] epoch:2718, lr:0.000847, total loss:0.000364\n",
            "[Train] epoch:2719, lr:0.000846, total loss:0.000383\n",
            "[Train] epoch:2720, lr:0.000846, total loss:0.000419\n",
            "[Train] epoch:2721, lr:0.000846, total loss:0.000453\n",
            "[Train] epoch:2722, lr:0.000846, total loss:0.000352\n",
            "[Train] epoch:2723, lr:0.000846, total loss:0.000373\n",
            "[Train] epoch:2724, lr:0.000846, total loss:0.000347\n",
            "[Train] epoch:2725, lr:0.000846, total loss:0.000329\n",
            "[Train] epoch:2726, lr:0.000846, total loss:0.000396\n",
            "[Train] epoch:2727, lr:0.000846, total loss:0.000349\n",
            "[Train] epoch:2728, lr:0.000845, total loss:0.000366\n",
            "[Train] epoch:2729, lr:0.000845, total loss:0.000366\n",
            "[Train] epoch:2730, lr:0.000845, total loss:0.000399\n",
            "[Train] epoch:2731, lr:0.000845, total loss:0.000415\n",
            "[Train] epoch:2732, lr:0.000845, total loss:0.000382\n",
            "[Train] epoch:2733, lr:0.000845, total loss:0.000349\n",
            "[Train] epoch:2734, lr:0.000845, total loss:0.000372\n",
            "[Train] epoch:2735, lr:0.000845, total loss:0.000431\n",
            "[Train] epoch:2736, lr:0.000845, total loss:0.000359\n",
            "[Train] epoch:2737, lr:0.000845, total loss:0.000347\n",
            "[Train] epoch:2738, lr:0.000844, total loss:0.000416\n",
            "[Train] epoch:2739, lr:0.000844, total loss:0.000359\n",
            "[Train] epoch:2740, lr:0.000844, total loss:0.000465\n",
            "[Train] epoch:2741, lr:0.000844, total loss:0.000420\n",
            "[Train] epoch:2742, lr:0.000844, total loss:0.000413\n",
            "[Train] epoch:2743, lr:0.000844, total loss:0.000416\n",
            "[Train] epoch:2744, lr:0.000844, total loss:0.000379\n",
            "[Train] epoch:2745, lr:0.000844, total loss:0.000357\n",
            "[Train] epoch:2746, lr:0.000844, total loss:0.000351\n",
            "[Train] epoch:2747, lr:0.000843, total loss:0.000373\n",
            "[Train] epoch:2748, lr:0.000843, total loss:0.000379\n",
            "[Train] epoch:2749, lr:0.000843, total loss:0.000380\n",
            "[Train] epoch:2750, lr:0.000843, total loss:0.000358\n",
            "[Train] epoch:2751, lr:0.000843, total loss:0.000359\n",
            "[Train] epoch:2752, lr:0.000843, total loss:0.000340\n",
            "[Train] epoch:2753, lr:0.000843, total loss:0.000357\n",
            "[Train] epoch:2754, lr:0.000843, total loss:0.000369\n",
            "[Train] epoch:2755, lr:0.000843, total loss:0.000328\n",
            "[Train] epoch:2756, lr:0.000842, total loss:0.000377\n",
            "[Train] epoch:2757, lr:0.000842, total loss:0.000324\n",
            "[Train] epoch:2758, lr:0.000842, total loss:0.000350\n",
            "[Train] epoch:2759, lr:0.000842, total loss:0.000338\n",
            "[Train] epoch:2760, lr:0.000842, total loss:0.000366\n",
            "[Train] epoch:2761, lr:0.000842, total loss:0.000359\n",
            "[Train] epoch:2762, lr:0.000842, total loss:0.000336\n",
            "[Train] epoch:2763, lr:0.000842, total loss:0.000386\n",
            "[Train] epoch:2764, lr:0.000842, total loss:0.000398\n",
            "[Train] epoch:2765, lr:0.000842, total loss:0.000386\n",
            "[Train] epoch:2766, lr:0.000841, total loss:0.000355\n",
            "[Train] epoch:2767, lr:0.000841, total loss:0.000428\n",
            "[Train] epoch:2768, lr:0.000841, total loss:0.000473\n",
            "[Train] epoch:2769, lr:0.000841, total loss:0.000472\n",
            "[Train] epoch:2770, lr:0.000841, total loss:0.000395\n",
            "[Train] epoch:2771, lr:0.000841, total loss:0.000518\n",
            "[Train] epoch:2772, lr:0.000841, total loss:0.000461\n",
            "[Train] epoch:2773, lr:0.000841, total loss:0.000446\n",
            "[Train] epoch:2774, lr:0.000841, total loss:0.000383\n",
            "[Train] epoch:2775, lr:0.000840, total loss:0.000333\n",
            "[Train] epoch:2776, lr:0.000840, total loss:0.000432\n",
            "[Train] epoch:2777, lr:0.000840, total loss:0.000384\n",
            "[Train] epoch:2778, lr:0.000840, total loss:0.000418\n",
            "[Train] epoch:2779, lr:0.000840, total loss:0.000404\n",
            "[Train] epoch:2780, lr:0.000840, total loss:0.000367\n",
            "[Train] epoch:2781, lr:0.000840, total loss:0.000323\n",
            "[Train] epoch:2782, lr:0.000840, total loss:0.000361\n",
            "[Train] epoch:2783, lr:0.000840, total loss:0.000378\n",
            "[Train] epoch:2784, lr:0.000839, total loss:0.000354\n",
            "[Train] epoch:2785, lr:0.000839, total loss:0.000365\n",
            "[Train] epoch:2786, lr:0.000839, total loss:0.000423\n",
            "[Train] epoch:2787, lr:0.000839, total loss:0.000357\n",
            "[Train] epoch:2788, lr:0.000839, total loss:0.000388\n",
            "[Train] epoch:2789, lr:0.000839, total loss:0.000374\n",
            "[Train] epoch:2790, lr:0.000839, total loss:0.000479\n",
            "[Train] epoch:2791, lr:0.000839, total loss:0.000411\n",
            "[Train] epoch:2792, lr:0.000839, total loss:0.000431\n",
            "[Train] epoch:2793, lr:0.000838, total loss:0.000348\n",
            "[Train] epoch:2794, lr:0.000838, total loss:0.000402\n",
            "[Train] epoch:2795, lr:0.000838, total loss:0.000367\n",
            "[Train] epoch:2796, lr:0.000838, total loss:0.000353\n",
            "[Train] epoch:2797, lr:0.000838, total loss:0.000324\n",
            "[Train] epoch:2798, lr:0.000838, total loss:0.000314\n",
            "[Train] epoch:2799, lr:0.000838, total loss:0.000379\n",
            "[Train] epoch:2800, lr:0.000838, total loss:0.000398\n",
            "[Train] epoch:2801, lr:0.000838, total loss:0.000384\n",
            "[Train] epoch:2802, lr:0.000838, total loss:0.000398\n",
            "[Train] epoch:2803, lr:0.000837, total loss:0.000353\n",
            "[Train] epoch:2804, lr:0.000837, total loss:0.000349\n",
            "[Train] epoch:2805, lr:0.000837, total loss:0.000347\n",
            "[Train] epoch:2806, lr:0.000837, total loss:0.000338\n",
            "[Train] epoch:2807, lr:0.000837, total loss:0.000305\n",
            "[Train] epoch:2808, lr:0.000837, total loss:0.000311\n",
            "[Train] epoch:2809, lr:0.000837, total loss:0.000348\n",
            "[Train] epoch:2810, lr:0.000837, total loss:0.000348\n",
            "[Train] epoch:2811, lr:0.000837, total loss:0.000346\n",
            "[Train] epoch:2812, lr:0.000836, total loss:0.000309\n",
            "[Train] epoch:2813, lr:0.000836, total loss:0.000345\n",
            "[Train] epoch:2814, lr:0.000836, total loss:0.000322\n",
            "[Train] epoch:2815, lr:0.000836, total loss:0.000328\n",
            "[Train] epoch:2816, lr:0.000836, total loss:0.000317\n",
            "[Train] epoch:2817, lr:0.000836, total loss:0.000355\n",
            "[Train] epoch:2818, lr:0.000836, total loss:0.000344\n",
            "[Train] epoch:2819, lr:0.000836, total loss:0.000360\n",
            "[Train] epoch:2820, lr:0.000836, total loss:0.000349\n",
            "[Train] epoch:2821, lr:0.000835, total loss:0.000366\n",
            "[Train] epoch:2822, lr:0.000835, total loss:0.000359\n",
            "[Train] epoch:2823, lr:0.000835, total loss:0.000379\n",
            "[Train] epoch:2824, lr:0.000835, total loss:0.000458\n",
            "[Train] epoch:2825, lr:0.000835, total loss:0.000341\n",
            "[Train] epoch:2826, lr:0.000835, total loss:0.000388\n",
            "[Train] epoch:2827, lr:0.000835, total loss:0.000391\n",
            "[Train] epoch:2828, lr:0.000835, total loss:0.000443\n",
            "[Train] epoch:2829, lr:0.000835, total loss:0.000358\n",
            "[Train] epoch:2830, lr:0.000834, total loss:0.000367\n",
            "[Train] epoch:2831, lr:0.000834, total loss:0.000373\n",
            "[Train] epoch:2832, lr:0.000834, total loss:0.000419\n",
            "[Train] epoch:2833, lr:0.000834, total loss:0.000360\n",
            "[Train] epoch:2834, lr:0.000834, total loss:0.000333\n",
            "[Train] epoch:2835, lr:0.000834, total loss:0.000472\n",
            "[Train] epoch:2836, lr:0.000834, total loss:0.000442\n",
            "[Train] epoch:2837, lr:0.000834, total loss:0.000419\n",
            "[Train] epoch:2838, lr:0.000834, total loss:0.000468\n",
            "[Train] epoch:2839, lr:0.000833, total loss:0.000406\n",
            "[Train] epoch:2840, lr:0.000833, total loss:0.000379\n",
            "[Train] epoch:2841, lr:0.000833, total loss:0.000364\n",
            "[Train] epoch:2842, lr:0.000833, total loss:0.000341\n",
            "[Train] epoch:2843, lr:0.000833, total loss:0.000356\n",
            "[Train] epoch:2844, lr:0.000833, total loss:0.000401\n",
            "[Train] epoch:2845, lr:0.000833, total loss:0.000415\n",
            "[Train] epoch:2846, lr:0.000833, total loss:0.000378\n",
            "[Train] epoch:2847, lr:0.000833, total loss:0.000349\n",
            "[Train] epoch:2848, lr:0.000832, total loss:0.000388\n",
            "[Train] epoch:2849, lr:0.000832, total loss:0.000393\n",
            "[Train] epoch:2850, lr:0.000832, total loss:0.000367\n",
            "[Train] epoch:2851, lr:0.000832, total loss:0.000336\n",
            "[Train] epoch:2852, lr:0.000832, total loss:0.000315\n",
            "[Train] epoch:2853, lr:0.000832, total loss:0.000382\n",
            "[Train] epoch:2854, lr:0.000832, total loss:0.000335\n",
            "[Train] epoch:2855, lr:0.000832, total loss:0.000364\n",
            "[Train] epoch:2856, lr:0.000832, total loss:0.000320\n",
            "[Train] epoch:2857, lr:0.000831, total loss:0.000345\n",
            "[Train] epoch:2858, lr:0.000831, total loss:0.000362\n",
            "[Train] epoch:2859, lr:0.000831, total loss:0.000341\n",
            "[Train] epoch:2860, lr:0.000831, total loss:0.000345\n",
            "[Train] epoch:2861, lr:0.000831, total loss:0.000441\n",
            "[Train] epoch:2862, lr:0.000831, total loss:0.000366\n",
            "[Train] epoch:2863, lr:0.000831, total loss:0.000314\n",
            "[Train] epoch:2864, lr:0.000831, total loss:0.000373\n",
            "[Train] epoch:2865, lr:0.000831, total loss:0.000348\n",
            "[Train] epoch:2866, lr:0.000830, total loss:0.000401\n",
            "[Train] epoch:2867, lr:0.000830, total loss:0.000361\n",
            "[Train] epoch:2868, lr:0.000830, total loss:0.000392\n",
            "[Train] epoch:2869, lr:0.000830, total loss:0.000363\n",
            "[Train] epoch:2870, lr:0.000830, total loss:0.000388\n",
            "[Train] epoch:2871, lr:0.000830, total loss:0.000379\n",
            "[Train] epoch:2872, lr:0.000830, total loss:0.000354\n",
            "[Train] epoch:2873, lr:0.000830, total loss:0.000355\n",
            "[Train] epoch:2874, lr:0.000830, total loss:0.000373\n",
            "[Train] epoch:2875, lr:0.000829, total loss:0.000396\n",
            "[Train] epoch:2876, lr:0.000829, total loss:0.000369\n",
            "[Train] epoch:2877, lr:0.000829, total loss:0.000385\n",
            "[Train] epoch:2878, lr:0.000829, total loss:0.000386\n",
            "[Train] epoch:2879, lr:0.000829, total loss:0.000386\n",
            "[Train] epoch:2880, lr:0.000829, total loss:0.000358\n",
            "[Train] epoch:2881, lr:0.000829, total loss:0.000437\n",
            "[Train] epoch:2882, lr:0.000829, total loss:0.000379\n",
            "[Train] epoch:2883, lr:0.000829, total loss:0.000351\n",
            "[Train] epoch:2884, lr:0.000828, total loss:0.000349\n",
            "[Train] epoch:2885, lr:0.000828, total loss:0.000324\n",
            "[Train] epoch:2886, lr:0.000828, total loss:0.000357\n",
            "[Train] epoch:2887, lr:0.000828, total loss:0.000368\n",
            "[Train] epoch:2888, lr:0.000828, total loss:0.000326\n",
            "[Train] epoch:2889, lr:0.000828, total loss:0.000338\n",
            "[Train] epoch:2890, lr:0.000828, total loss:0.000335\n",
            "[Train] epoch:2891, lr:0.000828, total loss:0.000379\n",
            "[Train] epoch:2892, lr:0.000828, total loss:0.000382\n",
            "[Train] epoch:2893, lr:0.000827, total loss:0.000450\n",
            "[Train] epoch:2894, lr:0.000827, total loss:0.000394\n",
            "[Train] epoch:2895, lr:0.000827, total loss:0.000379\n",
            "[Train] epoch:2896, lr:0.000827, total loss:0.000371\n",
            "[Train] epoch:2897, lr:0.000827, total loss:0.000373\n",
            "[Train] epoch:2898, lr:0.000827, total loss:0.000338\n",
            "[Train] epoch:2899, lr:0.000827, total loss:0.000340\n",
            "[Train] epoch:2900, lr:0.000827, total loss:0.000328\n",
            "[Train] epoch:2901, lr:0.000827, total loss:0.000326\n",
            "[Train] epoch:2902, lr:0.000826, total loss:0.000364\n",
            "[Train] epoch:2903, lr:0.000826, total loss:0.000348\n",
            "[Train] epoch:2904, lr:0.000826, total loss:0.000381\n",
            "[Train] epoch:2905, lr:0.000826, total loss:0.000391\n",
            "[Train] epoch:2906, lr:0.000826, total loss:0.000374\n",
            "[Train] epoch:2907, lr:0.000826, total loss:0.000366\n",
            "[Train] epoch:2908, lr:0.000826, total loss:0.000341\n",
            "[Train] epoch:2909, lr:0.000826, total loss:0.000342\n",
            "[Train] epoch:2910, lr:0.000826, total loss:0.000350\n",
            "[Train] epoch:2911, lr:0.000825, total loss:0.000336\n",
            "[Train] epoch:2912, lr:0.000825, total loss:0.000328\n",
            "[Train] epoch:2913, lr:0.000825, total loss:0.000355\n",
            "[Train] epoch:2914, lr:0.000825, total loss:0.000318\n",
            "[Train] epoch:2915, lr:0.000825, total loss:0.000307\n",
            "[Train] epoch:2916, lr:0.000825, total loss:0.000364\n",
            "[Train] epoch:2917, lr:0.000825, total loss:0.000379\n",
            "[Train] epoch:2918, lr:0.000825, total loss:0.000373\n",
            "[Train] epoch:2919, lr:0.000825, total loss:0.000353\n",
            "[Train] epoch:2920, lr:0.000824, total loss:0.000334\n",
            "[Train] epoch:2921, lr:0.000824, total loss:0.000357\n",
            "[Train] epoch:2922, lr:0.000824, total loss:0.000344\n",
            "[Train] epoch:2923, lr:0.000824, total loss:0.000352\n",
            "[Train] epoch:2924, lr:0.000824, total loss:0.000379\n",
            "[Train] epoch:2925, lr:0.000824, total loss:0.000328\n",
            "[Train] epoch:2926, lr:0.000824, total loss:0.000309\n",
            "[Train] epoch:2927, lr:0.000824, total loss:0.000319\n",
            "[Train] epoch:2928, lr:0.000824, total loss:0.000325\n",
            "[Train] epoch:2929, lr:0.000823, total loss:0.000348\n",
            "[Train] epoch:2930, lr:0.000823, total loss:0.000323\n",
            "[Train] epoch:2931, lr:0.000823, total loss:0.000301\n",
            "[Train] epoch:2932, lr:0.000823, total loss:0.000318\n",
            "[Train] epoch:2933, lr:0.000823, total loss:0.000405\n",
            "[Train] epoch:2934, lr:0.000823, total loss:0.000384\n",
            "[Train] epoch:2935, lr:0.000823, total loss:0.000421\n",
            "[Train] epoch:2936, lr:0.000823, total loss:0.000431\n",
            "[Train] epoch:2937, lr:0.000823, total loss:0.000373\n",
            "[Train] epoch:2938, lr:0.000822, total loss:0.000384\n",
            "[Train] epoch:2939, lr:0.000822, total loss:0.000327\n",
            "[Train] epoch:2940, lr:0.000822, total loss:0.000373\n",
            "[Train] epoch:2941, lr:0.000822, total loss:0.000380\n",
            "[Train] epoch:2942, lr:0.000822, total loss:0.000332\n",
            "[Train] epoch:2943, lr:0.000822, total loss:0.000377\n",
            "[Train] epoch:2944, lr:0.000822, total loss:0.000310\n",
            "[Train] epoch:2945, lr:0.000822, total loss:0.000344\n",
            "[Train] epoch:2946, lr:0.000822, total loss:0.000367\n",
            "[Train] epoch:2947, lr:0.000821, total loss:0.000309\n",
            "[Train] epoch:2948, lr:0.000821, total loss:0.000340\n",
            "[Train] epoch:2949, lr:0.000821, total loss:0.000307\n",
            "[Train] epoch:2950, lr:0.000821, total loss:0.000297\n",
            "[Train] epoch:2951, lr:0.000821, total loss:0.000349\n",
            "[Train] epoch:2952, lr:0.000821, total loss:0.000331\n",
            "[Train] epoch:2953, lr:0.000821, total loss:0.000322\n",
            "[Train] epoch:2954, lr:0.000821, total loss:0.000321\n",
            "[Train] epoch:2955, lr:0.000821, total loss:0.000304\n",
            "[Train] epoch:2956, lr:0.000820, total loss:0.000346\n",
            "[Train] epoch:2957, lr:0.000820, total loss:0.000349\n",
            "[Train] epoch:2958, lr:0.000820, total loss:0.000509\n",
            "[Train] epoch:2959, lr:0.000820, total loss:0.000412\n",
            "[Train] epoch:2960, lr:0.000820, total loss:0.000460\n",
            "[Train] epoch:2961, lr:0.000820, total loss:0.000408\n",
            "[Train] epoch:2962, lr:0.000820, total loss:0.000398\n",
            "[Train] epoch:2963, lr:0.000820, total loss:0.000396\n",
            "[Train] epoch:2964, lr:0.000820, total loss:0.000434\n",
            "[Train] epoch:2965, lr:0.000819, total loss:0.000397\n",
            "[Train] epoch:2966, lr:0.000819, total loss:0.000357\n",
            "[Train] epoch:2967, lr:0.000819, total loss:0.000359\n",
            "[Train] epoch:2968, lr:0.000819, total loss:0.000361\n",
            "[Train] epoch:2969, lr:0.000819, total loss:0.000314\n",
            "[Train] epoch:2970, lr:0.000819, total loss:0.000384\n",
            "[Train] epoch:2971, lr:0.000819, total loss:0.000383\n",
            "[Train] epoch:2972, lr:0.000819, total loss:0.000356\n",
            "[Train] epoch:2973, lr:0.000818, total loss:0.000347\n",
            "[Train] epoch:2974, lr:0.000818, total loss:0.000359\n",
            "[Train] epoch:2975, lr:0.000818, total loss:0.000333\n",
            "[Train] epoch:2976, lr:0.000818, total loss:0.000360\n",
            "[Train] epoch:2977, lr:0.000818, total loss:0.000350\n",
            "[Train] epoch:2978, lr:0.000818, total loss:0.000336\n",
            "[Train] epoch:2979, lr:0.000818, total loss:0.000375\n",
            "[Train] epoch:2980, lr:0.000818, total loss:0.000336\n",
            "[Train] epoch:2981, lr:0.000818, total loss:0.000391\n",
            "[Train] epoch:2982, lr:0.000817, total loss:0.000352\n",
            "[Train] epoch:2983, lr:0.000817, total loss:0.000346\n",
            "[Train] epoch:2984, lr:0.000817, total loss:0.000323\n",
            "[Train] epoch:2985, lr:0.000817, total loss:0.000339\n",
            "[Train] epoch:2986, lr:0.000817, total loss:0.000327\n",
            "[Train] epoch:2987, lr:0.000817, total loss:0.000305\n",
            "[Train] epoch:2988, lr:0.000817, total loss:0.000312\n",
            "[Train] epoch:2989, lr:0.000817, total loss:0.000338\n",
            "[Train] epoch:2990, lr:0.000817, total loss:0.000302\n",
            "[Train] epoch:2991, lr:0.000816, total loss:0.000379\n",
            "[Train] epoch:2992, lr:0.000816, total loss:0.000336\n",
            "[Train] epoch:2993, lr:0.000816, total loss:0.000319\n",
            "[Train] epoch:2994, lr:0.000816, total loss:0.000306\n",
            "[Train] epoch:2995, lr:0.000816, total loss:0.000338\n",
            "[Train] epoch:2996, lr:0.000816, total loss:0.000387\n",
            "[Train] epoch:2997, lr:0.000816, total loss:0.000296\n",
            "[Train] epoch:2998, lr:0.000816, total loss:0.000356\n",
            "[Train] epoch:2999, lr:0.000816, total loss:0.000310\n",
            "[Train] epoch:3000, lr:0.000815, total loss:0.000385\n",
            "[Train] epoch:3001, lr:0.000815, total loss:0.000371\n",
            "[Train] epoch:3002, lr:0.000815, total loss:0.000371\n",
            "[Train] epoch:3003, lr:0.000815, total loss:0.000415\n",
            "[Train] epoch:3004, lr:0.000815, total loss:0.000341\n",
            "[Train] epoch:3005, lr:0.000815, total loss:0.000444\n",
            "[Train] epoch:3006, lr:0.000815, total loss:0.000560\n",
            "[Train] epoch:3007, lr:0.000815, total loss:0.000498\n",
            "[Train] epoch:3008, lr:0.000815, total loss:0.000396\n",
            "[Train] epoch:3009, lr:0.000814, total loss:0.000382\n",
            "[Train] epoch:3010, lr:0.000814, total loss:0.000360\n",
            "[Train] epoch:3011, lr:0.000814, total loss:0.000447\n",
            "[Train] epoch:3012, lr:0.000814, total loss:0.000424\n",
            "[Train] epoch:3013, lr:0.000814, total loss:0.000456\n",
            "[Train] epoch:3014, lr:0.000814, total loss:0.000376\n",
            "[Train] epoch:3015, lr:0.000814, total loss:0.000407\n",
            "[Train] epoch:3016, lr:0.000814, total loss:0.000388\n",
            "[Train] epoch:3017, lr:0.000813, total loss:0.000384\n",
            "[Train] epoch:3018, lr:0.000813, total loss:0.000461\n",
            "[Train] epoch:3019, lr:0.000813, total loss:0.000413\n",
            "[Train] epoch:3020, lr:0.000813, total loss:0.000387\n",
            "[Train] epoch:3021, lr:0.000813, total loss:0.000430\n",
            "[Train] epoch:3022, lr:0.000813, total loss:0.000462\n",
            "[Train] epoch:3023, lr:0.000813, total loss:0.000505\n",
            "[Train] epoch:3024, lr:0.000813, total loss:0.000489\n",
            "[Train] epoch:3025, lr:0.000813, total loss:0.000369\n",
            "[Train] epoch:3026, lr:0.000812, total loss:0.000370\n",
            "[Train] epoch:3027, lr:0.000812, total loss:0.000435\n",
            "[Train] epoch:3028, lr:0.000812, total loss:0.000384\n",
            "[Train] epoch:3029, lr:0.000812, total loss:0.000365\n",
            "[Train] epoch:3030, lr:0.000812, total loss:0.000376\n",
            "[Train] epoch:3031, lr:0.000812, total loss:0.000337\n",
            "[Train] epoch:3032, lr:0.000812, total loss:0.000354\n",
            "[Train] epoch:3033, lr:0.000812, total loss:0.000331\n",
            "[Train] epoch:3034, lr:0.000812, total loss:0.000381\n",
            "[Train] epoch:3035, lr:0.000811, total loss:0.000394\n",
            "[Train] epoch:3036, lr:0.000811, total loss:0.000374\n",
            "[Train] epoch:3037, lr:0.000811, total loss:0.000315\n",
            "[Train] epoch:3038, lr:0.000811, total loss:0.000362\n",
            "[Train] epoch:3039, lr:0.000811, total loss:0.000375\n",
            "[Train] epoch:3040, lr:0.000811, total loss:0.000325\n",
            "[Train] epoch:3041, lr:0.000811, total loss:0.000321\n",
            "[Train] epoch:3042, lr:0.000811, total loss:0.000346\n",
            "[Train] epoch:3043, lr:0.000810, total loss:0.000324\n",
            "[Train] epoch:3044, lr:0.000810, total loss:0.000361\n",
            "[Train] epoch:3045, lr:0.000810, total loss:0.000331\n",
            "[Train] epoch:3046, lr:0.000810, total loss:0.000391\n",
            "[Train] epoch:3047, lr:0.000810, total loss:0.000476\n",
            "[Train] epoch:3048, lr:0.000810, total loss:0.000346\n",
            "[Train] epoch:3049, lr:0.000810, total loss:0.000409\n",
            "[Train] epoch:3050, lr:0.000810, total loss:0.000391\n",
            "[Train] epoch:3051, lr:0.000810, total loss:0.000360\n",
            "[Train] epoch:3052, lr:0.000809, total loss:0.000429\n",
            "[Train] epoch:3053, lr:0.000809, total loss:0.000364\n",
            "[Train] epoch:3054, lr:0.000809, total loss:0.000458\n",
            "[Train] epoch:3055, lr:0.000809, total loss:0.000331\n",
            "[Train] epoch:3056, lr:0.000809, total loss:0.000367\n",
            "[Train] epoch:3057, lr:0.000809, total loss:0.000348\n",
            "[Train] epoch:3058, lr:0.000809, total loss:0.000407\n",
            "[Train] epoch:3059, lr:0.000809, total loss:0.000358\n",
            "[Train] epoch:3060, lr:0.000809, total loss:0.000371\n",
            "[Train] epoch:3061, lr:0.000808, total loss:0.000384\n",
            "[Train] epoch:3062, lr:0.000808, total loss:0.000354\n",
            "[Train] epoch:3063, lr:0.000808, total loss:0.000349\n",
            "[Train] epoch:3064, lr:0.000808, total loss:0.000338\n",
            "[Train] epoch:3065, lr:0.000808, total loss:0.000388\n",
            "[Train] epoch:3066, lr:0.000808, total loss:0.000372\n",
            "[Train] epoch:3067, lr:0.000808, total loss:0.000518\n",
            "[Train] epoch:3068, lr:0.000808, total loss:0.000391\n",
            "[Train] epoch:3069, lr:0.000807, total loss:0.000369\n",
            "[Train] epoch:3070, lr:0.000807, total loss:0.000341\n",
            "[Train] epoch:3071, lr:0.000807, total loss:0.000369\n",
            "[Train] epoch:3072, lr:0.000807, total loss:0.000470\n",
            "[Train] epoch:3073, lr:0.000807, total loss:0.000345\n",
            "[Train] epoch:3074, lr:0.000807, total loss:0.000418\n",
            "[Train] epoch:3075, lr:0.000807, total loss:0.000559\n",
            "[Train] epoch:3076, lr:0.000807, total loss:0.000484\n",
            "[Train] epoch:3077, lr:0.000807, total loss:0.000512\n",
            "[Train] epoch:3078, lr:0.000806, total loss:0.000404\n",
            "[Train] epoch:3079, lr:0.000806, total loss:0.000410\n",
            "[Train] epoch:3080, lr:0.000806, total loss:0.000371\n",
            "[Train] epoch:3081, lr:0.000806, total loss:0.000424\n",
            "[Train] epoch:3082, lr:0.000806, total loss:0.000367\n",
            "[Train] epoch:3083, lr:0.000806, total loss:0.000364\n",
            "[Train] epoch:3084, lr:0.000806, total loss:0.000340\n",
            "[Train] epoch:3085, lr:0.000806, total loss:0.000325\n",
            "[Train] epoch:3086, lr:0.000805, total loss:0.000338\n",
            "[Train] epoch:3087, lr:0.000805, total loss:0.000316\n",
            "[Train] epoch:3088, lr:0.000805, total loss:0.000317\n",
            "[Train] epoch:3089, lr:0.000805, total loss:0.000322\n",
            "[Train] epoch:3090, lr:0.000805, total loss:0.000308\n",
            "[Train] epoch:3091, lr:0.000805, total loss:0.000310\n",
            "[Train] epoch:3092, lr:0.000805, total loss:0.000295\n",
            "[Train] epoch:3093, lr:0.000805, total loss:0.000320\n",
            "[Train] epoch:3094, lr:0.000805, total loss:0.000320\n",
            "[Train] epoch:3095, lr:0.000804, total loss:0.000323\n",
            "[Train] epoch:3096, lr:0.000804, total loss:0.000326\n",
            "[Train] epoch:3097, lr:0.000804, total loss:0.000308\n",
            "[Train] epoch:3098, lr:0.000804, total loss:0.000347\n",
            "[Train] epoch:3099, lr:0.000804, total loss:0.000324\n",
            "[Train] epoch:3100, lr:0.000804, total loss:0.000340\n",
            "[Train] epoch:3101, lr:0.000804, total loss:0.000342\n",
            "[Train] epoch:3102, lr:0.000804, total loss:0.000313\n",
            "[Train] epoch:3103, lr:0.000804, total loss:0.000339\n",
            "[Train] epoch:3104, lr:0.000803, total loss:0.000321\n",
            "[Train] epoch:3105, lr:0.000803, total loss:0.000306\n",
            "[Train] epoch:3106, lr:0.000803, total loss:0.000304\n",
            "[Train] epoch:3107, lr:0.000803, total loss:0.000300\n",
            "[Train] epoch:3108, lr:0.000803, total loss:0.000312\n",
            "[Train] epoch:3109, lr:0.000803, total loss:0.000317\n",
            "[Train] epoch:3110, lr:0.000803, total loss:0.000311\n",
            "[Train] epoch:3111, lr:0.000803, total loss:0.000346\n",
            "[Train] epoch:3112, lr:0.000802, total loss:0.000313\n",
            "[Train] epoch:3113, lr:0.000802, total loss:0.000331\n",
            "[Train] epoch:3114, lr:0.000802, total loss:0.000334\n",
            "[Train] epoch:3115, lr:0.000802, total loss:0.000326\n",
            "[Train] epoch:3116, lr:0.000802, total loss:0.000360\n",
            "[Train] epoch:3117, lr:0.000802, total loss:0.000341\n",
            "[Train] epoch:3118, lr:0.000802, total loss:0.000349\n",
            "[Train] epoch:3119, lr:0.000802, total loss:0.000376\n",
            "[Train] epoch:3120, lr:0.000802, total loss:0.000380\n",
            "[Train] epoch:3121, lr:0.000801, total loss:0.000396\n",
            "[Train] epoch:3122, lr:0.000801, total loss:0.000311\n",
            "[Train] epoch:3123, lr:0.000801, total loss:0.000356\n",
            "[Train] epoch:3124, lr:0.000801, total loss:0.000339\n",
            "[Train] epoch:3125, lr:0.000801, total loss:0.000339\n",
            "[Train] epoch:3126, lr:0.000801, total loss:0.000353\n",
            "[Train] epoch:3127, lr:0.000801, total loss:0.000387\n",
            "[Train] epoch:3128, lr:0.000801, total loss:0.000345\n",
            "[Train] epoch:3129, lr:0.000800, total loss:0.000348\n",
            "[Train] epoch:3130, lr:0.000800, total loss:0.000310\n",
            "[Train] epoch:3131, lr:0.000800, total loss:0.000314\n",
            "[Train] epoch:3132, lr:0.000800, total loss:0.000334\n",
            "[Train] epoch:3133, lr:0.000800, total loss:0.000362\n",
            "[Train] epoch:3134, lr:0.000800, total loss:0.000298\n",
            "[Train] epoch:3135, lr:0.000800, total loss:0.000369\n",
            "[Train] epoch:3136, lr:0.000800, total loss:0.000329\n",
            "[Train] epoch:3137, lr:0.000800, total loss:0.000493\n",
            "[Train] epoch:3138, lr:0.000799, total loss:0.000530\n",
            "[Train] epoch:3139, lr:0.000799, total loss:0.000592\n",
            "[Train] epoch:3140, lr:0.000799, total loss:0.000496\n",
            "[Train] epoch:3141, lr:0.000799, total loss:0.000387\n",
            "[Train] epoch:3142, lr:0.000799, total loss:0.000397\n",
            "[Train] epoch:3143, lr:0.000799, total loss:0.000343\n",
            "[Train] epoch:3144, lr:0.000799, total loss:0.000338\n",
            "[Train] epoch:3145, lr:0.000799, total loss:0.000350\n",
            "[Train] epoch:3146, lr:0.000798, total loss:0.000327\n",
            "[Train] epoch:3147, lr:0.000798, total loss:0.000356\n",
            "[Train] epoch:3148, lr:0.000798, total loss:0.000353\n",
            "[Train] epoch:3149, lr:0.000798, total loss:0.000296\n",
            "[Train] epoch:3150, lr:0.000798, total loss:0.000295\n",
            "[Train] epoch:3151, lr:0.000798, total loss:0.000352\n",
            "[Train] epoch:3152, lr:0.000798, total loss:0.000328\n",
            "[Train] epoch:3153, lr:0.000798, total loss:0.000374\n",
            "[Train] epoch:3154, lr:0.000798, total loss:0.000333\n",
            "[Train] epoch:3155, lr:0.000797, total loss:0.000380\n",
            "[Train] epoch:3156, lr:0.000797, total loss:0.000417\n",
            "[Train] epoch:3157, lr:0.000797, total loss:0.000390\n",
            "[Train] epoch:3158, lr:0.000797, total loss:0.000348\n",
            "[Train] epoch:3159, lr:0.000797, total loss:0.000367\n",
            "[Train] epoch:3160, lr:0.000797, total loss:0.000327\n",
            "[Train] epoch:3161, lr:0.000797, total loss:0.000437\n",
            "[Train] epoch:3162, lr:0.000797, total loss:0.000361\n",
            "[Train] epoch:3163, lr:0.000796, total loss:0.000378\n",
            "[Train] epoch:3164, lr:0.000796, total loss:0.000400\n",
            "[Train] epoch:3165, lr:0.000796, total loss:0.000298\n",
            "[Train] epoch:3166, lr:0.000796, total loss:0.000339\n",
            "[Train] epoch:3167, lr:0.000796, total loss:0.000400\n",
            "[Train] epoch:3168, lr:0.000796, total loss:0.000364\n",
            "[Train] epoch:3169, lr:0.000796, total loss:0.000382\n",
            "[Train] epoch:3170, lr:0.000796, total loss:0.000399\n",
            "[Train] epoch:3171, lr:0.000796, total loss:0.000369\n",
            "[Train] epoch:3172, lr:0.000795, total loss:0.000358\n",
            "[Train] epoch:3173, lr:0.000795, total loss:0.000388\n",
            "[Train] epoch:3174, lr:0.000795, total loss:0.000388\n",
            "[Train] epoch:3175, lr:0.000795, total loss:0.000385\n",
            "[Train] epoch:3176, lr:0.000795, total loss:0.000396\n",
            "[Train] epoch:3177, lr:0.000795, total loss:0.000352\n",
            "[Train] epoch:3178, lr:0.000795, total loss:0.000301\n",
            "[Train] epoch:3179, lr:0.000795, total loss:0.000349\n",
            "[Train] epoch:3180, lr:0.000794, total loss:0.000387\n",
            "[Train] epoch:3181, lr:0.000794, total loss:0.000356\n",
            "[Train] epoch:3182, lr:0.000794, total loss:0.000339\n",
            "[Train] epoch:3183, lr:0.000794, total loss:0.000374\n",
            "[Train] epoch:3184, lr:0.000794, total loss:0.000332\n",
            "[Train] epoch:3185, lr:0.000794, total loss:0.000333\n",
            "[Train] epoch:3186, lr:0.000794, total loss:0.000329\n",
            "[Train] epoch:3187, lr:0.000794, total loss:0.000326\n",
            "[Train] epoch:3188, lr:0.000793, total loss:0.000353\n",
            "[Train] epoch:3189, lr:0.000793, total loss:0.000330\n",
            "[Train] epoch:3190, lr:0.000793, total loss:0.000350\n",
            "[Train] epoch:3191, lr:0.000793, total loss:0.000317\n",
            "[Train] epoch:3192, lr:0.000793, total loss:0.000338\n",
            "[Train] epoch:3193, lr:0.000793, total loss:0.000373\n",
            "[Train] epoch:3194, lr:0.000793, total loss:0.000332\n",
            "[Train] epoch:3195, lr:0.000793, total loss:0.000319\n",
            "[Train] epoch:3196, lr:0.000793, total loss:0.000316\n",
            "[Train] epoch:3197, lr:0.000792, total loss:0.000310\n",
            "[Train] epoch:3198, lr:0.000792, total loss:0.000317\n",
            "[Train] epoch:3199, lr:0.000792, total loss:0.000313\n",
            "[Train] epoch:3200, lr:0.000792, total loss:0.000322\n",
            "[Train] epoch:3201, lr:0.000792, total loss:0.000313\n",
            "[Train] epoch:3202, lr:0.000792, total loss:0.000380\n",
            "[Train] epoch:3203, lr:0.000792, total loss:0.000368\n",
            "[Train] epoch:3204, lr:0.000792, total loss:0.000332\n",
            "[Train] epoch:3205, lr:0.000791, total loss:0.000344\n",
            "[Train] epoch:3206, lr:0.000791, total loss:0.000331\n",
            "[Train] epoch:3207, lr:0.000791, total loss:0.000298\n",
            "[Train] epoch:3208, lr:0.000791, total loss:0.000345\n",
            "[Train] epoch:3209, lr:0.000791, total loss:0.000314\n",
            "[Train] epoch:3210, lr:0.000791, total loss:0.000360\n",
            "[Train] epoch:3211, lr:0.000791, total loss:0.000453\n",
            "[Train] epoch:3212, lr:0.000791, total loss:0.000376\n",
            "[Train] epoch:3213, lr:0.000791, total loss:0.000286\n",
            "[Train] epoch:3214, lr:0.000790, total loss:0.000356\n",
            "[Train] epoch:3215, lr:0.000790, total loss:0.000330\n",
            "[Train] epoch:3216, lr:0.000790, total loss:0.000323\n",
            "[Train] epoch:3217, lr:0.000790, total loss:0.000349\n",
            "[Train] epoch:3218, lr:0.000790, total loss:0.000358\n",
            "[Train] epoch:3219, lr:0.000790, total loss:0.000319\n",
            "[Train] epoch:3220, lr:0.000790, total loss:0.000337\n",
            "[Train] epoch:3221, lr:0.000790, total loss:0.000312\n",
            "[Train] epoch:3222, lr:0.000789, total loss:0.000288\n",
            "[Train] epoch:3223, lr:0.000789, total loss:0.000339\n",
            "[Train] epoch:3224, lr:0.000789, total loss:0.000333\n",
            "[Train] epoch:3225, lr:0.000789, total loss:0.000322\n",
            "[Train] epoch:3226, lr:0.000789, total loss:0.000330\n",
            "[Train] epoch:3227, lr:0.000789, total loss:0.000341\n",
            "[Train] epoch:3228, lr:0.000789, total loss:0.000371\n",
            "[Train] epoch:3229, lr:0.000789, total loss:0.000306\n",
            "[Train] epoch:3230, lr:0.000788, total loss:0.000304\n",
            "[Train] epoch:3231, lr:0.000788, total loss:0.000357\n",
            "[Train] epoch:3232, lr:0.000788, total loss:0.000325\n",
            "[Train] epoch:3233, lr:0.000788, total loss:0.000321\n",
            "[Train] epoch:3234, lr:0.000788, total loss:0.000320\n",
            "[Train] epoch:3235, lr:0.000788, total loss:0.000306\n",
            "[Train] epoch:3236, lr:0.000788, total loss:0.000323\n",
            "[Train] epoch:3237, lr:0.000788, total loss:0.000308\n",
            "[Train] epoch:3238, lr:0.000788, total loss:0.000346\n",
            "[Train] epoch:3239, lr:0.000787, total loss:0.000355\n",
            "[Train] epoch:3240, lr:0.000787, total loss:0.000311\n",
            "[Train] epoch:3241, lr:0.000787, total loss:0.000319\n",
            "[Train] epoch:3242, lr:0.000787, total loss:0.000377\n",
            "[Train] epoch:3243, lr:0.000787, total loss:0.000290\n",
            "[Train] epoch:3244, lr:0.000787, total loss:0.000306\n",
            "[Train] epoch:3245, lr:0.000787, total loss:0.000349\n",
            "[Train] epoch:3246, lr:0.000787, total loss:0.000344\n",
            "[Train] epoch:3247, lr:0.000786, total loss:0.000320\n",
            "[Train] epoch:3248, lr:0.000786, total loss:0.000475\n",
            "[Train] epoch:3249, lr:0.000786, total loss:0.000390\n",
            "[Train] epoch:3250, lr:0.000786, total loss:0.000318\n",
            "[Train] epoch:3251, lr:0.000786, total loss:0.000386\n",
            "[Train] epoch:3252, lr:0.000786, total loss:0.000364\n",
            "[Train] epoch:3253, lr:0.000786, total loss:0.000361\n",
            "[Train] epoch:3254, lr:0.000786, total loss:0.000400\n",
            "[Train] epoch:3255, lr:0.000785, total loss:0.000344\n",
            "[Train] epoch:3256, lr:0.000785, total loss:0.000351\n",
            "[Train] epoch:3257, lr:0.000785, total loss:0.000386\n",
            "[Train] epoch:3258, lr:0.000785, total loss:0.000359\n",
            "[Train] epoch:3259, lr:0.000785, total loss:0.000374\n",
            "[Train] epoch:3260, lr:0.000785, total loss:0.000316\n",
            "[Train] epoch:3261, lr:0.000785, total loss:0.000325\n",
            "[Train] epoch:3262, lr:0.000785, total loss:0.000382\n",
            "[Train] epoch:3263, lr:0.000784, total loss:0.000370\n",
            "[Train] epoch:3264, lr:0.000784, total loss:0.000362\n",
            "[Train] epoch:3265, lr:0.000784, total loss:0.000360\n",
            "[Train] epoch:3266, lr:0.000784, total loss:0.000369\n",
            "[Train] epoch:3267, lr:0.000784, total loss:0.000341\n",
            "[Train] epoch:3268, lr:0.000784, total loss:0.000329\n",
            "[Train] epoch:3269, lr:0.000784, total loss:0.000408\n",
            "[Train] epoch:3270, lr:0.000784, total loss:0.000400\n",
            "[Train] epoch:3271, lr:0.000784, total loss:0.000329\n",
            "[Train] epoch:3272, lr:0.000783, total loss:0.000329\n",
            "[Train] epoch:3273, lr:0.000783, total loss:0.000334\n",
            "[Train] epoch:3274, lr:0.000783, total loss:0.000310\n",
            "[Train] epoch:3275, lr:0.000783, total loss:0.000314\n",
            "[Train] epoch:3276, lr:0.000783, total loss:0.000433\n",
            "[Train] epoch:3277, lr:0.000783, total loss:0.000383\n",
            "[Train] epoch:3278, lr:0.000783, total loss:0.000353\n",
            "[Train] epoch:3279, lr:0.000783, total loss:0.000342\n",
            "[Train] epoch:3280, lr:0.000782, total loss:0.000368\n",
            "[Train] epoch:3281, lr:0.000782, total loss:0.000282\n",
            "[Train] epoch:3282, lr:0.000782, total loss:0.000329\n",
            "[Train] epoch:3283, lr:0.000782, total loss:0.000327\n",
            "[Train] epoch:3284, lr:0.000782, total loss:0.000314\n",
            "[Train] epoch:3285, lr:0.000782, total loss:0.000332\n",
            "[Train] epoch:3286, lr:0.000782, total loss:0.000307\n",
            "[Train] epoch:3287, lr:0.000782, total loss:0.000340\n",
            "[Train] epoch:3288, lr:0.000781, total loss:0.000361\n",
            "[Train] epoch:3289, lr:0.000781, total loss:0.000330\n",
            "[Train] epoch:3290, lr:0.000781, total loss:0.000331\n",
            "[Train] epoch:3291, lr:0.000781, total loss:0.000298\n",
            "[Train] epoch:3292, lr:0.000781, total loss:0.000349\n",
            "[Train] epoch:3293, lr:0.000781, total loss:0.000312\n",
            "[Train] epoch:3294, lr:0.000781, total loss:0.000394\n",
            "[Train] epoch:3295, lr:0.000781, total loss:0.000580\n",
            "[Train] epoch:3296, lr:0.000780, total loss:0.000405\n",
            "[Train] epoch:3297, lr:0.000780, total loss:0.000395\n",
            "[Train] epoch:3298, lr:0.000780, total loss:0.000315\n",
            "[Train] epoch:3299, lr:0.000780, total loss:0.000321\n",
            "[Train] epoch:3300, lr:0.000780, total loss:0.000321\n",
            "[Train] epoch:3301, lr:0.000780, total loss:0.000324\n",
            "[Train] epoch:3302, lr:0.000780, total loss:0.000297\n",
            "[Train] epoch:3303, lr:0.000780, total loss:0.000310\n",
            "[Train] epoch:3304, lr:0.000780, total loss:0.000341\n",
            "[Train] epoch:3305, lr:0.000779, total loss:0.000336\n",
            "[Train] epoch:3306, lr:0.000779, total loss:0.000337\n",
            "[Train] epoch:3307, lr:0.000779, total loss:0.000411\n",
            "[Train] epoch:3308, lr:0.000779, total loss:0.000373\n",
            "[Train] epoch:3309, lr:0.000779, total loss:0.000357\n",
            "[Train] epoch:3310, lr:0.000779, total loss:0.000352\n",
            "[Train] epoch:3311, lr:0.000779, total loss:0.000304\n",
            "[Train] epoch:3312, lr:0.000779, total loss:0.000328\n",
            "[Train] epoch:3313, lr:0.000778, total loss:0.000326\n",
            "[Train] epoch:3314, lr:0.000778, total loss:0.000320\n",
            "[Train] epoch:3315, lr:0.000778, total loss:0.000288\n",
            "[Train] epoch:3316, lr:0.000778, total loss:0.000299\n",
            "[Train] epoch:3317, lr:0.000778, total loss:0.000278\n",
            "[Train] epoch:3318, lr:0.000778, total loss:0.000312\n",
            "[Train] epoch:3319, lr:0.000778, total loss:0.000290\n",
            "[Train] epoch:3320, lr:0.000778, total loss:0.000304\n",
            "[Train] epoch:3321, lr:0.000777, total loss:0.000308\n",
            "[Train] epoch:3322, lr:0.000777, total loss:0.000286\n",
            "[Train] epoch:3323, lr:0.000777, total loss:0.000304\n",
            "[Train] epoch:3324, lr:0.000777, total loss:0.000284\n",
            "[Train] epoch:3325, lr:0.000777, total loss:0.000289\n",
            "[Train] epoch:3326, lr:0.000777, total loss:0.000313\n",
            "[Train] epoch:3327, lr:0.000777, total loss:0.000301\n",
            "[Train] epoch:3328, lr:0.000777, total loss:0.000324\n",
            "[Train] epoch:3329, lr:0.000776, total loss:0.000307\n",
            "[Train] epoch:3330, lr:0.000776, total loss:0.000322\n",
            "[Train] epoch:3331, lr:0.000776, total loss:0.000305\n",
            "[Train] epoch:3332, lr:0.000776, total loss:0.000314\n",
            "[Train] epoch:3333, lr:0.000776, total loss:0.000328\n",
            "[Train] epoch:3334, lr:0.000776, total loss:0.000319\n",
            "[Train] epoch:3335, lr:0.000776, total loss:0.000379\n",
            "[Train] epoch:3336, lr:0.000776, total loss:0.000313\n",
            "[Train] epoch:3337, lr:0.000775, total loss:0.000323\n",
            "[Train] epoch:3338, lr:0.000775, total loss:0.000373\n",
            "[Train] epoch:3339, lr:0.000775, total loss:0.000314\n",
            "[Train] epoch:3340, lr:0.000775, total loss:0.000305\n",
            "[Train] epoch:3341, lr:0.000775, total loss:0.000276\n",
            "[Train] epoch:3342, lr:0.000775, total loss:0.000298\n",
            "[Train] epoch:3343, lr:0.000775, total loss:0.000300\n",
            "[Train] epoch:3344, lr:0.000775, total loss:0.000327\n",
            "[Train] epoch:3345, lr:0.000775, total loss:0.000315\n",
            "[Train] epoch:3346, lr:0.000774, total loss:0.000307\n",
            "[Train] epoch:3347, lr:0.000774, total loss:0.000343\n",
            "[Train] epoch:3348, lr:0.000774, total loss:0.000311\n",
            "[Train] epoch:3349, lr:0.000774, total loss:0.000285\n",
            "[Train] epoch:3350, lr:0.000774, total loss:0.000279\n",
            "[Train] epoch:3351, lr:0.000774, total loss:0.000333\n",
            "[Train] epoch:3352, lr:0.000774, total loss:0.000367\n",
            "[Train] epoch:3353, lr:0.000774, total loss:0.000340\n",
            "[Train] epoch:3354, lr:0.000773, total loss:0.000351\n",
            "[Train] epoch:3355, lr:0.000773, total loss:0.000466\n",
            "[Train] epoch:3356, lr:0.000773, total loss:0.000432\n",
            "[Train] epoch:3357, lr:0.000773, total loss:0.000479\n",
            "[Train] epoch:3358, lr:0.000773, total loss:0.000524\n",
            "[Train] epoch:3359, lr:0.000773, total loss:0.000416\n",
            "[Train] epoch:3360, lr:0.000773, total loss:0.000431\n",
            "[Train] epoch:3361, lr:0.000773, total loss:0.000488\n",
            "[Train] epoch:3362, lr:0.000772, total loss:0.000326\n",
            "[Train] epoch:3363, lr:0.000772, total loss:0.000343\n",
            "[Train] epoch:3364, lr:0.000772, total loss:0.000309\n",
            "[Train] epoch:3365, lr:0.000772, total loss:0.000314\n",
            "[Train] epoch:3366, lr:0.000772, total loss:0.000307\n",
            "[Train] epoch:3367, lr:0.000772, total loss:0.000311\n",
            "[Train] epoch:3368, lr:0.000772, total loss:0.000312\n",
            "[Train] epoch:3369, lr:0.000772, total loss:0.000292\n",
            "[Train] epoch:3370, lr:0.000771, total loss:0.000341\n",
            "[Train] epoch:3371, lr:0.000771, total loss:0.000343\n",
            "[Train] epoch:3372, lr:0.000771, total loss:0.000351\n",
            "[Train] epoch:3373, lr:0.000771, total loss:0.000329\n",
            "[Train] epoch:3374, lr:0.000771, total loss:0.000290\n",
            "[Train] epoch:3375, lr:0.000771, total loss:0.000354\n",
            "[Train] epoch:3376, lr:0.000771, total loss:0.000389\n",
            "[Train] epoch:3377, lr:0.000771, total loss:0.000363\n",
            "[Train] epoch:3378, lr:0.000770, total loss:0.000343\n",
            "[Train] epoch:3379, lr:0.000770, total loss:0.000315\n",
            "[Train] epoch:3380, lr:0.000770, total loss:0.000282\n",
            "[Train] epoch:3381, lr:0.000770, total loss:0.000311\n",
            "[Train] epoch:3382, lr:0.000770, total loss:0.000307\n",
            "[Train] epoch:3383, lr:0.000770, total loss:0.000349\n",
            "[Train] epoch:3384, lr:0.000770, total loss:0.000318\n",
            "[Train] epoch:3385, lr:0.000770, total loss:0.000338\n",
            "[Train] epoch:3386, lr:0.000769, total loss:0.000418\n",
            "[Train] epoch:3387, lr:0.000769, total loss:0.000399\n",
            "[Train] epoch:3388, lr:0.000769, total loss:0.000410\n",
            "[Train] epoch:3389, lr:0.000769, total loss:0.000383\n",
            "[Train] epoch:3390, lr:0.000769, total loss:0.000355\n",
            "[Train] epoch:3391, lr:0.000769, total loss:0.000329\n",
            "[Train] epoch:3392, lr:0.000769, total loss:0.000356\n",
            "[Train] epoch:3393, lr:0.000769, total loss:0.000364\n",
            "[Train] epoch:3394, lr:0.000768, total loss:0.000357\n",
            "[Train] epoch:3395, lr:0.000768, total loss:0.000333\n",
            "[Train] epoch:3396, lr:0.000768, total loss:0.000311\n",
            "[Train] epoch:3397, lr:0.000768, total loss:0.000329\n",
            "[Train] epoch:3398, lr:0.000768, total loss:0.000289\n",
            "[Train] epoch:3399, lr:0.000768, total loss:0.000342\n",
            "[Train] epoch:3400, lr:0.000768, total loss:0.000342\n",
            "[Train] epoch:3401, lr:0.000768, total loss:0.000278\n",
            "[Train] epoch:3402, lr:0.000767, total loss:0.000321\n",
            "[Train] epoch:3403, lr:0.000767, total loss:0.000299\n",
            "[Train] epoch:3404, lr:0.000767, total loss:0.000296\n",
            "[Train] epoch:3405, lr:0.000767, total loss:0.000321\n",
            "[Train] epoch:3406, lr:0.000767, total loss:0.000357\n",
            "[Train] epoch:3407, lr:0.000767, total loss:0.000322\n",
            "[Train] epoch:3408, lr:0.000767, total loss:0.000329\n",
            "[Train] epoch:3409, lr:0.000767, total loss:0.000372\n",
            "[Train] epoch:3410, lr:0.000766, total loss:0.000302\n",
            "[Train] epoch:3411, lr:0.000766, total loss:0.000334\n",
            "[Train] epoch:3412, lr:0.000766, total loss:0.000283\n",
            "[Train] epoch:3413, lr:0.000766, total loss:0.000285\n",
            "[Train] epoch:3414, lr:0.000766, total loss:0.000313\n",
            "[Train] epoch:3415, lr:0.000766, total loss:0.000313\n",
            "[Train] epoch:3416, lr:0.000766, total loss:0.000305\n",
            "[Train] epoch:3417, lr:0.000766, total loss:0.000329\n",
            "[Train] epoch:3418, lr:0.000765, total loss:0.000297\n",
            "[Train] epoch:3419, lr:0.000765, total loss:0.000312\n",
            "[Train] epoch:3420, lr:0.000765, total loss:0.000296\n",
            "[Train] epoch:3421, lr:0.000765, total loss:0.000300\n",
            "[Train] epoch:3422, lr:0.000765, total loss:0.000325\n",
            "[Train] epoch:3423, lr:0.000765, total loss:0.000316\n",
            "[Train] epoch:3424, lr:0.000765, total loss:0.000363\n",
            "[Train] epoch:3425, lr:0.000765, total loss:0.000445\n",
            "[Train] epoch:3426, lr:0.000765, total loss:0.000428\n",
            "[Train] epoch:3427, lr:0.000764, total loss:0.000400\n",
            "[Train] epoch:3428, lr:0.000764, total loss:0.000505\n",
            "[Train] epoch:3429, lr:0.000764, total loss:0.000453\n",
            "[Train] epoch:3430, lr:0.000764, total loss:0.000409\n",
            "[Train] epoch:3431, lr:0.000764, total loss:0.000382\n",
            "[Train] epoch:3432, lr:0.000764, total loss:0.000352\n",
            "[Train] epoch:3433, lr:0.000764, total loss:0.000358\n",
            "[Train] epoch:3434, lr:0.000764, total loss:0.000373\n",
            "[Train] epoch:3435, lr:0.000763, total loss:0.000391\n",
            "[Train] epoch:3436, lr:0.000763, total loss:0.000396\n",
            "[Train] epoch:3437, lr:0.000763, total loss:0.000384\n",
            "[Train] epoch:3438, lr:0.000763, total loss:0.000304\n",
            "[Train] epoch:3439, lr:0.000763, total loss:0.000339\n",
            "[Train] epoch:3440, lr:0.000763, total loss:0.000385\n",
            "[Train] epoch:3441, lr:0.000763, total loss:0.000321\n",
            "[Train] epoch:3442, lr:0.000763, total loss:0.000329\n",
            "[Train] epoch:3443, lr:0.000762, total loss:0.000301\n",
            "[Train] epoch:3444, lr:0.000762, total loss:0.000314\n",
            "[Train] epoch:3445, lr:0.000762, total loss:0.000293\n",
            "[Train] epoch:3446, lr:0.000762, total loss:0.000289\n",
            "[Train] epoch:3447, lr:0.000762, total loss:0.000312\n",
            "[Train] epoch:3448, lr:0.000762, total loss:0.000425\n",
            "[Train] epoch:3449, lr:0.000762, total loss:0.000349\n",
            "[Train] epoch:3450, lr:0.000762, total loss:0.000306\n",
            "[Train] epoch:3451, lr:0.000761, total loss:0.000305\n",
            "[Train] epoch:3452, lr:0.000761, total loss:0.000383\n",
            "[Train] epoch:3453, lr:0.000761, total loss:0.000344\n",
            "[Train] epoch:3454, lr:0.000761, total loss:0.000310\n",
            "[Train] epoch:3455, lr:0.000761, total loss:0.000322\n",
            "[Train] epoch:3456, lr:0.000761, total loss:0.000323\n",
            "[Train] epoch:3457, lr:0.000761, total loss:0.000300\n",
            "[Train] epoch:3458, lr:0.000761, total loss:0.000292\n",
            "[Train] epoch:3459, lr:0.000760, total loss:0.000314\n",
            "[Train] epoch:3460, lr:0.000760, total loss:0.000322\n",
            "[Train] epoch:3461, lr:0.000760, total loss:0.000309\n",
            "[Train] epoch:3462, lr:0.000760, total loss:0.000446\n",
            "[Train] epoch:3463, lr:0.000760, total loss:0.000355\n",
            "[Train] epoch:3464, lr:0.000760, total loss:0.000334\n",
            "[Train] epoch:3465, lr:0.000760, total loss:0.000347\n",
            "[Train] epoch:3466, lr:0.000760, total loss:0.000312\n",
            "[Train] epoch:3467, lr:0.000759, total loss:0.000288\n",
            "[Train] epoch:3468, lr:0.000759, total loss:0.000293\n",
            "[Train] epoch:3469, lr:0.000759, total loss:0.000321\n",
            "[Train] epoch:3470, lr:0.000759, total loss:0.000308\n",
            "[Train] epoch:3471, lr:0.000759, total loss:0.000336\n",
            "[Train] epoch:3472, lr:0.000759, total loss:0.000300\n",
            "[Train] epoch:3473, lr:0.000759, total loss:0.000321\n",
            "[Train] epoch:3474, lr:0.000759, total loss:0.000340\n",
            "[Train] epoch:3475, lr:0.000758, total loss:0.000314\n",
            "[Train] epoch:3476, lr:0.000758, total loss:0.000337\n",
            "[Train] epoch:3477, lr:0.000758, total loss:0.000301\n",
            "[Train] epoch:3478, lr:0.000758, total loss:0.000343\n",
            "[Train] epoch:3479, lr:0.000758, total loss:0.000319\n",
            "[Train] epoch:3480, lr:0.000758, total loss:0.000334\n",
            "[Train] epoch:3481, lr:0.000758, total loss:0.000371\n",
            "[Train] epoch:3482, lr:0.000758, total loss:0.000321\n",
            "[Train] epoch:3483, lr:0.000757, total loss:0.000328\n",
            "[Train] epoch:3484, lr:0.000757, total loss:0.000298\n",
            "[Train] epoch:3485, lr:0.000757, total loss:0.000327\n",
            "[Train] epoch:3486, lr:0.000757, total loss:0.000287\n",
            "[Train] epoch:3487, lr:0.000757, total loss:0.000322\n",
            "[Train] epoch:3488, lr:0.000757, total loss:0.000304\n",
            "[Train] epoch:3489, lr:0.000757, total loss:0.000296\n",
            "[Train] epoch:3490, lr:0.000756, total loss:0.000361\n",
            "[Train] epoch:3491, lr:0.000756, total loss:0.000328\n",
            "[Train] epoch:3492, lr:0.000756, total loss:0.000372\n",
            "[Train] epoch:3493, lr:0.000756, total loss:0.000332\n",
            "[Train] epoch:3494, lr:0.000756, total loss:0.000305\n",
            "[Train] epoch:3495, lr:0.000756, total loss:0.000325\n",
            "[Train] epoch:3496, lr:0.000756, total loss:0.000296\n",
            "[Train] epoch:3497, lr:0.000756, total loss:0.000295\n",
            "[Train] epoch:3498, lr:0.000755, total loss:0.000370\n",
            "[Train] epoch:3499, lr:0.000755, total loss:0.000275\n",
            "[Train] epoch:3500, lr:0.000755, total loss:0.000295\n",
            "[Train] epoch:3501, lr:0.000755, total loss:0.000300\n",
            "[Train] epoch:3502, lr:0.000755, total loss:0.000418\n",
            "[Train] epoch:3503, lr:0.000755, total loss:0.000320\n",
            "[Train] epoch:3504, lr:0.000755, total loss:0.000316\n",
            "[Train] epoch:3505, lr:0.000755, total loss:0.000312\n",
            "[Train] epoch:3506, lr:0.000754, total loss:0.000304\n",
            "[Train] epoch:3507, lr:0.000754, total loss:0.000274\n",
            "[Train] epoch:3508, lr:0.000754, total loss:0.000314\n",
            "[Train] epoch:3509, lr:0.000754, total loss:0.000367\n",
            "[Train] epoch:3510, lr:0.000754, total loss:0.000307\n",
            "[Train] epoch:3511, lr:0.000754, total loss:0.000305\n",
            "[Train] epoch:3512, lr:0.000754, total loss:0.000328\n",
            "[Train] epoch:3513, lr:0.000754, total loss:0.000315\n",
            "[Train] epoch:3514, lr:0.000753, total loss:0.000307\n",
            "[Train] epoch:3515, lr:0.000753, total loss:0.000325\n",
            "[Train] epoch:3516, lr:0.000753, total loss:0.000307\n",
            "[Train] epoch:3517, lr:0.000753, total loss:0.000303\n",
            "[Train] epoch:3518, lr:0.000753, total loss:0.000296\n",
            "[Train] epoch:3519, lr:0.000753, total loss:0.000317\n",
            "[Train] epoch:3520, lr:0.000753, total loss:0.000297\n",
            "[Train] epoch:3521, lr:0.000753, total loss:0.000342\n",
            "[Train] epoch:3522, lr:0.000752, total loss:0.000376\n",
            "[Train] epoch:3523, lr:0.000752, total loss:0.000296\n",
            "[Train] epoch:3524, lr:0.000752, total loss:0.000312\n",
            "[Train] epoch:3525, lr:0.000752, total loss:0.000286\n",
            "[Train] epoch:3526, lr:0.000752, total loss:0.000284\n",
            "[Train] epoch:3527, lr:0.000752, total loss:0.000296\n",
            "[Train] epoch:3528, lr:0.000752, total loss:0.000294\n",
            "[Train] epoch:3529, lr:0.000752, total loss:0.000321\n",
            "[Train] epoch:3530, lr:0.000751, total loss:0.000291\n",
            "[Train] epoch:3531, lr:0.000751, total loss:0.000331\n",
            "[Train] epoch:3532, lr:0.000751, total loss:0.000290\n",
            "[Train] epoch:3533, lr:0.000751, total loss:0.000287\n",
            "[Train] epoch:3534, lr:0.000751, total loss:0.000330\n",
            "[Train] epoch:3535, lr:0.000751, total loss:0.000323\n",
            "[Train] epoch:3536, lr:0.000751, total loss:0.000380\n",
            "[Train] epoch:3537, lr:0.000751, total loss:0.000277\n",
            "[Train] epoch:3538, lr:0.000750, total loss:0.000284\n",
            "[Train] epoch:3539, lr:0.000750, total loss:0.000356\n",
            "[Train] epoch:3540, lr:0.000750, total loss:0.000335\n",
            "[Train] epoch:3541, lr:0.000750, total loss:0.000347\n",
            "[Train] epoch:3542, lr:0.000750, total loss:0.000345\n",
            "[Train] epoch:3543, lr:0.000750, total loss:0.000361\n",
            "[Train] epoch:3544, lr:0.000750, total loss:0.000295\n",
            "[Train] epoch:3545, lr:0.000750, total loss:0.000303\n",
            "[Train] epoch:3546, lr:0.000749, total loss:0.000326\n",
            "[Train] epoch:3547, lr:0.000749, total loss:0.000297\n",
            "[Train] epoch:3548, lr:0.000749, total loss:0.000317\n",
            "[Train] epoch:3549, lr:0.000749, total loss:0.000295\n",
            "[Train] epoch:3550, lr:0.000749, total loss:0.000341\n",
            "[Train] epoch:3551, lr:0.000749, total loss:0.000410\n",
            "[Train] epoch:3552, lr:0.000749, total loss:0.000471\n",
            "[Train] epoch:3553, lr:0.000749, total loss:0.000379\n",
            "[Train] epoch:3554, lr:0.000748, total loss:0.000352\n",
            "[Train] epoch:3555, lr:0.000748, total loss:0.000352\n",
            "[Train] epoch:3556, lr:0.000748, total loss:0.000349\n",
            "[Train] epoch:3557, lr:0.000748, total loss:0.000329\n",
            "[Train] epoch:3558, lr:0.000748, total loss:0.000316\n",
            "[Train] epoch:3559, lr:0.000748, total loss:0.000325\n",
            "[Train] epoch:3560, lr:0.000748, total loss:0.000294\n",
            "[Train] epoch:3561, lr:0.000748, total loss:0.000379\n",
            "[Train] epoch:3562, lr:0.000747, total loss:0.000341\n",
            "[Train] epoch:3563, lr:0.000747, total loss:0.000396\n",
            "[Train] epoch:3564, lr:0.000747, total loss:0.000338\n",
            "[Train] epoch:3565, lr:0.000747, total loss:0.000358\n",
            "[Train] epoch:3566, lr:0.000747, total loss:0.000305\n",
            "[Train] epoch:3567, lr:0.000747, total loss:0.000360\n",
            "[Train] epoch:3568, lr:0.000747, total loss:0.000357\n",
            "[Train] epoch:3569, lr:0.000747, total loss:0.000340\n",
            "[Train] epoch:3570, lr:0.000746, total loss:0.000310\n",
            "[Train] epoch:3571, lr:0.000746, total loss:0.000293\n",
            "[Train] epoch:3572, lr:0.000746, total loss:0.000279\n",
            "[Train] epoch:3573, lr:0.000746, total loss:0.000321\n",
            "[Train] epoch:3574, lr:0.000746, total loss:0.000317\n",
            "[Train] epoch:3575, lr:0.000746, total loss:0.000306\n",
            "[Train] epoch:3576, lr:0.000746, total loss:0.000307\n",
            "[Train] epoch:3577, lr:0.000745, total loss:0.000305\n",
            "[Train] epoch:3578, lr:0.000745, total loss:0.000324\n",
            "[Train] epoch:3579, lr:0.000745, total loss:0.000300\n",
            "[Train] epoch:3580, lr:0.000745, total loss:0.000296\n",
            "[Train] epoch:3581, lr:0.000745, total loss:0.000316\n",
            "[Train] epoch:3582, lr:0.000745, total loss:0.000305\n",
            "[Train] epoch:3583, lr:0.000745, total loss:0.000337\n",
            "[Train] epoch:3584, lr:0.000745, total loss:0.000299\n",
            "[Train] epoch:3585, lr:0.000744, total loss:0.000290\n",
            "[Train] epoch:3586, lr:0.000744, total loss:0.000296\n",
            "[Train] epoch:3587, lr:0.000744, total loss:0.000318\n",
            "[Train] epoch:3588, lr:0.000744, total loss:0.000308\n",
            "[Train] epoch:3589, lr:0.000744, total loss:0.000292\n",
            "[Train] epoch:3590, lr:0.000744, total loss:0.000273\n",
            "[Train] epoch:3591, lr:0.000744, total loss:0.000339\n",
            "[Train] epoch:3592, lr:0.000744, total loss:0.000307\n",
            "[Train] epoch:3593, lr:0.000743, total loss:0.000334\n",
            "[Train] epoch:3594, lr:0.000743, total loss:0.000502\n",
            "[Train] epoch:3595, lr:0.000743, total loss:0.000368\n",
            "[Train] epoch:3596, lr:0.000743, total loss:0.000435\n",
            "[Train] epoch:3597, lr:0.000743, total loss:0.000362\n",
            "[Train] epoch:3598, lr:0.000743, total loss:0.000341\n",
            "[Train] epoch:3599, lr:0.000743, total loss:0.000313\n",
            "[Train] epoch:3600, lr:0.000743, total loss:0.000340\n",
            "[Train] epoch:3601, lr:0.000742, total loss:0.000346\n",
            "[Train] epoch:3602, lr:0.000742, total loss:0.000316\n",
            "[Train] epoch:3603, lr:0.000742, total loss:0.000291\n",
            "[Train] epoch:3604, lr:0.000742, total loss:0.000347\n",
            "[Train] epoch:3605, lr:0.000742, total loss:0.000308\n",
            "[Train] epoch:3606, lr:0.000742, total loss:0.000338\n",
            "[Train] epoch:3607, lr:0.000742, total loss:0.000311\n",
            "[Train] epoch:3608, lr:0.000742, total loss:0.000327\n",
            "[Train] epoch:3609, lr:0.000741, total loss:0.000348\n",
            "[Train] epoch:3610, lr:0.000741, total loss:0.000337\n",
            "[Train] epoch:3611, lr:0.000741, total loss:0.000311\n",
            "[Train] epoch:3612, lr:0.000741, total loss:0.000284\n",
            "[Train] epoch:3613, lr:0.000741, total loss:0.000311\n",
            "[Train] epoch:3614, lr:0.000741, total loss:0.000311\n",
            "[Train] epoch:3615, lr:0.000741, total loss:0.000356\n",
            "[Train] epoch:3616, lr:0.000740, total loss:0.000353\n",
            "[Train] epoch:3617, lr:0.000740, total loss:0.000368\n",
            "[Train] epoch:3618, lr:0.000740, total loss:0.000401\n",
            "[Train] epoch:3619, lr:0.000740, total loss:0.000411\n",
            "[Train] epoch:3620, lr:0.000740, total loss:0.000370\n",
            "[Train] epoch:3621, lr:0.000740, total loss:0.000396\n",
            "[Train] epoch:3622, lr:0.000740, total loss:0.000347\n",
            "[Train] epoch:3623, lr:0.000740, total loss:0.000321\n",
            "[Train] epoch:3624, lr:0.000739, total loss:0.000301\n",
            "[Train] epoch:3625, lr:0.000739, total loss:0.000319\n",
            "[Train] epoch:3626, lr:0.000739, total loss:0.000296\n",
            "[Train] epoch:3627, lr:0.000739, total loss:0.000324\n",
            "[Train] epoch:3628, lr:0.000739, total loss:0.000359\n",
            "[Train] epoch:3629, lr:0.000739, total loss:0.000335\n",
            "[Train] epoch:3630, lr:0.000739, total loss:0.000325\n",
            "[Train] epoch:3631, lr:0.000739, total loss:0.000286\n",
            "[Train] epoch:3632, lr:0.000738, total loss:0.000287\n",
            "[Train] epoch:3633, lr:0.000738, total loss:0.000297\n",
            "[Train] epoch:3634, lr:0.000738, total loss:0.000283\n",
            "[Train] epoch:3635, lr:0.000738, total loss:0.000327\n",
            "[Train] epoch:3636, lr:0.000738, total loss:0.000349\n",
            "[Train] epoch:3637, lr:0.000738, total loss:0.000277\n",
            "[Train] epoch:3638, lr:0.000738, total loss:0.000285\n",
            "[Train] epoch:3639, lr:0.000738, total loss:0.000346\n",
            "[Train] epoch:3640, lr:0.000737, total loss:0.000331\n",
            "[Train] epoch:3641, lr:0.000737, total loss:0.000337\n",
            "[Train] epoch:3642, lr:0.000737, total loss:0.000299\n",
            "[Train] epoch:3643, lr:0.000737, total loss:0.000295\n",
            "[Train] epoch:3644, lr:0.000737, total loss:0.000283\n",
            "[Train] epoch:3645, lr:0.000737, total loss:0.000296\n",
            "[Train] epoch:3646, lr:0.000737, total loss:0.000271\n",
            "[Train] epoch:3647, lr:0.000737, total loss:0.000304\n",
            "[Train] epoch:3648, lr:0.000736, total loss:0.000295\n",
            "[Train] epoch:3649, lr:0.000736, total loss:0.000290\n",
            "[Train] epoch:3650, lr:0.000736, total loss:0.000304\n",
            "[Train] epoch:3651, lr:0.000736, total loss:0.000324\n",
            "[Train] epoch:3652, lr:0.000736, total loss:0.000289\n",
            "[Train] epoch:3653, lr:0.000736, total loss:0.000288\n",
            "[Train] epoch:3654, lr:0.000736, total loss:0.000291\n",
            "[Train] epoch:3655, lr:0.000735, total loss:0.000309\n",
            "[Train] epoch:3656, lr:0.000735, total loss:0.000311\n",
            "[Train] epoch:3657, lr:0.000735, total loss:0.000375\n",
            "[Train] epoch:3658, lr:0.000735, total loss:0.000390\n",
            "[Train] epoch:3659, lr:0.000735, total loss:0.000414\n",
            "[Train] epoch:3660, lr:0.000735, total loss:0.000400\n",
            "[Train] epoch:3661, lr:0.000735, total loss:0.000364\n",
            "[Train] epoch:3662, lr:0.000735, total loss:0.000324\n",
            "[Train] epoch:3663, lr:0.000734, total loss:0.000289\n",
            "[Train] epoch:3664, lr:0.000734, total loss:0.000290\n",
            "[Train] epoch:3665, lr:0.000734, total loss:0.000298\n",
            "[Train] epoch:3666, lr:0.000734, total loss:0.000290\n",
            "[Train] epoch:3667, lr:0.000734, total loss:0.000323\n",
            "[Train] epoch:3668, lr:0.000734, total loss:0.000341\n",
            "[Train] epoch:3669, lr:0.000734, total loss:0.000334\n",
            "[Train] epoch:3670, lr:0.000734, total loss:0.000311\n",
            "[Train] epoch:3671, lr:0.000733, total loss:0.000301\n",
            "[Train] epoch:3672, lr:0.000733, total loss:0.000308\n",
            "[Train] epoch:3673, lr:0.000733, total loss:0.000303\n",
            "[Train] epoch:3674, lr:0.000733, total loss:0.000314\n",
            "[Train] epoch:3675, lr:0.000733, total loss:0.000332\n",
            "[Train] epoch:3676, lr:0.000733, total loss:0.000288\n",
            "[Train] epoch:3677, lr:0.000733, total loss:0.000290\n",
            "[Train] epoch:3678, lr:0.000733, total loss:0.000329\n",
            "[Train] epoch:3679, lr:0.000732, total loss:0.000304\n",
            "[Train] epoch:3680, lr:0.000732, total loss:0.000311\n",
            "[Train] epoch:3681, lr:0.000732, total loss:0.000433\n",
            "[Train] epoch:3682, lr:0.000732, total loss:0.000370\n",
            "[Train] epoch:3683, lr:0.000732, total loss:0.000319\n",
            "[Train] epoch:3684, lr:0.000732, total loss:0.000310\n",
            "[Train] epoch:3685, lr:0.000732, total loss:0.000308\n",
            "[Train] epoch:3686, lr:0.000731, total loss:0.000298\n",
            "[Train] epoch:3687, lr:0.000731, total loss:0.000314\n",
            "[Train] epoch:3688, lr:0.000731, total loss:0.000310\n",
            "[Train] epoch:3689, lr:0.000731, total loss:0.000304\n",
            "[Train] epoch:3690, lr:0.000731, total loss:0.000309\n",
            "[Train] epoch:3691, lr:0.000731, total loss:0.000296\n",
            "[Train] epoch:3692, lr:0.000731, total loss:0.000281\n",
            "[Train] epoch:3693, lr:0.000731, total loss:0.000297\n",
            "[Train] epoch:3694, lr:0.000730, total loss:0.000337\n",
            "[Train] epoch:3695, lr:0.000730, total loss:0.000301\n",
            "[Train] epoch:3696, lr:0.000730, total loss:0.000272\n",
            "[Train] epoch:3697, lr:0.000730, total loss:0.000344\n",
            "[Train] epoch:3698, lr:0.000730, total loss:0.000293\n",
            "[Train] epoch:3699, lr:0.000730, total loss:0.000315\n",
            "[Train] epoch:3700, lr:0.000730, total loss:0.000306\n",
            "[Train] epoch:3701, lr:0.000730, total loss:0.000288\n",
            "[Train] epoch:3702, lr:0.000729, total loss:0.000315\n",
            "[Train] epoch:3703, lr:0.000729, total loss:0.000301\n",
            "[Train] epoch:3704, lr:0.000729, total loss:0.000280\n",
            "[Train] epoch:3705, lr:0.000729, total loss:0.000346\n",
            "[Train] epoch:3706, lr:0.000729, total loss:0.000332\n",
            "[Train] epoch:3707, lr:0.000729, total loss:0.000316\n",
            "[Train] epoch:3708, lr:0.000729, total loss:0.000310\n",
            "[Train] epoch:3709, lr:0.000728, total loss:0.000317\n",
            "[Train] epoch:3710, lr:0.000728, total loss:0.000308\n",
            "[Train] epoch:3711, lr:0.000728, total loss:0.000286\n",
            "[Train] epoch:3712, lr:0.000728, total loss:0.000274\n",
            "[Train] epoch:3713, lr:0.000728, total loss:0.000284\n",
            "[Train] epoch:3714, lr:0.000728, total loss:0.000297\n",
            "[Train] epoch:3715, lr:0.000728, total loss:0.000288\n",
            "[Train] epoch:3716, lr:0.000728, total loss:0.000283\n",
            "[Train] epoch:3717, lr:0.000727, total loss:0.000322\n",
            "[Train] epoch:3718, lr:0.000727, total loss:0.000294\n",
            "[Train] epoch:3719, lr:0.000727, total loss:0.000297\n",
            "[Train] epoch:3720, lr:0.000727, total loss:0.000360\n",
            "[Train] epoch:3721, lr:0.000727, total loss:0.000310\n",
            "[Train] epoch:3722, lr:0.000727, total loss:0.000297\n",
            "[Train] epoch:3723, lr:0.000727, total loss:0.000309\n",
            "[Train] epoch:3724, lr:0.000727, total loss:0.000346\n",
            "[Train] epoch:3725, lr:0.000726, total loss:0.000334\n",
            "[Train] epoch:3726, lr:0.000726, total loss:0.000327\n",
            "[Train] epoch:3727, lr:0.000726, total loss:0.000304\n",
            "[Train] epoch:3728, lr:0.000726, total loss:0.000335\n",
            "[Train] epoch:3729, lr:0.000726, total loss:0.000311\n",
            "[Train] epoch:3730, lr:0.000726, total loss:0.000441\n",
            "[Train] epoch:3731, lr:0.000726, total loss:0.000475\n",
            "[Train] epoch:3732, lr:0.000726, total loss:0.000484\n",
            "[Train] epoch:3733, lr:0.000725, total loss:0.000437\n",
            "[Train] epoch:3734, lr:0.000725, total loss:0.000419\n",
            "[Train] epoch:3735, lr:0.000725, total loss:0.000406\n",
            "[Train] epoch:3736, lr:0.000725, total loss:0.000369\n",
            "[Train] epoch:3737, lr:0.000725, total loss:0.000337\n",
            "[Train] epoch:3738, lr:0.000725, total loss:0.000313\n",
            "[Train] epoch:3739, lr:0.000725, total loss:0.000312\n",
            "[Train] epoch:3740, lr:0.000724, total loss:0.000285\n",
            "[Train] epoch:3741, lr:0.000724, total loss:0.000294\n",
            "[Train] epoch:3742, lr:0.000724, total loss:0.000303\n",
            "[Train] epoch:3743, lr:0.000724, total loss:0.000281\n",
            "[Train] epoch:3744, lr:0.000724, total loss:0.000300\n",
            "[Train] epoch:3745, lr:0.000724, total loss:0.000318\n",
            "[Train] epoch:3746, lr:0.000724, total loss:0.000354\n",
            "[Train] epoch:3747, lr:0.000724, total loss:0.000311\n",
            "[Train] epoch:3748, lr:0.000723, total loss:0.000336\n",
            "[Train] epoch:3749, lr:0.000723, total loss:0.000294\n",
            "[Train] epoch:3750, lr:0.000723, total loss:0.000323\n",
            "[Train] epoch:3751, lr:0.000723, total loss:0.000267\n",
            "[Train] epoch:3752, lr:0.000723, total loss:0.000350\n",
            "[Train] epoch:3753, lr:0.000723, total loss:0.000307\n",
            "[Train] epoch:3754, lr:0.000723, total loss:0.000293\n",
            "[Train] epoch:3755, lr:0.000723, total loss:0.000326\n",
            "[Train] epoch:3756, lr:0.000722, total loss:0.000313\n",
            "[Train] epoch:3757, lr:0.000722, total loss:0.000289\n",
            "[Train] epoch:3758, lr:0.000722, total loss:0.000428\n",
            "[Train] epoch:3759, lr:0.000722, total loss:0.000331\n",
            "[Train] epoch:3760, lr:0.000722, total loss:0.000361\n",
            "[Train] epoch:3761, lr:0.000722, total loss:0.000325\n",
            "[Train] epoch:3762, lr:0.000722, total loss:0.000315\n",
            "[Train] epoch:3763, lr:0.000721, total loss:0.000290\n",
            "[Train] epoch:3764, lr:0.000721, total loss:0.000313\n",
            "[Train] epoch:3765, lr:0.000721, total loss:0.000303\n",
            "[Train] epoch:3766, lr:0.000721, total loss:0.000285\n",
            "[Train] epoch:3767, lr:0.000721, total loss:0.000269\n",
            "[Train] epoch:3768, lr:0.000721, total loss:0.000274\n",
            "[Train] epoch:3769, lr:0.000721, total loss:0.000307\n",
            "[Train] epoch:3770, lr:0.000721, total loss:0.000313\n",
            "[Train] epoch:3771, lr:0.000720, total loss:0.000307\n",
            "[Train] epoch:3772, lr:0.000720, total loss:0.000272\n",
            "[Train] epoch:3773, lr:0.000720, total loss:0.000294\n",
            "[Train] epoch:3774, lr:0.000720, total loss:0.000298\n",
            "[Train] epoch:3775, lr:0.000720, total loss:0.000299\n",
            "[Train] epoch:3776, lr:0.000720, total loss:0.000279\n",
            "[Train] epoch:3777, lr:0.000720, total loss:0.000312\n",
            "[Train] epoch:3778, lr:0.000719, total loss:0.000266\n",
            "[Train] epoch:3779, lr:0.000719, total loss:0.000277\n",
            "[Train] epoch:3780, lr:0.000719, total loss:0.000291\n",
            "[Train] epoch:3781, lr:0.000719, total loss:0.000286\n",
            "[Train] epoch:3782, lr:0.000719, total loss:0.000290\n",
            "[Train] epoch:3783, lr:0.000719, total loss:0.000308\n",
            "[Train] epoch:3784, lr:0.000719, total loss:0.000249\n",
            "[Train] epoch:3785, lr:0.000719, total loss:0.000261\n",
            "[Train] epoch:3786, lr:0.000718, total loss:0.000320\n",
            "[Train] epoch:3787, lr:0.000718, total loss:0.000277\n",
            "[Train] epoch:3788, lr:0.000718, total loss:0.000296\n",
            "[Train] epoch:3789, lr:0.000718, total loss:0.000342\n",
            "[Train] epoch:3790, lr:0.000718, total loss:0.000292\n",
            "[Train] epoch:3791, lr:0.000718, total loss:0.000329\n",
            "[Train] epoch:3792, lr:0.000718, total loss:0.000346\n",
            "[Train] epoch:3793, lr:0.000718, total loss:0.000293\n",
            "[Train] epoch:3794, lr:0.000717, total loss:0.000338\n",
            "[Train] epoch:3795, lr:0.000717, total loss:0.000278\n",
            "[Train] epoch:3796, lr:0.000717, total loss:0.000298\n",
            "[Train] epoch:3797, lr:0.000717, total loss:0.000279\n",
            "[Train] epoch:3798, lr:0.000717, total loss:0.000291\n",
            "[Train] epoch:3799, lr:0.000717, total loss:0.000340\n",
            "[Train] epoch:3800, lr:0.000717, total loss:0.000400\n",
            "[Train] epoch:3801, lr:0.000716, total loss:0.000359\n",
            "[Train] epoch:3802, lr:0.000716, total loss:0.000335\n",
            "[Train] epoch:3803, lr:0.000716, total loss:0.000321\n",
            "[Train] epoch:3804, lr:0.000716, total loss:0.000324\n",
            "[Train] epoch:3805, lr:0.000716, total loss:0.000326\n",
            "[Train] epoch:3806, lr:0.000716, total loss:0.000289\n",
            "[Train] epoch:3807, lr:0.000716, total loss:0.000305\n",
            "[Train] epoch:3808, lr:0.000716, total loss:0.000337\n",
            "[Train] epoch:3809, lr:0.000715, total loss:0.000302\n",
            "[Train] epoch:3810, lr:0.000715, total loss:0.000297\n",
            "[Train] epoch:3811, lr:0.000715, total loss:0.000321\n",
            "[Train] epoch:3812, lr:0.000715, total loss:0.000287\n",
            "[Train] epoch:3813, lr:0.000715, total loss:0.000286\n",
            "[Train] epoch:3814, lr:0.000715, total loss:0.000296\n",
            "[Train] epoch:3815, lr:0.000715, total loss:0.000394\n",
            "[Train] epoch:3816, lr:0.000714, total loss:0.000309\n",
            "[Train] epoch:3817, lr:0.000714, total loss:0.000372\n",
            "[Train] epoch:3818, lr:0.000714, total loss:0.000360\n",
            "[Train] epoch:3819, lr:0.000714, total loss:0.000324\n",
            "[Train] epoch:3820, lr:0.000714, total loss:0.000323\n",
            "[Train] epoch:3821, lr:0.000714, total loss:0.000307\n",
            "[Train] epoch:3822, lr:0.000714, total loss:0.000308\n",
            "[Train] epoch:3823, lr:0.000714, total loss:0.000299\n",
            "[Train] epoch:3824, lr:0.000713, total loss:0.000327\n",
            "[Train] epoch:3825, lr:0.000713, total loss:0.000303\n",
            "[Train] epoch:3826, lr:0.000713, total loss:0.000327\n",
            "[Train] epoch:3827, lr:0.000713, total loss:0.000345\n",
            "[Train] epoch:3828, lr:0.000713, total loss:0.000328\n",
            "[Train] epoch:3829, lr:0.000713, total loss:0.000307\n",
            "[Train] epoch:3830, lr:0.000713, total loss:0.000302\n",
            "[Train] epoch:3831, lr:0.000713, total loss:0.000301\n",
            "[Train] epoch:3832, lr:0.000712, total loss:0.000314\n",
            "[Train] epoch:3833, lr:0.000712, total loss:0.000281\n",
            "[Train] epoch:3834, lr:0.000712, total loss:0.000302\n",
            "[Train] epoch:3835, lr:0.000712, total loss:0.000318\n",
            "[Train] epoch:3836, lr:0.000712, total loss:0.000330\n",
            "[Train] epoch:3837, lr:0.000712, total loss:0.000317\n",
            "[Train] epoch:3838, lr:0.000712, total loss:0.000300\n",
            "[Train] epoch:3839, lr:0.000711, total loss:0.000296\n",
            "[Train] epoch:3840, lr:0.000711, total loss:0.000325\n",
            "[Train] epoch:3841, lr:0.000711, total loss:0.000330\n",
            "[Train] epoch:3842, lr:0.000711, total loss:0.000261\n",
            "[Train] epoch:3843, lr:0.000711, total loss:0.000293\n",
            "[Train] epoch:3844, lr:0.000711, total loss:0.000294\n",
            "[Train] epoch:3845, lr:0.000711, total loss:0.000311\n",
            "[Train] epoch:3846, lr:0.000711, total loss:0.000315\n",
            "[Train] epoch:3847, lr:0.000710, total loss:0.000297\n",
            "[Train] epoch:3848, lr:0.000710, total loss:0.000333\n",
            "[Train] epoch:3849, lr:0.000710, total loss:0.000332\n",
            "[Train] epoch:3850, lr:0.000710, total loss:0.000294\n",
            "[Train] epoch:3851, lr:0.000710, total loss:0.000368\n",
            "[Train] epoch:3852, lr:0.000710, total loss:0.000320\n",
            "[Train] epoch:3853, lr:0.000710, total loss:0.000298\n",
            "[Train] epoch:3854, lr:0.000709, total loss:0.000325\n",
            "[Train] epoch:3855, lr:0.000709, total loss:0.000291\n",
            "[Train] epoch:3856, lr:0.000709, total loss:0.000301\n",
            "[Train] epoch:3857, lr:0.000709, total loss:0.000300\n",
            "[Train] epoch:3858, lr:0.000709, total loss:0.000289\n",
            "[Train] epoch:3859, lr:0.000709, total loss:0.000303\n",
            "[Train] epoch:3860, lr:0.000709, total loss:0.000298\n",
            "[Train] epoch:3861, lr:0.000709, total loss:0.000280\n",
            "[Train] epoch:3862, lr:0.000708, total loss:0.000293\n",
            "[Train] epoch:3863, lr:0.000708, total loss:0.000301\n",
            "[Train] epoch:3864, lr:0.000708, total loss:0.000296\n",
            "[Train] epoch:3865, lr:0.000708, total loss:0.000269\n",
            "[Train] epoch:3866, lr:0.000708, total loss:0.000283\n",
            "[Train] epoch:3867, lr:0.000708, total loss:0.000274\n",
            "[Train] epoch:3868, lr:0.000708, total loss:0.000284\n",
            "[Train] epoch:3869, lr:0.000707, total loss:0.000290\n",
            "[Train] epoch:3870, lr:0.000707, total loss:0.000311\n",
            "[Train] epoch:3871, lr:0.000707, total loss:0.000302\n",
            "[Train] epoch:3872, lr:0.000707, total loss:0.000288\n",
            "[Train] epoch:3873, lr:0.000707, total loss:0.000259\n",
            "[Train] epoch:3874, lr:0.000707, total loss:0.000269\n",
            "[Train] epoch:3875, lr:0.000707, total loss:0.000305\n",
            "[Train] epoch:3876, lr:0.000707, total loss:0.000262\n",
            "[Train] epoch:3877, lr:0.000706, total loss:0.000284\n",
            "[Train] epoch:3878, lr:0.000706, total loss:0.000307\n",
            "[Train] epoch:3879, lr:0.000706, total loss:0.000303\n",
            "[Train] epoch:3880, lr:0.000706, total loss:0.000265\n",
            "[Train] epoch:3881, lr:0.000706, total loss:0.000302\n",
            "[Train] epoch:3882, lr:0.000706, total loss:0.000308\n",
            "[Train] epoch:3883, lr:0.000706, total loss:0.000310\n",
            "[Train] epoch:3884, lr:0.000706, total loss:0.000289\n",
            "[Train] epoch:3885, lr:0.000705, total loss:0.000283\n",
            "[Train] epoch:3886, lr:0.000705, total loss:0.000264\n",
            "[Train] epoch:3887, lr:0.000705, total loss:0.000286\n",
            "[Train] epoch:3888, lr:0.000705, total loss:0.000291\n",
            "[Train] epoch:3889, lr:0.000705, total loss:0.000269\n",
            "[Train] epoch:3890, lr:0.000705, total loss:0.000296\n",
            "[Train] epoch:3891, lr:0.000705, total loss:0.000269\n",
            "[Train] epoch:3892, lr:0.000704, total loss:0.000381\n",
            "[Train] epoch:3893, lr:0.000704, total loss:0.000373\n",
            "[Train] epoch:3894, lr:0.000704, total loss:0.000412\n",
            "[Train] epoch:3895, lr:0.000704, total loss:0.000398\n",
            "[Train] epoch:3896, lr:0.000704, total loss:0.000365\n",
            "[Train] epoch:3897, lr:0.000704, total loss:0.000368\n",
            "[Train] epoch:3898, lr:0.000704, total loss:0.000322\n",
            "[Train] epoch:3899, lr:0.000704, total loss:0.000337\n",
            "[Train] epoch:3900, lr:0.000703, total loss:0.000325\n",
            "[Train] epoch:3901, lr:0.000703, total loss:0.000326\n",
            "[Train] epoch:3902, lr:0.000703, total loss:0.000329\n",
            "[Train] epoch:3903, lr:0.000703, total loss:0.000285\n",
            "[Train] epoch:3904, lr:0.000703, total loss:0.000283\n",
            "[Train] epoch:3905, lr:0.000703, total loss:0.000290\n",
            "[Train] epoch:3906, lr:0.000703, total loss:0.000298\n",
            "[Train] epoch:3907, lr:0.000702, total loss:0.000290\n",
            "[Train] epoch:3908, lr:0.000702, total loss:0.000302\n",
            "[Train] epoch:3909, lr:0.000702, total loss:0.000348\n",
            "[Train] epoch:3910, lr:0.000702, total loss:0.000335\n",
            "[Train] epoch:3911, lr:0.000702, total loss:0.000331\n",
            "[Train] epoch:3912, lr:0.000702, total loss:0.000358\n",
            "[Train] epoch:3913, lr:0.000702, total loss:0.000385\n",
            "[Train] epoch:3914, lr:0.000702, total loss:0.000367\n",
            "[Train] epoch:3915, lr:0.000701, total loss:0.000325\n",
            "[Train] epoch:3916, lr:0.000701, total loss:0.000306\n",
            "[Train] epoch:3917, lr:0.000701, total loss:0.000286\n",
            "[Train] epoch:3918, lr:0.000701, total loss:0.000293\n",
            "[Train] epoch:3919, lr:0.000701, total loss:0.000295\n",
            "[Train] epoch:3920, lr:0.000701, total loss:0.000306\n",
            "[Train] epoch:3921, lr:0.000701, total loss:0.000290\n",
            "[Train] epoch:3922, lr:0.000700, total loss:0.000347\n",
            "[Train] epoch:3923, lr:0.000700, total loss:0.000422\n",
            "[Train] epoch:3924, lr:0.000700, total loss:0.000282\n",
            "[Train] epoch:3925, lr:0.000700, total loss:0.000350\n",
            "[Train] epoch:3926, lr:0.000700, total loss:0.000354\n",
            "[Train] epoch:3927, lr:0.000700, total loss:0.000313\n",
            "[Train] epoch:3928, lr:0.000700, total loss:0.000332\n",
            "[Train] epoch:3929, lr:0.000700, total loss:0.000321\n",
            "[Train] epoch:3930, lr:0.000699, total loss:0.000301\n",
            "[Train] epoch:3931, lr:0.000699, total loss:0.000329\n",
            "[Train] epoch:3932, lr:0.000699, total loss:0.000308\n",
            "[Train] epoch:3933, lr:0.000699, total loss:0.000293\n",
            "[Train] epoch:3934, lr:0.000699, total loss:0.000331\n",
            "[Train] epoch:3935, lr:0.000699, total loss:0.000337\n",
            "[Train] epoch:3936, lr:0.000699, total loss:0.000298\n",
            "[Train] epoch:3937, lr:0.000698, total loss:0.000307\n",
            "[Train] epoch:3938, lr:0.000698, total loss:0.000290\n",
            "[Train] epoch:3939, lr:0.000698, total loss:0.000310\n",
            "[Train] epoch:3940, lr:0.000698, total loss:0.000345\n",
            "[Train] epoch:3941, lr:0.000698, total loss:0.000328\n",
            "[Train] epoch:3942, lr:0.000698, total loss:0.000355\n",
            "[Train] epoch:3943, lr:0.000698, total loss:0.000345\n",
            "[Train] epoch:3944, lr:0.000698, total loss:0.000306\n",
            "[Train] epoch:3945, lr:0.000697, total loss:0.000281\n",
            "[Train] epoch:3946, lr:0.000697, total loss:0.000266\n",
            "[Train] epoch:3947, lr:0.000697, total loss:0.000277\n",
            "[Train] epoch:3948, lr:0.000697, total loss:0.000331\n",
            "[Train] epoch:3949, lr:0.000697, total loss:0.000313\n",
            "[Train] epoch:3950, lr:0.000697, total loss:0.000302\n",
            "[Train] epoch:3951, lr:0.000697, total loss:0.000301\n",
            "[Train] epoch:3952, lr:0.000696, total loss:0.000249\n",
            "[Train] epoch:3953, lr:0.000696, total loss:0.000284\n",
            "[Train] epoch:3954, lr:0.000696, total loss:0.000303\n",
            "[Train] epoch:3955, lr:0.000696, total loss:0.000324\n",
            "[Train] epoch:3956, lr:0.000696, total loss:0.000290\n",
            "[Train] epoch:3957, lr:0.000696, total loss:0.000297\n",
            "[Train] epoch:3958, lr:0.000696, total loss:0.000277\n",
            "[Train] epoch:3959, lr:0.000696, total loss:0.000328\n",
            "[Train] epoch:3960, lr:0.000695, total loss:0.000277\n",
            "[Train] epoch:3961, lr:0.000695, total loss:0.000292\n",
            "[Train] epoch:3962, lr:0.000695, total loss:0.000246\n",
            "[Train] epoch:3963, lr:0.000695, total loss:0.000269\n",
            "[Train] epoch:3964, lr:0.000695, total loss:0.000287\n",
            "[Train] epoch:3965, lr:0.000695, total loss:0.000281\n",
            "[Train] epoch:3966, lr:0.000695, total loss:0.000260\n",
            "[Train] epoch:3967, lr:0.000694, total loss:0.000258\n",
            "[Train] epoch:3968, lr:0.000694, total loss:0.000307\n",
            "[Train] epoch:3969, lr:0.000694, total loss:0.000303\n",
            "[Train] epoch:3970, lr:0.000694, total loss:0.000252\n",
            "[Train] epoch:3971, lr:0.000694, total loss:0.000270\n",
            "[Train] epoch:3972, lr:0.000694, total loss:0.000278\n",
            "[Train] epoch:3973, lr:0.000694, total loss:0.000264\n",
            "[Train] epoch:3974, lr:0.000693, total loss:0.000261\n",
            "[Train] epoch:3975, lr:0.000693, total loss:0.000285\n",
            "[Train] epoch:3976, lr:0.000693, total loss:0.000275\n",
            "[Train] epoch:3977, lr:0.000693, total loss:0.000273\n",
            "[Train] epoch:3978, lr:0.000693, total loss:0.000262\n",
            "[Train] epoch:3979, lr:0.000693, total loss:0.000284\n",
            "[Train] epoch:3980, lr:0.000693, total loss:0.000307\n",
            "[Train] epoch:3981, lr:0.000693, total loss:0.000323\n",
            "[Train] epoch:3982, lr:0.000692, total loss:0.000305\n",
            "[Train] epoch:3983, lr:0.000692, total loss:0.000284\n",
            "[Train] epoch:3984, lr:0.000692, total loss:0.000262\n",
            "[Train] epoch:3985, lr:0.000692, total loss:0.000290\n",
            "[Train] epoch:3986, lr:0.000692, total loss:0.000291\n",
            "[Train] epoch:3987, lr:0.000692, total loss:0.000296\n",
            "[Train] epoch:3988, lr:0.000692, total loss:0.000270\n",
            "[Train] epoch:3989, lr:0.000691, total loss:0.000296\n",
            "[Train] epoch:3990, lr:0.000691, total loss:0.000285\n",
            "[Train] epoch:3991, lr:0.000691, total loss:0.000330\n",
            "[Train] epoch:3992, lr:0.000691, total loss:0.000320\n",
            "[Train] epoch:3993, lr:0.000691, total loss:0.000325\n",
            "[Train] epoch:3994, lr:0.000691, total loss:0.000316\n",
            "[Train] epoch:3995, lr:0.000691, total loss:0.000300\n",
            "[Train] epoch:3996, lr:0.000691, total loss:0.000335\n",
            "[Train] epoch:3997, lr:0.000690, total loss:0.000326\n",
            "[Train] epoch:3998, lr:0.000690, total loss:0.000330\n",
            "[Train] epoch:3999, lr:0.000690, total loss:0.000307\n",
            "[Train] epoch:4000, lr:0.000690, total loss:0.000311\n",
            "[Train] epoch:4001, lr:0.000690, total loss:0.000323\n",
            "[Train] epoch:4002, lr:0.000690, total loss:0.000344\n",
            "[Train] epoch:4003, lr:0.000690, total loss:0.000273\n",
            "[Train] epoch:4004, lr:0.000689, total loss:0.000294\n",
            "[Train] epoch:4005, lr:0.000689, total loss:0.000289\n",
            "[Train] epoch:4006, lr:0.000689, total loss:0.000297\n",
            "[Train] epoch:4007, lr:0.000689, total loss:0.000255\n",
            "[Train] epoch:4008, lr:0.000689, total loss:0.000266\n",
            "[Train] epoch:4009, lr:0.000689, total loss:0.000346\n",
            "[Train] epoch:4010, lr:0.000689, total loss:0.000263\n",
            "[Train] epoch:4011, lr:0.000689, total loss:0.000276\n",
            "[Train] epoch:4012, lr:0.000688, total loss:0.000304\n",
            "[Train] epoch:4013, lr:0.000688, total loss:0.000319\n",
            "[Train] epoch:4014, lr:0.000688, total loss:0.000322\n",
            "[Train] epoch:4015, lr:0.000688, total loss:0.000297\n",
            "[Train] epoch:4016, lr:0.000688, total loss:0.000268\n",
            "[Train] epoch:4017, lr:0.000688, total loss:0.000290\n",
            "[Train] epoch:4018, lr:0.000688, total loss:0.000278\n",
            "[Train] epoch:4019, lr:0.000687, total loss:0.000309\n",
            "[Train] epoch:4020, lr:0.000687, total loss:0.000330\n",
            "[Train] epoch:4021, lr:0.000687, total loss:0.000322\n",
            "[Train] epoch:4022, lr:0.000687, total loss:0.000280\n",
            "[Train] epoch:4023, lr:0.000687, total loss:0.000313\n",
            "[Train] epoch:4024, lr:0.000687, total loss:0.000282\n",
            "[Train] epoch:4025, lr:0.000687, total loss:0.000316\n",
            "[Train] epoch:4026, lr:0.000686, total loss:0.000304\n",
            "[Train] epoch:4027, lr:0.000686, total loss:0.000308\n",
            "[Train] epoch:4028, lr:0.000686, total loss:0.000308\n",
            "[Train] epoch:4029, lr:0.000686, total loss:0.000290\n",
            "[Train] epoch:4030, lr:0.000686, total loss:0.000291\n",
            "[Train] epoch:4031, lr:0.000686, total loss:0.000262\n",
            "[Train] epoch:4032, lr:0.000686, total loss:0.000279\n",
            "[Train] epoch:4033, lr:0.000686, total loss:0.000296\n",
            "[Train] epoch:4034, lr:0.000685, total loss:0.000300\n",
            "[Train] epoch:4035, lr:0.000685, total loss:0.000279\n",
            "[Train] epoch:4036, lr:0.000685, total loss:0.000269\n",
            "[Train] epoch:4037, lr:0.000685, total loss:0.000295\n",
            "[Train] epoch:4038, lr:0.000685, total loss:0.000340\n",
            "[Train] epoch:4039, lr:0.000685, total loss:0.000332\n",
            "[Train] epoch:4040, lr:0.000685, total loss:0.000312\n",
            "[Train] epoch:4041, lr:0.000684, total loss:0.000286\n",
            "[Train] epoch:4042, lr:0.000684, total loss:0.000277\n",
            "[Train] epoch:4043, lr:0.000684, total loss:0.000286\n",
            "[Train] epoch:4044, lr:0.000684, total loss:0.000316\n",
            "[Train] epoch:4045, lr:0.000684, total loss:0.000318\n",
            "[Train] epoch:4046, lr:0.000684, total loss:0.000330\n",
            "[Train] epoch:4047, lr:0.000684, total loss:0.000309\n",
            "[Train] epoch:4048, lr:0.000684, total loss:0.000283\n",
            "[Train] epoch:4049, lr:0.000683, total loss:0.000281\n",
            "[Train] epoch:4050, lr:0.000683, total loss:0.000301\n",
            "[Train] epoch:4051, lr:0.000683, total loss:0.000325\n",
            "[Train] epoch:4052, lr:0.000683, total loss:0.000285\n",
            "[Train] epoch:4053, lr:0.000683, total loss:0.000324\n",
            "[Train] epoch:4054, lr:0.000683, total loss:0.000327\n",
            "[Train] epoch:4055, lr:0.000683, total loss:0.000287\n",
            "[Train] epoch:4056, lr:0.000682, total loss:0.000333\n",
            "[Train] epoch:4057, lr:0.000682, total loss:0.000279\n",
            "[Train] epoch:4058, lr:0.000682, total loss:0.000283\n",
            "[Train] epoch:4059, lr:0.000682, total loss:0.000287\n",
            "[Train] epoch:4060, lr:0.000682, total loss:0.000269\n",
            "[Train] epoch:4061, lr:0.000682, total loss:0.000287\n",
            "[Train] epoch:4062, lr:0.000682, total loss:0.000243\n",
            "[Train] epoch:4063, lr:0.000681, total loss:0.000328\n",
            "[Train] epoch:4064, lr:0.000681, total loss:0.000270\n",
            "[Train] epoch:4065, lr:0.000681, total loss:0.000273\n",
            "[Train] epoch:4066, lr:0.000681, total loss:0.000285\n",
            "[Train] epoch:4067, lr:0.000681, total loss:0.000258\n",
            "[Train] epoch:4068, lr:0.000681, total loss:0.000253\n",
            "[Train] epoch:4069, lr:0.000681, total loss:0.000278\n",
            "[Train] epoch:4070, lr:0.000681, total loss:0.000264\n",
            "[Train] epoch:4071, lr:0.000680, total loss:0.000374\n",
            "[Train] epoch:4072, lr:0.000680, total loss:0.000329\n",
            "[Train] epoch:4073, lr:0.000680, total loss:0.000260\n",
            "[Train] epoch:4074, lr:0.000680, total loss:0.000272\n",
            "[Train] epoch:4075, lr:0.000680, total loss:0.000326\n",
            "[Train] epoch:4076, lr:0.000680, total loss:0.000285\n",
            "[Train] epoch:4077, lr:0.000680, total loss:0.000297\n",
            "[Train] epoch:4078, lr:0.000679, total loss:0.000288\n",
            "[Train] epoch:4079, lr:0.000679, total loss:0.000344\n",
            "[Train] epoch:4080, lr:0.000679, total loss:0.000296\n",
            "[Train] epoch:4081, lr:0.000679, total loss:0.000308\n",
            "[Train] epoch:4082, lr:0.000679, total loss:0.000283\n",
            "[Train] epoch:4083, lr:0.000679, total loss:0.000256\n",
            "[Train] epoch:4084, lr:0.000679, total loss:0.000278\n",
            "[Train] epoch:4085, lr:0.000679, total loss:0.000284\n",
            "[Train] epoch:4086, lr:0.000678, total loss:0.000311\n",
            "[Train] epoch:4087, lr:0.000678, total loss:0.000294\n",
            "[Train] epoch:4088, lr:0.000678, total loss:0.000273\n",
            "[Train] epoch:4089, lr:0.000678, total loss:0.000277\n",
            "[Train] epoch:4090, lr:0.000678, total loss:0.000272\n",
            "[Train] epoch:4091, lr:0.000678, total loss:0.000272\n",
            "[Train] epoch:4092, lr:0.000678, total loss:0.000283\n",
            "[Train] epoch:4093, lr:0.000677, total loss:0.000276\n",
            "[Train] epoch:4094, lr:0.000677, total loss:0.000259\n",
            "[Train] epoch:4095, lr:0.000677, total loss:0.000312\n",
            "[Train] epoch:4096, lr:0.000677, total loss:0.000273\n",
            "[Train] epoch:4097, lr:0.000677, total loss:0.000273\n",
            "[Train] epoch:4098, lr:0.000677, total loss:0.000277\n",
            "[Train] epoch:4099, lr:0.000677, total loss:0.000278\n",
            "[Train] epoch:4100, lr:0.000676, total loss:0.000291\n",
            "[Train] epoch:4101, lr:0.000676, total loss:0.000309\n",
            "[Train] epoch:4102, lr:0.000676, total loss:0.000330\n",
            "[Train] epoch:4103, lr:0.000676, total loss:0.000293\n",
            "[Train] epoch:4104, lr:0.000676, total loss:0.000284\n",
            "[Train] epoch:4105, lr:0.000676, total loss:0.000254\n",
            "[Train] epoch:4106, lr:0.000676, total loss:0.000287\n",
            "[Train] epoch:4107, lr:0.000676, total loss:0.000327\n",
            "[Train] epoch:4108, lr:0.000675, total loss:0.000307\n",
            "[Train] epoch:4109, lr:0.000675, total loss:0.000293\n",
            "[Train] epoch:4110, lr:0.000675, total loss:0.000323\n",
            "[Train] epoch:4111, lr:0.000675, total loss:0.000292\n",
            "[Train] epoch:4112, lr:0.000675, total loss:0.000328\n",
            "[Train] epoch:4113, lr:0.000675, total loss:0.000343\n",
            "[Train] epoch:4114, lr:0.000675, total loss:0.000295\n",
            "[Train] epoch:4115, lr:0.000674, total loss:0.000311\n",
            "[Train] epoch:4116, lr:0.000674, total loss:0.000317\n",
            "[Train] epoch:4117, lr:0.000674, total loss:0.000294\n",
            "[Train] epoch:4118, lr:0.000674, total loss:0.000391\n",
            "[Train] epoch:4119, lr:0.000674, total loss:0.000383\n",
            "[Train] epoch:4120, lr:0.000674, total loss:0.000315\n",
            "[Train] epoch:4121, lr:0.000674, total loss:0.000331\n",
            "[Train] epoch:4122, lr:0.000673, total loss:0.000298\n",
            "[Train] epoch:4123, lr:0.000673, total loss:0.000244\n",
            "[Train] epoch:4124, lr:0.000673, total loss:0.000275\n",
            "[Train] epoch:4125, lr:0.000673, total loss:0.000278\n",
            "[Train] epoch:4126, lr:0.000673, total loss:0.000265\n",
            "[Train] epoch:4127, lr:0.000673, total loss:0.000268\n",
            "[Train] epoch:4128, lr:0.000673, total loss:0.000272\n",
            "[Train] epoch:4129, lr:0.000673, total loss:0.000288\n",
            "[Train] epoch:4130, lr:0.000672, total loss:0.000300\n",
            "[Train] epoch:4131, lr:0.000672, total loss:0.000321\n",
            "[Train] epoch:4132, lr:0.000672, total loss:0.000322\n",
            "[Train] epoch:4133, lr:0.000672, total loss:0.000412\n",
            "[Train] epoch:4134, lr:0.000672, total loss:0.000323\n",
            "[Train] epoch:4135, lr:0.000672, total loss:0.000315\n",
            "[Train] epoch:4136, lr:0.000672, total loss:0.000303\n",
            "[Train] epoch:4137, lr:0.000671, total loss:0.000290\n",
            "[Train] epoch:4138, lr:0.000671, total loss:0.000309\n",
            "[Train] epoch:4139, lr:0.000671, total loss:0.000294\n",
            "[Train] epoch:4140, lr:0.000671, total loss:0.000333\n",
            "[Train] epoch:4141, lr:0.000671, total loss:0.000278\n",
            "[Train] epoch:4142, lr:0.000671, total loss:0.000299\n",
            "[Train] epoch:4143, lr:0.000671, total loss:0.000293\n",
            "[Train] epoch:4144, lr:0.000670, total loss:0.000302\n",
            "[Train] epoch:4145, lr:0.000670, total loss:0.000283\n",
            "[Train] epoch:4146, lr:0.000670, total loss:0.000265\n",
            "[Train] epoch:4147, lr:0.000670, total loss:0.000304\n",
            "[Train] epoch:4148, lr:0.000670, total loss:0.000314\n",
            "[Train] epoch:4149, lr:0.000670, total loss:0.000341\n",
            "[Train] epoch:4150, lr:0.000670, total loss:0.000280\n",
            "[Train] epoch:4151, lr:0.000670, total loss:0.000293\n",
            "[Train] epoch:4152, lr:0.000669, total loss:0.000298\n",
            "[Train] epoch:4153, lr:0.000669, total loss:0.000330\n",
            "[Train] epoch:4154, lr:0.000669, total loss:0.000325\n",
            "[Train] epoch:4155, lr:0.000669, total loss:0.000316\n",
            "[Train] epoch:4156, lr:0.000669, total loss:0.000406\n",
            "[Train] epoch:4157, lr:0.000669, total loss:0.000391\n",
            "[Train] epoch:4158, lr:0.000669, total loss:0.000420\n",
            "[Train] epoch:4159, lr:0.000668, total loss:0.000397\n",
            "[Train] epoch:4160, lr:0.000668, total loss:0.000291\n",
            "[Train] epoch:4161, lr:0.000668, total loss:0.000346\n",
            "[Train] epoch:4162, lr:0.000668, total loss:0.000349\n",
            "[Train] epoch:4163, lr:0.000668, total loss:0.000294\n",
            "[Train] epoch:4164, lr:0.000668, total loss:0.000293\n",
            "[Train] epoch:4165, lr:0.000668, total loss:0.000319\n",
            "[Train] epoch:4166, lr:0.000667, total loss:0.000309\n",
            "[Train] epoch:4167, lr:0.000667, total loss:0.000322\n",
            "[Train] epoch:4168, lr:0.000667, total loss:0.000291\n",
            "[Train] epoch:4169, lr:0.000667, total loss:0.000282\n",
            "[Train] epoch:4170, lr:0.000667, total loss:0.000429\n",
            "[Train] epoch:4171, lr:0.000667, total loss:0.000347\n",
            "[Train] epoch:4172, lr:0.000667, total loss:0.000300\n",
            "[Train] epoch:4173, lr:0.000667, total loss:0.000289\n",
            "[Train] epoch:4174, lr:0.000666, total loss:0.000285\n",
            "[Train] epoch:4175, lr:0.000666, total loss:0.000302\n",
            "[Train] epoch:4176, lr:0.000666, total loss:0.000331\n",
            "[Train] epoch:4177, lr:0.000666, total loss:0.000288\n",
            "[Train] epoch:4178, lr:0.000666, total loss:0.000327\n",
            "[Train] epoch:4179, lr:0.000666, total loss:0.000325\n",
            "[Train] epoch:4180, lr:0.000666, total loss:0.000265\n",
            "[Train] epoch:4181, lr:0.000665, total loss:0.000277\n",
            "[Train] epoch:4182, lr:0.000665, total loss:0.000299\n",
            "[Train] epoch:4183, lr:0.000665, total loss:0.000268\n",
            "[Train] epoch:4184, lr:0.000665, total loss:0.000293\n",
            "[Train] epoch:4185, lr:0.000665, total loss:0.000267\n",
            "[Train] epoch:4186, lr:0.000665, total loss:0.000293\n",
            "[Train] epoch:4187, lr:0.000665, total loss:0.000287\n",
            "[Train] epoch:4188, lr:0.000664, total loss:0.000346\n",
            "[Train] epoch:4189, lr:0.000664, total loss:0.000331\n",
            "[Train] epoch:4190, lr:0.000664, total loss:0.000275\n",
            "[Train] epoch:4191, lr:0.000664, total loss:0.000310\n",
            "[Train] epoch:4192, lr:0.000664, total loss:0.000280\n",
            "[Train] epoch:4193, lr:0.000664, total loss:0.000269\n",
            "[Train] epoch:4194, lr:0.000664, total loss:0.000351\n",
            "[Train] epoch:4195, lr:0.000664, total loss:0.000297\n",
            "[Train] epoch:4196, lr:0.000663, total loss:0.000270\n",
            "[Train] epoch:4197, lr:0.000663, total loss:0.000269\n",
            "[Train] epoch:4198, lr:0.000663, total loss:0.000266\n",
            "[Train] epoch:4199, lr:0.000663, total loss:0.000287\n",
            "[Train] epoch:4200, lr:0.000663, total loss:0.000290\n",
            "[Train] epoch:4201, lr:0.000663, total loss:0.000264\n",
            "[Train] epoch:4202, lr:0.000663, total loss:0.000270\n",
            "[Train] epoch:4203, lr:0.000662, total loss:0.000264\n",
            "[Train] epoch:4204, lr:0.000662, total loss:0.000267\n",
            "[Train] epoch:4205, lr:0.000662, total loss:0.000287\n",
            "[Train] epoch:4206, lr:0.000662, total loss:0.000282\n",
            "[Train] epoch:4207, lr:0.000662, total loss:0.000298\n",
            "[Train] epoch:4208, lr:0.000662, total loss:0.000277\n",
            "[Train] epoch:4209, lr:0.000662, total loss:0.000253\n",
            "[Train] epoch:4210, lr:0.000661, total loss:0.000287\n",
            "[Train] epoch:4211, lr:0.000661, total loss:0.000311\n",
            "[Train] epoch:4212, lr:0.000661, total loss:0.000310\n",
            "[Train] epoch:4213, lr:0.000661, total loss:0.000329\n",
            "[Train] epoch:4214, lr:0.000661, total loss:0.000295\n",
            "[Train] epoch:4215, lr:0.000661, total loss:0.000320\n",
            "[Train] epoch:4216, lr:0.000661, total loss:0.000300\n",
            "[Train] epoch:4217, lr:0.000661, total loss:0.000311\n",
            "[Train] epoch:4218, lr:0.000660, total loss:0.000291\n",
            "[Train] epoch:4219, lr:0.000660, total loss:0.000322\n",
            "[Train] epoch:4220, lr:0.000660, total loss:0.000303\n",
            "[Train] epoch:4221, lr:0.000660, total loss:0.000301\n",
            "[Train] epoch:4222, lr:0.000660, total loss:0.000288\n",
            "[Train] epoch:4223, lr:0.000660, total loss:0.000272\n",
            "[Train] epoch:4224, lr:0.000660, total loss:0.000283\n",
            "[Train] epoch:4225, lr:0.000659, total loss:0.000243\n",
            "[Train] epoch:4226, lr:0.000659, total loss:0.000283\n",
            "[Train] epoch:4227, lr:0.000659, total loss:0.000296\n",
            "[Train] epoch:4228, lr:0.000659, total loss:0.000281\n",
            "[Train] epoch:4229, lr:0.000659, total loss:0.000321\n",
            "[Train] epoch:4230, lr:0.000659, total loss:0.000305\n",
            "[Train] epoch:4231, lr:0.000659, total loss:0.000282\n",
            "[Train] epoch:4232, lr:0.000658, total loss:0.000278\n",
            "[Train] epoch:4233, lr:0.000658, total loss:0.000283\n",
            "[Train] epoch:4234, lr:0.000658, total loss:0.000302\n",
            "[Train] epoch:4235, lr:0.000658, total loss:0.000268\n",
            "[Train] epoch:4236, lr:0.000658, total loss:0.000298\n",
            "[Train] epoch:4237, lr:0.000658, total loss:0.000266\n",
            "[Train] epoch:4238, lr:0.000658, total loss:0.000280\n",
            "[Train] epoch:4239, lr:0.000657, total loss:0.000282\n",
            "[Train] epoch:4240, lr:0.000657, total loss:0.000248\n",
            "[Train] epoch:4241, lr:0.000657, total loss:0.000286\n",
            "[Train] epoch:4242, lr:0.000657, total loss:0.000313\n",
            "[Train] epoch:4243, lr:0.000657, total loss:0.000297\n",
            "[Train] epoch:4244, lr:0.000657, total loss:0.000271\n",
            "[Train] epoch:4245, lr:0.000657, total loss:0.000299\n",
            "[Train] epoch:4246, lr:0.000657, total loss:0.000271\n",
            "[Train] epoch:4247, lr:0.000656, total loss:0.000282\n",
            "[Train] epoch:4248, lr:0.000656, total loss:0.000282\n",
            "[Train] epoch:4249, lr:0.000656, total loss:0.000272\n",
            "[Train] epoch:4250, lr:0.000656, total loss:0.000289\n",
            "[Train] epoch:4251, lr:0.000656, total loss:0.000346\n",
            "[Train] epoch:4252, lr:0.000656, total loss:0.000282\n",
            "[Train] epoch:4253, lr:0.000656, total loss:0.000254\n",
            "[Train] epoch:4254, lr:0.000655, total loss:0.000289\n",
            "[Train] epoch:4255, lr:0.000655, total loss:0.000268\n",
            "[Train] epoch:4256, lr:0.000655, total loss:0.000276\n",
            "[Train] epoch:4257, lr:0.000655, total loss:0.000265\n",
            "[Train] epoch:4258, lr:0.000655, total loss:0.000282\n",
            "[Train] epoch:4259, lr:0.000655, total loss:0.000309\n",
            "[Train] epoch:4260, lr:0.000655, total loss:0.000357\n",
            "[Train] epoch:4261, lr:0.000654, total loss:0.000349\n",
            "[Train] epoch:4262, lr:0.000654, total loss:0.000357\n",
            "[Train] epoch:4263, lr:0.000654, total loss:0.000340\n",
            "[Train] epoch:4264, lr:0.000654, total loss:0.000303\n",
            "[Train] epoch:4265, lr:0.000654, total loss:0.000322\n",
            "[Train] epoch:4266, lr:0.000654, total loss:0.000264\n",
            "[Train] epoch:4267, lr:0.000654, total loss:0.000290\n",
            "[Train] epoch:4268, lr:0.000654, total loss:0.000339\n",
            "[Train] epoch:4269, lr:0.000653, total loss:0.000291\n",
            "[Train] epoch:4270, lr:0.000653, total loss:0.000288\n",
            "[Train] epoch:4271, lr:0.000653, total loss:0.000317\n",
            "[Train] epoch:4272, lr:0.000653, total loss:0.000293\n",
            "[Train] epoch:4273, lr:0.000653, total loss:0.000273\n",
            "[Train] epoch:4274, lr:0.000653, total loss:0.000309\n",
            "[Train] epoch:4275, lr:0.000653, total loss:0.000317\n",
            "[Train] epoch:4276, lr:0.000652, total loss:0.000260\n",
            "[Train] epoch:4277, lr:0.000652, total loss:0.000349\n",
            "[Train] epoch:4278, lr:0.000652, total loss:0.000338\n",
            "[Train] epoch:4279, lr:0.000652, total loss:0.000313\n",
            "[Train] epoch:4280, lr:0.000652, total loss:0.000328\n",
            "[Train] epoch:4281, lr:0.000652, total loss:0.000318\n",
            "[Train] epoch:4282, lr:0.000652, total loss:0.000294\n",
            "[Train] epoch:4283, lr:0.000651, total loss:0.000283\n",
            "[Train] epoch:4284, lr:0.000651, total loss:0.000290\n",
            "[Train] epoch:4285, lr:0.000651, total loss:0.000258\n",
            "[Train] epoch:4286, lr:0.000651, total loss:0.000284\n",
            "[Train] epoch:4287, lr:0.000651, total loss:0.000286\n",
            "[Train] epoch:4288, lr:0.000651, total loss:0.000295\n",
            "[Train] epoch:4289, lr:0.000651, total loss:0.000305\n",
            "[Train] epoch:4290, lr:0.000650, total loss:0.000298\n",
            "[Train] epoch:4291, lr:0.000650, total loss:0.000285\n",
            "[Train] epoch:4292, lr:0.000650, total loss:0.000345\n",
            "[Train] epoch:4293, lr:0.000650, total loss:0.000316\n",
            "[Train] epoch:4294, lr:0.000650, total loss:0.000351\n",
            "[Train] epoch:4295, lr:0.000650, total loss:0.000287\n",
            "[Train] epoch:4296, lr:0.000650, total loss:0.000284\n",
            "[Train] epoch:4297, lr:0.000650, total loss:0.000281\n",
            "[Train] epoch:4298, lr:0.000649, total loss:0.000286\n",
            "[Train] epoch:4299, lr:0.000649, total loss:0.000292\n",
            "[Train] epoch:4300, lr:0.000649, total loss:0.000362\n",
            "[Train] epoch:4301, lr:0.000649, total loss:0.000297\n",
            "[Train] epoch:4302, lr:0.000649, total loss:0.000350\n",
            "[Train] epoch:4303, lr:0.000649, total loss:0.000320\n",
            "[Train] epoch:4304, lr:0.000649, total loss:0.000307\n",
            "[Train] epoch:4305, lr:0.000648, total loss:0.000266\n",
            "[Train] epoch:4306, lr:0.000648, total loss:0.000271\n",
            "[Train] epoch:4307, lr:0.000648, total loss:0.000327\n",
            "[Train] epoch:4308, lr:0.000648, total loss:0.000300\n",
            "[Train] epoch:4309, lr:0.000648, total loss:0.000304\n",
            "[Train] epoch:4310, lr:0.000648, total loss:0.000298\n",
            "[Train] epoch:4311, lr:0.000648, total loss:0.000264\n",
            "[Train] epoch:4312, lr:0.000647, total loss:0.000292\n",
            "[Train] epoch:4313, lr:0.000647, total loss:0.000285\n",
            "[Train] epoch:4314, lr:0.000647, total loss:0.000291\n",
            "[Train] epoch:4315, lr:0.000647, total loss:0.000364\n",
            "[Train] epoch:4316, lr:0.000647, total loss:0.000308\n",
            "[Train] epoch:4317, lr:0.000647, total loss:0.000290\n",
            "[Train] epoch:4318, lr:0.000647, total loss:0.000307\n",
            "[Train] epoch:4319, lr:0.000646, total loss:0.000330\n",
            "[Train] epoch:4320, lr:0.000646, total loss:0.000322\n",
            "[Train] epoch:4321, lr:0.000646, total loss:0.000268\n",
            "[Train] epoch:4322, lr:0.000646, total loss:0.000260\n",
            "[Train] epoch:4323, lr:0.000646, total loss:0.000262\n",
            "[Train] epoch:4324, lr:0.000646, total loss:0.000250\n",
            "[Train] epoch:4325, lr:0.000646, total loss:0.000322\n",
            "[Train] epoch:4326, lr:0.000645, total loss:0.000272\n",
            "[Train] epoch:4327, lr:0.000645, total loss:0.000285\n",
            "[Train] epoch:4328, lr:0.000645, total loss:0.000275\n",
            "[Train] epoch:4329, lr:0.000645, total loss:0.000261\n",
            "[Train] epoch:4330, lr:0.000645, total loss:0.000286\n",
            "[Train] epoch:4331, lr:0.000645, total loss:0.000280\n",
            "[Train] epoch:4332, lr:0.000645, total loss:0.000285\n",
            "[Train] epoch:4333, lr:0.000645, total loss:0.000270\n",
            "[Train] epoch:4334, lr:0.000644, total loss:0.000255\n",
            "[Train] epoch:4335, lr:0.000644, total loss:0.000298\n",
            "[Train] epoch:4336, lr:0.000644, total loss:0.000291\n",
            "[Train] epoch:4337, lr:0.000644, total loss:0.000286\n",
            "[Train] epoch:4338, lr:0.000644, total loss:0.000308\n",
            "[Train] epoch:4339, lr:0.000644, total loss:0.000287\n",
            "[Train] epoch:4340, lr:0.000644, total loss:0.000282\n",
            "[Train] epoch:4341, lr:0.000643, total loss:0.000290\n",
            "[Train] epoch:4342, lr:0.000643, total loss:0.000413\n",
            "[Train] epoch:4343, lr:0.000643, total loss:0.000350\n",
            "[Train] epoch:4344, lr:0.000643, total loss:0.000320\n",
            "[Train] epoch:4345, lr:0.000643, total loss:0.000287\n",
            "[Train] epoch:4346, lr:0.000643, total loss:0.000271\n",
            "[Train] epoch:4347, lr:0.000643, total loss:0.000260\n",
            "[Train] epoch:4348, lr:0.000642, total loss:0.000256\n",
            "[Train] epoch:4349, lr:0.000642, total loss:0.000250\n",
            "[Train] epoch:4350, lr:0.000642, total loss:0.000278\n",
            "[Train] epoch:4351, lr:0.000642, total loss:0.000273\n",
            "[Train] epoch:4352, lr:0.000642, total loss:0.000265\n",
            "[Train] epoch:4353, lr:0.000642, total loss:0.000306\n",
            "[Train] epoch:4354, lr:0.000642, total loss:0.000323\n",
            "[Train] epoch:4355, lr:0.000641, total loss:0.000294\n",
            "[Train] epoch:4356, lr:0.000641, total loss:0.000306\n",
            "[Train] epoch:4357, lr:0.000641, total loss:0.000265\n",
            "[Train] epoch:4358, lr:0.000641, total loss:0.000280\n",
            "[Train] epoch:4359, lr:0.000641, total loss:0.000280\n",
            "[Train] epoch:4360, lr:0.000641, total loss:0.000261\n",
            "[Train] epoch:4361, lr:0.000641, total loss:0.000276\n",
            "[Train] epoch:4362, lr:0.000641, total loss:0.000285\n",
            "[Train] epoch:4363, lr:0.000640, total loss:0.000296\n",
            "[Train] epoch:4364, lr:0.000640, total loss:0.000278\n",
            "[Train] epoch:4365, lr:0.000640, total loss:0.000289\n",
            "[Train] epoch:4366, lr:0.000640, total loss:0.000297\n",
            "[Train] epoch:4367, lr:0.000640, total loss:0.000268\n",
            "[Train] epoch:4368, lr:0.000640, total loss:0.000293\n",
            "[Train] epoch:4369, lr:0.000640, total loss:0.000260\n",
            "[Train] epoch:4370, lr:0.000639, total loss:0.000307\n",
            "[Train] epoch:4371, lr:0.000639, total loss:0.000264\n",
            "[Train] epoch:4372, lr:0.000639, total loss:0.000249\n",
            "[Train] epoch:4373, lr:0.000639, total loss:0.000269\n",
            "[Train] epoch:4374, lr:0.000639, total loss:0.000253\n",
            "[Train] epoch:4375, lr:0.000639, total loss:0.000232\n",
            "[Train] epoch:4376, lr:0.000639, total loss:0.000252\n",
            "[Train] epoch:4377, lr:0.000638, total loss:0.000269\n",
            "[Train] epoch:4378, lr:0.000638, total loss:0.000255\n",
            "[Train] epoch:4379, lr:0.000638, total loss:0.000281\n",
            "[Train] epoch:4380, lr:0.000638, total loss:0.000322\n",
            "[Train] epoch:4381, lr:0.000638, total loss:0.000276\n",
            "[Train] epoch:4382, lr:0.000638, total loss:0.000313\n",
            "[Train] epoch:4383, lr:0.000638, total loss:0.000306\n",
            "[Train] epoch:4384, lr:0.000637, total loss:0.000307\n",
            "[Train] epoch:4385, lr:0.000637, total loss:0.000292\n",
            "[Train] epoch:4386, lr:0.000637, total loss:0.000266\n",
            "[Train] epoch:4387, lr:0.000637, total loss:0.000249\n",
            "[Train] epoch:4388, lr:0.000637, total loss:0.000323\n",
            "[Train] epoch:4389, lr:0.000637, total loss:0.000333\n",
            "[Train] epoch:4390, lr:0.000637, total loss:0.000296\n",
            "[Train] epoch:4391, lr:0.000636, total loss:0.000279\n",
            "[Train] epoch:4392, lr:0.000636, total loss:0.000270\n",
            "[Train] epoch:4393, lr:0.000636, total loss:0.000261\n",
            "[Train] epoch:4394, lr:0.000636, total loss:0.000258\n",
            "[Train] epoch:4395, lr:0.000636, total loss:0.000286\n",
            "[Train] epoch:4396, lr:0.000636, total loss:0.000260\n",
            "[Train] epoch:4397, lr:0.000636, total loss:0.000343\n",
            "[Train] epoch:4398, lr:0.000636, total loss:0.000269\n",
            "[Train] epoch:4399, lr:0.000635, total loss:0.000263\n",
            "[Train] epoch:4400, lr:0.000635, total loss:0.000283\n",
            "[Train] epoch:4401, lr:0.000635, total loss:0.000360\n",
            "[Train] epoch:4402, lr:0.000635, total loss:0.000281\n",
            "[Train] epoch:4403, lr:0.000635, total loss:0.000259\n",
            "[Train] epoch:4404, lr:0.000635, total loss:0.000267\n",
            "[Train] epoch:4405, lr:0.000635, total loss:0.000268\n",
            "[Train] epoch:4406, lr:0.000634, total loss:0.000262\n",
            "[Train] epoch:4407, lr:0.000634, total loss:0.000286\n",
            "[Train] epoch:4408, lr:0.000634, total loss:0.000242\n",
            "[Train] epoch:4409, lr:0.000634, total loss:0.000309\n",
            "[Train] epoch:4410, lr:0.000634, total loss:0.000317\n",
            "[Train] epoch:4411, lr:0.000634, total loss:0.000290\n",
            "[Train] epoch:4412, lr:0.000634, total loss:0.000277\n",
            "[Train] epoch:4413, lr:0.000633, total loss:0.000266\n",
            "[Train] epoch:4414, lr:0.000633, total loss:0.000273\n",
            "[Train] epoch:4415, lr:0.000633, total loss:0.000257\n",
            "[Train] epoch:4416, lr:0.000633, total loss:0.000261\n",
            "[Train] epoch:4417, lr:0.000633, total loss:0.000302\n",
            "[Train] epoch:4418, lr:0.000633, total loss:0.000267\n",
            "[Train] epoch:4419, lr:0.000633, total loss:0.000287\n",
            "[Train] epoch:4420, lr:0.000632, total loss:0.000319\n",
            "[Train] epoch:4421, lr:0.000632, total loss:0.000350\n",
            "[Train] epoch:4422, lr:0.000632, total loss:0.000265\n",
            "[Train] epoch:4423, lr:0.000632, total loss:0.000269\n",
            "[Train] epoch:4424, lr:0.000632, total loss:0.000289\n",
            "[Train] epoch:4425, lr:0.000632, total loss:0.000283\n",
            "[Train] epoch:4426, lr:0.000632, total loss:0.000308\n",
            "[Train] epoch:4427, lr:0.000631, total loss:0.000267\n",
            "[Train] epoch:4428, lr:0.000631, total loss:0.000305\n",
            "[Train] epoch:4429, lr:0.000631, total loss:0.000280\n",
            "[Train] epoch:4430, lr:0.000631, total loss:0.000298\n",
            "[Train] epoch:4431, lr:0.000631, total loss:0.000243\n",
            "[Train] epoch:4432, lr:0.000631, total loss:0.000255\n",
            "[Train] epoch:4433, lr:0.000631, total loss:0.000258\n",
            "[Train] epoch:4434, lr:0.000631, total loss:0.000294\n",
            "[Train] epoch:4435, lr:0.000630, total loss:0.000296\n",
            "[Train] epoch:4436, lr:0.000630, total loss:0.000283\n",
            "[Train] epoch:4437, lr:0.000630, total loss:0.000255\n",
            "[Train] epoch:4438, lr:0.000630, total loss:0.000322\n",
            "[Train] epoch:4439, lr:0.000630, total loss:0.000310\n",
            "[Train] epoch:4440, lr:0.000630, total loss:0.000284\n",
            "[Train] epoch:4441, lr:0.000630, total loss:0.000308\n",
            "[Train] epoch:4442, lr:0.000629, total loss:0.000289\n",
            "[Train] epoch:4443, lr:0.000629, total loss:0.000301\n",
            "[Train] epoch:4444, lr:0.000629, total loss:0.000273\n",
            "[Train] epoch:4445, lr:0.000629, total loss:0.000280\n",
            "[Train] epoch:4446, lr:0.000629, total loss:0.000287\n",
            "[Train] epoch:4447, lr:0.000629, total loss:0.000255\n",
            "[Train] epoch:4448, lr:0.000629, total loss:0.000247\n",
            "[Train] epoch:4449, lr:0.000628, total loss:0.000288\n",
            "[Train] epoch:4450, lr:0.000628, total loss:0.000271\n",
            "[Train] epoch:4451, lr:0.000628, total loss:0.000271\n",
            "[Train] epoch:4452, lr:0.000628, total loss:0.000257\n",
            "[Train] epoch:4453, lr:0.000628, total loss:0.000258\n",
            "[Train] epoch:4454, lr:0.000628, total loss:0.000257\n",
            "[Train] epoch:4455, lr:0.000628, total loss:0.000261\n",
            "[Train] epoch:4456, lr:0.000627, total loss:0.000282\n",
            "[Train] epoch:4457, lr:0.000627, total loss:0.000277\n",
            "[Train] epoch:4458, lr:0.000627, total loss:0.000308\n",
            "[Train] epoch:4459, lr:0.000627, total loss:0.000255\n",
            "[Train] epoch:4460, lr:0.000627, total loss:0.000251\n",
            "[Train] epoch:4461, lr:0.000627, total loss:0.000275\n",
            "[Train] epoch:4462, lr:0.000627, total loss:0.000261\n",
            "[Train] epoch:4463, lr:0.000626, total loss:0.000255\n",
            "[Train] epoch:4464, lr:0.000626, total loss:0.000246\n",
            "[Train] epoch:4465, lr:0.000626, total loss:0.000266\n",
            "[Train] epoch:4466, lr:0.000626, total loss:0.000286\n",
            "[Train] epoch:4467, lr:0.000626, total loss:0.000261\n",
            "[Train] epoch:4468, lr:0.000626, total loss:0.000275\n",
            "[Train] epoch:4469, lr:0.000626, total loss:0.000271\n",
            "[Train] epoch:4470, lr:0.000625, total loss:0.000271\n",
            "[Train] epoch:4471, lr:0.000625, total loss:0.000249\n",
            "[Train] epoch:4472, lr:0.000625, total loss:0.000255\n",
            "[Train] epoch:4473, lr:0.000625, total loss:0.000266\n",
            "[Train] epoch:4474, lr:0.000625, total loss:0.000269\n",
            "[Train] epoch:4475, lr:0.000625, total loss:0.000291\n",
            "[Train] epoch:4476, lr:0.000625, total loss:0.000264\n",
            "[Train] epoch:4477, lr:0.000625, total loss:0.000285\n",
            "[Train] epoch:4478, lr:0.000624, total loss:0.000295\n",
            "[Train] epoch:4479, lr:0.000624, total loss:0.000270\n",
            "[Train] epoch:4480, lr:0.000624, total loss:0.000258\n",
            "[Train] epoch:4481, lr:0.000624, total loss:0.000262\n",
            "[Train] epoch:4482, lr:0.000624, total loss:0.000291\n",
            "[Train] epoch:4483, lr:0.000624, total loss:0.000303\n",
            "[Train] epoch:4484, lr:0.000624, total loss:0.000268\n",
            "[Train] epoch:4485, lr:0.000623, total loss:0.000258\n",
            "[Train] epoch:4486, lr:0.000623, total loss:0.000292\n",
            "[Train] epoch:4487, lr:0.000623, total loss:0.000271\n",
            "[Train] epoch:4488, lr:0.000623, total loss:0.000314\n",
            "[Train] epoch:4489, lr:0.000623, total loss:0.000246\n",
            "[Train] epoch:4490, lr:0.000623, total loss:0.000258\n",
            "[Train] epoch:4491, lr:0.000623, total loss:0.000254\n",
            "[Train] epoch:4492, lr:0.000622, total loss:0.000273\n",
            "[Train] epoch:4493, lr:0.000622, total loss:0.000264\n",
            "[Train] epoch:4494, lr:0.000622, total loss:0.000278\n",
            "[Train] epoch:4495, lr:0.000622, total loss:0.000260\n",
            "[Train] epoch:4496, lr:0.000622, total loss:0.000278\n",
            "[Train] epoch:4497, lr:0.000622, total loss:0.000238\n",
            "[Train] epoch:4498, lr:0.000622, total loss:0.000262\n",
            "[Train] epoch:4499, lr:0.000621, total loss:0.000277\n",
            "[Train] epoch:4500, lr:0.000621, total loss:0.000336\n",
            "[Train] epoch:4501, lr:0.000621, total loss:0.000302\n",
            "[Train] epoch:4502, lr:0.000621, total loss:0.000312\n",
            "[Train] epoch:4503, lr:0.000621, total loss:0.000302\n",
            "[Train] epoch:4504, lr:0.000621, total loss:0.000265\n",
            "[Train] epoch:4505, lr:0.000621, total loss:0.000279\n",
            "[Train] epoch:4506, lr:0.000620, total loss:0.000265\n",
            "[Train] epoch:4507, lr:0.000620, total loss:0.000285\n",
            "[Train] epoch:4508, lr:0.000620, total loss:0.000285\n",
            "[Train] epoch:4509, lr:0.000620, total loss:0.000267\n",
            "[Train] epoch:4510, lr:0.000620, total loss:0.000248\n",
            "[Train] epoch:4511, lr:0.000620, total loss:0.000275\n",
            "[Train] epoch:4512, lr:0.000620, total loss:0.000249\n",
            "[Train] epoch:4513, lr:0.000619, total loss:0.000319\n",
            "[Train] epoch:4514, lr:0.000619, total loss:0.000258\n",
            "[Train] epoch:4515, lr:0.000619, total loss:0.000302\n",
            "[Train] epoch:4516, lr:0.000619, total loss:0.000281\n",
            "[Train] epoch:4517, lr:0.000619, total loss:0.000271\n",
            "[Train] epoch:4518, lr:0.000619, total loss:0.000265\n",
            "[Train] epoch:4519, lr:0.000619, total loss:0.000294\n",
            "[Train] epoch:4520, lr:0.000619, total loss:0.000277\n",
            "[Train] epoch:4521, lr:0.000618, total loss:0.000272\n",
            "[Train] epoch:4522, lr:0.000618, total loss:0.000253\n",
            "[Train] epoch:4523, lr:0.000618, total loss:0.000264\n",
            "[Train] epoch:4524, lr:0.000618, total loss:0.000242\n",
            "[Train] epoch:4525, lr:0.000618, total loss:0.000250\n",
            "[Train] epoch:4526, lr:0.000618, total loss:0.000275\n",
            "[Train] epoch:4527, lr:0.000618, total loss:0.000236\n",
            "[Train] epoch:4528, lr:0.000617, total loss:0.000281\n",
            "[Train] epoch:4529, lr:0.000617, total loss:0.000300\n",
            "[Train] epoch:4530, lr:0.000617, total loss:0.000286\n",
            "[Train] epoch:4531, lr:0.000617, total loss:0.000287\n",
            "[Train] epoch:4532, lr:0.000617, total loss:0.000235\n",
            "[Train] epoch:4533, lr:0.000617, total loss:0.000280\n",
            "[Train] epoch:4534, lr:0.000617, total loss:0.000298\n",
            "[Train] epoch:4535, lr:0.000616, total loss:0.000239\n",
            "[Train] epoch:4536, lr:0.000616, total loss:0.000262\n",
            "[Train] epoch:4537, lr:0.000616, total loss:0.000264\n",
            "[Train] epoch:4538, lr:0.000616, total loss:0.000295\n",
            "[Train] epoch:4539, lr:0.000616, total loss:0.000274\n",
            "[Train] epoch:4540, lr:0.000616, total loss:0.000275\n",
            "[Train] epoch:4541, lr:0.000616, total loss:0.000278\n",
            "[Train] epoch:4542, lr:0.000615, total loss:0.000265\n",
            "[Train] epoch:4543, lr:0.000615, total loss:0.000274\n",
            "[Train] epoch:4544, lr:0.000615, total loss:0.000247\n",
            "[Train] epoch:4545, lr:0.000615, total loss:0.000305\n",
            "[Train] epoch:4546, lr:0.000615, total loss:0.000235\n",
            "[Train] epoch:4547, lr:0.000615, total loss:0.000281\n",
            "[Train] epoch:4548, lr:0.000615, total loss:0.000302\n",
            "[Train] epoch:4549, lr:0.000614, total loss:0.000268\n",
            "[Train] epoch:4550, lr:0.000614, total loss:0.000285\n",
            "[Train] epoch:4551, lr:0.000614, total loss:0.000297\n",
            "[Train] epoch:4552, lr:0.000614, total loss:0.000270\n",
            "[Train] epoch:4553, lr:0.000614, total loss:0.000288\n",
            "[Train] epoch:4554, lr:0.000614, total loss:0.000326\n",
            "[Train] epoch:4555, lr:0.000614, total loss:0.000314\n",
            "[Train] epoch:4556, lr:0.000613, total loss:0.000277\n",
            "[Train] epoch:4557, lr:0.000613, total loss:0.000293\n",
            "[Train] epoch:4558, lr:0.000613, total loss:0.000261\n",
            "[Train] epoch:4559, lr:0.000613, total loss:0.000254\n",
            "[Train] epoch:4560, lr:0.000613, total loss:0.000323\n",
            "[Train] epoch:4561, lr:0.000613, total loss:0.000294\n",
            "[Train] epoch:4562, lr:0.000613, total loss:0.000275\n",
            "[Train] epoch:4563, lr:0.000612, total loss:0.000261\n",
            "[Train] epoch:4564, lr:0.000612, total loss:0.000365\n",
            "[Train] epoch:4565, lr:0.000612, total loss:0.000266\n",
            "[Train] epoch:4566, lr:0.000612, total loss:0.000264\n",
            "[Train] epoch:4567, lr:0.000612, total loss:0.000273\n",
            "[Train] epoch:4568, lr:0.000612, total loss:0.000263\n",
            "[Train] epoch:4569, lr:0.000612, total loss:0.000255\n",
            "[Train] epoch:4570, lr:0.000612, total loss:0.000262\n",
            "[Train] epoch:4571, lr:0.000611, total loss:0.000277\n",
            "[Train] epoch:4572, lr:0.000611, total loss:0.000261\n",
            "[Train] epoch:4573, lr:0.000611, total loss:0.000255\n",
            "[Train] epoch:4574, lr:0.000611, total loss:0.000293\n",
            "[Train] epoch:4575, lr:0.000611, total loss:0.000311\n",
            "[Train] epoch:4576, lr:0.000611, total loss:0.000313\n",
            "[Train] epoch:4577, lr:0.000611, total loss:0.000234\n",
            "[Train] epoch:4578, lr:0.000610, total loss:0.000260\n",
            "[Train] epoch:4579, lr:0.000610, total loss:0.000287\n",
            "[Train] epoch:4580, lr:0.000610, total loss:0.000246\n",
            "[Train] epoch:4581, lr:0.000610, total loss:0.000260\n",
            "[Train] epoch:4582, lr:0.000610, total loss:0.000242\n",
            "[Train] epoch:4583, lr:0.000610, total loss:0.000272\n",
            "[Train] epoch:4584, lr:0.000610, total loss:0.000272\n",
            "[Train] epoch:4585, lr:0.000609, total loss:0.000266\n",
            "[Train] epoch:4586, lr:0.000609, total loss:0.000251\n",
            "[Train] epoch:4587, lr:0.000609, total loss:0.000259\n",
            "[Train] epoch:4588, lr:0.000609, total loss:0.000287\n",
            "[Train] epoch:4589, lr:0.000609, total loss:0.000298\n",
            "[Train] epoch:4590, lr:0.000609, total loss:0.000275\n",
            "[Train] epoch:4591, lr:0.000609, total loss:0.000264\n",
            "[Train] epoch:4592, lr:0.000608, total loss:0.000289\n",
            "[Train] epoch:4593, lr:0.000608, total loss:0.000271\n",
            "[Train] epoch:4594, lr:0.000608, total loss:0.000254\n",
            "[Train] epoch:4595, lr:0.000608, total loss:0.000254\n",
            "[Train] epoch:4596, lr:0.000608, total loss:0.000254\n",
            "[Train] epoch:4597, lr:0.000608, total loss:0.000272\n",
            "[Train] epoch:4598, lr:0.000608, total loss:0.000244\n",
            "[Train] epoch:4599, lr:0.000607, total loss:0.000289\n",
            "[Train] epoch:4600, lr:0.000607, total loss:0.000262\n",
            "[Train] epoch:4601, lr:0.000607, total loss:0.000244\n",
            "[Train] epoch:4602, lr:0.000607, total loss:0.000248\n",
            "[Train] epoch:4603, lr:0.000607, total loss:0.000242\n",
            "[Train] epoch:4604, lr:0.000607, total loss:0.000235\n",
            "[Train] epoch:4605, lr:0.000607, total loss:0.000240\n",
            "[Train] epoch:4606, lr:0.000606, total loss:0.000269\n",
            "[Train] epoch:4607, lr:0.000606, total loss:0.000266\n",
            "[Train] epoch:4608, lr:0.000606, total loss:0.000278\n",
            "[Train] epoch:4609, lr:0.000606, total loss:0.000341\n",
            "[Train] epoch:4610, lr:0.000606, total loss:0.000284\n",
            "[Train] epoch:4611, lr:0.000606, total loss:0.000265\n",
            "[Train] epoch:4612, lr:0.000606, total loss:0.000299\n",
            "[Train] epoch:4613, lr:0.000605, total loss:0.000279\n",
            "[Train] epoch:4614, lr:0.000605, total loss:0.000298\n",
            "[Train] epoch:4615, lr:0.000605, total loss:0.000294\n",
            "[Train] epoch:4616, lr:0.000605, total loss:0.000270\n",
            "[Train] epoch:4617, lr:0.000605, total loss:0.000282\n",
            "[Train] epoch:4618, lr:0.000605, total loss:0.000276\n",
            "[Train] epoch:4619, lr:0.000605, total loss:0.000299\n",
            "[Train] epoch:4620, lr:0.000604, total loss:0.000242\n",
            "[Train] epoch:4621, lr:0.000604, total loss:0.000252\n",
            "[Train] epoch:4622, lr:0.000604, total loss:0.000288\n",
            "[Train] epoch:4623, lr:0.000604, total loss:0.000254\n",
            "[Train] epoch:4624, lr:0.000604, total loss:0.000245\n",
            "[Train] epoch:4625, lr:0.000604, total loss:0.000254\n",
            "[Train] epoch:4626, lr:0.000604, total loss:0.000233\n",
            "[Train] epoch:4627, lr:0.000604, total loss:0.000276\n",
            "[Train] epoch:4628, lr:0.000603, total loss:0.000311\n",
            "[Train] epoch:4629, lr:0.000603, total loss:0.000318\n",
            "[Train] epoch:4630, lr:0.000603, total loss:0.000286\n",
            "[Train] epoch:4631, lr:0.000603, total loss:0.000313\n",
            "[Train] epoch:4632, lr:0.000603, total loss:0.000268\n",
            "[Train] epoch:4633, lr:0.000603, total loss:0.000278\n",
            "[Train] epoch:4634, lr:0.000603, total loss:0.000312\n",
            "[Train] epoch:4635, lr:0.000602, total loss:0.000255\n",
            "[Train] epoch:4636, lr:0.000602, total loss:0.000254\n",
            "[Train] epoch:4637, lr:0.000602, total loss:0.000266\n",
            "[Train] epoch:4638, lr:0.000602, total loss:0.000249\n",
            "[Train] epoch:4639, lr:0.000602, total loss:0.000266\n",
            "[Train] epoch:4640, lr:0.000602, total loss:0.000251\n",
            "[Train] epoch:4641, lr:0.000602, total loss:0.000291\n",
            "[Train] epoch:4642, lr:0.000601, total loss:0.000279\n",
            "[Train] epoch:4643, lr:0.000601, total loss:0.000300\n",
            "[Train] epoch:4644, lr:0.000601, total loss:0.000267\n",
            "[Train] epoch:4645, lr:0.000601, total loss:0.000276\n",
            "[Train] epoch:4646, lr:0.000601, total loss:0.000253\n",
            "[Train] epoch:4647, lr:0.000601, total loss:0.000270\n",
            "[Train] epoch:4648, lr:0.000601, total loss:0.000310\n",
            "[Train] epoch:4649, lr:0.000600, total loss:0.000260\n",
            "[Train] epoch:4650, lr:0.000600, total loss:0.000276\n",
            "[Train] epoch:4651, lr:0.000600, total loss:0.000366\n",
            "[Train] epoch:4652, lr:0.000600, total loss:0.000284\n",
            "[Train] epoch:4653, lr:0.000600, total loss:0.000330\n",
            "[Train] epoch:4654, lr:0.000600, total loss:0.000272\n",
            "[Train] epoch:4655, lr:0.000600, total loss:0.000286\n",
            "[Train] epoch:4656, lr:0.000599, total loss:0.000241\n",
            "[Train] epoch:4657, lr:0.000599, total loss:0.000229\n",
            "[Train] epoch:4658, lr:0.000599, total loss:0.000256\n",
            "[Train] epoch:4659, lr:0.000599, total loss:0.000292\n",
            "[Train] epoch:4660, lr:0.000599, total loss:0.000262\n",
            "[Train] epoch:4661, lr:0.000599, total loss:0.000301\n",
            "[Train] epoch:4662, lr:0.000599, total loss:0.000240\n",
            "[Train] epoch:4663, lr:0.000598, total loss:0.000285\n",
            "[Train] epoch:4664, lr:0.000598, total loss:0.000316\n",
            "[Train] epoch:4665, lr:0.000598, total loss:0.000318\n",
            "[Train] epoch:4666, lr:0.000598, total loss:0.000297\n",
            "[Train] epoch:4667, lr:0.000598, total loss:0.000270\n",
            "[Train] epoch:4668, lr:0.000598, total loss:0.000264\n",
            "[Train] epoch:4669, lr:0.000598, total loss:0.000275\n",
            "[Train] epoch:4670, lr:0.000597, total loss:0.000262\n",
            "[Train] epoch:4671, lr:0.000597, total loss:0.000290\n",
            "[Train] epoch:4672, lr:0.000597, total loss:0.000305\n",
            "[Train] epoch:4673, lr:0.000597, total loss:0.000290\n",
            "[Train] epoch:4674, lr:0.000597, total loss:0.000268\n",
            "[Train] epoch:4675, lr:0.000597, total loss:0.000252\n",
            "[Train] epoch:4676, lr:0.000597, total loss:0.000311\n",
            "[Train] epoch:4677, lr:0.000596, total loss:0.000283\n",
            "[Train] epoch:4678, lr:0.000596, total loss:0.000272\n",
            "[Train] epoch:4679, lr:0.000596, total loss:0.000250\n",
            "[Train] epoch:4680, lr:0.000596, total loss:0.000270\n",
            "[Train] epoch:4681, lr:0.000596, total loss:0.000254\n",
            "[Train] epoch:4682, lr:0.000596, total loss:0.000248\n",
            "[Train] epoch:4683, lr:0.000596, total loss:0.000252\n",
            "[Train] epoch:4684, lr:0.000595, total loss:0.000269\n",
            "[Train] epoch:4685, lr:0.000595, total loss:0.000258\n",
            "[Train] epoch:4686, lr:0.000595, total loss:0.000237\n",
            "[Train] epoch:4687, lr:0.000595, total loss:0.000251\n",
            "[Train] epoch:4688, lr:0.000595, total loss:0.000295\n",
            "[Train] epoch:4689, lr:0.000595, total loss:0.000266\n",
            "[Train] epoch:4690, lr:0.000595, total loss:0.000283\n",
            "[Train] epoch:4691, lr:0.000595, total loss:0.000288\n",
            "[Train] epoch:4692, lr:0.000594, total loss:0.000316\n",
            "[Train] epoch:4693, lr:0.000594, total loss:0.000318\n",
            "[Train] epoch:4694, lr:0.000594, total loss:0.000278\n",
            "[Train] epoch:4695, lr:0.000594, total loss:0.000313\n",
            "[Train] epoch:4696, lr:0.000594, total loss:0.000295\n",
            "[Train] epoch:4697, lr:0.000594, total loss:0.000269\n",
            "[Train] epoch:4698, lr:0.000594, total loss:0.000298\n",
            "[Train] epoch:4699, lr:0.000593, total loss:0.000278\n",
            "[Train] epoch:4700, lr:0.000593, total loss:0.000270\n",
            "[Train] epoch:4701, lr:0.000593, total loss:0.000261\n",
            "[Train] epoch:4702, lr:0.000593, total loss:0.000253\n",
            "[Train] epoch:4703, lr:0.000593, total loss:0.000287\n",
            "[Train] epoch:4704, lr:0.000593, total loss:0.000376\n",
            "[Train] epoch:4705, lr:0.000593, total loss:0.000308\n",
            "[Train] epoch:4706, lr:0.000592, total loss:0.000289\n",
            "[Train] epoch:4707, lr:0.000592, total loss:0.000291\n",
            "[Train] epoch:4708, lr:0.000592, total loss:0.000345\n",
            "[Train] epoch:4709, lr:0.000592, total loss:0.000294\n",
            "[Train] epoch:4710, lr:0.000592, total loss:0.000292\n",
            "[Train] epoch:4711, lr:0.000592, total loss:0.000279\n",
            "[Train] epoch:4712, lr:0.000592, total loss:0.000284\n",
            "[Train] epoch:4713, lr:0.000591, total loss:0.000261\n",
            "[Train] epoch:4714, lr:0.000591, total loss:0.000258\n",
            "[Train] epoch:4715, lr:0.000591, total loss:0.000242\n",
            "[Train] epoch:4716, lr:0.000591, total loss:0.000242\n",
            "[Train] epoch:4717, lr:0.000591, total loss:0.000257\n",
            "[Train] epoch:4718, lr:0.000591, total loss:0.000263\n",
            "[Train] epoch:4719, lr:0.000591, total loss:0.000272\n",
            "[Train] epoch:4720, lr:0.000590, total loss:0.000272\n",
            "[Train] epoch:4721, lr:0.000590, total loss:0.000255\n",
            "[Train] epoch:4722, lr:0.000590, total loss:0.000313\n",
            "[Train] epoch:4723, lr:0.000590, total loss:0.000247\n",
            "[Train] epoch:4724, lr:0.000590, total loss:0.000290\n",
            "[Train] epoch:4725, lr:0.000590, total loss:0.000244\n",
            "[Train] epoch:4726, lr:0.000590, total loss:0.000249\n",
            "[Train] epoch:4727, lr:0.000589, total loss:0.000235\n",
            "[Train] epoch:4728, lr:0.000589, total loss:0.000259\n",
            "[Train] epoch:4729, lr:0.000589, total loss:0.000244\n",
            "[Train] epoch:4730, lr:0.000589, total loss:0.000275\n",
            "[Train] epoch:4731, lr:0.000589, total loss:0.000257\n",
            "[Train] epoch:4732, lr:0.000589, total loss:0.000237\n",
            "[Train] epoch:4733, lr:0.000589, total loss:0.000262\n",
            "[Train] epoch:4734, lr:0.000588, total loss:0.000237\n",
            "[Train] epoch:4735, lr:0.000588, total loss:0.000240\n",
            "[Train] epoch:4736, lr:0.000588, total loss:0.000311\n",
            "[Train] epoch:4737, lr:0.000588, total loss:0.000291\n",
            "[Train] epoch:4738, lr:0.000588, total loss:0.000324\n",
            "[Train] epoch:4739, lr:0.000588, total loss:0.000277\n",
            "[Train] epoch:4740, lr:0.000588, total loss:0.000249\n",
            "[Train] epoch:4741, lr:0.000587, total loss:0.000274\n",
            "[Train] epoch:4742, lr:0.000587, total loss:0.000250\n",
            "[Train] epoch:4743, lr:0.000587, total loss:0.000256\n",
            "[Train] epoch:4744, lr:0.000587, total loss:0.000263\n",
            "[Train] epoch:4745, lr:0.000587, total loss:0.000248\n",
            "[Train] epoch:4746, lr:0.000587, total loss:0.000270\n",
            "[Train] epoch:4747, lr:0.000587, total loss:0.000256\n",
            "[Train] epoch:4748, lr:0.000586, total loss:0.000286\n",
            "[Train] epoch:4749, lr:0.000586, total loss:0.000259\n",
            "[Train] epoch:4750, lr:0.000586, total loss:0.000294\n",
            "[Train] epoch:4751, lr:0.000586, total loss:0.000295\n",
            "[Train] epoch:4752, lr:0.000586, total loss:0.000271\n",
            "[Train] epoch:4753, lr:0.000586, total loss:0.000311\n",
            "[Train] epoch:4754, lr:0.000586, total loss:0.000281\n",
            "[Train] epoch:4755, lr:0.000585, total loss:0.000253\n",
            "[Train] epoch:4756, lr:0.000585, total loss:0.000264\n",
            "[Train] epoch:4757, lr:0.000585, total loss:0.000244\n",
            "[Train] epoch:4758, lr:0.000585, total loss:0.000244\n",
            "[Train] epoch:4759, lr:0.000585, total loss:0.000249\n",
            "[Train] epoch:4760, lr:0.000585, total loss:0.000265\n",
            "[Train] epoch:4761, lr:0.000585, total loss:0.000254\n",
            "[Train] epoch:4762, lr:0.000584, total loss:0.000268\n",
            "[Train] epoch:4763, lr:0.000584, total loss:0.000272\n",
            "[Train] epoch:4764, lr:0.000584, total loss:0.000309\n",
            "[Train] epoch:4765, lr:0.000584, total loss:0.000275\n",
            "[Train] epoch:4766, lr:0.000584, total loss:0.000283\n",
            "[Train] epoch:4767, lr:0.000584, total loss:0.000281\n",
            "[Train] epoch:4768, lr:0.000584, total loss:0.000301\n",
            "[Train] epoch:4769, lr:0.000584, total loss:0.000273\n",
            "[Train] epoch:4770, lr:0.000583, total loss:0.000259\n",
            "[Train] epoch:4771, lr:0.000583, total loss:0.000265\n",
            "[Train] epoch:4772, lr:0.000583, total loss:0.000263\n",
            "[Train] epoch:4773, lr:0.000583, total loss:0.000264\n",
            "[Train] epoch:4774, lr:0.000583, total loss:0.000262\n",
            "[Train] epoch:4775, lr:0.000583, total loss:0.000279\n",
            "[Train] epoch:4776, lr:0.000583, total loss:0.000287\n",
            "[Train] epoch:4777, lr:0.000582, total loss:0.000331\n",
            "[Train] epoch:4778, lr:0.000582, total loss:0.000294\n",
            "[Train] epoch:4779, lr:0.000582, total loss:0.000303\n",
            "[Train] epoch:4780, lr:0.000582, total loss:0.000285\n",
            "[Train] epoch:4781, lr:0.000582, total loss:0.000423\n",
            "[Train] epoch:4782, lr:0.000582, total loss:0.000272\n",
            "[Train] epoch:4783, lr:0.000582, total loss:0.000270\n",
            "[Train] epoch:4784, lr:0.000581, total loss:0.000273\n",
            "[Train] epoch:4785, lr:0.000581, total loss:0.000294\n",
            "[Train] epoch:4786, lr:0.000581, total loss:0.000243\n",
            "[Train] epoch:4787, lr:0.000581, total loss:0.000261\n",
            "[Train] epoch:4788, lr:0.000581, total loss:0.000266\n",
            "[Train] epoch:4789, lr:0.000581, total loss:0.000244\n",
            "[Train] epoch:4790, lr:0.000581, total loss:0.000231\n",
            "[Train] epoch:4791, lr:0.000580, total loss:0.000247\n",
            "[Train] epoch:4792, lr:0.000580, total loss:0.000299\n",
            "[Train] epoch:4793, lr:0.000580, total loss:0.000266\n",
            "[Train] epoch:4794, lr:0.000580, total loss:0.000259\n",
            "[Train] epoch:4795, lr:0.000580, total loss:0.000336\n",
            "[Train] epoch:4796, lr:0.000580, total loss:0.000310\n",
            "[Train] epoch:4797, lr:0.000580, total loss:0.000297\n",
            "[Train] epoch:4798, lr:0.000579, total loss:0.000263\n",
            "[Train] epoch:4799, lr:0.000579, total loss:0.000291\n",
            "[Train] epoch:4800, lr:0.000579, total loss:0.000242\n",
            "[Train] epoch:4801, lr:0.000579, total loss:0.000294\n",
            "[Train] epoch:4802, lr:0.000579, total loss:0.000318\n",
            "[Train] epoch:4803, lr:0.000579, total loss:0.000332\n",
            "[Train] epoch:4804, lr:0.000579, total loss:0.000263\n",
            "[Train] epoch:4805, lr:0.000578, total loss:0.000289\n",
            "[Train] epoch:4806, lr:0.000578, total loss:0.000278\n",
            "[Train] epoch:4807, lr:0.000578, total loss:0.000288\n",
            "[Train] epoch:4808, lr:0.000578, total loss:0.000290\n",
            "[Train] epoch:4809, lr:0.000578, total loss:0.000284\n",
            "[Train] epoch:4810, lr:0.000578, total loss:0.000268\n",
            "[Train] epoch:4811, lr:0.000578, total loss:0.000272\n",
            "[Train] epoch:4812, lr:0.000577, total loss:0.000238\n",
            "[Train] epoch:4813, lr:0.000577, total loss:0.000244\n",
            "[Train] epoch:4814, lr:0.000577, total loss:0.000253\n",
            "[Train] epoch:4815, lr:0.000577, total loss:0.000241\n",
            "[Train] epoch:4816, lr:0.000577, total loss:0.000238\n",
            "[Train] epoch:4817, lr:0.000577, total loss:0.000256\n",
            "[Train] epoch:4818, lr:0.000577, total loss:0.000281\n",
            "[Train] epoch:4819, lr:0.000576, total loss:0.000273\n",
            "[Train] epoch:4820, lr:0.000576, total loss:0.000276\n",
            "[Train] epoch:4821, lr:0.000576, total loss:0.000255\n",
            "[Train] epoch:4822, lr:0.000576, total loss:0.000272\n",
            "[Train] epoch:4823, lr:0.000576, total loss:0.000314\n",
            "[Train] epoch:4824, lr:0.000576, total loss:0.000288\n",
            "[Train] epoch:4825, lr:0.000576, total loss:0.000300\n",
            "[Train] epoch:4826, lr:0.000575, total loss:0.000281\n",
            "[Train] epoch:4827, lr:0.000575, total loss:0.000250\n",
            "[Train] epoch:4828, lr:0.000575, total loss:0.000263\n",
            "[Train] epoch:4829, lr:0.000575, total loss:0.000257\n",
            "[Train] epoch:4830, lr:0.000575, total loss:0.000276\n",
            "[Train] epoch:4831, lr:0.000575, total loss:0.000273\n",
            "[Train] epoch:4832, lr:0.000575, total loss:0.000301\n",
            "[Train] epoch:4833, lr:0.000574, total loss:0.000259\n",
            "[Train] epoch:4834, lr:0.000574, total loss:0.000293\n",
            "[Train] epoch:4835, lr:0.000574, total loss:0.000245\n",
            "[Train] epoch:4836, lr:0.000574, total loss:0.000280\n",
            "[Train] epoch:4837, lr:0.000574, total loss:0.000285\n",
            "[Train] epoch:4838, lr:0.000574, total loss:0.000257\n",
            "[Train] epoch:4839, lr:0.000574, total loss:0.000242\n",
            "[Train] epoch:4840, lr:0.000573, total loss:0.000245\n",
            "[Train] epoch:4841, lr:0.000573, total loss:0.000236\n",
            "[Train] epoch:4842, lr:0.000573, total loss:0.000254\n",
            "[Train] epoch:4843, lr:0.000573, total loss:0.000384\n",
            "[Train] epoch:4844, lr:0.000573, total loss:0.000263\n",
            "[Train] epoch:4845, lr:0.000573, total loss:0.000265\n",
            "[Train] epoch:4846, lr:0.000573, total loss:0.000275\n",
            "[Train] epoch:4847, lr:0.000572, total loss:0.000251\n",
            "[Train] epoch:4848, lr:0.000572, total loss:0.000282\n",
            "[Train] epoch:4849, lr:0.000572, total loss:0.000306\n",
            "[Train] epoch:4850, lr:0.000572, total loss:0.000262\n",
            "[Train] epoch:4851, lr:0.000572, total loss:0.000247\n",
            "[Train] epoch:4852, lr:0.000572, total loss:0.000236\n",
            "[Train] epoch:4853, lr:0.000572, total loss:0.000245\n",
            "[Train] epoch:4854, lr:0.000572, total loss:0.000266\n",
            "[Train] epoch:4855, lr:0.000571, total loss:0.000274\n",
            "[Train] epoch:4856, lr:0.000571, total loss:0.000267\n",
            "[Train] epoch:4857, lr:0.000571, total loss:0.000290\n",
            "[Train] epoch:4858, lr:0.000571, total loss:0.000239\n",
            "[Train] epoch:4859, lr:0.000571, total loss:0.000268\n",
            "[Train] epoch:4860, lr:0.000571, total loss:0.000281\n",
            "[Train] epoch:4861, lr:0.000571, total loss:0.000255\n",
            "[Train] epoch:4862, lr:0.000570, total loss:0.000254\n",
            "[Train] epoch:4863, lr:0.000570, total loss:0.000289\n",
            "[Train] epoch:4864, lr:0.000570, total loss:0.000240\n",
            "[Train] epoch:4865, lr:0.000570, total loss:0.000287\n",
            "[Train] epoch:4866, lr:0.000570, total loss:0.000270\n",
            "[Train] epoch:4867, lr:0.000570, total loss:0.000238\n",
            "[Train] epoch:4868, lr:0.000570, total loss:0.000223\n",
            "[Train] epoch:4869, lr:0.000569, total loss:0.000251\n",
            "[Train] epoch:4870, lr:0.000569, total loss:0.000229\n",
            "[Train] epoch:4871, lr:0.000569, total loss:0.000252\n",
            "[Train] epoch:4872, lr:0.000569, total loss:0.000293\n",
            "[Train] epoch:4873, lr:0.000569, total loss:0.000251\n",
            "[Train] epoch:4874, lr:0.000569, total loss:0.000245\n",
            "[Train] epoch:4875, lr:0.000569, total loss:0.000243\n",
            "[Train] epoch:4876, lr:0.000568, total loss:0.000248\n",
            "[Train] epoch:4877, lr:0.000568, total loss:0.000254\n",
            "[Train] epoch:4878, lr:0.000568, total loss:0.000237\n",
            "[Train] epoch:4879, lr:0.000568, total loss:0.000225\n",
            "[Train] epoch:4880, lr:0.000568, total loss:0.000277\n",
            "[Train] epoch:4881, lr:0.000568, total loss:0.000242\n",
            "[Train] epoch:4882, lr:0.000568, total loss:0.000246\n",
            "[Train] epoch:4883, lr:0.000567, total loss:0.000269\n",
            "[Train] epoch:4884, lr:0.000567, total loss:0.000292\n",
            "[Train] epoch:4885, lr:0.000567, total loss:0.000248\n",
            "[Train] epoch:4886, lr:0.000567, total loss:0.000243\n",
            "[Train] epoch:4887, lr:0.000567, total loss:0.000256\n",
            "[Train] epoch:4888, lr:0.000567, total loss:0.000275\n",
            "[Train] epoch:4889, lr:0.000567, total loss:0.000259\n",
            "[Train] epoch:4890, lr:0.000566, total loss:0.000253\n",
            "[Train] epoch:4891, lr:0.000566, total loss:0.000244\n",
            "[Train] epoch:4892, lr:0.000566, total loss:0.000237\n",
            "[Train] epoch:4893, lr:0.000566, total loss:0.000232\n",
            "[Train] epoch:4894, lr:0.000566, total loss:0.000259\n",
            "[Train] epoch:4895, lr:0.000566, total loss:0.000246\n",
            "[Train] epoch:4896, lr:0.000566, total loss:0.000247\n",
            "[Train] epoch:4897, lr:0.000565, total loss:0.000223\n",
            "[Train] epoch:4898, lr:0.000565, total loss:0.000234\n",
            "[Train] epoch:4899, lr:0.000565, total loss:0.000348\n",
            "[Train] epoch:4900, lr:0.000565, total loss:0.000314\n",
            "[Train] epoch:4901, lr:0.000565, total loss:0.000285\n",
            "[Train] epoch:4902, lr:0.000565, total loss:0.000308\n",
            "[Train] epoch:4903, lr:0.000565, total loss:0.000252\n",
            "[Train] epoch:4904, lr:0.000564, total loss:0.000258\n",
            "[Train] epoch:4905, lr:0.000564, total loss:0.000248\n",
            "[Train] epoch:4906, lr:0.000564, total loss:0.000228\n",
            "[Train] epoch:4907, lr:0.000564, total loss:0.000231\n",
            "[Train] epoch:4908, lr:0.000564, total loss:0.000266\n",
            "[Train] epoch:4909, lr:0.000564, total loss:0.000290\n",
            "[Train] epoch:4910, lr:0.000564, total loss:0.000308\n",
            "[Train] epoch:4911, lr:0.000563, total loss:0.000240\n",
            "[Train] epoch:4912, lr:0.000563, total loss:0.000264\n",
            "[Train] epoch:4913, lr:0.000563, total loss:0.000272\n",
            "[Train] epoch:4914, lr:0.000563, total loss:0.000258\n",
            "[Train] epoch:4915, lr:0.000563, total loss:0.000282\n",
            "[Train] epoch:4916, lr:0.000563, total loss:0.000276\n",
            "[Train] epoch:4917, lr:0.000563, total loss:0.000264\n",
            "[Train] epoch:4918, lr:0.000562, total loss:0.000267\n",
            "[Train] epoch:4919, lr:0.000562, total loss:0.000293\n",
            "[Train] epoch:4920, lr:0.000562, total loss:0.000303\n",
            "[Train] epoch:4921, lr:0.000562, total loss:0.000280\n",
            "[Train] epoch:4922, lr:0.000562, total loss:0.000256\n",
            "[Train] epoch:4923, lr:0.000562, total loss:0.000247\n",
            "[Train] epoch:4924, lr:0.000562, total loss:0.000308\n",
            "[Train] epoch:4925, lr:0.000561, total loss:0.000280\n",
            "[Train] epoch:4926, lr:0.000561, total loss:0.000338\n",
            "[Train] epoch:4927, lr:0.000561, total loss:0.000276\n",
            "[Train] epoch:4928, lr:0.000561, total loss:0.000269\n",
            "[Train] epoch:4929, lr:0.000561, total loss:0.000288\n",
            "[Train] epoch:4930, lr:0.000561, total loss:0.000281\n",
            "[Train] epoch:4931, lr:0.000561, total loss:0.000277\n",
            "[Train] epoch:4932, lr:0.000560, total loss:0.000248\n",
            "[Train] epoch:4933, lr:0.000560, total loss:0.000250\n",
            "[Train] epoch:4934, lr:0.000560, total loss:0.000226\n",
            "[Train] epoch:4935, lr:0.000560, total loss:0.000255\n",
            "[Train] epoch:4936, lr:0.000560, total loss:0.000284\n",
            "[Train] epoch:4937, lr:0.000560, total loss:0.000282\n",
            "[Train] epoch:4938, lr:0.000560, total loss:0.000261\n",
            "[Train] epoch:4939, lr:0.000559, total loss:0.000250\n",
            "[Train] epoch:4940, lr:0.000559, total loss:0.000252\n",
            "[Train] epoch:4941, lr:0.000559, total loss:0.000242\n",
            "[Train] epoch:4942, lr:0.000559, total loss:0.000339\n",
            "[Train] epoch:4943, lr:0.000559, total loss:0.000317\n",
            "[Train] epoch:4944, lr:0.000559, total loss:0.000285\n",
            "[Train] epoch:4945, lr:0.000559, total loss:0.000257\n",
            "[Train] epoch:4946, lr:0.000558, total loss:0.000224\n",
            "[Train] epoch:4947, lr:0.000558, total loss:0.000256\n",
            "[Train] epoch:4948, lr:0.000558, total loss:0.000244\n",
            "[Train] epoch:4949, lr:0.000558, total loss:0.000279\n",
            "[Train] epoch:4950, lr:0.000558, total loss:0.000267\n",
            "[Train] epoch:4951, lr:0.000558, total loss:0.000216\n",
            "[Train] epoch:4952, lr:0.000558, total loss:0.000250\n",
            "[Train] epoch:4953, lr:0.000557, total loss:0.000241\n",
            "[Train] epoch:4954, lr:0.000557, total loss:0.000250\n",
            "[Train] epoch:4955, lr:0.000557, total loss:0.000272\n",
            "[Train] epoch:4956, lr:0.000557, total loss:0.000241\n",
            "[Train] epoch:4957, lr:0.000557, total loss:0.000244\n",
            "[Train] epoch:4958, lr:0.000557, total loss:0.000239\n",
            "[Train] epoch:4959, lr:0.000557, total loss:0.000260\n",
            "[Train] epoch:4960, lr:0.000557, total loss:0.000236\n",
            "[Train] epoch:4961, lr:0.000556, total loss:0.000229\n",
            "[Train] epoch:4962, lr:0.000556, total loss:0.000250\n",
            "[Train] epoch:4963, lr:0.000556, total loss:0.000229\n",
            "[Train] epoch:4964, lr:0.000556, total loss:0.000257\n",
            "[Train] epoch:4965, lr:0.000556, total loss:0.000226\n",
            "[Train] epoch:4966, lr:0.000556, total loss:0.000236\n",
            "[Train] epoch:4967, lr:0.000556, total loss:0.000226\n",
            "[Train] epoch:4968, lr:0.000555, total loss:0.000251\n",
            "[Train] epoch:4969, lr:0.000555, total loss:0.000269\n",
            "[Train] epoch:4970, lr:0.000555, total loss:0.000250\n",
            "[Train] epoch:4971, lr:0.000555, total loss:0.000248\n",
            "[Train] epoch:4972, lr:0.000555, total loss:0.000251\n",
            "[Train] epoch:4973, lr:0.000555, total loss:0.000232\n",
            "[Train] epoch:4974, lr:0.000555, total loss:0.000289\n",
            "[Train] epoch:4975, lr:0.000554, total loss:0.000256\n",
            "[Train] epoch:4976, lr:0.000554, total loss:0.000263\n",
            "[Train] epoch:4977, lr:0.000554, total loss:0.000251\n",
            "[Train] epoch:4978, lr:0.000554, total loss:0.000270\n",
            "[Train] epoch:4979, lr:0.000554, total loss:0.000398\n",
            "[Train] epoch:4980, lr:0.000554, total loss:0.000322\n",
            "[Train] epoch:4981, lr:0.000554, total loss:0.000293\n",
            "[Train] epoch:4982, lr:0.000553, total loss:0.000253\n",
            "[Train] epoch:4983, lr:0.000553, total loss:0.000264\n",
            "[Train] epoch:4984, lr:0.000553, total loss:0.000294\n",
            "[Train] epoch:4985, lr:0.000553, total loss:0.000310\n",
            "[Train] epoch:4986, lr:0.000553, total loss:0.000267\n",
            "[Train] epoch:4987, lr:0.000553, total loss:0.000265\n",
            "[Train] epoch:4988, lr:0.000553, total loss:0.000226\n",
            "[Train] epoch:4989, lr:0.000552, total loss:0.000341\n",
            "[Train] epoch:4990, lr:0.000552, total loss:0.000317\n",
            "[Train] epoch:4991, lr:0.000552, total loss:0.000291\n",
            "[Train] epoch:4992, lr:0.000552, total loss:0.000288\n",
            "[Train] epoch:4993, lr:0.000552, total loss:0.000267\n",
            "[Train] epoch:4994, lr:0.000552, total loss:0.000267\n",
            "[Train] epoch:4995, lr:0.000552, total loss:0.000251\n",
            "[Train] epoch:4996, lr:0.000551, total loss:0.000250\n",
            "[Train] epoch:4997, lr:0.000551, total loss:0.000240\n",
            "[Train] epoch:4998, lr:0.000551, total loss:0.000241\n",
            "[Train] epoch:4999, lr:0.000551, total loss:0.000293\n",
            "[Train] epoch:5000, lr:0.000551, total loss:0.000248\n",
            "[Train] epoch:5001, lr:0.000551, total loss:0.000302\n",
            "[Train] epoch:5002, lr:0.000551, total loss:0.000272\n",
            "[Train] epoch:5003, lr:0.000550, total loss:0.000251\n",
            "[Train] epoch:5004, lr:0.000550, total loss:0.000251\n",
            "[Train] epoch:5005, lr:0.000550, total loss:0.000230\n",
            "[Train] epoch:5006, lr:0.000550, total loss:0.000235\n",
            "[Train] epoch:5007, lr:0.000550, total loss:0.000255\n",
            "[Train] epoch:5008, lr:0.000550, total loss:0.000237\n",
            "[Train] epoch:5009, lr:0.000550, total loss:0.000264\n",
            "[Train] epoch:5010, lr:0.000549, total loss:0.000252\n",
            "[Train] epoch:5011, lr:0.000549, total loss:0.000268\n",
            "[Train] epoch:5012, lr:0.000549, total loss:0.000360\n",
            "[Train] epoch:5013, lr:0.000549, total loss:0.000272\n",
            "[Train] epoch:5014, lr:0.000549, total loss:0.000278\n",
            "[Train] epoch:5015, lr:0.000549, total loss:0.000239\n",
            "[Train] epoch:5016, lr:0.000549, total loss:0.000250\n",
            "[Train] epoch:5017, lr:0.000548, total loss:0.000261\n",
            "[Train] epoch:5018, lr:0.000548, total loss:0.000239\n",
            "[Train] epoch:5019, lr:0.000548, total loss:0.000265\n",
            "[Train] epoch:5020, lr:0.000548, total loss:0.000280\n",
            "[Train] epoch:5021, lr:0.000548, total loss:0.000331\n",
            "[Train] epoch:5022, lr:0.000548, total loss:0.000305\n",
            "[Train] epoch:5023, lr:0.000548, total loss:0.000263\n",
            "[Train] epoch:5024, lr:0.000547, total loss:0.000267\n",
            "[Train] epoch:5025, lr:0.000547, total loss:0.000244\n",
            "[Train] epoch:5026, lr:0.000547, total loss:0.000291\n",
            "[Train] epoch:5027, lr:0.000547, total loss:0.000288\n",
            "[Train] epoch:5028, lr:0.000547, total loss:0.000270\n",
            "[Train] epoch:5029, lr:0.000547, total loss:0.000293\n",
            "[Train] epoch:5030, lr:0.000547, total loss:0.000239\n",
            "[Train] epoch:5031, lr:0.000546, total loss:0.000362\n",
            "[Train] epoch:5032, lr:0.000546, total loss:0.000284\n",
            "[Train] epoch:5033, lr:0.000546, total loss:0.000244\n",
            "[Train] epoch:5034, lr:0.000546, total loss:0.000292\n",
            "[Train] epoch:5035, lr:0.000546, total loss:0.000264\n",
            "[Train] epoch:5036, lr:0.000546, total loss:0.000247\n",
            "[Train] epoch:5037, lr:0.000546, total loss:0.000244\n",
            "[Train] epoch:5038, lr:0.000545, total loss:0.000271\n",
            "[Train] epoch:5039, lr:0.000545, total loss:0.000237\n",
            "[Train] epoch:5040, lr:0.000545, total loss:0.000249\n",
            "[Train] epoch:5041, lr:0.000545, total loss:0.000237\n",
            "[Train] epoch:5042, lr:0.000545, total loss:0.000237\n",
            "[Train] epoch:5043, lr:0.000545, total loss:0.000220\n",
            "[Train] epoch:5044, lr:0.000545, total loss:0.000217\n",
            "[Train] epoch:5045, lr:0.000544, total loss:0.000219\n",
            "[Train] epoch:5046, lr:0.000544, total loss:0.000246\n",
            "[Train] epoch:5047, lr:0.000544, total loss:0.000239\n",
            "[Train] epoch:5048, lr:0.000544, total loss:0.000257\n",
            "[Train] epoch:5049, lr:0.000544, total loss:0.000258\n",
            "[Train] epoch:5050, lr:0.000544, total loss:0.000270\n",
            "[Train] epoch:5051, lr:0.000544, total loss:0.000279\n",
            "[Train] epoch:5052, lr:0.000543, total loss:0.000294\n",
            "[Train] epoch:5053, lr:0.000543, total loss:0.000242\n",
            "[Train] epoch:5054, lr:0.000543, total loss:0.000245\n",
            "[Train] epoch:5055, lr:0.000543, total loss:0.000256\n",
            "[Train] epoch:5056, lr:0.000543, total loss:0.000259\n",
            "[Train] epoch:5057, lr:0.000543, total loss:0.000246\n",
            "[Train] epoch:5058, lr:0.000543, total loss:0.000270\n",
            "[Train] epoch:5059, lr:0.000543, total loss:0.000295\n",
            "[Train] epoch:5060, lr:0.000542, total loss:0.000262\n",
            "[Train] epoch:5061, lr:0.000542, total loss:0.000266\n",
            "[Train] epoch:5062, lr:0.000542, total loss:0.000277\n",
            "[Train] epoch:5063, lr:0.000542, total loss:0.000261\n",
            "[Train] epoch:5064, lr:0.000542, total loss:0.000234\n",
            "[Train] epoch:5065, lr:0.000542, total loss:0.000253\n",
            "[Train] epoch:5066, lr:0.000542, total loss:0.000258\n",
            "[Train] epoch:5067, lr:0.000541, total loss:0.000226\n",
            "[Train] epoch:5068, lr:0.000541, total loss:0.000242\n",
            "[Train] epoch:5069, lr:0.000541, total loss:0.000235\n",
            "[Train] epoch:5070, lr:0.000541, total loss:0.000258\n",
            "[Train] epoch:5071, lr:0.000541, total loss:0.000247\n",
            "[Train] epoch:5072, lr:0.000541, total loss:0.000229\n",
            "[Train] epoch:5073, lr:0.000541, total loss:0.000239\n",
            "[Train] epoch:5074, lr:0.000540, total loss:0.000257\n",
            "[Train] epoch:5075, lr:0.000540, total loss:0.000242\n",
            "[Train] epoch:5076, lr:0.000540, total loss:0.000250\n",
            "[Train] epoch:5077, lr:0.000540, total loss:0.000250\n",
            "[Train] epoch:5078, lr:0.000540, total loss:0.000222\n",
            "[Train] epoch:5079, lr:0.000540, total loss:0.000238\n",
            "[Train] epoch:5080, lr:0.000540, total loss:0.000234\n",
            "[Train] epoch:5081, lr:0.000539, total loss:0.000223\n",
            "[Train] epoch:5082, lr:0.000539, total loss:0.000257\n",
            "[Train] epoch:5083, lr:0.000539, total loss:0.000252\n",
            "[Train] epoch:5084, lr:0.000539, total loss:0.000264\n",
            "[Train] epoch:5085, lr:0.000539, total loss:0.000279\n",
            "[Train] epoch:5086, lr:0.000539, total loss:0.000253\n",
            "[Train] epoch:5087, lr:0.000539, total loss:0.000271\n",
            "[Train] epoch:5088, lr:0.000538, total loss:0.000260\n",
            "[Train] epoch:5089, lr:0.000538, total loss:0.000229\n",
            "[Train] epoch:5090, lr:0.000538, total loss:0.000296\n",
            "[Train] epoch:5091, lr:0.000538, total loss:0.000361\n",
            "[Train] epoch:5092, lr:0.000538, total loss:0.000316\n",
            "[Train] epoch:5093, lr:0.000538, total loss:0.000259\n",
            "[Train] epoch:5094, lr:0.000538, total loss:0.000305\n",
            "[Train] epoch:5095, lr:0.000537, total loss:0.000275\n",
            "[Train] epoch:5096, lr:0.000537, total loss:0.000251\n",
            "[Train] epoch:5097, lr:0.000537, total loss:0.000221\n",
            "[Train] epoch:5098, lr:0.000537, total loss:0.000233\n",
            "[Train] epoch:5099, lr:0.000537, total loss:0.000274\n",
            "[Train] epoch:5100, lr:0.000537, total loss:0.000256\n",
            "[Train] epoch:5101, lr:0.000537, total loss:0.000246\n",
            "[Train] epoch:5102, lr:0.000536, total loss:0.000253\n",
            "[Train] epoch:5103, lr:0.000536, total loss:0.000261\n",
            "[Train] epoch:5104, lr:0.000536, total loss:0.000278\n",
            "[Train] epoch:5105, lr:0.000536, total loss:0.000272\n",
            "[Train] epoch:5106, lr:0.000536, total loss:0.000282\n",
            "[Train] epoch:5107, lr:0.000536, total loss:0.000270\n",
            "[Train] epoch:5108, lr:0.000536, total loss:0.000291\n",
            "[Train] epoch:5109, lr:0.000535, total loss:0.000350\n",
            "[Train] epoch:5110, lr:0.000535, total loss:0.000260\n",
            "[Train] epoch:5111, lr:0.000535, total loss:0.000256\n",
            "[Train] epoch:5112, lr:0.000535, total loss:0.000261\n",
            "[Train] epoch:5113, lr:0.000535, total loss:0.000275\n",
            "[Train] epoch:5114, lr:0.000535, total loss:0.000249\n",
            "[Train] epoch:5115, lr:0.000535, total loss:0.000259\n",
            "[Train] epoch:5116, lr:0.000534, total loss:0.000267\n",
            "[Train] epoch:5117, lr:0.000534, total loss:0.000230\n",
            "[Train] epoch:5118, lr:0.000534, total loss:0.000222\n",
            "[Train] epoch:5119, lr:0.000534, total loss:0.000227\n",
            "[Train] epoch:5120, lr:0.000534, total loss:0.000239\n",
            "[Train] epoch:5121, lr:0.000534, total loss:0.000246\n",
            "[Train] epoch:5122, lr:0.000534, total loss:0.000225\n",
            "[Train] epoch:5123, lr:0.000533, total loss:0.000225\n",
            "[Train] epoch:5124, lr:0.000533, total loss:0.000227\n",
            "[Train] epoch:5125, lr:0.000533, total loss:0.000238\n",
            "[Train] epoch:5126, lr:0.000533, total loss:0.000226\n",
            "[Train] epoch:5127, lr:0.000533, total loss:0.000254\n",
            "[Train] epoch:5128, lr:0.000533, total loss:0.000225\n",
            "[Train] epoch:5129, lr:0.000533, total loss:0.000230\n",
            "[Train] epoch:5130, lr:0.000532, total loss:0.000249\n",
            "[Train] epoch:5131, lr:0.000532, total loss:0.000306\n",
            "[Train] epoch:5132, lr:0.000532, total loss:0.000284\n",
            "[Train] epoch:5133, lr:0.000532, total loss:0.000285\n",
            "[Train] epoch:5134, lr:0.000532, total loss:0.000281\n",
            "[Train] epoch:5135, lr:0.000532, total loss:0.000257\n",
            "[Train] epoch:5136, lr:0.000532, total loss:0.000282\n",
            "[Train] epoch:5137, lr:0.000531, total loss:0.000260\n",
            "[Train] epoch:5138, lr:0.000531, total loss:0.000275\n",
            "[Train] epoch:5139, lr:0.000531, total loss:0.000245\n",
            "[Train] epoch:5140, lr:0.000531, total loss:0.000243\n",
            "[Train] epoch:5141, lr:0.000531, total loss:0.000242\n",
            "[Train] epoch:5142, lr:0.000531, total loss:0.000280\n",
            "[Train] epoch:5143, lr:0.000531, total loss:0.000239\n",
            "[Train] epoch:5144, lr:0.000530, total loss:0.000260\n",
            "[Train] epoch:5145, lr:0.000530, total loss:0.000249\n",
            "[Train] epoch:5146, lr:0.000530, total loss:0.000269\n",
            "[Train] epoch:5147, lr:0.000530, total loss:0.000263\n",
            "[Train] epoch:5148, lr:0.000530, total loss:0.000269\n",
            "[Train] epoch:5149, lr:0.000530, total loss:0.000258\n",
            "[Train] epoch:5150, lr:0.000530, total loss:0.000252\n",
            "[Train] epoch:5151, lr:0.000529, total loss:0.000221\n",
            "[Train] epoch:5152, lr:0.000529, total loss:0.000228\n",
            "[Train] epoch:5153, lr:0.000529, total loss:0.000245\n",
            "[Train] epoch:5154, lr:0.000529, total loss:0.000233\n",
            "[Train] epoch:5155, lr:0.000529, total loss:0.000258\n",
            "[Train] epoch:5156, lr:0.000529, total loss:0.000254\n",
            "[Train] epoch:5157, lr:0.000529, total loss:0.000245\n",
            "[Train] epoch:5158, lr:0.000528, total loss:0.000243\n",
            "[Train] epoch:5159, lr:0.000528, total loss:0.000248\n",
            "[Train] epoch:5160, lr:0.000528, total loss:0.000235\n",
            "[Train] epoch:5161, lr:0.000528, total loss:0.000271\n",
            "[Train] epoch:5162, lr:0.000528, total loss:0.000217\n",
            "[Train] epoch:5163, lr:0.000528, total loss:0.000286\n",
            "[Train] epoch:5164, lr:0.000528, total loss:0.000250\n",
            "[Train] epoch:5165, lr:0.000528, total loss:0.000257\n",
            "[Train] epoch:5166, lr:0.000527, total loss:0.000301\n",
            "[Train] epoch:5167, lr:0.000527, total loss:0.000255\n",
            "[Train] epoch:5168, lr:0.000527, total loss:0.000254\n",
            "[Train] epoch:5169, lr:0.000527, total loss:0.000248\n",
            "[Train] epoch:5170, lr:0.000527, total loss:0.000222\n",
            "[Train] epoch:5171, lr:0.000527, total loss:0.000231\n",
            "[Train] epoch:5172, lr:0.000527, total loss:0.000260\n",
            "[Train] epoch:5173, lr:0.000526, total loss:0.000239\n",
            "[Train] epoch:5174, lr:0.000526, total loss:0.000231\n",
            "[Train] epoch:5175, lr:0.000526, total loss:0.000246\n",
            "[Train] epoch:5176, lr:0.000526, total loss:0.000246\n",
            "[Train] epoch:5177, lr:0.000526, total loss:0.000217\n",
            "[Train] epoch:5178, lr:0.000526, total loss:0.000227\n",
            "[Train] epoch:5179, lr:0.000526, total loss:0.000257\n",
            "[Train] epoch:5180, lr:0.000525, total loss:0.000296\n",
            "[Train] epoch:5181, lr:0.000525, total loss:0.000245\n",
            "[Train] epoch:5182, lr:0.000525, total loss:0.000259\n",
            "[Train] epoch:5183, lr:0.000525, total loss:0.000256\n",
            "[Train] epoch:5184, lr:0.000525, total loss:0.000257\n",
            "[Train] epoch:5185, lr:0.000525, total loss:0.000255\n",
            "[Train] epoch:5186, lr:0.000525, total loss:0.000259\n",
            "[Train] epoch:5187, lr:0.000524, total loss:0.000251\n",
            "[Train] epoch:5188, lr:0.000524, total loss:0.000249\n",
            "[Train] epoch:5189, lr:0.000524, total loss:0.000248\n",
            "[Train] epoch:5190, lr:0.000524, total loss:0.000246\n",
            "[Train] epoch:5191, lr:0.000524, total loss:0.000238\n",
            "[Train] epoch:5192, lr:0.000524, total loss:0.000246\n",
            "[Train] epoch:5193, lr:0.000524, total loss:0.000249\n",
            "[Train] epoch:5194, lr:0.000523, total loss:0.000240\n",
            "[Train] epoch:5195, lr:0.000523, total loss:0.000256\n",
            "[Train] epoch:5196, lr:0.000523, total loss:0.000240\n",
            "[Train] epoch:5197, lr:0.000523, total loss:0.000266\n",
            "[Train] epoch:5198, lr:0.000523, total loss:0.000308\n",
            "[Train] epoch:5199, lr:0.000523, total loss:0.000278\n",
            "[Train] epoch:5200, lr:0.000523, total loss:0.000286\n",
            "[Train] epoch:5201, lr:0.000522, total loss:0.000281\n",
            "[Train] epoch:5202, lr:0.000522, total loss:0.000284\n",
            "[Train] epoch:5203, lr:0.000522, total loss:0.000252\n",
            "[Train] epoch:5204, lr:0.000522, total loss:0.000255\n",
            "[Train] epoch:5205, lr:0.000522, total loss:0.000249\n",
            "[Train] epoch:5206, lr:0.000522, total loss:0.000233\n",
            "[Train] epoch:5207, lr:0.000522, total loss:0.000245\n",
            "[Train] epoch:5208, lr:0.000521, total loss:0.000238\n",
            "[Train] epoch:5209, lr:0.000521, total loss:0.000236\n",
            "[Train] epoch:5210, lr:0.000521, total loss:0.000254\n",
            "[Train] epoch:5211, lr:0.000521, total loss:0.000235\n",
            "[Train] epoch:5212, lr:0.000521, total loss:0.000287\n",
            "[Train] epoch:5213, lr:0.000521, total loss:0.000250\n",
            "[Train] epoch:5214, lr:0.000521, total loss:0.000254\n",
            "[Train] epoch:5215, lr:0.000520, total loss:0.000246\n",
            "[Train] epoch:5216, lr:0.000520, total loss:0.000254\n",
            "[Train] epoch:5217, lr:0.000520, total loss:0.000253\n",
            "[Train] epoch:5218, lr:0.000520, total loss:0.000231\n",
            "[Train] epoch:5219, lr:0.000520, total loss:0.000269\n",
            "[Train] epoch:5220, lr:0.000520, total loss:0.000300\n",
            "[Train] epoch:5221, lr:0.000520, total loss:0.000254\n",
            "[Train] epoch:5222, lr:0.000519, total loss:0.000242\n",
            "[Train] epoch:5223, lr:0.000519, total loss:0.000243\n",
            "[Train] epoch:5224, lr:0.000519, total loss:0.000236\n",
            "[Train] epoch:5225, lr:0.000519, total loss:0.000241\n",
            "[Train] epoch:5226, lr:0.000519, total loss:0.000204\n",
            "[Train] epoch:5227, lr:0.000519, total loss:0.000262\n",
            "[Train] epoch:5228, lr:0.000519, total loss:0.000247\n",
            "[Train] epoch:5229, lr:0.000518, total loss:0.000248\n",
            "[Train] epoch:5230, lr:0.000518, total loss:0.000267\n",
            "[Train] epoch:5231, lr:0.000518, total loss:0.000259\n",
            "[Train] epoch:5232, lr:0.000518, total loss:0.000269\n",
            "[Train] epoch:5233, lr:0.000518, total loss:0.000284\n",
            "[Train] epoch:5234, lr:0.000518, total loss:0.000296\n",
            "[Train] epoch:5235, lr:0.000518, total loss:0.000290\n",
            "[Train] epoch:5236, lr:0.000517, total loss:0.000269\n",
            "[Train] epoch:5237, lr:0.000517, total loss:0.000241\n",
            "[Train] epoch:5238, lr:0.000517, total loss:0.000255\n",
            "[Train] epoch:5239, lr:0.000517, total loss:0.000227\n",
            "[Train] epoch:5240, lr:0.000517, total loss:0.000228\n",
            "[Train] epoch:5241, lr:0.000517, total loss:0.000240\n",
            "[Train] epoch:5242, lr:0.000517, total loss:0.000219\n",
            "[Train] epoch:5243, lr:0.000516, total loss:0.000240\n",
            "[Train] epoch:5244, lr:0.000516, total loss:0.000252\n",
            "[Train] epoch:5245, lr:0.000516, total loss:0.000252\n",
            "[Train] epoch:5246, lr:0.000516, total loss:0.000241\n",
            "[Train] epoch:5247, lr:0.000516, total loss:0.000238\n",
            "[Train] epoch:5248, lr:0.000516, total loss:0.000230\n",
            "[Train] epoch:5249, lr:0.000516, total loss:0.000235\n",
            "[Train] epoch:5250, lr:0.000516, total loss:0.000280\n",
            "[Train] epoch:5251, lr:0.000515, total loss:0.000232\n",
            "[Train] epoch:5252, lr:0.000515, total loss:0.000324\n",
            "[Train] epoch:5253, lr:0.000515, total loss:0.000234\n",
            "[Train] epoch:5254, lr:0.000515, total loss:0.000250\n",
            "[Train] epoch:5255, lr:0.000515, total loss:0.000228\n",
            "[Train] epoch:5256, lr:0.000515, total loss:0.000247\n",
            "[Train] epoch:5257, lr:0.000515, total loss:0.000264\n",
            "[Train] epoch:5258, lr:0.000514, total loss:0.000351\n",
            "[Train] epoch:5259, lr:0.000514, total loss:0.000335\n",
            "[Train] epoch:5260, lr:0.000514, total loss:0.000282\n",
            "[Train] epoch:5261, lr:0.000514, total loss:0.000282\n",
            "[Train] epoch:5262, lr:0.000514, total loss:0.000277\n",
            "[Train] epoch:5263, lr:0.000514, total loss:0.000304\n",
            "[Train] epoch:5264, lr:0.000514, total loss:0.000279\n",
            "[Train] epoch:5265, lr:0.000513, total loss:0.000242\n",
            "[Train] epoch:5266, lr:0.000513, total loss:0.000270\n",
            "[Train] epoch:5267, lr:0.000513, total loss:0.000249\n",
            "[Train] epoch:5268, lr:0.000513, total loss:0.000288\n",
            "[Train] epoch:5269, lr:0.000513, total loss:0.000279\n",
            "[Train] epoch:5270, lr:0.000513, total loss:0.000264\n",
            "[Train] epoch:5271, lr:0.000513, total loss:0.000275\n",
            "[Train] epoch:5272, lr:0.000512, total loss:0.000249\n",
            "[Train] epoch:5273, lr:0.000512, total loss:0.000256\n",
            "[Train] epoch:5274, lr:0.000512, total loss:0.000258\n",
            "[Train] epoch:5275, lr:0.000512, total loss:0.000234\n",
            "[Train] epoch:5276, lr:0.000512, total loss:0.000256\n",
            "[Train] epoch:5277, lr:0.000512, total loss:0.000253\n",
            "[Train] epoch:5278, lr:0.000512, total loss:0.000273\n",
            "[Train] epoch:5279, lr:0.000511, total loss:0.000246\n",
            "[Train] epoch:5280, lr:0.000511, total loss:0.000255\n",
            "[Train] epoch:5281, lr:0.000511, total loss:0.000234\n",
            "[Train] epoch:5282, lr:0.000511, total loss:0.000248\n",
            "[Train] epoch:5283, lr:0.000511, total loss:0.000276\n",
            "[Train] epoch:5284, lr:0.000511, total loss:0.000252\n",
            "[Train] epoch:5285, lr:0.000511, total loss:0.000256\n",
            "[Train] epoch:5286, lr:0.000510, total loss:0.000250\n",
            "[Train] epoch:5287, lr:0.000510, total loss:0.000253\n",
            "[Train] epoch:5288, lr:0.000510, total loss:0.000240\n",
            "[Train] epoch:5289, lr:0.000510, total loss:0.000274\n",
            "[Train] epoch:5290, lr:0.000510, total loss:0.000256\n",
            "[Train] epoch:5291, lr:0.000510, total loss:0.000224\n",
            "[Train] epoch:5292, lr:0.000510, total loss:0.000412\n",
            "[Train] epoch:5293, lr:0.000509, total loss:0.000338\n",
            "[Train] epoch:5294, lr:0.000509, total loss:0.000381\n",
            "[Train] epoch:5295, lr:0.000509, total loss:0.000335\n",
            "[Train] epoch:5296, lr:0.000509, total loss:0.000301\n",
            "[Train] epoch:5297, lr:0.000509, total loss:0.000295\n",
            "[Train] epoch:5298, lr:0.000509, total loss:0.000276\n",
            "[Train] epoch:5299, lr:0.000509, total loss:0.000249\n",
            "[Train] epoch:5300, lr:0.000508, total loss:0.000217\n",
            "[Train] epoch:5301, lr:0.000508, total loss:0.000239\n",
            "[Train] epoch:5302, lr:0.000508, total loss:0.000229\n",
            "[Train] epoch:5303, lr:0.000508, total loss:0.000274\n",
            "[Train] epoch:5304, lr:0.000508, total loss:0.000242\n",
            "[Train] epoch:5305, lr:0.000508, total loss:0.000228\n",
            "[Train] epoch:5306, lr:0.000508, total loss:0.000271\n",
            "[Train] epoch:5307, lr:0.000507, total loss:0.000244\n",
            "[Train] epoch:5308, lr:0.000507, total loss:0.000243\n",
            "[Train] epoch:5309, lr:0.000507, total loss:0.000259\n",
            "[Train] epoch:5310, lr:0.000507, total loss:0.000229\n",
            "[Train] epoch:5311, lr:0.000507, total loss:0.000242\n",
            "[Train] epoch:5312, lr:0.000507, total loss:0.000269\n",
            "[Train] epoch:5313, lr:0.000507, total loss:0.000285\n",
            "[Train] epoch:5314, lr:0.000506, total loss:0.000223\n",
            "[Train] epoch:5315, lr:0.000506, total loss:0.000251\n",
            "[Train] epoch:5316, lr:0.000506, total loss:0.000362\n",
            "[Train] epoch:5317, lr:0.000506, total loss:0.000318\n",
            "[Train] epoch:5318, lr:0.000506, total loss:0.000338\n",
            "[Train] epoch:5319, lr:0.000506, total loss:0.000298\n",
            "[Train] epoch:5320, lr:0.000506, total loss:0.000288\n",
            "[Train] epoch:5321, lr:0.000505, total loss:0.000277\n",
            "[Train] epoch:5322, lr:0.000505, total loss:0.000259\n",
            "[Train] epoch:5323, lr:0.000505, total loss:0.000265\n",
            "[Train] epoch:5324, lr:0.000505, total loss:0.000249\n",
            "[Train] epoch:5325, lr:0.000505, total loss:0.000249\n",
            "[Train] epoch:5326, lr:0.000505, total loss:0.000263\n",
            "[Train] epoch:5327, lr:0.000505, total loss:0.000260\n",
            "[Train] epoch:5328, lr:0.000505, total loss:0.000262\n",
            "[Train] epoch:5329, lr:0.000504, total loss:0.000254\n",
            "[Train] epoch:5330, lr:0.000504, total loss:0.000243\n",
            "[Train] epoch:5331, lr:0.000504, total loss:0.000264\n",
            "[Train] epoch:5332, lr:0.000504, total loss:0.000255\n",
            "[Train] epoch:5333, lr:0.000504, total loss:0.000254\n",
            "[Train] epoch:5334, lr:0.000504, total loss:0.000259\n",
            "[Train] epoch:5335, lr:0.000504, total loss:0.000280\n",
            "[Train] epoch:5336, lr:0.000503, total loss:0.000259\n",
            "[Train] epoch:5337, lr:0.000503, total loss:0.000303\n",
            "[Train] epoch:5338, lr:0.000503, total loss:0.000252\n",
            "[Train] epoch:5339, lr:0.000503, total loss:0.000248\n",
            "[Train] epoch:5340, lr:0.000503, total loss:0.000253\n",
            "[Train] epoch:5341, lr:0.000503, total loss:0.000233\n",
            "[Train] epoch:5342, lr:0.000503, total loss:0.000280\n",
            "[Train] epoch:5343, lr:0.000502, total loss:0.000257\n",
            "[Train] epoch:5344, lr:0.000502, total loss:0.000260\n",
            "[Train] epoch:5345, lr:0.000502, total loss:0.000242\n",
            "[Train] epoch:5346, lr:0.000502, total loss:0.000223\n",
            "[Train] epoch:5347, lr:0.000502, total loss:0.000224\n",
            "[Train] epoch:5348, lr:0.000502, total loss:0.000226\n",
            "[Train] epoch:5349, lr:0.000502, total loss:0.000210\n",
            "[Train] epoch:5350, lr:0.000501, total loss:0.000254\n",
            "[Train] epoch:5351, lr:0.000501, total loss:0.000253\n",
            "[Train] epoch:5352, lr:0.000501, total loss:0.000235\n",
            "[Train] epoch:5353, lr:0.000501, total loss:0.000230\n",
            "[Train] epoch:5354, lr:0.000501, total loss:0.000247\n",
            "[Train] epoch:5355, lr:0.000501, total loss:0.000256\n",
            "[Train] epoch:5356, lr:0.000501, total loss:0.000228\n",
            "[Train] epoch:5357, lr:0.000500, total loss:0.000225\n",
            "[Train] epoch:5358, lr:0.000500, total loss:0.000264\n",
            "[Train] epoch:5359, lr:0.000500, total loss:0.000259\n",
            "[Train] epoch:5360, lr:0.000500, total loss:0.000235\n",
            "[Train] epoch:5361, lr:0.000500, total loss:0.000247\n",
            "[Train] epoch:5362, lr:0.000500, total loss:0.000248\n",
            "[Train] epoch:5363, lr:0.000500, total loss:0.000230\n",
            "[Train] epoch:5364, lr:0.000499, total loss:0.000260\n",
            "[Train] epoch:5365, lr:0.000499, total loss:0.000234\n",
            "[Train] epoch:5366, lr:0.000499, total loss:0.000231\n",
            "[Train] epoch:5367, lr:0.000499, total loss:0.000238\n",
            "[Train] epoch:5368, lr:0.000499, total loss:0.000250\n",
            "[Train] epoch:5369, lr:0.000499, total loss:0.000231\n",
            "[Train] epoch:5370, lr:0.000499, total loss:0.000236\n",
            "[Train] epoch:5371, lr:0.000498, total loss:0.000223\n",
            "[Train] epoch:5372, lr:0.000498, total loss:0.000300\n",
            "[Train] epoch:5373, lr:0.000498, total loss:0.000263\n",
            "[Train] epoch:5374, lr:0.000498, total loss:0.000233\n",
            "[Train] epoch:5375, lr:0.000498, total loss:0.000243\n",
            "[Train] epoch:5376, lr:0.000498, total loss:0.000237\n",
            "[Train] epoch:5377, lr:0.000498, total loss:0.000316\n",
            "[Train] epoch:5378, lr:0.000497, total loss:0.000235\n",
            "[Train] epoch:5379, lr:0.000497, total loss:0.000232\n",
            "[Train] epoch:5380, lr:0.000497, total loss:0.000240\n",
            "[Train] epoch:5381, lr:0.000497, total loss:0.000286\n",
            "[Train] epoch:5382, lr:0.000497, total loss:0.000229\n",
            "[Train] epoch:5383, lr:0.000497, total loss:0.000231\n",
            "[Train] epoch:5384, lr:0.000497, total loss:0.000221\n",
            "[Train] epoch:5385, lr:0.000496, total loss:0.000234\n",
            "[Train] epoch:5386, lr:0.000496, total loss:0.000246\n",
            "[Train] epoch:5387, lr:0.000496, total loss:0.000241\n",
            "[Train] epoch:5388, lr:0.000496, total loss:0.000246\n",
            "[Train] epoch:5389, lr:0.000496, total loss:0.000240\n",
            "[Train] epoch:5390, lr:0.000496, total loss:0.000247\n",
            "[Train] epoch:5391, lr:0.000496, total loss:0.000224\n",
            "[Train] epoch:5392, lr:0.000496, total loss:0.000232\n",
            "[Train] epoch:5393, lr:0.000495, total loss:0.000244\n",
            "[Train] epoch:5394, lr:0.000495, total loss:0.000316\n",
            "[Train] epoch:5395, lr:0.000495, total loss:0.000308\n",
            "[Train] epoch:5396, lr:0.000495, total loss:0.000277\n",
            "[Train] epoch:5397, lr:0.000495, total loss:0.000229\n",
            "[Train] epoch:5398, lr:0.000495, total loss:0.000234\n",
            "[Train] epoch:5399, lr:0.000495, total loss:0.000217\n",
            "[Train] epoch:5400, lr:0.000494, total loss:0.000228\n",
            "[Train] epoch:5401, lr:0.000494, total loss:0.000238\n",
            "[Train] epoch:5402, lr:0.000494, total loss:0.000249\n",
            "[Train] epoch:5403, lr:0.000494, total loss:0.000243\n",
            "[Train] epoch:5404, lr:0.000494, total loss:0.000248\n",
            "[Train] epoch:5405, lr:0.000494, total loss:0.000291\n",
            "[Train] epoch:5406, lr:0.000494, total loss:0.000269\n",
            "[Train] epoch:5407, lr:0.000493, total loss:0.000298\n",
            "[Train] epoch:5408, lr:0.000493, total loss:0.000277\n",
            "[Train] epoch:5409, lr:0.000493, total loss:0.000269\n",
            "[Train] epoch:5410, lr:0.000493, total loss:0.000254\n",
            "[Train] epoch:5411, lr:0.000493, total loss:0.000235\n",
            "[Train] epoch:5412, lr:0.000493, total loss:0.000245\n",
            "[Train] epoch:5413, lr:0.000493, total loss:0.000206\n",
            "[Train] epoch:5414, lr:0.000492, total loss:0.000269\n",
            "[Train] epoch:5415, lr:0.000492, total loss:0.000232\n",
            "[Train] epoch:5416, lr:0.000492, total loss:0.000239\n",
            "[Train] epoch:5417, lr:0.000492, total loss:0.000223\n",
            "[Train] epoch:5418, lr:0.000492, total loss:0.000346\n",
            "[Train] epoch:5419, lr:0.000492, total loss:0.000257\n",
            "[Train] epoch:5420, lr:0.000492, total loss:0.000238\n",
            "[Train] epoch:5421, lr:0.000491, total loss:0.000260\n",
            "[Train] epoch:5422, lr:0.000491, total loss:0.000231\n",
            "[Train] epoch:5423, lr:0.000491, total loss:0.000249\n",
            "[Train] epoch:5424, lr:0.000491, total loss:0.000223\n",
            "[Train] epoch:5425, lr:0.000491, total loss:0.000242\n",
            "[Train] epoch:5426, lr:0.000491, total loss:0.000261\n",
            "[Train] epoch:5427, lr:0.000491, total loss:0.000248\n",
            "[Train] epoch:5428, lr:0.000490, total loss:0.000242\n",
            "[Train] epoch:5429, lr:0.000490, total loss:0.000257\n",
            "[Train] epoch:5430, lr:0.000490, total loss:0.000277\n",
            "[Train] epoch:5431, lr:0.000490, total loss:0.000221\n",
            "[Train] epoch:5432, lr:0.000490, total loss:0.000246\n",
            "[Train] epoch:5433, lr:0.000490, total loss:0.000267\n",
            "[Train] epoch:5434, lr:0.000490, total loss:0.000257\n",
            "[Train] epoch:5435, lr:0.000489, total loss:0.000256\n",
            "[Train] epoch:5436, lr:0.000489, total loss:0.000225\n",
            "[Train] epoch:5437, lr:0.000489, total loss:0.000255\n",
            "[Train] epoch:5438, lr:0.000489, total loss:0.000268\n",
            "[Train] epoch:5439, lr:0.000489, total loss:0.000226\n",
            "[Train] epoch:5440, lr:0.000489, total loss:0.000396\n",
            "[Train] epoch:5441, lr:0.000489, total loss:0.000292\n",
            "[Train] epoch:5442, lr:0.000488, total loss:0.000273\n",
            "[Train] epoch:5443, lr:0.000488, total loss:0.000265\n",
            "[Train] epoch:5444, lr:0.000488, total loss:0.000245\n",
            "[Train] epoch:5445, lr:0.000488, total loss:0.000273\n",
            "[Train] epoch:5446, lr:0.000488, total loss:0.000365\n",
            "[Train] epoch:5447, lr:0.000488, total loss:0.000322\n",
            "[Train] epoch:5448, lr:0.000488, total loss:0.000309\n",
            "[Train] epoch:5449, lr:0.000488, total loss:0.000324\n",
            "[Train] epoch:5450, lr:0.000487, total loss:0.000303\n",
            "[Train] epoch:5451, lr:0.000487, total loss:0.000282\n",
            "[Train] epoch:5452, lr:0.000487, total loss:0.000315\n",
            "[Train] epoch:5453, lr:0.000487, total loss:0.000256\n",
            "[Train] epoch:5454, lr:0.000487, total loss:0.000237\n",
            "[Train] epoch:5455, lr:0.000487, total loss:0.000246\n",
            "[Train] epoch:5456, lr:0.000487, total loss:0.000239\n",
            "[Train] epoch:5457, lr:0.000486, total loss:0.000297\n",
            "[Train] epoch:5458, lr:0.000486, total loss:0.000222\n",
            "[Train] epoch:5459, lr:0.000486, total loss:0.000224\n",
            "[Train] epoch:5460, lr:0.000486, total loss:0.000222\n",
            "[Train] epoch:5461, lr:0.000486, total loss:0.000229\n",
            "[Train] epoch:5462, lr:0.000486, total loss:0.000212\n",
            "[Train] epoch:5463, lr:0.000486, total loss:0.000233\n",
            "[Train] epoch:5464, lr:0.000485, total loss:0.000244\n",
            "[Train] epoch:5465, lr:0.000485, total loss:0.000231\n",
            "[Train] epoch:5466, lr:0.000485, total loss:0.000212\n",
            "[Train] epoch:5467, lr:0.000485, total loss:0.000219\n",
            "[Train] epoch:5468, lr:0.000485, total loss:0.000229\n",
            "[Train] epoch:5469, lr:0.000485, total loss:0.000273\n",
            "[Train] epoch:5470, lr:0.000485, total loss:0.000228\n",
            "[Train] epoch:5471, lr:0.000484, total loss:0.000228\n",
            "[Train] epoch:5472, lr:0.000484, total loss:0.000229\n",
            "[Train] epoch:5473, lr:0.000484, total loss:0.000254\n",
            "[Train] epoch:5474, lr:0.000484, total loss:0.000227\n",
            "[Train] epoch:5475, lr:0.000484, total loss:0.000231\n",
            "[Train] epoch:5476, lr:0.000484, total loss:0.000229\n",
            "[Train] epoch:5477, lr:0.000484, total loss:0.000231\n",
            "[Train] epoch:5478, lr:0.000483, total loss:0.000221\n",
            "[Train] epoch:5479, lr:0.000483, total loss:0.000241\n",
            "[Train] epoch:5480, lr:0.000483, total loss:0.000222\n",
            "[Train] epoch:5481, lr:0.000483, total loss:0.000219\n",
            "[Train] epoch:5482, lr:0.000483, total loss:0.000255\n",
            "[Train] epoch:5483, lr:0.000483, total loss:0.000284\n",
            "[Train] epoch:5484, lr:0.000483, total loss:0.000250\n",
            "[Train] epoch:5485, lr:0.000482, total loss:0.000239\n",
            "[Train] epoch:5486, lr:0.000482, total loss:0.000244\n",
            "[Train] epoch:5487, lr:0.000482, total loss:0.000253\n",
            "[Train] epoch:5488, lr:0.000482, total loss:0.000234\n",
            "[Train] epoch:5489, lr:0.000482, total loss:0.000223\n",
            "[Train] epoch:5490, lr:0.000482, total loss:0.000241\n",
            "[Train] epoch:5491, lr:0.000482, total loss:0.000238\n",
            "[Train] epoch:5492, lr:0.000481, total loss:0.000243\n",
            "[Train] epoch:5493, lr:0.000481, total loss:0.000319\n",
            "[Train] epoch:5494, lr:0.000481, total loss:0.000238\n",
            "[Train] epoch:5495, lr:0.000481, total loss:0.000216\n",
            "[Train] epoch:5496, lr:0.000481, total loss:0.000241\n",
            "[Train] epoch:5497, lr:0.000481, total loss:0.000236\n",
            "[Train] epoch:5498, lr:0.000481, total loss:0.000243\n",
            "[Train] epoch:5499, lr:0.000481, total loss:0.000239\n",
            "[Train] epoch:5500, lr:0.000480, total loss:0.000244\n",
            "[Train] epoch:5501, lr:0.000480, total loss:0.000239\n",
            "[Train] epoch:5502, lr:0.000480, total loss:0.000262\n",
            "[Train] epoch:5503, lr:0.000480, total loss:0.000260\n",
            "[Train] epoch:5504, lr:0.000480, total loss:0.000212\n",
            "[Train] epoch:5505, lr:0.000480, total loss:0.000246\n",
            "[Train] epoch:5506, lr:0.000480, total loss:0.000238\n",
            "[Train] epoch:5507, lr:0.000479, total loss:0.000231\n",
            "[Train] epoch:5508, lr:0.000479, total loss:0.000245\n",
            "[Train] epoch:5509, lr:0.000479, total loss:0.000228\n",
            "[Train] epoch:5510, lr:0.000479, total loss:0.000318\n",
            "[Train] epoch:5511, lr:0.000479, total loss:0.000233\n",
            "[Train] epoch:5512, lr:0.000479, total loss:0.000237\n",
            "[Train] epoch:5513, lr:0.000479, total loss:0.000234\n",
            "[Train] epoch:5514, lr:0.000478, total loss:0.000264\n",
            "[Train] epoch:5515, lr:0.000478, total loss:0.000298\n",
            "[Train] epoch:5516, lr:0.000478, total loss:0.000286\n",
            "[Train] epoch:5517, lr:0.000478, total loss:0.000264\n",
            "[Train] epoch:5518, lr:0.000478, total loss:0.000248\n",
            "[Train] epoch:5519, lr:0.000478, total loss:0.000276\n",
            "[Train] epoch:5520, lr:0.000478, total loss:0.000249\n",
            "[Train] epoch:5521, lr:0.000477, total loss:0.000256\n",
            "[Train] epoch:5522, lr:0.000477, total loss:0.000272\n",
            "[Train] epoch:5523, lr:0.000477, total loss:0.000230\n",
            "[Train] epoch:5524, lr:0.000477, total loss:0.000250\n",
            "[Train] epoch:5525, lr:0.000477, total loss:0.000247\n",
            "[Train] epoch:5526, lr:0.000477, total loss:0.000254\n",
            "[Train] epoch:5527, lr:0.000477, total loss:0.000255\n",
            "[Train] epoch:5528, lr:0.000476, total loss:0.000257\n",
            "[Train] epoch:5529, lr:0.000476, total loss:0.000233\n",
            "[Train] epoch:5530, lr:0.000476, total loss:0.000271\n",
            "[Train] epoch:5531, lr:0.000476, total loss:0.000250\n",
            "[Train] epoch:5532, lr:0.000476, total loss:0.000234\n",
            "[Train] epoch:5533, lr:0.000476, total loss:0.000241\n",
            "[Train] epoch:5534, lr:0.000476, total loss:0.000279\n",
            "[Train] epoch:5535, lr:0.000475, total loss:0.000261\n",
            "[Train] epoch:5536, lr:0.000475, total loss:0.000266\n",
            "[Train] epoch:5537, lr:0.000475, total loss:0.000240\n",
            "[Train] epoch:5538, lr:0.000475, total loss:0.000247\n",
            "[Train] epoch:5539, lr:0.000475, total loss:0.000242\n",
            "[Train] epoch:5540, lr:0.000475, total loss:0.000211\n",
            "[Train] epoch:5541, lr:0.000475, total loss:0.000206\n",
            "[Train] epoch:5542, lr:0.000475, total loss:0.000247\n",
            "[Train] epoch:5543, lr:0.000474, total loss:0.000240\n",
            "[Train] epoch:5544, lr:0.000474, total loss:0.000230\n",
            "[Train] epoch:5545, lr:0.000474, total loss:0.000213\n",
            "[Train] epoch:5546, lr:0.000474, total loss:0.000259\n",
            "[Train] epoch:5547, lr:0.000474, total loss:0.000222\n",
            "[Train] epoch:5548, lr:0.000474, total loss:0.000252\n",
            "[Train] epoch:5549, lr:0.000474, total loss:0.000317\n",
            "[Train] epoch:5550, lr:0.000473, total loss:0.000291\n",
            "[Train] epoch:5551, lr:0.000473, total loss:0.000287\n",
            "[Train] epoch:5552, lr:0.000473, total loss:0.000260\n",
            "[Train] epoch:5553, lr:0.000473, total loss:0.000280\n",
            "[Train] epoch:5554, lr:0.000473, total loss:0.000269\n",
            "[Train] epoch:5555, lr:0.000473, total loss:0.000239\n",
            "[Train] epoch:5556, lr:0.000473, total loss:0.000246\n",
            "[Train] epoch:5557, lr:0.000472, total loss:0.000331\n",
            "[Train] epoch:5558, lr:0.000472, total loss:0.000249\n",
            "[Train] epoch:5559, lr:0.000472, total loss:0.000291\n",
            "[Train] epoch:5560, lr:0.000472, total loss:0.000269\n",
            "[Train] epoch:5561, lr:0.000472, total loss:0.000253\n",
            "[Train] epoch:5562, lr:0.000472, total loss:0.000243\n",
            "[Train] epoch:5563, lr:0.000472, total loss:0.000287\n",
            "[Train] epoch:5564, lr:0.000471, total loss:0.000219\n",
            "[Train] epoch:5565, lr:0.000471, total loss:0.000256\n",
            "[Train] epoch:5566, lr:0.000471, total loss:0.000248\n",
            "[Train] epoch:5567, lr:0.000471, total loss:0.000262\n",
            "[Train] epoch:5568, lr:0.000471, total loss:0.000228\n",
            "[Train] epoch:5569, lr:0.000471, total loss:0.000223\n",
            "[Train] epoch:5570, lr:0.000471, total loss:0.000245\n",
            "[Train] epoch:5571, lr:0.000470, total loss:0.000225\n",
            "[Train] epoch:5572, lr:0.000470, total loss:0.000223\n",
            "[Train] epoch:5573, lr:0.000470, total loss:0.000260\n",
            "[Train] epoch:5574, lr:0.000470, total loss:0.000260\n",
            "[Train] epoch:5575, lr:0.000470, total loss:0.000227\n",
            "[Train] epoch:5576, lr:0.000470, total loss:0.000252\n",
            "[Train] epoch:5577, lr:0.000470, total loss:0.000272\n",
            "[Train] epoch:5578, lr:0.000469, total loss:0.000278\n",
            "[Train] epoch:5579, lr:0.000469, total loss:0.000271\n",
            "[Train] epoch:5580, lr:0.000469, total loss:0.000241\n",
            "[Train] epoch:5581, lr:0.000469, total loss:0.000240\n",
            "[Train] epoch:5582, lr:0.000469, total loss:0.000308\n",
            "[Train] epoch:5583, lr:0.000469, total loss:0.000223\n",
            "[Train] epoch:5584, lr:0.000469, total loss:0.000216\n",
            "[Train] epoch:5585, lr:0.000469, total loss:0.000277\n",
            "[Train] epoch:5586, lr:0.000468, total loss:0.000232\n",
            "[Train] epoch:5587, lr:0.000468, total loss:0.000272\n",
            "[Train] epoch:5588, lr:0.000468, total loss:0.000290\n",
            "[Train] epoch:5589, lr:0.000468, total loss:0.000258\n",
            "[Train] epoch:5590, lr:0.000468, total loss:0.000231\n",
            "[Train] epoch:5591, lr:0.000468, total loss:0.000219\n",
            "[Train] epoch:5592, lr:0.000468, total loss:0.000444\n",
            "[Train] epoch:5593, lr:0.000467, total loss:0.000354\n",
            "[Train] epoch:5594, lr:0.000467, total loss:0.000285\n",
            "[Train] epoch:5595, lr:0.000467, total loss:0.000258\n",
            "[Train] epoch:5596, lr:0.000467, total loss:0.000241\n",
            "[Train] epoch:5597, lr:0.000467, total loss:0.000225\n",
            "[Train] epoch:5598, lr:0.000467, total loss:0.000256\n",
            "[Train] epoch:5599, lr:0.000467, total loss:0.000231\n",
            "[Train] epoch:5600, lr:0.000466, total loss:0.000263\n",
            "[Train] epoch:5601, lr:0.000466, total loss:0.000228\n",
            "[Train] epoch:5602, lr:0.000466, total loss:0.000274\n",
            "[Train] epoch:5603, lr:0.000466, total loss:0.000224\n",
            "[Train] epoch:5604, lr:0.000466, total loss:0.000232\n",
            "[Train] epoch:5605, lr:0.000466, total loss:0.000242\n",
            "[Train] epoch:5606, lr:0.000466, total loss:0.000228\n",
            "[Train] epoch:5607, lr:0.000465, total loss:0.000240\n",
            "[Train] epoch:5608, lr:0.000465, total loss:0.000260\n",
            "[Train] epoch:5609, lr:0.000465, total loss:0.000241\n",
            "[Train] epoch:5610, lr:0.000465, total loss:0.000258\n",
            "[Train] epoch:5611, lr:0.000465, total loss:0.000237\n",
            "[Train] epoch:5612, lr:0.000465, total loss:0.000245\n",
            "[Train] epoch:5613, lr:0.000465, total loss:0.000221\n",
            "[Train] epoch:5614, lr:0.000464, total loss:0.000224\n",
            "[Train] epoch:5615, lr:0.000464, total loss:0.000208\n",
            "[Train] epoch:5616, lr:0.000464, total loss:0.000207\n",
            "[Train] epoch:5617, lr:0.000464, total loss:0.000240\n",
            "[Train] epoch:5618, lr:0.000464, total loss:0.000234\n",
            "[Train] epoch:5619, lr:0.000464, total loss:0.000236\n",
            "[Train] epoch:5620, lr:0.000464, total loss:0.000219\n",
            "[Train] epoch:5621, lr:0.000464, total loss:0.000215\n",
            "[Train] epoch:5622, lr:0.000463, total loss:0.000218\n",
            "[Train] epoch:5623, lr:0.000463, total loss:0.000204\n",
            "[Train] epoch:5624, lr:0.000463, total loss:0.000227\n",
            "[Train] epoch:5625, lr:0.000463, total loss:0.000208\n",
            "[Train] epoch:5626, lr:0.000463, total loss:0.000227\n",
            "[Train] epoch:5627, lr:0.000463, total loss:0.000232\n",
            "[Train] epoch:5628, lr:0.000463, total loss:0.000220\n",
            "[Train] epoch:5629, lr:0.000462, total loss:0.000223\n",
            "[Train] epoch:5630, lr:0.000462, total loss:0.000300\n",
            "[Train] epoch:5631, lr:0.000462, total loss:0.000312\n",
            "[Train] epoch:5632, lr:0.000462, total loss:0.000281\n",
            "[Train] epoch:5633, lr:0.000462, total loss:0.000271\n",
            "[Train] epoch:5634, lr:0.000462, total loss:0.000253\n",
            "[Train] epoch:5635, lr:0.000462, total loss:0.000283\n",
            "[Train] epoch:5636, lr:0.000461, total loss:0.000458\n",
            "[Train] epoch:5637, lr:0.000461, total loss:0.000310\n",
            "[Train] epoch:5638, lr:0.000461, total loss:0.000307\n",
            "[Train] epoch:5639, lr:0.000461, total loss:0.000309\n",
            "[Train] epoch:5640, lr:0.000461, total loss:0.000293\n",
            "[Train] epoch:5641, lr:0.000461, total loss:0.000399\n",
            "[Train] epoch:5642, lr:0.000461, total loss:0.000332\n",
            "[Train] epoch:5643, lr:0.000460, total loss:0.000266\n",
            "[Train] epoch:5644, lr:0.000460, total loss:0.000295\n",
            "[Train] epoch:5645, lr:0.000460, total loss:0.000241\n",
            "[Train] epoch:5646, lr:0.000460, total loss:0.000235\n",
            "[Train] epoch:5647, lr:0.000460, total loss:0.000235\n",
            "[Train] epoch:5648, lr:0.000460, total loss:0.000226\n",
            "[Train] epoch:5649, lr:0.000460, total loss:0.000221\n",
            "[Train] epoch:5650, lr:0.000459, total loss:0.000264\n",
            "[Train] epoch:5651, lr:0.000459, total loss:0.000235\n",
            "[Train] epoch:5652, lr:0.000459, total loss:0.000230\n",
            "[Train] epoch:5653, lr:0.000459, total loss:0.000221\n",
            "[Train] epoch:5654, lr:0.000459, total loss:0.000212\n",
            "[Train] epoch:5655, lr:0.000459, total loss:0.000242\n",
            "[Train] epoch:5656, lr:0.000459, total loss:0.000240\n",
            "[Train] epoch:5657, lr:0.000459, total loss:0.000228\n",
            "[Train] epoch:5658, lr:0.000458, total loss:0.000252\n",
            "[Train] epoch:5659, lr:0.000458, total loss:0.000228\n",
            "[Train] epoch:5660, lr:0.000458, total loss:0.000235\n",
            "[Train] epoch:5661, lr:0.000458, total loss:0.000226\n",
            "[Train] epoch:5662, lr:0.000458, total loss:0.000234\n",
            "[Train] epoch:5663, lr:0.000458, total loss:0.000260\n",
            "[Train] epoch:5664, lr:0.000458, total loss:0.000231\n",
            "[Train] epoch:5665, lr:0.000457, total loss:0.000259\n",
            "[Train] epoch:5666, lr:0.000457, total loss:0.000243\n",
            "[Train] epoch:5667, lr:0.000457, total loss:0.000230\n",
            "[Train] epoch:5668, lr:0.000457, total loss:0.000254\n",
            "[Train] epoch:5669, lr:0.000457, total loss:0.000272\n",
            "[Train] epoch:5670, lr:0.000457, total loss:0.000223\n",
            "[Train] epoch:5671, lr:0.000457, total loss:0.000230\n",
            "[Train] epoch:5672, lr:0.000456, total loss:0.000256\n",
            "[Train] epoch:5673, lr:0.000456, total loss:0.000216\n",
            "[Train] epoch:5674, lr:0.000456, total loss:0.000221\n",
            "[Train] epoch:5675, lr:0.000456, total loss:0.000246\n",
            "[Train] epoch:5676, lr:0.000456, total loss:0.000248\n",
            "[Train] epoch:5677, lr:0.000456, total loss:0.000217\n",
            "[Train] epoch:5678, lr:0.000456, total loss:0.000243\n",
            "[Train] epoch:5679, lr:0.000455, total loss:0.000248\n",
            "[Train] epoch:5680, lr:0.000455, total loss:0.000253\n",
            "[Train] epoch:5681, lr:0.000455, total loss:0.000272\n",
            "[Train] epoch:5682, lr:0.000455, total loss:0.000279\n",
            "[Train] epoch:5683, lr:0.000455, total loss:0.000225\n",
            "[Train] epoch:5684, lr:0.000455, total loss:0.000235\n",
            "[Train] epoch:5685, lr:0.000455, total loss:0.000245\n",
            "[Train] epoch:5686, lr:0.000455, total loss:0.000239\n",
            "[Train] epoch:5687, lr:0.000454, total loss:0.000230\n",
            "[Train] epoch:5688, lr:0.000454, total loss:0.000248\n",
            "[Train] epoch:5689, lr:0.000454, total loss:0.000274\n",
            "[Train] epoch:5690, lr:0.000454, total loss:0.000262\n",
            "[Train] epoch:5691, lr:0.000454, total loss:0.000277\n",
            "[Train] epoch:5692, lr:0.000454, total loss:0.000219\n",
            "[Train] epoch:5693, lr:0.000454, total loss:0.000222\n",
            "[Train] epoch:5694, lr:0.000453, total loss:0.000223\n",
            "[Train] epoch:5695, lr:0.000453, total loss:0.000217\n",
            "[Train] epoch:5696, lr:0.000453, total loss:0.000220\n",
            "[Train] epoch:5697, lr:0.000453, total loss:0.000218\n",
            "[Train] epoch:5698, lr:0.000453, total loss:0.000201\n",
            "[Train] epoch:5699, lr:0.000453, total loss:0.000217\n",
            "[Train] epoch:5700, lr:0.000453, total loss:0.000218\n",
            "[Train] epoch:5701, lr:0.000452, total loss:0.000229\n",
            "[Train] epoch:5702, lr:0.000452, total loss:0.000302\n",
            "[Train] epoch:5703, lr:0.000452, total loss:0.000263\n",
            "[Train] epoch:5704, lr:0.000452, total loss:0.000241\n",
            "[Train] epoch:5705, lr:0.000452, total loss:0.000234\n",
            "[Train] epoch:5706, lr:0.000452, total loss:0.000272\n",
            "[Train] epoch:5707, lr:0.000452, total loss:0.000247\n",
            "[Train] epoch:5708, lr:0.000451, total loss:0.000323\n",
            "[Train] epoch:5709, lr:0.000451, total loss:0.000234\n",
            "[Train] epoch:5710, lr:0.000451, total loss:0.000219\n",
            "[Train] epoch:5711, lr:0.000451, total loss:0.000261\n",
            "[Train] epoch:5712, lr:0.000451, total loss:0.000240\n",
            "[Train] epoch:5713, lr:0.000451, total loss:0.000217\n",
            "[Train] epoch:5714, lr:0.000451, total loss:0.000242\n",
            "[Train] epoch:5715, lr:0.000450, total loss:0.000285\n",
            "[Train] epoch:5716, lr:0.000450, total loss:0.000240\n",
            "[Train] epoch:5717, lr:0.000450, total loss:0.000234\n",
            "[Train] epoch:5718, lr:0.000450, total loss:0.000231\n",
            "[Train] epoch:5719, lr:0.000450, total loss:0.000228\n",
            "[Train] epoch:5720, lr:0.000450, total loss:0.000239\n",
            "[Train] epoch:5721, lr:0.000450, total loss:0.000250\n",
            "[Train] epoch:5722, lr:0.000450, total loss:0.000248\n",
            "[Train] epoch:5723, lr:0.000449, total loss:0.000241\n",
            "[Train] epoch:5724, lr:0.000449, total loss:0.000253\n",
            "[Train] epoch:5725, lr:0.000449, total loss:0.000241\n",
            "[Train] epoch:5726, lr:0.000449, total loss:0.000220\n",
            "[Train] epoch:5727, lr:0.000449, total loss:0.000256\n",
            "[Train] epoch:5728, lr:0.000449, total loss:0.000228\n",
            "[Train] epoch:5729, lr:0.000449, total loss:0.000258\n",
            "[Train] epoch:5730, lr:0.000448, total loss:0.000226\n",
            "[Train] epoch:5731, lr:0.000448, total loss:0.000236\n",
            "[Train] epoch:5732, lr:0.000448, total loss:0.000236\n",
            "[Train] epoch:5733, lr:0.000448, total loss:0.000234\n",
            "[Train] epoch:5734, lr:0.000448, total loss:0.000229\n",
            "[Train] epoch:5735, lr:0.000448, total loss:0.000197\n",
            "[Train] epoch:5736, lr:0.000448, total loss:0.000204\n",
            "[Train] epoch:5737, lr:0.000447, total loss:0.000231\n",
            "[Train] epoch:5738, lr:0.000447, total loss:0.000229\n",
            "[Train] epoch:5739, lr:0.000447, total loss:0.000285\n",
            "[Train] epoch:5740, lr:0.000447, total loss:0.000238\n",
            "[Train] epoch:5741, lr:0.000447, total loss:0.000217\n",
            "[Train] epoch:5742, lr:0.000447, total loss:0.000229\n",
            "[Train] epoch:5743, lr:0.000447, total loss:0.000205\n",
            "[Train] epoch:5744, lr:0.000446, total loss:0.000238\n",
            "[Train] epoch:5745, lr:0.000446, total loss:0.000242\n",
            "[Train] epoch:5746, lr:0.000446, total loss:0.000213\n",
            "[Train] epoch:5747, lr:0.000446, total loss:0.000237\n",
            "[Train] epoch:5748, lr:0.000446, total loss:0.000216\n",
            "[Train] epoch:5749, lr:0.000446, total loss:0.000245\n",
            "[Train] epoch:5750, lr:0.000446, total loss:0.000229\n",
            "[Train] epoch:5751, lr:0.000446, total loss:0.000227\n",
            "[Train] epoch:5752, lr:0.000445, total loss:0.000232\n",
            "[Train] epoch:5753, lr:0.000445, total loss:0.000242\n",
            "[Train] epoch:5754, lr:0.000445, total loss:0.000229\n",
            "[Train] epoch:5755, lr:0.000445, total loss:0.000249\n",
            "[Train] epoch:5756, lr:0.000445, total loss:0.000213\n",
            "[Train] epoch:5757, lr:0.000445, total loss:0.000222\n",
            "[Train] epoch:5758, lr:0.000445, total loss:0.000261\n",
            "[Train] epoch:5759, lr:0.000444, total loss:0.000282\n",
            "[Train] epoch:5760, lr:0.000444, total loss:0.000271\n",
            "[Train] epoch:5761, lr:0.000444, total loss:0.000223\n",
            "[Train] epoch:5762, lr:0.000444, total loss:0.000262\n",
            "[Train] epoch:5763, lr:0.000444, total loss:0.000293\n",
            "[Train] epoch:5764, lr:0.000444, total loss:0.000239\n",
            "[Train] epoch:5765, lr:0.000444, total loss:0.000244\n",
            "[Train] epoch:5766, lr:0.000443, total loss:0.000239\n",
            "[Train] epoch:5767, lr:0.000443, total loss:0.000237\n",
            "[Train] epoch:5768, lr:0.000443, total loss:0.000220\n",
            "[Train] epoch:5769, lr:0.000443, total loss:0.000221\n",
            "[Train] epoch:5770, lr:0.000443, total loss:0.000204\n",
            "[Train] epoch:5771, lr:0.000443, total loss:0.000216\n",
            "[Train] epoch:5772, lr:0.000443, total loss:0.000213\n",
            "[Train] epoch:5773, lr:0.000443, total loss:0.000208\n",
            "[Train] epoch:5774, lr:0.000442, total loss:0.000216\n",
            "[Train] epoch:5775, lr:0.000442, total loss:0.000234\n",
            "[Train] epoch:5776, lr:0.000442, total loss:0.000236\n",
            "[Train] epoch:5777, lr:0.000442, total loss:0.000216\n",
            "[Train] epoch:5778, lr:0.000442, total loss:0.000234\n",
            "[Train] epoch:5779, lr:0.000442, total loss:0.000220\n",
            "[Train] epoch:5780, lr:0.000442, total loss:0.000240\n",
            "[Train] epoch:5781, lr:0.000441, total loss:0.000246\n",
            "[Train] epoch:5782, lr:0.000441, total loss:0.000221\n",
            "[Train] epoch:5783, lr:0.000441, total loss:0.000237\n",
            "[Train] epoch:5784, lr:0.000441, total loss:0.000235\n",
            "[Train] epoch:5785, lr:0.000441, total loss:0.000226\n",
            "[Train] epoch:5786, lr:0.000441, total loss:0.000216\n",
            "[Train] epoch:5787, lr:0.000441, total loss:0.000241\n",
            "[Train] epoch:5788, lr:0.000440, total loss:0.000268\n",
            "[Train] epoch:5789, lr:0.000440, total loss:0.000249\n",
            "[Train] epoch:5790, lr:0.000440, total loss:0.000254\n",
            "[Train] epoch:5791, lr:0.000440, total loss:0.000311\n",
            "[Train] epoch:5792, lr:0.000440, total loss:0.000227\n",
            "[Train] epoch:5793, lr:0.000440, total loss:0.000246\n",
            "[Train] epoch:5794, lr:0.000440, total loss:0.000256\n",
            "[Train] epoch:5795, lr:0.000439, total loss:0.000220\n",
            "[Train] epoch:5796, lr:0.000439, total loss:0.000240\n",
            "[Train] epoch:5797, lr:0.000439, total loss:0.000241\n",
            "[Train] epoch:5798, lr:0.000439, total loss:0.000216\n",
            "[Train] epoch:5799, lr:0.000439, total loss:0.000223\n",
            "[Train] epoch:5800, lr:0.000439, total loss:0.000218\n",
            "[Train] epoch:5801, lr:0.000439, total loss:0.000223\n",
            "[Train] epoch:5802, lr:0.000439, total loss:0.000236\n",
            "[Train] epoch:5803, lr:0.000438, total loss:0.000219\n",
            "[Train] epoch:5804, lr:0.000438, total loss:0.000237\n",
            "[Train] epoch:5805, lr:0.000438, total loss:0.000227\n",
            "[Train] epoch:5806, lr:0.000438, total loss:0.000219\n",
            "[Train] epoch:5807, lr:0.000438, total loss:0.000232\n",
            "[Train] epoch:5808, lr:0.000438, total loss:0.000295\n",
            "[Train] epoch:5809, lr:0.000438, total loss:0.000281\n",
            "[Train] epoch:5810, lr:0.000437, total loss:0.000285\n",
            "[Train] epoch:5811, lr:0.000437, total loss:0.000250\n",
            "[Train] epoch:5812, lr:0.000437, total loss:0.000273\n",
            "[Train] epoch:5813, lr:0.000437, total loss:0.000245\n",
            "[Train] epoch:5814, lr:0.000437, total loss:0.000230\n",
            "[Train] epoch:5815, lr:0.000437, total loss:0.000222\n",
            "[Train] epoch:5816, lr:0.000437, total loss:0.000221\n",
            "[Train] epoch:5817, lr:0.000436, total loss:0.000207\n",
            "[Train] epoch:5818, lr:0.000436, total loss:0.000225\n",
            "[Train] epoch:5819, lr:0.000436, total loss:0.000268\n",
            "[Train] epoch:5820, lr:0.000436, total loss:0.000216\n",
            "[Train] epoch:5821, lr:0.000436, total loss:0.000208\n",
            "[Train] epoch:5822, lr:0.000436, total loss:0.000230\n",
            "[Train] epoch:5823, lr:0.000436, total loss:0.000232\n",
            "[Train] epoch:5824, lr:0.000436, total loss:0.000241\n",
            "[Train] epoch:5825, lr:0.000435, total loss:0.000242\n",
            "[Train] epoch:5826, lr:0.000435, total loss:0.000222\n",
            "[Train] epoch:5827, lr:0.000435, total loss:0.000229\n",
            "[Train] epoch:5828, lr:0.000435, total loss:0.000252\n",
            "[Train] epoch:5829, lr:0.000435, total loss:0.000251\n",
            "[Train] epoch:5830, lr:0.000435, total loss:0.000265\n",
            "[Train] epoch:5831, lr:0.000435, total loss:0.000224\n",
            "[Train] epoch:5832, lr:0.000434, total loss:0.000217\n",
            "[Train] epoch:5833, lr:0.000434, total loss:0.000204\n",
            "[Train] epoch:5834, lr:0.000434, total loss:0.000240\n",
            "[Train] epoch:5835, lr:0.000434, total loss:0.000238\n",
            "[Train] epoch:5836, lr:0.000434, total loss:0.000283\n",
            "[Train] epoch:5837, lr:0.000434, total loss:0.000255\n",
            "[Train] epoch:5838, lr:0.000434, total loss:0.000285\n",
            "[Train] epoch:5839, lr:0.000433, total loss:0.000293\n",
            "[Train] epoch:5840, lr:0.000433, total loss:0.000379\n",
            "[Train] epoch:5841, lr:0.000433, total loss:0.000299\n",
            "[Train] epoch:5842, lr:0.000433, total loss:0.000314\n",
            "[Train] epoch:5843, lr:0.000433, total loss:0.000252\n",
            "[Train] epoch:5844, lr:0.000433, total loss:0.000258\n",
            "[Train] epoch:5845, lr:0.000433, total loss:0.000248\n",
            "[Train] epoch:5846, lr:0.000433, total loss:0.000261\n",
            "[Train] epoch:5847, lr:0.000432, total loss:0.000236\n",
            "[Train] epoch:5848, lr:0.000432, total loss:0.000261\n",
            "[Train] epoch:5849, lr:0.000432, total loss:0.000236\n",
            "[Train] epoch:5850, lr:0.000432, total loss:0.000214\n",
            "[Train] epoch:5851, lr:0.000432, total loss:0.000255\n",
            "[Train] epoch:5852, lr:0.000432, total loss:0.000257\n",
            "[Train] epoch:5853, lr:0.000432, total loss:0.000245\n",
            "[Train] epoch:5854, lr:0.000431, total loss:0.000230\n",
            "[Train] epoch:5855, lr:0.000431, total loss:0.000216\n",
            "[Train] epoch:5856, lr:0.000431, total loss:0.000250\n",
            "[Train] epoch:5857, lr:0.000431, total loss:0.000220\n",
            "[Train] epoch:5858, lr:0.000431, total loss:0.000221\n",
            "[Train] epoch:5859, lr:0.000431, total loss:0.000215\n",
            "[Train] epoch:5860, lr:0.000431, total loss:0.000211\n",
            "[Train] epoch:5861, lr:0.000430, total loss:0.000235\n",
            "[Train] epoch:5862, lr:0.000430, total loss:0.000217\n",
            "[Train] epoch:5863, lr:0.000430, total loss:0.000220\n",
            "[Train] epoch:5864, lr:0.000430, total loss:0.000220\n",
            "[Train] epoch:5865, lr:0.000430, total loss:0.000249\n",
            "[Train] epoch:5866, lr:0.000430, total loss:0.000232\n",
            "[Train] epoch:5867, lr:0.000430, total loss:0.000228\n",
            "[Train] epoch:5868, lr:0.000430, total loss:0.000211\n",
            "[Train] epoch:5869, lr:0.000429, total loss:0.000218\n",
            "[Train] epoch:5870, lr:0.000429, total loss:0.000261\n",
            "[Train] epoch:5871, lr:0.000429, total loss:0.000259\n",
            "[Train] epoch:5872, lr:0.000429, total loss:0.000250\n",
            "[Train] epoch:5873, lr:0.000429, total loss:0.000244\n",
            "[Train] epoch:5874, lr:0.000429, total loss:0.000268\n",
            "[Train] epoch:5875, lr:0.000429, total loss:0.000257\n",
            "[Train] epoch:5876, lr:0.000428, total loss:0.000204\n",
            "[Train] epoch:5877, lr:0.000428, total loss:0.000220\n",
            "[Train] epoch:5878, lr:0.000428, total loss:0.000214\n",
            "[Train] epoch:5879, lr:0.000428, total loss:0.000211\n",
            "[Train] epoch:5880, lr:0.000428, total loss:0.000234\n",
            "[Train] epoch:5881, lr:0.000428, total loss:0.000233\n",
            "[Train] epoch:5882, lr:0.000428, total loss:0.000241\n",
            "[Train] epoch:5883, lr:0.000427, total loss:0.000280\n",
            "[Train] epoch:5884, lr:0.000427, total loss:0.000239\n",
            "[Train] epoch:5885, lr:0.000427, total loss:0.000216\n",
            "[Train] epoch:5886, lr:0.000427, total loss:0.000207\n",
            "[Train] epoch:5887, lr:0.000427, total loss:0.000208\n",
            "[Train] epoch:5888, lr:0.000427, total loss:0.000237\n",
            "[Train] epoch:5889, lr:0.000427, total loss:0.000205\n",
            "[Train] epoch:5890, lr:0.000427, total loss:0.000203\n",
            "[Train] epoch:5891, lr:0.000426, total loss:0.000212\n",
            "[Train] epoch:5892, lr:0.000426, total loss:0.000220\n",
            "[Train] epoch:5893, lr:0.000426, total loss:0.000239\n",
            "[Train] epoch:5894, lr:0.000426, total loss:0.000226\n",
            "[Train] epoch:5895, lr:0.000426, total loss:0.000308\n",
            "[Train] epoch:5896, lr:0.000426, total loss:0.000247\n",
            "[Train] epoch:5897, lr:0.000426, total loss:0.000233\n",
            "[Train] epoch:5898, lr:0.000425, total loss:0.000246\n",
            "[Train] epoch:5899, lr:0.000425, total loss:0.000227\n",
            "[Train] epoch:5900, lr:0.000425, total loss:0.000229\n",
            "[Train] epoch:5901, lr:0.000425, total loss:0.000246\n",
            "[Train] epoch:5902, lr:0.000425, total loss:0.000205\n",
            "[Train] epoch:5903, lr:0.000425, total loss:0.000217\n",
            "[Train] epoch:5904, lr:0.000425, total loss:0.000250\n",
            "[Train] epoch:5905, lr:0.000424, total loss:0.000226\n",
            "[Train] epoch:5906, lr:0.000424, total loss:0.000230\n",
            "[Train] epoch:5907, lr:0.000424, total loss:0.000243\n",
            "[Train] epoch:5908, lr:0.000424, total loss:0.000252\n",
            "[Train] epoch:5909, lr:0.000424, total loss:0.000201\n",
            "[Train] epoch:5910, lr:0.000424, total loss:0.000222\n",
            "[Train] epoch:5911, lr:0.000424, total loss:0.000232\n",
            "[Train] epoch:5912, lr:0.000424, total loss:0.000246\n",
            "[Train] epoch:5913, lr:0.000423, total loss:0.000275\n",
            "[Train] epoch:5914, lr:0.000423, total loss:0.000258\n",
            "[Train] epoch:5915, lr:0.000423, total loss:0.000220\n",
            "[Train] epoch:5916, lr:0.000423, total loss:0.000245\n",
            "[Train] epoch:5917, lr:0.000423, total loss:0.000242\n",
            "[Train] epoch:5918, lr:0.000423, total loss:0.000219\n",
            "[Train] epoch:5919, lr:0.000423, total loss:0.000212\n",
            "[Train] epoch:5920, lr:0.000422, total loss:0.000256\n",
            "[Train] epoch:5921, lr:0.000422, total loss:0.000253\n",
            "[Train] epoch:5922, lr:0.000422, total loss:0.000235\n",
            "[Train] epoch:5923, lr:0.000422, total loss:0.000251\n",
            "[Train] epoch:5924, lr:0.000422, total loss:0.000223\n",
            "[Train] epoch:5925, lr:0.000422, total loss:0.000237\n",
            "[Train] epoch:5926, lr:0.000422, total loss:0.000284\n",
            "[Train] epoch:5927, lr:0.000421, total loss:0.000217\n",
            "[Train] epoch:5928, lr:0.000421, total loss:0.000392\n",
            "[Train] epoch:5929, lr:0.000421, total loss:0.000263\n",
            "[Train] epoch:5930, lr:0.000421, total loss:0.000255\n",
            "[Train] epoch:5931, lr:0.000421, total loss:0.000246\n",
            "[Train] epoch:5932, lr:0.000421, total loss:0.000219\n",
            "[Train] epoch:5933, lr:0.000421, total loss:0.000213\n",
            "[Train] epoch:5934, lr:0.000421, total loss:0.000227\n",
            "[Train] epoch:5935, lr:0.000420, total loss:0.000231\n",
            "[Train] epoch:5936, lr:0.000420, total loss:0.000209\n",
            "[Train] epoch:5937, lr:0.000420, total loss:0.000215\n",
            "[Train] epoch:5938, lr:0.000420, total loss:0.000212\n",
            "[Train] epoch:5939, lr:0.000420, total loss:0.000216\n",
            "[Train] epoch:5940, lr:0.000420, total loss:0.000229\n",
            "[Train] epoch:5941, lr:0.000420, total loss:0.000193\n",
            "[Train] epoch:5942, lr:0.000419, total loss:0.000216\n",
            "[Train] epoch:5943, lr:0.000419, total loss:0.000216\n",
            "[Train] epoch:5944, lr:0.000419, total loss:0.000238\n",
            "[Train] epoch:5945, lr:0.000419, total loss:0.000265\n",
            "[Train] epoch:5946, lr:0.000419, total loss:0.000220\n",
            "[Train] epoch:5947, lr:0.000419, total loss:0.000313\n",
            "[Train] epoch:5948, lr:0.000419, total loss:0.000210\n",
            "[Train] epoch:5949, lr:0.000419, total loss:0.000220\n",
            "[Train] epoch:5950, lr:0.000418, total loss:0.000206\n",
            "[Train] epoch:5951, lr:0.000418, total loss:0.000202\n",
            "[Train] epoch:5952, lr:0.000418, total loss:0.000224\n",
            "[Train] epoch:5953, lr:0.000418, total loss:0.000208\n",
            "[Train] epoch:5954, lr:0.000418, total loss:0.000222\n",
            "[Train] epoch:5955, lr:0.000418, total loss:0.000231\n",
            "[Train] epoch:5956, lr:0.000418, total loss:0.000247\n",
            "[Train] epoch:5957, lr:0.000417, total loss:0.000247\n",
            "[Train] epoch:5958, lr:0.000417, total loss:0.000218\n",
            "[Train] epoch:5959, lr:0.000417, total loss:0.000254\n",
            "[Train] epoch:5960, lr:0.000417, total loss:0.000221\n",
            "[Train] epoch:5961, lr:0.000417, total loss:0.000264\n",
            "[Train] epoch:5962, lr:0.000417, total loss:0.000211\n",
            "[Train] epoch:5963, lr:0.000417, total loss:0.000235\n",
            "[Train] epoch:5964, lr:0.000416, total loss:0.000210\n",
            "[Train] epoch:5965, lr:0.000416, total loss:0.000263\n",
            "[Train] epoch:5966, lr:0.000416, total loss:0.000261\n",
            "[Train] epoch:5967, lr:0.000416, total loss:0.000222\n",
            "[Train] epoch:5968, lr:0.000416, total loss:0.000246\n",
            "[Train] epoch:5969, lr:0.000416, total loss:0.000315\n",
            "[Train] epoch:5970, lr:0.000416, total loss:0.000213\n",
            "[Train] epoch:5971, lr:0.000416, total loss:0.000206\n",
            "[Train] epoch:5972, lr:0.000415, total loss:0.000220\n",
            "[Train] epoch:5973, lr:0.000415, total loss:0.000203\n",
            "[Train] epoch:5974, lr:0.000415, total loss:0.000208\n",
            "[Train] epoch:5975, lr:0.000415, total loss:0.000195\n",
            "[Train] epoch:5976, lr:0.000415, total loss:0.000238\n",
            "[Train] epoch:5977, lr:0.000415, total loss:0.000231\n",
            "[Train] epoch:5978, lr:0.000415, total loss:0.000223\n",
            "[Train] epoch:5979, lr:0.000414, total loss:0.000239\n",
            "[Train] epoch:5980, lr:0.000414, total loss:0.000219\n",
            "[Train] epoch:5981, lr:0.000414, total loss:0.000221\n",
            "[Train] epoch:5982, lr:0.000414, total loss:0.000221\n",
            "[Train] epoch:5983, lr:0.000414, total loss:0.000266\n",
            "[Train] epoch:5984, lr:0.000414, total loss:0.000245\n",
            "[Train] epoch:5985, lr:0.000414, total loss:0.000215\n",
            "[Train] epoch:5986, lr:0.000414, total loss:0.000220\n",
            "[Train] epoch:5987, lr:0.000413, total loss:0.000228\n",
            "[Train] epoch:5988, lr:0.000413, total loss:0.000209\n",
            "[Train] epoch:5989, lr:0.000413, total loss:0.000216\n",
            "[Train] epoch:5990, lr:0.000413, total loss:0.000214\n",
            "[Train] epoch:5991, lr:0.000413, total loss:0.000230\n",
            "[Train] epoch:5992, lr:0.000413, total loss:0.000245\n",
            "[Train] epoch:5993, lr:0.000413, total loss:0.000246\n",
            "[Train] epoch:5994, lr:0.000412, total loss:0.000229\n",
            "[Train] epoch:5995, lr:0.000412, total loss:0.000231\n",
            "[Train] epoch:5996, lr:0.000412, total loss:0.000277\n",
            "[Train] epoch:5997, lr:0.000412, total loss:0.000252\n",
            "[Train] epoch:5998, lr:0.000412, total loss:0.000258\n",
            "[Train] epoch:5999, lr:0.000412, total loss:0.000240\n",
            "[Train] epoch:6000, lr:0.000412, total loss:0.000229\n",
            "[Train] epoch:6001, lr:0.000411, total loss:0.000242\n",
            "[Train] epoch:6002, lr:0.000411, total loss:0.000216\n",
            "[Train] epoch:6003, lr:0.000411, total loss:0.000220\n",
            "[Train] epoch:6004, lr:0.000411, total loss:0.000238\n",
            "[Train] epoch:6005, lr:0.000411, total loss:0.000243\n",
            "[Train] epoch:6006, lr:0.000411, total loss:0.000208\n",
            "[Train] epoch:6007, lr:0.000411, total loss:0.000243\n",
            "[Train] epoch:6008, lr:0.000411, total loss:0.000217\n",
            "[Train] epoch:6009, lr:0.000410, total loss:0.000217\n",
            "[Train] epoch:6010, lr:0.000410, total loss:0.000229\n",
            "[Train] epoch:6011, lr:0.000410, total loss:0.000207\n",
            "[Train] epoch:6012, lr:0.000410, total loss:0.000211\n",
            "[Train] epoch:6013, lr:0.000410, total loss:0.000232\n",
            "[Train] epoch:6014, lr:0.000410, total loss:0.000192\n",
            "[Train] epoch:6015, lr:0.000410, total loss:0.000239\n",
            "[Train] epoch:6016, lr:0.000409, total loss:0.000222\n",
            "[Train] epoch:6017, lr:0.000409, total loss:0.000236\n",
            "[Train] epoch:6018, lr:0.000409, total loss:0.000264\n",
            "[Train] epoch:6019, lr:0.000409, total loss:0.000317\n",
            "[Train] epoch:6020, lr:0.000409, total loss:0.000249\n",
            "[Train] epoch:6021, lr:0.000409, total loss:0.000270\n",
            "[Train] epoch:6022, lr:0.000409, total loss:0.000265\n",
            "[Train] epoch:6023, lr:0.000409, total loss:0.000211\n",
            "[Train] epoch:6024, lr:0.000408, total loss:0.000228\n",
            "[Train] epoch:6025, lr:0.000408, total loss:0.000258\n",
            "[Train] epoch:6026, lr:0.000408, total loss:0.000292\n",
            "[Train] epoch:6027, lr:0.000408, total loss:0.000262\n",
            "[Train] epoch:6028, lr:0.000408, total loss:0.000245\n",
            "[Train] epoch:6029, lr:0.000408, total loss:0.000228\n",
            "[Train] epoch:6030, lr:0.000408, total loss:0.000216\n",
            "[Train] epoch:6031, lr:0.000407, total loss:0.000226\n",
            "[Train] epoch:6032, lr:0.000407, total loss:0.000291\n",
            "[Train] epoch:6033, lr:0.000407, total loss:0.000270\n",
            "[Train] epoch:6034, lr:0.000407, total loss:0.000236\n",
            "[Train] epoch:6035, lr:0.000407, total loss:0.000226\n",
            "[Train] epoch:6036, lr:0.000407, total loss:0.000209\n",
            "[Train] epoch:6037, lr:0.000407, total loss:0.000216\n",
            "[Train] epoch:6038, lr:0.000407, total loss:0.000227\n",
            "[Train] epoch:6039, lr:0.000406, total loss:0.000240\n",
            "[Train] epoch:6040, lr:0.000406, total loss:0.000276\n",
            "[Train] epoch:6041, lr:0.000406, total loss:0.000228\n",
            "[Train] epoch:6042, lr:0.000406, total loss:0.000235\n",
            "[Train] epoch:6043, lr:0.000406, total loss:0.000242\n",
            "[Train] epoch:6044, lr:0.000406, total loss:0.000241\n",
            "[Train] epoch:6045, lr:0.000406, total loss:0.000217\n",
            "[Train] epoch:6046, lr:0.000405, total loss:0.000197\n",
            "[Train] epoch:6047, lr:0.000405, total loss:0.000214\n",
            "[Train] epoch:6048, lr:0.000405, total loss:0.000211\n",
            "[Train] epoch:6049, lr:0.000405, total loss:0.000231\n",
            "[Train] epoch:6050, lr:0.000405, total loss:0.000234\n",
            "[Train] epoch:6051, lr:0.000405, total loss:0.000194\n",
            "[Train] epoch:6052, lr:0.000405, total loss:0.000200\n",
            "[Train] epoch:6053, lr:0.000404, total loss:0.000209\n",
            "[Train] epoch:6054, lr:0.000404, total loss:0.000244\n",
            "[Train] epoch:6055, lr:0.000404, total loss:0.000219\n",
            "[Train] epoch:6056, lr:0.000404, total loss:0.000204\n",
            "[Train] epoch:6057, lr:0.000404, total loss:0.000199\n",
            "[Train] epoch:6058, lr:0.000404, total loss:0.000257\n",
            "[Train] epoch:6059, lr:0.000404, total loss:0.000219\n",
            "[Train] epoch:6060, lr:0.000404, total loss:0.000240\n",
            "[Train] epoch:6061, lr:0.000403, total loss:0.000212\n",
            "[Train] epoch:6062, lr:0.000403, total loss:0.000209\n",
            "[Train] epoch:6063, lr:0.000403, total loss:0.000201\n",
            "[Train] epoch:6064, lr:0.000403, total loss:0.000232\n",
            "[Train] epoch:6065, lr:0.000403, total loss:0.000207\n",
            "[Train] epoch:6066, lr:0.000403, total loss:0.000235\n",
            "[Train] epoch:6067, lr:0.000403, total loss:0.000236\n",
            "[Train] epoch:6068, lr:0.000402, total loss:0.000229\n",
            "[Train] epoch:6069, lr:0.000402, total loss:0.000236\n",
            "[Train] epoch:6070, lr:0.000402, total loss:0.000216\n",
            "[Train] epoch:6071, lr:0.000402, total loss:0.000243\n",
            "[Train] epoch:6072, lr:0.000402, total loss:0.000255\n",
            "[Train] epoch:6073, lr:0.000402, total loss:0.000267\n",
            "[Train] epoch:6074, lr:0.000402, total loss:0.000234\n",
            "[Train] epoch:6075, lr:0.000402, total loss:0.000242\n",
            "[Train] epoch:6076, lr:0.000401, total loss:0.000253\n",
            "[Train] epoch:6077, lr:0.000401, total loss:0.000234\n",
            "[Train] epoch:6078, lr:0.000401, total loss:0.000219\n",
            "[Train] epoch:6079, lr:0.000401, total loss:0.000263\n",
            "[Train] epoch:6080, lr:0.000401, total loss:0.000230\n",
            "[Train] epoch:6081, lr:0.000401, total loss:0.000230\n",
            "[Train] epoch:6082, lr:0.000401, total loss:0.000210\n",
            "[Train] epoch:6083, lr:0.000400, total loss:0.000234\n",
            "[Train] epoch:6084, lr:0.000400, total loss:0.000243\n",
            "[Train] epoch:6085, lr:0.000400, total loss:0.000239\n",
            "[Train] epoch:6086, lr:0.000400, total loss:0.000232\n",
            "[Train] epoch:6087, lr:0.000400, total loss:0.000215\n",
            "[Train] epoch:6088, lr:0.000400, total loss:0.000235\n",
            "[Train] epoch:6089, lr:0.000400, total loss:0.000252\n",
            "[Train] epoch:6090, lr:0.000400, total loss:0.000219\n",
            "[Train] epoch:6091, lr:0.000399, total loss:0.000210\n",
            "[Train] epoch:6092, lr:0.000399, total loss:0.000231\n",
            "[Train] epoch:6093, lr:0.000399, total loss:0.000202\n",
            "[Train] epoch:6094, lr:0.000399, total loss:0.000213\n",
            "[Train] epoch:6095, lr:0.000399, total loss:0.000211\n",
            "[Train] epoch:6096, lr:0.000399, total loss:0.000226\n",
            "[Train] epoch:6097, lr:0.000399, total loss:0.000242\n",
            "[Train] epoch:6098, lr:0.000398, total loss:0.000201\n",
            "[Train] epoch:6099, lr:0.000398, total loss:0.000249\n",
            "[Train] epoch:6100, lr:0.000398, total loss:0.000313\n",
            "[Train] epoch:6101, lr:0.000398, total loss:0.000250\n",
            "[Train] epoch:6102, lr:0.000398, total loss:0.000257\n",
            "[Train] epoch:6103, lr:0.000398, total loss:0.000240\n",
            "[Train] epoch:6104, lr:0.000398, total loss:0.000211\n",
            "[Train] epoch:6105, lr:0.000398, total loss:0.000210\n",
            "[Train] epoch:6106, lr:0.000397, total loss:0.000197\n",
            "[Train] epoch:6107, lr:0.000397, total loss:0.000205\n",
            "[Train] epoch:6108, lr:0.000397, total loss:0.000242\n",
            "[Train] epoch:6109, lr:0.000397, total loss:0.000238\n",
            "[Train] epoch:6110, lr:0.000397, total loss:0.000222\n",
            "[Train] epoch:6111, lr:0.000397, total loss:0.000220\n",
            "[Train] epoch:6112, lr:0.000397, total loss:0.000244\n",
            "[Train] epoch:6113, lr:0.000396, total loss:0.000242\n",
            "[Train] epoch:6114, lr:0.000396, total loss:0.000237\n",
            "[Train] epoch:6115, lr:0.000396, total loss:0.000202\n",
            "[Train] epoch:6116, lr:0.000396, total loss:0.000241\n",
            "[Train] epoch:6117, lr:0.000396, total loss:0.000241\n",
            "[Train] epoch:6118, lr:0.000396, total loss:0.000230\n",
            "[Train] epoch:6119, lr:0.000396, total loss:0.000216\n",
            "[Train] epoch:6120, lr:0.000396, total loss:0.000209\n",
            "[Train] epoch:6121, lr:0.000395, total loss:0.000271\n",
            "[Train] epoch:6122, lr:0.000395, total loss:0.000225\n",
            "[Train] epoch:6123, lr:0.000395, total loss:0.000224\n",
            "[Train] epoch:6124, lr:0.000395, total loss:0.000208\n",
            "[Train] epoch:6125, lr:0.000395, total loss:0.000249\n",
            "[Train] epoch:6126, lr:0.000395, total loss:0.000248\n",
            "[Train] epoch:6127, lr:0.000395, total loss:0.000258\n",
            "[Train] epoch:6128, lr:0.000394, total loss:0.000220\n",
            "[Train] epoch:6129, lr:0.000394, total loss:0.000222\n",
            "[Train] epoch:6130, lr:0.000394, total loss:0.000251\n",
            "[Train] epoch:6131, lr:0.000394, total loss:0.000246\n",
            "[Train] epoch:6132, lr:0.000394, total loss:0.000210\n",
            "[Train] epoch:6133, lr:0.000394, total loss:0.000245\n",
            "[Train] epoch:6134, lr:0.000394, total loss:0.000219\n",
            "[Train] epoch:6135, lr:0.000394, total loss:0.000214\n",
            "[Train] epoch:6136, lr:0.000393, total loss:0.000210\n",
            "[Train] epoch:6137, lr:0.000393, total loss:0.000222\n",
            "[Train] epoch:6138, lr:0.000393, total loss:0.000259\n",
            "[Train] epoch:6139, lr:0.000393, total loss:0.000265\n",
            "[Train] epoch:6140, lr:0.000393, total loss:0.000216\n",
            "[Train] epoch:6141, lr:0.000393, total loss:0.000217\n",
            "[Train] epoch:6142, lr:0.000393, total loss:0.000211\n",
            "[Train] epoch:6143, lr:0.000393, total loss:0.000223\n",
            "[Train] epoch:6144, lr:0.000392, total loss:0.000234\n",
            "[Train] epoch:6145, lr:0.000392, total loss:0.000237\n",
            "[Train] epoch:6146, lr:0.000392, total loss:0.000229\n",
            "[Train] epoch:6147, lr:0.000392, total loss:0.000208\n",
            "[Train] epoch:6148, lr:0.000392, total loss:0.000223\n",
            "[Train] epoch:6149, lr:0.000392, total loss:0.000221\n",
            "[Train] epoch:6150, lr:0.000392, total loss:0.000244\n",
            "[Train] epoch:6151, lr:0.000391, total loss:0.000237\n",
            "[Train] epoch:6152, lr:0.000391, total loss:0.000214\n",
            "[Train] epoch:6153, lr:0.000391, total loss:0.000218\n",
            "[Train] epoch:6154, lr:0.000391, total loss:0.000248\n",
            "[Train] epoch:6155, lr:0.000391, total loss:0.000250\n",
            "[Train] epoch:6156, lr:0.000391, total loss:0.000334\n",
            "[Train] epoch:6157, lr:0.000391, total loss:0.000226\n",
            "[Train] epoch:6158, lr:0.000391, total loss:0.000229\n",
            "[Train] epoch:6159, lr:0.000390, total loss:0.000210\n",
            "[Train] epoch:6160, lr:0.000390, total loss:0.000236\n",
            "[Train] epoch:6161, lr:0.000390, total loss:0.000276\n",
            "[Train] epoch:6162, lr:0.000390, total loss:0.000212\n",
            "[Train] epoch:6163, lr:0.000390, total loss:0.000230\n",
            "[Train] epoch:6164, lr:0.000390, total loss:0.000217\n",
            "[Train] epoch:6165, lr:0.000390, total loss:0.000214\n",
            "[Train] epoch:6166, lr:0.000389, total loss:0.000218\n",
            "[Train] epoch:6167, lr:0.000389, total loss:0.000202\n",
            "[Train] epoch:6168, lr:0.000389, total loss:0.000229\n",
            "[Train] epoch:6169, lr:0.000389, total loss:0.000219\n",
            "[Train] epoch:6170, lr:0.000389, total loss:0.000200\n",
            "[Train] epoch:6171, lr:0.000389, total loss:0.000203\n",
            "[Train] epoch:6172, lr:0.000389, total loss:0.000244\n",
            "[Train] epoch:6173, lr:0.000389, total loss:0.000210\n",
            "[Train] epoch:6174, lr:0.000388, total loss:0.000233\n",
            "[Train] epoch:6175, lr:0.000388, total loss:0.000222\n",
            "[Train] epoch:6176, lr:0.000388, total loss:0.000207\n",
            "[Train] epoch:6177, lr:0.000388, total loss:0.000233\n",
            "[Train] epoch:6178, lr:0.000388, total loss:0.000299\n",
            "[Train] epoch:6179, lr:0.000388, total loss:0.000243\n",
            "[Train] epoch:6180, lr:0.000388, total loss:0.000233\n",
            "[Train] epoch:6181, lr:0.000387, total loss:0.000207\n",
            "[Train] epoch:6182, lr:0.000387, total loss:0.000203\n",
            "[Train] epoch:6183, lr:0.000387, total loss:0.000207\n",
            "[Train] epoch:6184, lr:0.000387, total loss:0.000210\n",
            "[Train] epoch:6185, lr:0.000387, total loss:0.000220\n",
            "[Train] epoch:6186, lr:0.000387, total loss:0.000215\n",
            "[Train] epoch:6187, lr:0.000387, total loss:0.000213\n",
            "[Train] epoch:6188, lr:0.000387, total loss:0.000216\n",
            "[Train] epoch:6189, lr:0.000386, total loss:0.000222\n",
            "[Train] epoch:6190, lr:0.000386, total loss:0.000207\n",
            "[Train] epoch:6191, lr:0.000386, total loss:0.000209\n",
            "[Train] epoch:6192, lr:0.000386, total loss:0.000246\n",
            "[Train] epoch:6193, lr:0.000386, total loss:0.000257\n",
            "[Train] epoch:6194, lr:0.000386, total loss:0.000205\n",
            "[Train] epoch:6195, lr:0.000386, total loss:0.000196\n",
            "[Train] epoch:6196, lr:0.000386, total loss:0.000225\n",
            "[Train] epoch:6197, lr:0.000385, total loss:0.000214\n",
            "[Train] epoch:6198, lr:0.000385, total loss:0.000233\n",
            "[Train] epoch:6199, lr:0.000385, total loss:0.000210\n",
            "[Train] epoch:6200, lr:0.000385, total loss:0.000239\n",
            "[Train] epoch:6201, lr:0.000385, total loss:0.000224\n",
            "[Train] epoch:6202, lr:0.000385, total loss:0.000216\n",
            "Early stopping at:  6202\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compare metrics, plot loss curve and save best model"
      ],
      "metadata": {
        "id": "CosxEBy_G2IF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "if overwrite:\n",
        "  torch.save(pinn.best_model, \"/content/drive/MyDrive/Machine Learning Project (ECE 228)/Battery Dataset/pinn_second_derivative.pth\")\n",
        "  pinn.load_model(\"/content/drive/MyDrive/Machine Learning Project (ECE 228)/Battery Dataset/pinn_second_derivative.pth\")\n",
        "true_label, pred_label = pinn.Test(loader['test_3'])\n",
        "mse = metrics.mean_squared_error(torch.tensor(true_label), torch.tensor(pred_label))\n",
        "mae = metrics.mean_absolute_error(torch.tensor(true_label), torch.tensor(pred_label))\n",
        "r2 = metrics.r2_score(torch.tensor(true_label), torch.tensor(pred_label))\n",
        "mape = metrics.mean_absolute_percentage_error(torch.tensor(true_label), torch.tensor(pred_label))\n",
        "\n",
        "print(mse, mae, r2, mape)\n",
        "\n",
        "print(count_parameters(pinn))\n",
        "result_dict = [{\n",
        "    'run name': 'second derivative regular features',\n",
        "    'mse': mse,\n",
        "    'mae': mae,\n",
        "    'r2': r2,\n",
        "    'mape': mape,\n",
        "    'param count': count_parameters(pinn)\n",
        "}]\n",
        "df = pd.DataFrame(result_dict)\n",
        "try:\n",
        "  results = pd.read_csv('/content/drive/MyDrive/Machine Learning Project (ECE 228)/Battery Dataset/results.csv')\n",
        "  results = pd.concat([results, df], ignore_index=True)\n",
        "except:\n",
        "  results = df\n",
        "\n",
        "if overwrite:\n",
        "  results.to_csv('/content/drive/MyDrive/Machine Learning Project (ECE 228)/Battery Dataset/results.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXyQsEQLG6sV",
        "outputId": "6a79e44c-0691-49c0-9a99-91e70ba65f5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0005406623612911314 0.013582779346617928 0.9614307894851389 0.01823025792990187\n",
            "14682\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the loss curve\n",
        "import matplotlib.pyplot as plt\n",
        "x = list(range(1,len(losses_second_derivative) + 1))\n",
        "plt.yscale('log')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Log-Scale Loss plot for second derivative model')\n",
        "plt.plot(x, losses_second_derivative)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "bFbdSgpfG-yd",
        "outputId": "98c09db1-2223-4357-b9b3-bf7c402d641f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7d352edfa0d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Second Derivatives on Extended Features\n"
      ],
      "metadata": {
        "id": "M9T3VWamKH09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# little bit of hee hee haha data manipulation\n",
        "df = pd.read_csv('/content/drive/MyDrive/Machine Learning Project (ECE 228)/Battery Dataset/features_extended.csv')\n",
        "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "df.dropna(inplace=True)\n",
        "df = df.drop(columns='fft_dominant_freq_i') # normalizing constants leads to div by zero. it's all the same because duh you can't do the FT on a linearly increasing value and then pick the dominant frequency without just basically getting the linear term back (i.e. index 1) (kind of mixing fourier and taylor series for this complaint but it doesn't remove the fact that it's the same number for every single entry)\n",
        "# makes no sense to run the FT on a function that only behaves linearly in the window but it seems the spectral entropy helped so I'm not goint to throw a fit\n",
        "df = df.drop(columns='fft_dominant_freq_v') # normalizing constants leads to div by zero\n",
        "df.to_csv(\"clean_features_extended.csv\", index=False)"
      ],
      "metadata": {
        "id": "dLpcPxYRKUhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class args:\n",
        "  pass\n",
        "\n",
        "# hehe tricks\n",
        "arg = args()\n",
        "\n",
        "arg.data = 'our_stuff'\n",
        "arg.batch = 10\n",
        "arg.batch_size = 256\n",
        "arg.normalization_method = 'z-score'\n",
        "arg.epochs = 10000\n",
        "arg.lr = 1e-3\n",
        "arg.warmup_epochs = 10\n",
        "arg.warmup_lr = 5e-4\n",
        "arg.final_lr = 1e-4\n",
        "arg.lr_F = 1e-3\n",
        "arg.iter_per_epoch = 1\n",
        "arg.F_layers_num = 3\n",
        "arg.F_hidden_dim = 60\n",
        "arg.alpha = 1\n",
        "arg.beta = 1\n",
        "arg.early_stop = 500\n",
        "arg.feature_length = 23 # use 23 for extended features\n",
        "arg.second_derivatives = True # use to not include second derivatives\n"
      ],
      "metadata": {
        "id": "3kamU077KVh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ok let's load a dataset\n",
        "reader = DF(arg)"
      ],
      "metadata": {
        "id": "G_xT8gObKc40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's get a loader back\n",
        "loader = reader.load_all([\"./clean_features_extended.csv\"], nominal_capacity=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cKZQhgQKgxW",
        "outputId": "114fc2f3-193e-4a9d-e513-8ef8c2f02eb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      charge_CC_mean_V  charge_CC_std_V  charge_CC_kurtosis_V  \\\n",
            "0             0.719302         0.060387             -0.416205   \n",
            "1             1.744366        -1.770584              2.239328   \n",
            "2             0.813665        -0.168417              0.106946   \n",
            "3             0.746640        -1.103780              0.819037   \n",
            "4             0.615206        -0.662110              0.482629   \n",
            "...                ...              ...                   ...   \n",
            "2396         -0.431346         0.728308             -0.611656   \n",
            "2397         -0.442060         0.650396             -0.614654   \n",
            "2398         -0.409993         0.631472             -0.612084   \n",
            "2399         -0.503042         0.695169             -0.596058   \n",
            "2400         -0.406144         0.605715             -0.612426   \n",
            "\n",
            "      charge_CC_skew_V  charge_CC_time_V  charge_CC_charge  charge_CC_slope_V  \\\n",
            "0            -0.490447         -2.652343         -2.639229           9.289918   \n",
            "1            -2.047867         -0.189973         -0.164574          -0.220123   \n",
            "2            -0.758163         -0.717865         -0.694388           0.094567   \n",
            "3            -1.162773         -0.754520         -0.731544           0.129686   \n",
            "4            -0.731586         -0.945440         -0.923880           0.269518   \n",
            "...                ...               ...               ...                ...   \n",
            "2396          0.406049         -0.405084         -0.379109          -0.081770   \n",
            "2397          0.412729         -0.422070         -0.396967          -0.079425   \n",
            "2398          0.416279         -0.445634         -0.419898          -0.069482   \n",
            "2399          0.488817         -0.188997         -0.162619          -0.196982   \n",
            "2400          0.394161         -0.469825         -0.444168          -0.057148   \n",
            "\n",
            "      charge_CC_entropy_V  charge_CV_mean_I  charge_CV_std_dev_I  ...  \\\n",
            "0               -3.767662         -0.533985             0.280544  ...   \n",
            "1               -0.544218         -1.468485            -0.737614  ...   \n",
            "2               -0.855270         -0.163320             0.369320  ...   \n",
            "3               -0.882921         -1.905650            -1.639340  ...   \n",
            "4               -1.019545         -1.985914            -1.116553  ...   \n",
            "...                   ...               ...                  ...  ...   \n",
            "2396            -0.749642          0.422425            -1.409878  ...   \n",
            "2397            -0.766719          0.408411            -1.681065  ...   \n",
            "2398            -0.784003          0.514787            -1.252275  ...   \n",
            "2399            -0.651206          0.310818            -1.725393  ...   \n",
            "2400            -0.810322          0.734143            -1.147519  ...   \n",
            "\n",
            "      charge_CV_slope_I  charge_CV_entropy_I  voltage_range_CC  \\\n",
            "0              0.465429            -0.965529         -4.062558   \n",
            "1              1.264524             0.204064         -1.228839   \n",
            "2              1.252285             0.136015         -0.012242   \n",
            "3              1.323510             0.211991          0.447969   \n",
            "4              1.383567             0.303588         -0.008673   \n",
            "...                 ...                  ...               ...   \n",
            "2396          -0.365561            -1.571977          0.733176   \n",
            "2397          -0.406831            -1.618121          0.347949   \n",
            "2398          -0.355108            -1.601372          0.184281   \n",
            "2399          -0.089845            -1.410325          0.222597   \n",
            "2400          -0.315553            -1.585471          0.123136   \n",
            "\n",
            "      current_range_CV  voltage_rms_CC  current_rms_CV  \\\n",
            "0            -0.600508        0.720590       -0.441106   \n",
            "1             0.548698        1.742806       -1.505971   \n",
            "2            -0.691349        0.814510       -0.081481   \n",
            "3            -0.751668        0.744993       -2.088463   \n",
            "4             0.169692        0.614443       -2.058659   \n",
            "...                ...             ...             ...   \n",
            "2396         -0.862895       -0.430183        0.119307   \n",
            "2397         -0.993078       -0.441131        0.055572   \n",
            "2398         -1.117055       -0.409062        0.235115   \n",
            "2399         -1.185204       -0.502091       -0.043943   \n",
            "2400         -1.004965       -0.405277        0.459621   \n",
            "\n",
            "      fft_spectral_entropy_v  fft_spectral_entropy_i  cycle_index  capacity  \n",
            "0                  -3.136304               -0.893938    -0.841376  0.350731  \n",
            "1                   2.002703                1.520689    -1.228564  0.782358  \n",
            "2                   0.320334               -0.247006    -1.208186  0.779403  \n",
            "3                   1.530118                1.452607    -1.187807  0.772999  \n",
            "4                   1.146405                0.749716    -1.167429  0.767229  \n",
            "...                      ...                     ...          ...       ...  \n",
            "2396               -0.625367                0.165608     1.339104  0.698253  \n",
            "2397               -0.633874                0.230246     1.359483  0.697567  \n",
            "2398               -0.642112                0.013432     1.379861  0.689922  \n",
            "2399               -0.642174                0.490014     1.400239  0.689690  \n",
            "2400               -0.642954                0.105679     1.420618  0.691855  \n",
            "\n",
            "[2401 rows x 24 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pinn = PINN(arg)"
      ],
      "metadata": {
        "id": "59EScmaxKldE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses_second_derivative_extended = pinn.Train(trainloader=loader['train'],\n",
        "           validloader=loader['valid'],\n",
        "           testloader=loader['test'], debug=True)\n",
        "if overwrite:\n",
        "  np.save('/content/drive/MyDrive/Machine Learning Project (ECE 228)/Battery Dataset/loss_second_derivative_extended.npy', losses_second_derivative_extended)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sldu48FCKorN",
        "outputId": "dd1ad0ce-73ef-4524-e4f6-58e1de397dbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Train] epoch:1, lr:0.000500, total loss:0.988908\n",
            "[Train] epoch:2, lr:0.000556, total loss:0.647699\n",
            "[Train] epoch:3, lr:0.000611, total loss:0.467781\n",
            "[Train] epoch:4, lr:0.000667, total loss:0.339736\n",
            "[Train] epoch:5, lr:0.000722, total loss:0.273898\n",
            "[Train] epoch:6, lr:0.000778, total loss:0.211649\n",
            "[Train] epoch:7, lr:0.000833, total loss:0.181018\n",
            "[Train] epoch:8, lr:0.000889, total loss:0.140800\n",
            "[Train] epoch:9, lr:0.000944, total loss:0.123733\n",
            "[Train] epoch:10, lr:0.001000, total loss:0.104587\n",
            "[Train] epoch:11, lr:0.001000, total loss:0.090636\n",
            "[Train] epoch:12, lr:0.001000, total loss:0.080529\n",
            "[Train] epoch:13, lr:0.001000, total loss:0.068424\n",
            "[Train] epoch:14, lr:0.001000, total loss:0.062603\n",
            "[Train] epoch:15, lr:0.001000, total loss:0.055970\n",
            "[Train] epoch:16, lr:0.001000, total loss:0.050336\n",
            "[Train] epoch:17, lr:0.001000, total loss:0.043932\n",
            "[Train] epoch:18, lr:0.001000, total loss:0.042022\n",
            "[Train] epoch:19, lr:0.001000, total loss:0.038801\n",
            "[Train] epoch:20, lr:0.001000, total loss:0.036558\n",
            "[Train] epoch:21, lr:0.001000, total loss:0.036193\n",
            "[Train] epoch:22, lr:0.001000, total loss:0.030478\n",
            "[Train] epoch:23, lr:0.001000, total loss:0.031144\n",
            "[Train] epoch:24, lr:0.001000, total loss:0.027956\n",
            "[Train] epoch:25, lr:0.001000, total loss:0.025383\n",
            "[Train] epoch:26, lr:0.001000, total loss:0.024934\n",
            "[Train] epoch:27, lr:0.001000, total loss:0.024602\n",
            "[Train] epoch:28, lr:0.001000, total loss:0.020973\n",
            "[Train] epoch:29, lr:0.001000, total loss:0.021257\n",
            "[Train] epoch:30, lr:0.001000, total loss:0.020047\n",
            "[Train] epoch:31, lr:0.001000, total loss:0.019223\n",
            "[Train] epoch:32, lr:0.001000, total loss:0.018322\n",
            "[Train] epoch:33, lr:0.001000, total loss:0.016373\n",
            "[Train] epoch:34, lr:0.001000, total loss:0.015848\n",
            "[Train] epoch:35, lr:0.001000, total loss:0.014609\n",
            "[Train] epoch:36, lr:0.001000, total loss:0.014239\n",
            "[Train] epoch:37, lr:0.001000, total loss:0.014196\n",
            "[Train] epoch:38, lr:0.001000, total loss:0.012688\n",
            "[Train] epoch:39, lr:0.001000, total loss:0.012179\n",
            "[Train] epoch:40, lr:0.001000, total loss:0.011989\n",
            "[Train] epoch:41, lr:0.001000, total loss:0.011901\n",
            "[Train] epoch:42, lr:0.001000, total loss:0.011482\n",
            "[Train] epoch:43, lr:0.001000, total loss:0.011335\n",
            "[Train] epoch:44, lr:0.001000, total loss:0.011118\n",
            "[Train] epoch:45, lr:0.001000, total loss:0.009982\n",
            "[Train] epoch:46, lr:0.001000, total loss:0.009953\n",
            "[Train] epoch:47, lr:0.001000, total loss:0.009096\n",
            "[Train] epoch:48, lr:0.001000, total loss:0.009210\n",
            "[Train] epoch:49, lr:0.001000, total loss:0.008665\n",
            "[Train] epoch:50, lr:0.001000, total loss:0.009144\n",
            "[Train] epoch:51, lr:0.001000, total loss:0.008823\n",
            "[Train] epoch:52, lr:0.001000, total loss:0.008311\n",
            "[Train] epoch:53, lr:0.001000, total loss:0.007815\n",
            "[Train] epoch:54, lr:0.001000, total loss:0.007716\n",
            "[Train] epoch:55, lr:0.001000, total loss:0.007996\n",
            "[Train] epoch:56, lr:0.001000, total loss:0.007357\n",
            "[Train] epoch:57, lr:0.001000, total loss:0.007184\n",
            "[Train] epoch:58, lr:0.001000, total loss:0.007136\n",
            "[Train] epoch:59, lr:0.001000, total loss:0.007277\n",
            "[Train] epoch:60, lr:0.001000, total loss:0.006983\n",
            "[Train] epoch:61, lr:0.001000, total loss:0.006718\n",
            "[Train] epoch:62, lr:0.001000, total loss:0.006442\n",
            "[Train] epoch:63, lr:0.001000, total loss:0.006627\n",
            "[Train] epoch:64, lr:0.001000, total loss:0.006665\n",
            "[Train] epoch:65, lr:0.001000, total loss:0.006104\n",
            "[Train] epoch:66, lr:0.001000, total loss:0.005662\n",
            "[Train] epoch:67, lr:0.001000, total loss:0.005898\n",
            "[Train] epoch:68, lr:0.001000, total loss:0.005879\n",
            "[Train] epoch:69, lr:0.001000, total loss:0.005645\n",
            "[Train] epoch:70, lr:0.001000, total loss:0.005516\n",
            "[Train] epoch:71, lr:0.001000, total loss:0.005599\n",
            "[Train] epoch:72, lr:0.001000, total loss:0.005574\n",
            "[Train] epoch:73, lr:0.001000, total loss:0.005535\n",
            "[Train] epoch:74, lr:0.001000, total loss:0.005534\n",
            "[Train] epoch:75, lr:0.001000, total loss:0.005262\n",
            "[Train] epoch:76, lr:0.001000, total loss:0.005292\n",
            "[Train] epoch:77, lr:0.001000, total loss:0.005064\n",
            "[Train] epoch:78, lr:0.001000, total loss:0.004942\n",
            "[Train] epoch:79, lr:0.001000, total loss:0.004907\n",
            "[Train] epoch:80, lr:0.001000, total loss:0.004852\n",
            "[Train] epoch:81, lr:0.001000, total loss:0.004852\n",
            "[Train] epoch:82, lr:0.001000, total loss:0.004543\n",
            "[Train] epoch:83, lr:0.001000, total loss:0.004579\n",
            "[Train] epoch:84, lr:0.001000, total loss:0.004759\n",
            "[Train] epoch:85, lr:0.001000, total loss:0.004313\n",
            "[Train] epoch:86, lr:0.001000, total loss:0.004366\n",
            "[Train] epoch:87, lr:0.001000, total loss:0.004367\n",
            "[Train] epoch:88, lr:0.001000, total loss:0.004761\n",
            "[Train] epoch:89, lr:0.001000, total loss:0.004314\n",
            "[Train] epoch:90, lr:0.001000, total loss:0.004404\n",
            "[Train] epoch:91, lr:0.001000, total loss:0.004432\n",
            "[Train] epoch:92, lr:0.001000, total loss:0.004218\n",
            "[Train] epoch:93, lr:0.001000, total loss:0.003973\n",
            "[Train] epoch:94, lr:0.001000, total loss:0.004054\n",
            "[Train] epoch:95, lr:0.001000, total loss:0.003844\n",
            "[Train] epoch:96, lr:0.001000, total loss:0.004209\n",
            "[Train] epoch:97, lr:0.001000, total loss:0.004104\n",
            "[Train] epoch:98, lr:0.001000, total loss:0.004014\n",
            "[Train] epoch:99, lr:0.001000, total loss:0.003921\n",
            "[Train] epoch:100, lr:0.001000, total loss:0.003738\n",
            "[Train] epoch:101, lr:0.001000, total loss:0.003599\n",
            "[Train] epoch:102, lr:0.001000, total loss:0.003855\n",
            "[Train] epoch:103, lr:0.001000, total loss:0.003623\n",
            "[Train] epoch:104, lr:0.001000, total loss:0.003552\n",
            "[Train] epoch:105, lr:0.001000, total loss:0.003782\n",
            "[Train] epoch:106, lr:0.001000, total loss:0.003453\n",
            "[Train] epoch:107, lr:0.001000, total loss:0.003487\n",
            "[Train] epoch:108, lr:0.001000, total loss:0.003538\n",
            "[Train] epoch:109, lr:0.001000, total loss:0.003545\n",
            "[Train] epoch:110, lr:0.001000, total loss:0.003596\n",
            "[Train] epoch:111, lr:0.001000, total loss:0.003515\n",
            "[Train] epoch:112, lr:0.001000, total loss:0.003267\n",
            "[Train] epoch:113, lr:0.001000, total loss:0.003439\n",
            "[Train] epoch:114, lr:0.001000, total loss:0.003491\n",
            "[Train] epoch:115, lr:0.001000, total loss:0.003438\n",
            "[Train] epoch:116, lr:0.001000, total loss:0.003291\n",
            "[Train] epoch:117, lr:0.001000, total loss:0.003276\n",
            "[Train] epoch:118, lr:0.001000, total loss:0.003256\n",
            "[Train] epoch:119, lr:0.001000, total loss:0.003223\n",
            "[Train] epoch:120, lr:0.001000, total loss:0.003301\n",
            "[Train] epoch:121, lr:0.001000, total loss:0.003241\n",
            "[Train] epoch:122, lr:0.001000, total loss:0.003129\n",
            "[Train] epoch:123, lr:0.001000, total loss:0.003105\n",
            "[Train] epoch:124, lr:0.001000, total loss:0.003048\n",
            "[Train] epoch:125, lr:0.001000, total loss:0.003140\n",
            "[Train] epoch:126, lr:0.001000, total loss:0.003194\n",
            "[Train] epoch:127, lr:0.001000, total loss:0.003076\n",
            "[Train] epoch:128, lr:0.001000, total loss:0.003146\n",
            "[Train] epoch:129, lr:0.001000, total loss:0.003102\n",
            "[Train] epoch:130, lr:0.001000, total loss:0.003008\n",
            "[Train] epoch:131, lr:0.001000, total loss:0.003001\n",
            "[Train] epoch:132, lr:0.001000, total loss:0.003097\n",
            "[Train] epoch:133, lr:0.001000, total loss:0.002866\n",
            "[Train] epoch:134, lr:0.001000, total loss:0.002925\n",
            "[Train] epoch:135, lr:0.001000, total loss:0.002853\n",
            "[Train] epoch:136, lr:0.001000, total loss:0.002872\n",
            "[Train] epoch:137, lr:0.001000, total loss:0.002817\n",
            "[Train] epoch:138, lr:0.001000, total loss:0.002884\n",
            "[Train] epoch:139, lr:0.001000, total loss:0.002942\n",
            "[Train] epoch:140, lr:0.001000, total loss:0.002893\n",
            "[Train] epoch:141, lr:0.001000, total loss:0.002817\n",
            "[Train] epoch:142, lr:0.001000, total loss:0.002735\n",
            "[Train] epoch:143, lr:0.001000, total loss:0.002817\n",
            "[Train] epoch:144, lr:0.001000, total loss:0.002761\n",
            "[Train] epoch:145, lr:0.001000, total loss:0.002763\n",
            "[Train] epoch:146, lr:0.001000, total loss:0.002759\n",
            "[Train] epoch:147, lr:0.001000, total loss:0.002684\n",
            "[Train] epoch:148, lr:0.001000, total loss:0.002767\n",
            "[Train] epoch:149, lr:0.001000, total loss:0.002790\n",
            "[Train] epoch:150, lr:0.001000, total loss:0.002681\n",
            "[Train] epoch:151, lr:0.001000, total loss:0.002656\n",
            "[Train] epoch:152, lr:0.001000, total loss:0.002551\n",
            "[Train] epoch:153, lr:0.001000, total loss:0.002596\n",
            "[Train] epoch:154, lr:0.001000, total loss:0.002487\n",
            "[Train] epoch:155, lr:0.001000, total loss:0.002455\n",
            "[Train] epoch:156, lr:0.001000, total loss:0.002517\n",
            "[Train] epoch:157, lr:0.001000, total loss:0.002526\n",
            "[Train] epoch:158, lr:0.001000, total loss:0.002568\n",
            "[Train] epoch:159, lr:0.001000, total loss:0.002565\n",
            "[Train] epoch:160, lr:0.001000, total loss:0.002526\n",
            "[Train] epoch:161, lr:0.000999, total loss:0.002489\n",
            "[Train] epoch:162, lr:0.000999, total loss:0.002449\n",
            "[Train] epoch:163, lr:0.000999, total loss:0.002510\n",
            "[Train] epoch:164, lr:0.000999, total loss:0.002373\n",
            "[Train] epoch:165, lr:0.000999, total loss:0.002511\n",
            "[Train] epoch:166, lr:0.000999, total loss:0.002456\n",
            "[Train] epoch:167, lr:0.000999, total loss:0.002497\n",
            "[Train] epoch:168, lr:0.000999, total loss:0.002312\n",
            "[Train] epoch:169, lr:0.000999, total loss:0.002305\n",
            "[Train] epoch:170, lr:0.000999, total loss:0.002412\n",
            "[Train] epoch:171, lr:0.000999, total loss:0.002411\n",
            "[Train] epoch:172, lr:0.000999, total loss:0.002516\n",
            "[Train] epoch:173, lr:0.000999, total loss:0.002470\n",
            "[Train] epoch:174, lr:0.000999, total loss:0.002435\n",
            "[Train] epoch:175, lr:0.000999, total loss:0.002432\n",
            "[Train] epoch:176, lr:0.000999, total loss:0.002381\n",
            "[Train] epoch:177, lr:0.000999, total loss:0.002343\n",
            "[Train] epoch:178, lr:0.000999, total loss:0.002353\n",
            "[Train] epoch:179, lr:0.000999, total loss:0.002298\n",
            "[Train] epoch:180, lr:0.000999, total loss:0.002282\n",
            "[Train] epoch:181, lr:0.000999, total loss:0.002364\n",
            "[Train] epoch:182, lr:0.000999, total loss:0.002221\n",
            "[Train] epoch:183, lr:0.000999, total loss:0.002243\n",
            "[Train] epoch:184, lr:0.000999, total loss:0.002181\n",
            "[Train] epoch:185, lr:0.000999, total loss:0.002280\n",
            "[Train] epoch:186, lr:0.000999, total loss:0.002131\n",
            "[Train] epoch:187, lr:0.000999, total loss:0.002247\n",
            "[Train] epoch:188, lr:0.000999, total loss:0.002211\n",
            "[Train] epoch:189, lr:0.000999, total loss:0.002260\n",
            "[Train] epoch:190, lr:0.000999, total loss:0.002213\n",
            "[Train] epoch:191, lr:0.000999, total loss:0.002225\n",
            "[Train] epoch:192, lr:0.000999, total loss:0.002349\n",
            "[Train] epoch:193, lr:0.000999, total loss:0.002304\n",
            "[Train] epoch:194, lr:0.000999, total loss:0.002231\n",
            "[Train] epoch:195, lr:0.000999, total loss:0.002136\n",
            "[Train] epoch:196, lr:0.000999, total loss:0.002131\n",
            "[Train] epoch:197, lr:0.000999, total loss:0.002145\n",
            "[Train] epoch:198, lr:0.000999, total loss:0.002216\n",
            "[Train] epoch:199, lr:0.000999, total loss:0.002095\n",
            "[Train] epoch:200, lr:0.000999, total loss:0.002037\n",
            "[Train] epoch:201, lr:0.000999, total loss:0.002096\n",
            "[Train] epoch:202, lr:0.000999, total loss:0.002120\n",
            "[Train] epoch:203, lr:0.000999, total loss:0.002076\n",
            "[Train] epoch:204, lr:0.000999, total loss:0.002073\n",
            "[Train] epoch:205, lr:0.000999, total loss:0.002005\n",
            "[Train] epoch:206, lr:0.000999, total loss:0.002115\n",
            "[Train] epoch:207, lr:0.000999, total loss:0.002092\n",
            "[Train] epoch:208, lr:0.000999, total loss:0.002134\n",
            "[Train] epoch:209, lr:0.000999, total loss:0.002160\n",
            "[Train] epoch:210, lr:0.000999, total loss:0.002230\n",
            "[Train] epoch:211, lr:0.000999, total loss:0.002005\n",
            "[Train] epoch:212, lr:0.000999, total loss:0.001929\n",
            "[Train] epoch:213, lr:0.000999, total loss:0.001995\n",
            "[Train] epoch:214, lr:0.000999, total loss:0.002074\n",
            "[Train] epoch:215, lr:0.000999, total loss:0.001998\n",
            "[Train] epoch:216, lr:0.000999, total loss:0.001923\n",
            "[Train] epoch:217, lr:0.000999, total loss:0.001959\n",
            "[Train] epoch:218, lr:0.000999, total loss:0.001970\n",
            "[Train] epoch:219, lr:0.000999, total loss:0.001954\n",
            "[Train] epoch:220, lr:0.000999, total loss:0.001888\n",
            "[Train] epoch:221, lr:0.000999, total loss:0.001988\n",
            "[Train] epoch:222, lr:0.000999, total loss:0.002005\n",
            "[Train] epoch:223, lr:0.000999, total loss:0.001886\n",
            "[Train] epoch:224, lr:0.000999, total loss:0.001927\n",
            "[Train] epoch:225, lr:0.000999, total loss:0.001953\n",
            "[Train] epoch:226, lr:0.000999, total loss:0.001871\n",
            "[Train] epoch:227, lr:0.000999, total loss:0.001930\n",
            "[Train] epoch:228, lr:0.000999, total loss:0.001946\n",
            "[Train] epoch:229, lr:0.000999, total loss:0.001918\n",
            "[Train] epoch:230, lr:0.000999, total loss:0.001892\n",
            "[Train] epoch:231, lr:0.000999, total loss:0.001935\n",
            "[Train] epoch:232, lr:0.000999, total loss:0.002001\n",
            "[Train] epoch:233, lr:0.000999, total loss:0.001947\n",
            "[Train] epoch:234, lr:0.000999, total loss:0.001922\n",
            "[Train] epoch:235, lr:0.000999, total loss:0.001945\n",
            "[Train] epoch:236, lr:0.000999, total loss:0.001892\n",
            "[Train] epoch:237, lr:0.000999, total loss:0.001869\n",
            "[Train] epoch:238, lr:0.000999, total loss:0.001819\n",
            "[Train] epoch:239, lr:0.000999, total loss:0.001849\n",
            "[Train] epoch:240, lr:0.000999, total loss:0.001846\n",
            "[Train] epoch:241, lr:0.000999, total loss:0.001923\n",
            "[Train] epoch:242, lr:0.000999, total loss:0.001900\n",
            "[Train] epoch:243, lr:0.000999, total loss:0.001856\n",
            "[Train] epoch:244, lr:0.000999, total loss:0.001767\n",
            "[Train] epoch:245, lr:0.000999, total loss:0.001856\n",
            "[Train] epoch:246, lr:0.000999, total loss:0.001833\n",
            "[Train] epoch:247, lr:0.000999, total loss:0.001850\n",
            "[Train] epoch:248, lr:0.000999, total loss:0.001809\n",
            "[Train] epoch:249, lr:0.000999, total loss:0.001812\n",
            "[Train] epoch:250, lr:0.000999, total loss:0.001820\n",
            "[Train] epoch:251, lr:0.000999, total loss:0.001802\n",
            "[Train] epoch:252, lr:0.000999, total loss:0.001688\n",
            "[Train] epoch:253, lr:0.000999, total loss:0.001818\n",
            "[Train] epoch:254, lr:0.000999, total loss:0.001802\n",
            "[Train] epoch:255, lr:0.000999, total loss:0.001873\n",
            "[Train] epoch:256, lr:0.000999, total loss:0.001770\n",
            "[Train] epoch:257, lr:0.000999, total loss:0.001732\n",
            "[Train] epoch:258, lr:0.000999, total loss:0.001789\n",
            "[Train] epoch:259, lr:0.000999, total loss:0.001773\n",
            "[Train] epoch:260, lr:0.000999, total loss:0.001670\n",
            "[Train] epoch:261, lr:0.000999, total loss:0.001733\n",
            "[Train] epoch:262, lr:0.000999, total loss:0.001732\n",
            "[Train] epoch:263, lr:0.000999, total loss:0.001821\n",
            "[Train] epoch:264, lr:0.000999, total loss:0.001704\n",
            "[Train] epoch:265, lr:0.000999, total loss:0.001751\n",
            "[Train] epoch:266, lr:0.000999, total loss:0.001793\n",
            "[Train] epoch:267, lr:0.000999, total loss:0.001689\n",
            "[Train] epoch:268, lr:0.000999, total loss:0.001675\n",
            "[Train] epoch:269, lr:0.000999, total loss:0.001788\n",
            "[Train] epoch:270, lr:0.000999, total loss:0.001803\n",
            "[Train] epoch:271, lr:0.000998, total loss:0.001738\n",
            "[Train] epoch:272, lr:0.000998, total loss:0.001744\n",
            "[Train] epoch:273, lr:0.000998, total loss:0.001714\n",
            "[Train] epoch:274, lr:0.000998, total loss:0.001651\n",
            "[Train] epoch:275, lr:0.000998, total loss:0.001627\n",
            "[Train] epoch:276, lr:0.000998, total loss:0.001685\n",
            "[Train] epoch:277, lr:0.000998, total loss:0.001660\n",
            "[Train] epoch:278, lr:0.000998, total loss:0.001693\n",
            "[Train] epoch:279, lr:0.000998, total loss:0.001701\n",
            "[Train] epoch:280, lr:0.000998, total loss:0.001695\n",
            "[Train] epoch:281, lr:0.000998, total loss:0.001791\n",
            "[Train] epoch:282, lr:0.000998, total loss:0.001748\n",
            "[Train] epoch:283, lr:0.000998, total loss:0.001715\n",
            "[Train] epoch:284, lr:0.000998, total loss:0.001651\n",
            "[Train] epoch:285, lr:0.000998, total loss:0.001716\n",
            "[Train] epoch:286, lr:0.000998, total loss:0.001602\n",
            "[Train] epoch:287, lr:0.000998, total loss:0.001600\n",
            "[Train] epoch:288, lr:0.000998, total loss:0.001699\n",
            "[Train] epoch:289, lr:0.000998, total loss:0.001597\n",
            "[Train] epoch:290, lr:0.000998, total loss:0.001602\n",
            "[Train] epoch:291, lr:0.000998, total loss:0.001629\n",
            "[Train] epoch:292, lr:0.000998, total loss:0.001642\n",
            "[Train] epoch:293, lr:0.000998, total loss:0.001596\n",
            "[Train] epoch:294, lr:0.000998, total loss:0.001669\n",
            "[Train] epoch:295, lr:0.000998, total loss:0.001590\n",
            "[Train] epoch:296, lr:0.000998, total loss:0.001575\n",
            "[Train] epoch:297, lr:0.000998, total loss:0.001607\n",
            "[Train] epoch:298, lr:0.000998, total loss:0.001558\n",
            "[Train] epoch:299, lr:0.000998, total loss:0.001642\n",
            "[Train] epoch:300, lr:0.000998, total loss:0.001561\n",
            "[Train] epoch:301, lr:0.000998, total loss:0.001565\n",
            "[Train] epoch:302, lr:0.000998, total loss:0.001555\n",
            "[Train] epoch:303, lr:0.000998, total loss:0.001658\n",
            "[Train] epoch:304, lr:0.000998, total loss:0.001557\n",
            "[Train] epoch:305, lr:0.000998, total loss:0.001605\n",
            "[Train] epoch:306, lr:0.000998, total loss:0.001588\n",
            "[Train] epoch:307, lr:0.000998, total loss:0.001611\n",
            "[Train] epoch:308, lr:0.000998, total loss:0.001547\n",
            "[Train] epoch:309, lr:0.000998, total loss:0.001518\n",
            "[Train] epoch:310, lr:0.000998, total loss:0.001594\n",
            "[Train] epoch:311, lr:0.000998, total loss:0.001609\n",
            "[Train] epoch:312, lr:0.000998, total loss:0.001574\n",
            "[Train] epoch:313, lr:0.000998, total loss:0.001564\n",
            "[Train] epoch:314, lr:0.000998, total loss:0.001521\n",
            "[Train] epoch:315, lr:0.000998, total loss:0.001575\n",
            "[Train] epoch:316, lr:0.000998, total loss:0.001508\n",
            "[Train] epoch:317, lr:0.000998, total loss:0.001531\n",
            "[Train] epoch:318, lr:0.000998, total loss:0.001567\n",
            "[Train] epoch:319, lr:0.000998, total loss:0.001510\n",
            "[Train] epoch:320, lr:0.000998, total loss:0.001537\n",
            "[Train] epoch:321, lr:0.000998, total loss:0.001520\n",
            "[Train] epoch:322, lr:0.000998, total loss:0.001524\n",
            "[Train] epoch:323, lr:0.000998, total loss:0.001559\n",
            "[Train] epoch:324, lr:0.000998, total loss:0.001466\n",
            "[Train] epoch:325, lr:0.000998, total loss:0.001521\n",
            "[Train] epoch:326, lr:0.000998, total loss:0.001549\n",
            "[Train] epoch:327, lr:0.000998, total loss:0.001484\n",
            "[Train] epoch:328, lr:0.000998, total loss:0.001515\n",
            "[Train] epoch:329, lr:0.000998, total loss:0.001492\n",
            "[Train] epoch:330, lr:0.000998, total loss:0.001581\n",
            "[Train] epoch:331, lr:0.000998, total loss:0.001588\n",
            "[Train] epoch:332, lr:0.000998, total loss:0.001556\n",
            "[Train] epoch:333, lr:0.000998, total loss:0.001520\n",
            "[Train] epoch:334, lr:0.000998, total loss:0.001478\n",
            "[Train] epoch:335, lr:0.000998, total loss:0.001451\n",
            "[Train] epoch:336, lr:0.000998, total loss:0.001542\n",
            "[Train] epoch:337, lr:0.000998, total loss:0.001520\n",
            "[Train] epoch:338, lr:0.000998, total loss:0.001540\n",
            "[Train] epoch:339, lr:0.000998, total loss:0.001429\n",
            "[Train] epoch:340, lr:0.000998, total loss:0.001562\n",
            "[Train] epoch:341, lr:0.000998, total loss:0.001456\n",
            "[Train] epoch:342, lr:0.000998, total loss:0.001427\n",
            "[Train] epoch:343, lr:0.000998, total loss:0.001476\n",
            "[Train] epoch:344, lr:0.000998, total loss:0.001570\n",
            "[Train] epoch:345, lr:0.000998, total loss:0.001483\n",
            "[Train] epoch:346, lr:0.000998, total loss:0.001518\n",
            "[Train] epoch:347, lr:0.000997, total loss:0.001444\n",
            "[Train] epoch:348, lr:0.000997, total loss:0.001494\n",
            "[Train] epoch:349, lr:0.000997, total loss:0.001488\n",
            "[Train] epoch:350, lr:0.000997, total loss:0.001530\n",
            "[Train] epoch:351, lr:0.000997, total loss:0.001559\n",
            "[Train] epoch:352, lr:0.000997, total loss:0.001447\n",
            "[Train] epoch:353, lr:0.000997, total loss:0.001457\n",
            "[Train] epoch:354, lr:0.000997, total loss:0.001426\n",
            "[Train] epoch:355, lr:0.000997, total loss:0.001446\n",
            "[Train] epoch:356, lr:0.000997, total loss:0.001437\n",
            "[Train] epoch:357, lr:0.000997, total loss:0.001429\n",
            "[Train] epoch:358, lr:0.000997, total loss:0.001406\n",
            "[Train] epoch:359, lr:0.000997, total loss:0.001413\n",
            "[Train] epoch:360, lr:0.000997, total loss:0.001419\n",
            "[Train] epoch:361, lr:0.000997, total loss:0.001445\n",
            "[Train] epoch:362, lr:0.000997, total loss:0.001420\n",
            "[Train] epoch:363, lr:0.000997, total loss:0.001365\n",
            "[Train] epoch:364, lr:0.000997, total loss:0.001368\n",
            "[Train] epoch:365, lr:0.000997, total loss:0.001401\n",
            "[Train] epoch:366, lr:0.000997, total loss:0.001433\n",
            "[Train] epoch:367, lr:0.000997, total loss:0.001412\n",
            "[Train] epoch:368, lr:0.000997, total loss:0.001409\n",
            "[Train] epoch:369, lr:0.000997, total loss:0.001341\n",
            "[Train] epoch:370, lr:0.000997, total loss:0.001346\n",
            "[Train] epoch:371, lr:0.000997, total loss:0.001509\n",
            "[Train] epoch:372, lr:0.000997, total loss:0.001418\n",
            "[Train] epoch:373, lr:0.000997, total loss:0.001343\n",
            "[Train] epoch:374, lr:0.000997, total loss:0.001365\n",
            "[Train] epoch:375, lr:0.000997, total loss:0.001430\n",
            "[Train] epoch:376, lr:0.000997, total loss:0.001264\n",
            "[Train] epoch:377, lr:0.000997, total loss:0.001397\n",
            "[Train] epoch:378, lr:0.000997, total loss:0.001373\n",
            "[Train] epoch:379, lr:0.000997, total loss:0.001386\n",
            "[Train] epoch:380, lr:0.000997, total loss:0.001382\n",
            "[Train] epoch:381, lr:0.000997, total loss:0.001400\n",
            "[Train] epoch:382, lr:0.000997, total loss:0.001331\n",
            "[Train] epoch:383, lr:0.000997, total loss:0.001295\n",
            "[Train] epoch:384, lr:0.000997, total loss:0.001311\n",
            "[Train] epoch:385, lr:0.000997, total loss:0.001382\n",
            "[Train] epoch:386, lr:0.000997, total loss:0.001383\n",
            "[Train] epoch:387, lr:0.000997, total loss:0.001309\n",
            "[Train] epoch:388, lr:0.000997, total loss:0.001337\n",
            "[Train] epoch:389, lr:0.000997, total loss:0.001343\n",
            "[Train] epoch:390, lr:0.000997, total loss:0.001312\n",
            "[Train] epoch:391, lr:0.000997, total loss:0.001364\n",
            "[Train] epoch:392, lr:0.000997, total loss:0.001371\n",
            "[Train] epoch:393, lr:0.000997, total loss:0.001285\n",
            "[Train] epoch:394, lr:0.000997, total loss:0.001342\n",
            "[Train] epoch:395, lr:0.000997, total loss:0.001313\n",
            "[Train] epoch:396, lr:0.000997, total loss:0.001276\n",
            "[Train] epoch:397, lr:0.000997, total loss:0.001316\n",
            "[Train] epoch:398, lr:0.000997, total loss:0.001380\n",
            "[Train] epoch:399, lr:0.000997, total loss:0.001245\n",
            "[Train] epoch:400, lr:0.000997, total loss:0.001350\n",
            "[Train] epoch:401, lr:0.000997, total loss:0.001278\n",
            "[Train] epoch:402, lr:0.000997, total loss:0.001326\n",
            "[Train] epoch:403, lr:0.000997, total loss:0.001345\n",
            "[Train] epoch:404, lr:0.000997, total loss:0.001315\n",
            "[Train] epoch:405, lr:0.000997, total loss:0.001278\n",
            "[Train] epoch:406, lr:0.000997, total loss:0.001220\n",
            "[Train] epoch:407, lr:0.000997, total loss:0.001272\n",
            "[Train] epoch:408, lr:0.000996, total loss:0.001237\n",
            "[Train] epoch:409, lr:0.000996, total loss:0.001255\n",
            "[Train] epoch:410, lr:0.000996, total loss:0.001264\n",
            "[Train] epoch:411, lr:0.000996, total loss:0.001324\n",
            "[Train] epoch:412, lr:0.000996, total loss:0.001217\n",
            "[Train] epoch:413, lr:0.000996, total loss:0.001197\n",
            "[Train] epoch:414, lr:0.000996, total loss:0.001300\n",
            "[Train] epoch:415, lr:0.000996, total loss:0.001287\n",
            "[Train] epoch:416, lr:0.000996, total loss:0.001366\n",
            "[Train] epoch:417, lr:0.000996, total loss:0.001303\n",
            "[Train] epoch:418, lr:0.000996, total loss:0.001338\n",
            "[Train] epoch:419, lr:0.000996, total loss:0.001240\n",
            "[Train] epoch:420, lr:0.000996, total loss:0.001309\n",
            "[Train] epoch:421, lr:0.000996, total loss:0.001290\n",
            "[Train] epoch:422, lr:0.000996, total loss:0.001297\n",
            "[Train] epoch:423, lr:0.000996, total loss:0.001256\n",
            "[Train] epoch:424, lr:0.000996, total loss:0.001201\n",
            "[Train] epoch:425, lr:0.000996, total loss:0.001273\n",
            "[Train] epoch:426, lr:0.000996, total loss:0.001261\n",
            "[Train] epoch:427, lr:0.000996, total loss:0.001276\n",
            "[Train] epoch:428, lr:0.000996, total loss:0.001216\n",
            "[Train] epoch:429, lr:0.000996, total loss:0.001256\n",
            "[Train] epoch:430, lr:0.000996, total loss:0.001264\n",
            "[Train] epoch:431, lr:0.000996, total loss:0.001201\n",
            "[Train] epoch:432, lr:0.000996, total loss:0.001230\n",
            "[Train] epoch:433, lr:0.000996, total loss:0.001264\n",
            "[Train] epoch:434, lr:0.000996, total loss:0.001201\n",
            "[Train] epoch:435, lr:0.000996, total loss:0.001237\n",
            "[Train] epoch:436, lr:0.000996, total loss:0.001199\n",
            "[Train] epoch:437, lr:0.000996, total loss:0.001213\n",
            "[Train] epoch:438, lr:0.000996, total loss:0.001230\n",
            "[Train] epoch:439, lr:0.000996, total loss:0.001184\n",
            "[Train] epoch:440, lr:0.000996, total loss:0.001233\n",
            "[Train] epoch:441, lr:0.000996, total loss:0.001247\n",
            "[Train] epoch:442, lr:0.000996, total loss:0.001209\n",
            "[Train] epoch:443, lr:0.000996, total loss:0.001162\n",
            "[Train] epoch:444, lr:0.000996, total loss:0.001196\n",
            "[Train] epoch:445, lr:0.000996, total loss:0.001228\n",
            "[Train] epoch:446, lr:0.000996, total loss:0.001234\n",
            "[Train] epoch:447, lr:0.000996, total loss:0.001210\n",
            "[Train] epoch:448, lr:0.000996, total loss:0.001193\n",
            "[Train] epoch:449, lr:0.000996, total loss:0.001193\n",
            "[Train] epoch:450, lr:0.000996, total loss:0.001140\n",
            "[Train] epoch:451, lr:0.000996, total loss:0.001159\n",
            "[Train] epoch:452, lr:0.000996, total loss:0.001235\n",
            "[Train] epoch:453, lr:0.000996, total loss:0.001201\n",
            "[Train] epoch:454, lr:0.000996, total loss:0.001187\n",
            "[Train] epoch:455, lr:0.000996, total loss:0.001173\n",
            "[Train] epoch:456, lr:0.000996, total loss:0.001253\n",
            "[Train] epoch:457, lr:0.000996, total loss:0.001318\n",
            "[Train] epoch:458, lr:0.000996, total loss:0.001292\n",
            "[Train] epoch:459, lr:0.000996, total loss:0.001218\n",
            "[Train] epoch:460, lr:0.000996, total loss:0.001157\n",
            "[Train] epoch:461, lr:0.000996, total loss:0.001185\n",
            "[Train] epoch:462, lr:0.000995, total loss:0.001145\n",
            "[Train] epoch:463, lr:0.000995, total loss:0.001181\n",
            "[Train] epoch:464, lr:0.000995, total loss:0.001171\n",
            "[Train] epoch:465, lr:0.000995, total loss:0.001215\n",
            "[Train] epoch:466, lr:0.000995, total loss:0.001230\n",
            "[Train] epoch:467, lr:0.000995, total loss:0.001122\n",
            "[Train] epoch:468, lr:0.000995, total loss:0.001207\n",
            "[Train] epoch:469, lr:0.000995, total loss:0.001191\n",
            "[Train] epoch:470, lr:0.000995, total loss:0.001210\n",
            "[Train] epoch:471, lr:0.000995, total loss:0.001146\n",
            "[Train] epoch:472, lr:0.000995, total loss:0.001132\n",
            "[Train] epoch:473, lr:0.000995, total loss:0.001229\n",
            "[Train] epoch:474, lr:0.000995, total loss:0.001142\n",
            "[Train] epoch:475, lr:0.000995, total loss:0.001122\n",
            "[Train] epoch:476, lr:0.000995, total loss:0.001242\n",
            "[Train] epoch:477, lr:0.000995, total loss:0.001153\n",
            "[Train] epoch:478, lr:0.000995, total loss:0.001224\n",
            "[Train] epoch:479, lr:0.000995, total loss:0.001174\n",
            "[Train] epoch:480, lr:0.000995, total loss:0.001075\n",
            "[Train] epoch:481, lr:0.000995, total loss:0.001109\n",
            "[Train] epoch:482, lr:0.000995, total loss:0.001097\n",
            "[Train] epoch:483, lr:0.000995, total loss:0.001209\n",
            "[Train] epoch:484, lr:0.000995, total loss:0.001134\n",
            "[Train] epoch:485, lr:0.000995, total loss:0.001215\n",
            "[Train] epoch:486, lr:0.000995, total loss:0.001184\n",
            "[Train] epoch:487, lr:0.000995, total loss:0.001129\n",
            "[Train] epoch:488, lr:0.000995, total loss:0.001143\n",
            "[Train] epoch:489, lr:0.000995, total loss:0.001133\n",
            "[Train] epoch:490, lr:0.000995, total loss:0.001102\n",
            "[Train] epoch:491, lr:0.000995, total loss:0.001086\n",
            "[Train] epoch:492, lr:0.000995, total loss:0.001114\n",
            "[Train] epoch:493, lr:0.000995, total loss:0.001067\n",
            "[Train] epoch:494, lr:0.000995, total loss:0.001115\n",
            "[Train] epoch:495, lr:0.000995, total loss:0.001074\n",
            "[Train] epoch:496, lr:0.000995, total loss:0.001113\n",
            "[Train] epoch:497, lr:0.000995, total loss:0.001052\n",
            "[Train] epoch:498, lr:0.000995, total loss:0.001101\n",
            "[Train] epoch:499, lr:0.000995, total loss:0.001093\n",
            "[Train] epoch:500, lr:0.000995, total loss:0.001192\n",
            "[Train] epoch:501, lr:0.000995, total loss:0.001067\n",
            "[Train] epoch:502, lr:0.000995, total loss:0.001142\n",
            "[Train] epoch:503, lr:0.000995, total loss:0.001025\n",
            "[Train] epoch:504, lr:0.000995, total loss:0.001102\n",
            "[Train] epoch:505, lr:0.000995, total loss:0.001098\n",
            "[Train] epoch:506, lr:0.000995, total loss:0.001184\n",
            "[Train] epoch:507, lr:0.000995, total loss:0.001134\n",
            "[Train] epoch:508, lr:0.000995, total loss:0.001138\n",
            "[Train] epoch:509, lr:0.000994, total loss:0.001049\n",
            "[Train] epoch:510, lr:0.000994, total loss:0.001134\n",
            "[Train] epoch:511, lr:0.000994, total loss:0.001053\n",
            "[Train] epoch:512, lr:0.000994, total loss:0.001103\n",
            "[Train] epoch:513, lr:0.000994, total loss:0.001096\n",
            "[Train] epoch:514, lr:0.000994, total loss:0.001085\n",
            "[Train] epoch:515, lr:0.000994, total loss:0.001048\n",
            "[Train] epoch:516, lr:0.000994, total loss:0.001105\n",
            "[Train] epoch:517, lr:0.000994, total loss:0.001101\n",
            "[Train] epoch:518, lr:0.000994, total loss:0.001081\n",
            "[Train] epoch:519, lr:0.000994, total loss:0.001047\n",
            "[Train] epoch:520, lr:0.000994, total loss:0.001090\n",
            "[Train] epoch:521, lr:0.000994, total loss:0.001173\n",
            "[Train] epoch:522, lr:0.000994, total loss:0.001103\n",
            "[Train] epoch:523, lr:0.000994, total loss:0.001060\n",
            "[Train] epoch:524, lr:0.000994, total loss:0.001069\n",
            "[Train] epoch:525, lr:0.000994, total loss:0.001079\n",
            "[Train] epoch:526, lr:0.000994, total loss:0.001064\n",
            "[Train] epoch:527, lr:0.000994, total loss:0.000973\n",
            "[Train] epoch:528, lr:0.000994, total loss:0.001081\n",
            "[Train] epoch:529, lr:0.000994, total loss:0.001069\n",
            "[Train] epoch:530, lr:0.000994, total loss:0.001047\n",
            "[Train] epoch:531, lr:0.000994, total loss:0.001045\n",
            "[Train] epoch:532, lr:0.000994, total loss:0.001101\n",
            "[Train] epoch:533, lr:0.000994, total loss:0.001061\n",
            "[Train] epoch:534, lr:0.000994, total loss:0.001080\n",
            "[Train] epoch:535, lr:0.000994, total loss:0.001007\n",
            "[Train] epoch:536, lr:0.000994, total loss:0.000993\n",
            "[Train] epoch:537, lr:0.000994, total loss:0.001003\n",
            "[Train] epoch:538, lr:0.000994, total loss:0.001052\n",
            "[Train] epoch:539, lr:0.000994, total loss:0.000966\n",
            "[Train] epoch:540, lr:0.000994, total loss:0.001060\n",
            "[Train] epoch:541, lr:0.000994, total loss:0.001089\n",
            "[Train] epoch:542, lr:0.000994, total loss:0.001093\n",
            "[Train] epoch:543, lr:0.000994, total loss:0.001006\n",
            "[Train] epoch:544, lr:0.000994, total loss:0.000952\n",
            "[Train] epoch:545, lr:0.000994, total loss:0.001042\n",
            "[Train] epoch:546, lr:0.000994, total loss:0.001059\n",
            "[Train] epoch:547, lr:0.000994, total loss:0.001055\n",
            "[Train] epoch:548, lr:0.000994, total loss:0.000994\n",
            "[Train] epoch:549, lr:0.000994, total loss:0.001045\n",
            "[Train] epoch:550, lr:0.000994, total loss:0.001021\n",
            "[Train] epoch:551, lr:0.000994, total loss:0.001055\n",
            "[Train] epoch:552, lr:0.000994, total loss:0.000967\n",
            "[Train] epoch:553, lr:0.000993, total loss:0.001029\n",
            "[Train] epoch:554, lr:0.000993, total loss:0.001008\n",
            "[Train] epoch:555, lr:0.000993, total loss:0.001030\n",
            "[Train] epoch:556, lr:0.000993, total loss:0.001000\n",
            "[Train] epoch:557, lr:0.000993, total loss:0.001016\n",
            "[Train] epoch:558, lr:0.000993, total loss:0.001045\n",
            "[Train] epoch:559, lr:0.000993, total loss:0.001061\n",
            "[Train] epoch:560, lr:0.000993, total loss:0.001012\n",
            "[Train] epoch:561, lr:0.000993, total loss:0.000939\n",
            "[Train] epoch:562, lr:0.000993, total loss:0.001007\n",
            "[Train] epoch:563, lr:0.000993, total loss:0.001020\n",
            "[Train] epoch:564, lr:0.000993, total loss:0.001076\n",
            "[Train] epoch:565, lr:0.000993, total loss:0.000974\n",
            "[Train] epoch:566, lr:0.000993, total loss:0.001028\n",
            "[Train] epoch:567, lr:0.000993, total loss:0.001039\n",
            "[Train] epoch:568, lr:0.000993, total loss:0.001061\n",
            "[Train] epoch:569, lr:0.000993, total loss:0.000977\n",
            "[Train] epoch:570, lr:0.000993, total loss:0.001030\n",
            "[Train] epoch:571, lr:0.000993, total loss:0.001019\n",
            "[Train] epoch:572, lr:0.000993, total loss:0.000984\n",
            "[Train] epoch:573, lr:0.000993, total loss:0.000984\n",
            "[Train] epoch:574, lr:0.000993, total loss:0.000998\n",
            "[Train] epoch:575, lr:0.000993, total loss:0.001087\n",
            "[Train] epoch:576, lr:0.000993, total loss:0.001019\n",
            "[Train] epoch:577, lr:0.000993, total loss:0.001064\n",
            "[Train] epoch:578, lr:0.000993, total loss:0.000989\n",
            "[Train] epoch:579, lr:0.000993, total loss:0.001028\n",
            "[Train] epoch:580, lr:0.000993, total loss:0.000948\n",
            "[Train] epoch:581, lr:0.000993, total loss:0.000971\n",
            "[Train] epoch:582, lr:0.000993, total loss:0.000926\n",
            "[Train] epoch:583, lr:0.000993, total loss:0.000918\n",
            "[Train] epoch:584, lr:0.000993, total loss:0.000983\n",
            "[Train] epoch:585, lr:0.000993, total loss:0.000933\n",
            "[Train] epoch:586, lr:0.000993, total loss:0.000933\n",
            "[Train] epoch:587, lr:0.000993, total loss:0.000954\n",
            "[Train] epoch:588, lr:0.000993, total loss:0.000945\n",
            "[Train] epoch:589, lr:0.000993, total loss:0.000965\n",
            "[Train] epoch:590, lr:0.000993, total loss:0.001003\n",
            "[Train] epoch:591, lr:0.000993, total loss:0.000939\n",
            "[Train] epoch:592, lr:0.000993, total loss:0.000951\n",
            "[Train] epoch:593, lr:0.000992, total loss:0.000903\n",
            "[Train] epoch:594, lr:0.000992, total loss:0.000922\n",
            "[Train] epoch:595, lr:0.000992, total loss:0.000945\n",
            "[Train] epoch:596, lr:0.000992, total loss:0.000993\n",
            "[Train] epoch:597, lr:0.000992, total loss:0.000906\n",
            "[Train] epoch:598, lr:0.000992, total loss:0.000933\n",
            "[Train] epoch:599, lr:0.000992, total loss:0.000952\n",
            "[Train] epoch:600, lr:0.000992, total loss:0.001029\n",
            "[Train] epoch:601, lr:0.000992, total loss:0.000986\n",
            "[Train] epoch:602, lr:0.000992, total loss:0.000946\n",
            "[Train] epoch:603, lr:0.000992, total loss:0.000927\n",
            "[Train] epoch:604, lr:0.000992, total loss:0.000957\n",
            "[Train] epoch:605, lr:0.000992, total loss:0.000914\n",
            "[Train] epoch:606, lr:0.000992, total loss:0.000904\n",
            "[Train] epoch:607, lr:0.000992, total loss:0.000951\n",
            "[Train] epoch:608, lr:0.000992, total loss:0.000939\n",
            "[Train] epoch:609, lr:0.000992, total loss:0.000927\n",
            "[Train] epoch:610, lr:0.000992, total loss:0.000911\n",
            "[Train] epoch:611, lr:0.000992, total loss:0.000949\n",
            "[Train] epoch:612, lr:0.000992, total loss:0.000963\n",
            "[Train] epoch:613, lr:0.000992, total loss:0.000912\n",
            "[Train] epoch:614, lr:0.000992, total loss:0.000892\n",
            "[Train] epoch:615, lr:0.000992, total loss:0.000943\n",
            "[Train] epoch:616, lr:0.000992, total loss:0.000945\n",
            "[Train] epoch:617, lr:0.000992, total loss:0.000946\n",
            "[Train] epoch:618, lr:0.000992, total loss:0.000958\n",
            "[Train] epoch:619, lr:0.000992, total loss:0.000938\n",
            "[Train] epoch:620, lr:0.000992, total loss:0.000903\n",
            "[Train] epoch:621, lr:0.000992, total loss:0.000937\n",
            "[Train] epoch:622, lr:0.000992, total loss:0.000919\n",
            "[Train] epoch:623, lr:0.000992, total loss:0.000962\n",
            "[Train] epoch:624, lr:0.000992, total loss:0.000981\n",
            "[Train] epoch:625, lr:0.000992, total loss:0.000910\n",
            "[Train] epoch:626, lr:0.000992, total loss:0.000923\n",
            "[Train] epoch:627, lr:0.000992, total loss:0.000947\n",
            "[Train] epoch:628, lr:0.000992, total loss:0.001003\n",
            "[Train] epoch:629, lr:0.000992, total loss:0.000929\n",
            "[Train] epoch:630, lr:0.000992, total loss:0.000950\n",
            "[Train] epoch:631, lr:0.000991, total loss:0.000942\n",
            "[Train] epoch:632, lr:0.000991, total loss:0.000879\n",
            "[Train] epoch:633, lr:0.000991, total loss:0.000919\n",
            "[Train] epoch:634, lr:0.000991, total loss:0.000966\n",
            "[Train] epoch:635, lr:0.000991, total loss:0.000892\n",
            "[Train] epoch:636, lr:0.000991, total loss:0.000886\n",
            "[Train] epoch:637, lr:0.000991, total loss:0.000895\n",
            "[Train] epoch:638, lr:0.000991, total loss:0.000847\n",
            "[Train] epoch:639, lr:0.000991, total loss:0.000915\n",
            "[Train] epoch:640, lr:0.000991, total loss:0.000868\n",
            "[Train] epoch:641, lr:0.000991, total loss:0.000848\n",
            "[Train] epoch:642, lr:0.000991, total loss:0.000866\n",
            "[Train] epoch:643, lr:0.000991, total loss:0.000835\n",
            "[Train] epoch:644, lr:0.000991, total loss:0.000855\n",
            "[Train] epoch:645, lr:0.000991, total loss:0.000822\n",
            "[Train] epoch:646, lr:0.000991, total loss:0.000897\n",
            "[Train] epoch:647, lr:0.000991, total loss:0.000872\n",
            "[Train] epoch:648, lr:0.000991, total loss:0.000839\n",
            "[Train] epoch:649, lr:0.000991, total loss:0.000829\n",
            "[Train] epoch:650, lr:0.000991, total loss:0.000806\n",
            "[Train] epoch:651, lr:0.000991, total loss:0.000832\n",
            "[Train] epoch:652, lr:0.000991, total loss:0.000860\n",
            "[Train] epoch:653, lr:0.000991, total loss:0.000811\n",
            "[Train] epoch:654, lr:0.000991, total loss:0.000893\n",
            "[Train] epoch:655, lr:0.000991, total loss:0.000825\n",
            "[Train] epoch:656, lr:0.000991, total loss:0.000849\n",
            "[Train] epoch:657, lr:0.000991, total loss:0.000866\n",
            "[Train] epoch:658, lr:0.000991, total loss:0.000848\n",
            "[Train] epoch:659, lr:0.000991, total loss:0.000840\n",
            "[Train] epoch:660, lr:0.000991, total loss:0.000846\n",
            "[Train] epoch:661, lr:0.000991, total loss:0.000812\n",
            "[Train] epoch:662, lr:0.000991, total loss:0.000898\n",
            "[Train] epoch:663, lr:0.000991, total loss:0.000839\n",
            "[Train] epoch:664, lr:0.000991, total loss:0.000865\n",
            "[Train] epoch:665, lr:0.000991, total loss:0.000897\n",
            "[Train] epoch:666, lr:0.000990, total loss:0.000899\n",
            "[Train] epoch:667, lr:0.000990, total loss:0.000864\n",
            "[Train] epoch:668, lr:0.000990, total loss:0.000902\n",
            "[Train] epoch:669, lr:0.000990, total loss:0.000881\n",
            "[Train] epoch:670, lr:0.000990, total loss:0.000789\n",
            "[Train] epoch:671, lr:0.000990, total loss:0.000863\n",
            "[Train] epoch:672, lr:0.000990, total loss:0.000873\n",
            "[Train] epoch:673, lr:0.000990, total loss:0.000851\n",
            "[Train] epoch:674, lr:0.000990, total loss:0.000892\n",
            "[Train] epoch:675, lr:0.000990, total loss:0.000826\n",
            "[Train] epoch:676, lr:0.000990, total loss:0.000844\n",
            "[Train] epoch:677, lr:0.000990, total loss:0.000870\n",
            "[Train] epoch:678, lr:0.000990, total loss:0.000849\n",
            "[Train] epoch:679, lr:0.000990, total loss:0.000888\n",
            "[Train] epoch:680, lr:0.000990, total loss:0.000835\n",
            "[Train] epoch:681, lr:0.000990, total loss:0.000883\n",
            "[Train] epoch:682, lr:0.000990, total loss:0.000871\n",
            "[Train] epoch:683, lr:0.000990, total loss:0.000837\n",
            "[Train] epoch:684, lr:0.000990, total loss:0.000888\n",
            "[Train] epoch:685, lr:0.000990, total loss:0.000882\n",
            "[Train] epoch:686, lr:0.000990, total loss:0.000872\n",
            "[Train] epoch:687, lr:0.000990, total loss:0.000894\n",
            "[Train] epoch:688, lr:0.000990, total loss:0.000877\n",
            "[Train] epoch:689, lr:0.000990, total loss:0.000834\n",
            "[Train] epoch:690, lr:0.000990, total loss:0.000810\n",
            "[Train] epoch:691, lr:0.000990, total loss:0.000787\n",
            "[Train] epoch:692, lr:0.000990, total loss:0.000828\n",
            "[Train] epoch:693, lr:0.000990, total loss:0.000826\n",
            "[Train] epoch:694, lr:0.000990, total loss:0.000810\n",
            "[Train] epoch:695, lr:0.000990, total loss:0.000798\n",
            "[Train] epoch:696, lr:0.000990, total loss:0.000809\n",
            "[Train] epoch:697, lr:0.000990, total loss:0.000821\n",
            "[Train] epoch:698, lr:0.000990, total loss:0.000815\n",
            "[Train] epoch:699, lr:0.000990, total loss:0.000803\n",
            "[Train] epoch:700, lr:0.000989, total loss:0.000838\n",
            "[Train] epoch:701, lr:0.000989, total loss:0.000828\n",
            "[Train] epoch:702, lr:0.000989, total loss:0.000849\n",
            "[Train] epoch:703, lr:0.000989, total loss:0.000851\n",
            "[Train] epoch:704, lr:0.000989, total loss:0.000793\n",
            "[Train] epoch:705, lr:0.000989, total loss:0.000811\n",
            "[Train] epoch:706, lr:0.000989, total loss:0.000824\n",
            "[Train] epoch:707, lr:0.000989, total loss:0.000842\n",
            "[Train] epoch:708, lr:0.000989, total loss:0.000807\n",
            "[Train] epoch:709, lr:0.000989, total loss:0.000824\n",
            "[Train] epoch:710, lr:0.000989, total loss:0.000861\n",
            "[Train] epoch:711, lr:0.000989, total loss:0.000837\n",
            "[Train] epoch:712, lr:0.000989, total loss:0.000786\n",
            "[Train] epoch:713, lr:0.000989, total loss:0.000836\n",
            "[Train] epoch:714, lr:0.000989, total loss:0.000856\n",
            "[Train] epoch:715, lr:0.000989, total loss:0.000825\n",
            "[Train] epoch:716, lr:0.000989, total loss:0.000794\n",
            "[Train] epoch:717, lr:0.000989, total loss:0.000794\n",
            "[Train] epoch:718, lr:0.000989, total loss:0.000816\n",
            "[Train] epoch:719, lr:0.000989, total loss:0.000843\n",
            "[Train] epoch:720, lr:0.000989, total loss:0.000887\n",
            "[Train] epoch:721, lr:0.000989, total loss:0.000796\n",
            "[Train] epoch:722, lr:0.000989, total loss:0.000822\n",
            "[Train] epoch:723, lr:0.000989, total loss:0.000748\n",
            "[Train] epoch:724, lr:0.000989, total loss:0.000744\n",
            "[Train] epoch:725, lr:0.000989, total loss:0.000714\n",
            "[Train] epoch:726, lr:0.000989, total loss:0.000810\n",
            "[Train] epoch:727, lr:0.000989, total loss:0.000782\n",
            "[Train] epoch:728, lr:0.000989, total loss:0.000829\n",
            "[Train] epoch:729, lr:0.000989, total loss:0.000739\n",
            "[Train] epoch:730, lr:0.000989, total loss:0.000885\n",
            "[Train] epoch:731, lr:0.000989, total loss:0.000789\n",
            "[Train] epoch:732, lr:0.000988, total loss:0.000799\n",
            "[Train] epoch:733, lr:0.000988, total loss:0.000778\n",
            "[Train] epoch:734, lr:0.000988, total loss:0.000829\n",
            "[Train] epoch:735, lr:0.000988, total loss:0.000751\n",
            "[Train] epoch:736, lr:0.000988, total loss:0.000811\n",
            "[Train] epoch:737, lr:0.000988, total loss:0.000821\n",
            "[Train] epoch:738, lr:0.000988, total loss:0.000812\n",
            "[Train] epoch:739, lr:0.000988, total loss:0.000761\n",
            "[Train] epoch:740, lr:0.000988, total loss:0.000779\n",
            "[Train] epoch:741, lr:0.000988, total loss:0.000792\n",
            "[Train] epoch:742, lr:0.000988, total loss:0.000771\n",
            "[Train] epoch:743, lr:0.000988, total loss:0.000736\n",
            "[Train] epoch:744, lr:0.000988, total loss:0.000772\n",
            "[Train] epoch:745, lr:0.000988, total loss:0.000807\n",
            "[Train] epoch:746, lr:0.000988, total loss:0.000852\n",
            "[Train] epoch:747, lr:0.000988, total loss:0.000777\n",
            "[Train] epoch:748, lr:0.000988, total loss:0.000736\n",
            "[Train] epoch:749, lr:0.000988, total loss:0.000790\n",
            "[Train] epoch:750, lr:0.000988, total loss:0.000830\n",
            "[Train] epoch:751, lr:0.000988, total loss:0.000839\n",
            "[Train] epoch:752, lr:0.000988, total loss:0.000814\n",
            "[Train] epoch:753, lr:0.000988, total loss:0.000776\n",
            "[Train] epoch:754, lr:0.000988, total loss:0.000801\n",
            "[Train] epoch:755, lr:0.000988, total loss:0.000729\n",
            "[Train] epoch:756, lr:0.000988, total loss:0.000758\n",
            "[Train] epoch:757, lr:0.000988, total loss:0.000698\n",
            "[Train] epoch:758, lr:0.000988, total loss:0.000731\n",
            "[Train] epoch:759, lr:0.000988, total loss:0.000736\n",
            "[Train] epoch:760, lr:0.000988, total loss:0.000760\n",
            "[Train] epoch:761, lr:0.000988, total loss:0.000742\n",
            "[Train] epoch:762, lr:0.000988, total loss:0.000761\n",
            "[Train] epoch:763, lr:0.000987, total loss:0.000741\n",
            "[Train] epoch:764, lr:0.000987, total loss:0.000750\n",
            "[Train] epoch:765, lr:0.000987, total loss:0.000729\n",
            "[Train] epoch:766, lr:0.000987, total loss:0.000745\n",
            "[Train] epoch:767, lr:0.000987, total loss:0.000772\n",
            "[Train] epoch:768, lr:0.000987, total loss:0.000779\n",
            "[Train] epoch:769, lr:0.000987, total loss:0.000748\n",
            "[Train] epoch:770, lr:0.000987, total loss:0.000759\n",
            "[Train] epoch:771, lr:0.000987, total loss:0.000698\n",
            "[Train] epoch:772, lr:0.000987, total loss:0.000761\n",
            "[Train] epoch:773, lr:0.000987, total loss:0.000705\n",
            "[Train] epoch:774, lr:0.000987, total loss:0.000771\n",
            "[Train] epoch:775, lr:0.000987, total loss:0.000774\n",
            "[Train] epoch:776, lr:0.000987, total loss:0.000724\n",
            "[Train] epoch:777, lr:0.000987, total loss:0.000770\n",
            "[Train] epoch:778, lr:0.000987, total loss:0.000765\n",
            "[Train] epoch:779, lr:0.000987, total loss:0.000794\n",
            "[Train] epoch:780, lr:0.000987, total loss:0.000740\n",
            "[Train] epoch:781, lr:0.000987, total loss:0.000774\n",
            "[Train] epoch:782, lr:0.000987, total loss:0.000720\n",
            "[Train] epoch:783, lr:0.000987, total loss:0.000721\n",
            "[Train] epoch:784, lr:0.000987, total loss:0.000745\n",
            "[Train] epoch:785, lr:0.000987, total loss:0.000747\n",
            "[Train] epoch:786, lr:0.000987, total loss:0.000812\n",
            "[Train] epoch:787, lr:0.000987, total loss:0.000751\n",
            "[Train] epoch:788, lr:0.000987, total loss:0.000781\n",
            "[Train] epoch:789, lr:0.000987, total loss:0.000740\n",
            "[Train] epoch:790, lr:0.000987, total loss:0.000699\n",
            "[Train] epoch:791, lr:0.000987, total loss:0.000733\n",
            "[Train] epoch:792, lr:0.000986, total loss:0.000748\n",
            "[Train] epoch:793, lr:0.000986, total loss:0.000706\n",
            "[Train] epoch:794, lr:0.000986, total loss:0.000717\n",
            "[Train] epoch:795, lr:0.000986, total loss:0.000694\n",
            "[Train] epoch:796, lr:0.000986, total loss:0.000732\n",
            "[Train] epoch:797, lr:0.000986, total loss:0.000699\n",
            "[Train] epoch:798, lr:0.000986, total loss:0.000721\n",
            "[Train] epoch:799, lr:0.000986, total loss:0.000731\n",
            "[Train] epoch:800, lr:0.000986, total loss:0.000673\n",
            "[Train] epoch:801, lr:0.000986, total loss:0.000700\n",
            "[Train] epoch:802, lr:0.000986, total loss:0.000736\n",
            "[Train] epoch:803, lr:0.000986, total loss:0.000754\n",
            "[Train] epoch:804, lr:0.000986, total loss:0.000719\n",
            "[Train] epoch:805, lr:0.000986, total loss:0.000718\n",
            "[Train] epoch:806, lr:0.000986, total loss:0.000676\n",
            "[Train] epoch:807, lr:0.000986, total loss:0.000744\n",
            "[Train] epoch:808, lr:0.000986, total loss:0.000705\n",
            "[Train] epoch:809, lr:0.000986, total loss:0.000719\n",
            "[Train] epoch:810, lr:0.000986, total loss:0.000730\n",
            "[Train] epoch:811, lr:0.000986, total loss:0.000727\n",
            "[Train] epoch:812, lr:0.000986, total loss:0.000741\n",
            "[Train] epoch:813, lr:0.000986, total loss:0.000749\n",
            "[Train] epoch:814, lr:0.000986, total loss:0.000752\n",
            "[Train] epoch:815, lr:0.000986, total loss:0.000717\n",
            "[Train] epoch:816, lr:0.000986, total loss:0.000723\n",
            "[Train] epoch:817, lr:0.000986, total loss:0.000699\n",
            "[Train] epoch:818, lr:0.000986, total loss:0.000732\n",
            "[Train] epoch:819, lr:0.000986, total loss:0.000685\n",
            "[Train] epoch:820, lr:0.000986, total loss:0.000725\n",
            "[Train] epoch:821, lr:0.000985, total loss:0.000669\n",
            "[Train] epoch:822, lr:0.000985, total loss:0.000718\n",
            "[Train] epoch:823, lr:0.000985, total loss:0.000694\n",
            "[Train] epoch:824, lr:0.000985, total loss:0.000736\n",
            "[Train] epoch:825, lr:0.000985, total loss:0.000678\n",
            "[Train] epoch:826, lr:0.000985, total loss:0.000705\n",
            "[Train] epoch:827, lr:0.000985, total loss:0.000665\n",
            "[Train] epoch:828, lr:0.000985, total loss:0.000683\n",
            "[Train] epoch:829, lr:0.000985, total loss:0.000712\n",
            "[Train] epoch:830, lr:0.000985, total loss:0.000715\n",
            "[Train] epoch:831, lr:0.000985, total loss:0.000696\n",
            "[Train] epoch:832, lr:0.000985, total loss:0.000749\n",
            "[Train] epoch:833, lr:0.000985, total loss:0.000676\n",
            "[Train] epoch:834, lr:0.000985, total loss:0.000688\n",
            "[Train] epoch:835, lr:0.000985, total loss:0.000764\n",
            "[Train] epoch:836, lr:0.000985, total loss:0.000761\n",
            "[Train] epoch:837, lr:0.000985, total loss:0.000729\n",
            "[Train] epoch:838, lr:0.000985, total loss:0.000707\n",
            "[Train] epoch:839, lr:0.000985, total loss:0.000688\n",
            "[Train] epoch:840, lr:0.000985, total loss:0.000697\n",
            "[Train] epoch:841, lr:0.000985, total loss:0.000734\n",
            "[Train] epoch:842, lr:0.000985, total loss:0.000672\n",
            "[Train] epoch:843, lr:0.000985, total loss:0.000721\n",
            "[Train] epoch:844, lr:0.000985, total loss:0.000711\n",
            "[Train] epoch:845, lr:0.000985, total loss:0.000681\n",
            "[Train] epoch:846, lr:0.000985, total loss:0.000717\n",
            "[Train] epoch:847, lr:0.000985, total loss:0.000693\n",
            "[Train] epoch:848, lr:0.000985, total loss:0.000658\n",
            "[Train] epoch:849, lr:0.000984, total loss:0.000720\n",
            "[Train] epoch:850, lr:0.000984, total loss:0.000713\n",
            "[Train] epoch:851, lr:0.000984, total loss:0.000691\n",
            "[Train] epoch:852, lr:0.000984, total loss:0.000674\n",
            "[Train] epoch:853, lr:0.000984, total loss:0.000683\n",
            "[Train] epoch:854, lr:0.000984, total loss:0.000688\n",
            "[Train] epoch:855, lr:0.000984, total loss:0.000689\n",
            "[Train] epoch:856, lr:0.000984, total loss:0.000748\n",
            "[Train] epoch:857, lr:0.000984, total loss:0.000746\n",
            "[Train] epoch:858, lr:0.000984, total loss:0.000741\n",
            "[Train] epoch:859, lr:0.000984, total loss:0.000719\n",
            "[Train] epoch:860, lr:0.000984, total loss:0.000708\n",
            "[Train] epoch:861, lr:0.000984, total loss:0.000719\n",
            "[Train] epoch:862, lr:0.000984, total loss:0.000686\n",
            "[Train] epoch:863, lr:0.000984, total loss:0.000717\n",
            "[Train] epoch:864, lr:0.000984, total loss:0.000678\n",
            "[Train] epoch:865, lr:0.000984, total loss:0.000699\n",
            "[Train] epoch:866, lr:0.000984, total loss:0.000683\n",
            "[Train] epoch:867, lr:0.000984, total loss:0.000685\n",
            "[Train] epoch:868, lr:0.000984, total loss:0.000694\n",
            "[Train] epoch:869, lr:0.000984, total loss:0.000670\n",
            "[Train] epoch:870, lr:0.000984, total loss:0.000690\n",
            "[Train] epoch:871, lr:0.000984, total loss:0.000658\n",
            "[Train] epoch:872, lr:0.000984, total loss:0.000662\n",
            "[Train] epoch:873, lr:0.000984, total loss:0.000716\n",
            "[Train] epoch:874, lr:0.000984, total loss:0.000673\n",
            "[Train] epoch:875, lr:0.000983, total loss:0.000722\n",
            "[Train] epoch:876, lr:0.000983, total loss:0.000688\n",
            "[Train] epoch:877, lr:0.000983, total loss:0.000687\n",
            "[Train] epoch:878, lr:0.000983, total loss:0.000692\n",
            "[Train] epoch:879, lr:0.000983, total loss:0.000722\n",
            "[Train] epoch:880, lr:0.000983, total loss:0.000666\n",
            "[Train] epoch:881, lr:0.000983, total loss:0.000622\n",
            "[Train] epoch:882, lr:0.000983, total loss:0.000699\n",
            "[Train] epoch:883, lr:0.000983, total loss:0.000689\n",
            "[Train] epoch:884, lr:0.000983, total loss:0.000693\n",
            "[Train] epoch:885, lr:0.000983, total loss:0.000689\n",
            "[Train] epoch:886, lr:0.000983, total loss:0.000691\n",
            "[Train] epoch:887, lr:0.000983, total loss:0.000637\n",
            "[Train] epoch:888, lr:0.000983, total loss:0.000661\n",
            "[Train] epoch:889, lr:0.000983, total loss:0.000614\n",
            "[Train] epoch:890, lr:0.000983, total loss:0.000701\n",
            "[Train] epoch:891, lr:0.000983, total loss:0.000684\n",
            "[Train] epoch:892, lr:0.000983, total loss:0.000664\n",
            "[Train] epoch:893, lr:0.000983, total loss:0.000688\n",
            "[Train] epoch:894, lr:0.000983, total loss:0.000635\n",
            "[Train] epoch:895, lr:0.000983, total loss:0.000646\n",
            "[Train] epoch:896, lr:0.000983, total loss:0.000673\n",
            "[Train] epoch:897, lr:0.000983, total loss:0.000662\n",
            "[Train] epoch:898, lr:0.000983, total loss:0.000659\n",
            "[Train] epoch:899, lr:0.000983, total loss:0.000604\n",
            "[Train] epoch:900, lr:0.000983, total loss:0.000651\n",
            "[Train] epoch:901, lr:0.000982, total loss:0.000586\n",
            "[Train] epoch:902, lr:0.000982, total loss:0.000634\n",
            "[Train] epoch:903, lr:0.000982, total loss:0.000660\n",
            "[Train] epoch:904, lr:0.000982, total loss:0.000675\n",
            "[Train] epoch:905, lr:0.000982, total loss:0.000668\n",
            "[Train] epoch:906, lr:0.000982, total loss:0.000680\n",
            "[Train] epoch:907, lr:0.000982, total loss:0.000640\n",
            "[Train] epoch:908, lr:0.000982, total loss:0.000632\n",
            "[Train] epoch:909, lr:0.000982, total loss:0.000613\n",
            "[Train] epoch:910, lr:0.000982, total loss:0.000623\n",
            "[Train] epoch:911, lr:0.000982, total loss:0.000652\n",
            "[Train] epoch:912, lr:0.000982, total loss:0.000665\n",
            "[Train] epoch:913, lr:0.000982, total loss:0.000691\n",
            "[Train] epoch:914, lr:0.000982, total loss:0.000648\n",
            "[Train] epoch:915, lr:0.000982, total loss:0.000618\n",
            "[Train] epoch:916, lr:0.000982, total loss:0.000611\n",
            "[Train] epoch:917, lr:0.000982, total loss:0.000618\n",
            "[Train] epoch:918, lr:0.000982, total loss:0.000673\n",
            "[Train] epoch:919, lr:0.000982, total loss:0.000709\n",
            "[Train] epoch:920, lr:0.000982, total loss:0.000634\n",
            "[Train] epoch:921, lr:0.000982, total loss:0.000656\n",
            "[Train] epoch:922, lr:0.000982, total loss:0.000703\n",
            "[Train] epoch:923, lr:0.000982, total loss:0.000650\n",
            "[Train] epoch:924, lr:0.000982, total loss:0.000640\n",
            "[Train] epoch:925, lr:0.000982, total loss:0.000632\n",
            "[Train] epoch:926, lr:0.000981, total loss:0.000647\n",
            "[Train] epoch:927, lr:0.000981, total loss:0.000626\n",
            "[Train] epoch:928, lr:0.000981, total loss:0.000632\n",
            "[Train] epoch:929, lr:0.000981, total loss:0.000614\n",
            "[Train] epoch:930, lr:0.000981, total loss:0.000614\n",
            "[Train] epoch:931, lr:0.000981, total loss:0.000621\n",
            "[Train] epoch:932, lr:0.000981, total loss:0.000670\n",
            "[Train] epoch:933, lr:0.000981, total loss:0.000695\n",
            "[Train] epoch:934, lr:0.000981, total loss:0.000638\n",
            "[Train] epoch:935, lr:0.000981, total loss:0.000642\n",
            "[Train] epoch:936, lr:0.000981, total loss:0.000677\n",
            "[Train] epoch:937, lr:0.000981, total loss:0.000648\n",
            "[Train] epoch:938, lr:0.000981, total loss:0.000646\n",
            "[Train] epoch:939, lr:0.000981, total loss:0.000638\n",
            "[Train] epoch:940, lr:0.000981, total loss:0.000690\n",
            "[Train] epoch:941, lr:0.000981, total loss:0.000618\n",
            "[Train] epoch:942, lr:0.000981, total loss:0.000651\n",
            "[Train] epoch:943, lr:0.000981, total loss:0.000688\n",
            "[Train] epoch:944, lr:0.000981, total loss:0.000674\n",
            "[Train] epoch:945, lr:0.000981, total loss:0.000661\n",
            "[Train] epoch:946, lr:0.000981, total loss:0.000674\n",
            "[Train] epoch:947, lr:0.000981, total loss:0.000674\n",
            "[Train] epoch:948, lr:0.000981, total loss:0.000629\n",
            "[Train] epoch:949, lr:0.000981, total loss:0.000622\n",
            "[Train] epoch:950, lr:0.000981, total loss:0.000640\n",
            "[Train] epoch:951, lr:0.000980, total loss:0.000625\n",
            "[Train] epoch:952, lr:0.000980, total loss:0.000593\n",
            "[Train] epoch:953, lr:0.000980, total loss:0.000595\n",
            "[Train] epoch:954, lr:0.000980, total loss:0.000570\n",
            "[Train] epoch:955, lr:0.000980, total loss:0.000619\n",
            "[Train] epoch:956, lr:0.000980, total loss:0.000585\n",
            "[Train] epoch:957, lr:0.000980, total loss:0.000650\n",
            "[Train] epoch:958, lr:0.000980, total loss:0.000598\n",
            "[Train] epoch:959, lr:0.000980, total loss:0.000627\n",
            "[Train] epoch:960, lr:0.000980, total loss:0.000624\n",
            "[Train] epoch:961, lr:0.000980, total loss:0.000616\n",
            "[Train] epoch:962, lr:0.000980, total loss:0.000645\n",
            "[Train] epoch:963, lr:0.000980, total loss:0.000665\n",
            "[Train] epoch:964, lr:0.000980, total loss:0.000645\n",
            "[Train] epoch:965, lr:0.000980, total loss:0.000610\n",
            "[Train] epoch:966, lr:0.000980, total loss:0.000659\n",
            "[Train] epoch:967, lr:0.000980, total loss:0.000624\n",
            "[Train] epoch:968, lr:0.000980, total loss:0.000631\n",
            "[Train] epoch:969, lr:0.000980, total loss:0.000574\n",
            "[Train] epoch:970, lr:0.000980, total loss:0.000607\n",
            "[Train] epoch:971, lr:0.000980, total loss:0.000603\n",
            "[Train] epoch:972, lr:0.000980, total loss:0.000600\n",
            "[Train] epoch:973, lr:0.000980, total loss:0.000613\n",
            "[Train] epoch:974, lr:0.000980, total loss:0.000669\n",
            "[Train] epoch:975, lr:0.000979, total loss:0.000605\n",
            "[Train] epoch:976, lr:0.000979, total loss:0.000603\n",
            "[Train] epoch:977, lr:0.000979, total loss:0.000627\n",
            "[Train] epoch:978, lr:0.000979, total loss:0.000652\n",
            "[Train] epoch:979, lr:0.000979, total loss:0.000617\n",
            "[Train] epoch:980, lr:0.000979, total loss:0.000618\n",
            "[Train] epoch:981, lr:0.000979, total loss:0.000611\n",
            "[Train] epoch:982, lr:0.000979, total loss:0.000608\n",
            "[Train] epoch:983, lr:0.000979, total loss:0.000590\n",
            "[Train] epoch:984, lr:0.000979, total loss:0.000706\n",
            "[Train] epoch:985, lr:0.000979, total loss:0.000616\n",
            "[Train] epoch:986, lr:0.000979, total loss:0.000617\n",
            "[Train] epoch:987, lr:0.000979, total loss:0.000622\n",
            "[Train] epoch:988, lr:0.000979, total loss:0.000578\n",
            "[Train] epoch:989, lr:0.000979, total loss:0.000602\n",
            "[Train] epoch:990, lr:0.000979, total loss:0.000655\n",
            "[Train] epoch:991, lr:0.000979, total loss:0.000631\n",
            "[Train] epoch:992, lr:0.000979, total loss:0.000571\n",
            "[Train] epoch:993, lr:0.000979, total loss:0.000644\n",
            "[Train] epoch:994, lr:0.000979, total loss:0.000594\n",
            "[Train] epoch:995, lr:0.000979, total loss:0.000574\n",
            "[Train] epoch:996, lr:0.000979, total loss:0.000591\n",
            "[Train] epoch:997, lr:0.000979, total loss:0.000618\n",
            "[Train] epoch:998, lr:0.000978, total loss:0.000584\n",
            "[Train] epoch:999, lr:0.000978, total loss:0.000566\n",
            "[Train] epoch:1000, lr:0.000978, total loss:0.000591\n",
            "[Train] epoch:1001, lr:0.000978, total loss:0.000561\n",
            "[Train] epoch:1002, lr:0.000978, total loss:0.000593\n",
            "[Train] epoch:1003, lr:0.000978, total loss:0.000623\n",
            "[Train] epoch:1004, lr:0.000978, total loss:0.000613\n",
            "[Train] epoch:1005, lr:0.000978, total loss:0.000569\n",
            "[Train] epoch:1006, lr:0.000978, total loss:0.000584\n",
            "[Train] epoch:1007, lr:0.000978, total loss:0.000598\n",
            "[Train] epoch:1008, lr:0.000978, total loss:0.000570\n",
            "[Train] epoch:1009, lr:0.000978, total loss:0.000593\n",
            "[Train] epoch:1010, lr:0.000978, total loss:0.000618\n",
            "[Train] epoch:1011, lr:0.000978, total loss:0.000567\n",
            "[Train] epoch:1012, lr:0.000978, total loss:0.000605\n",
            "[Train] epoch:1013, lr:0.000978, total loss:0.000607\n",
            "[Train] epoch:1014, lr:0.000978, total loss:0.000598\n",
            "[Train] epoch:1015, lr:0.000978, total loss:0.000574\n",
            "[Train] epoch:1016, lr:0.000978, total loss:0.000567\n",
            "[Train] epoch:1017, lr:0.000978, total loss:0.000584\n",
            "[Train] epoch:1018, lr:0.000978, total loss:0.000560\n",
            "[Train] epoch:1019, lr:0.000978, total loss:0.000588\n",
            "[Train] epoch:1020, lr:0.000978, total loss:0.000572\n",
            "[Train] epoch:1021, lr:0.000977, total loss:0.000560\n",
            "[Train] epoch:1022, lr:0.000977, total loss:0.000579\n",
            "[Train] epoch:1023, lr:0.000977, total loss:0.000596\n",
            "[Train] epoch:1024, lr:0.000977, total loss:0.000570\n",
            "[Train] epoch:1025, lr:0.000977, total loss:0.000578\n",
            "[Train] epoch:1026, lr:0.000977, total loss:0.000645\n",
            "[Train] epoch:1027, lr:0.000977, total loss:0.000586\n",
            "[Train] epoch:1028, lr:0.000977, total loss:0.000579\n",
            "[Train] epoch:1029, lr:0.000977, total loss:0.000620\n",
            "[Train] epoch:1030, lr:0.000977, total loss:0.000590\n",
            "[Train] epoch:1031, lr:0.000977, total loss:0.000586\n",
            "[Train] epoch:1032, lr:0.000977, total loss:0.000556\n",
            "[Train] epoch:1033, lr:0.000977, total loss:0.000556\n",
            "[Train] epoch:1034, lr:0.000977, total loss:0.000572\n",
            "[Train] epoch:1035, lr:0.000977, total loss:0.000654\n",
            "[Train] epoch:1036, lr:0.000977, total loss:0.000577\n",
            "[Train] epoch:1037, lr:0.000977, total loss:0.000576\n",
            "[Train] epoch:1038, lr:0.000977, total loss:0.000581\n",
            "[Train] epoch:1039, lr:0.000977, total loss:0.000580\n",
            "[Train] epoch:1040, lr:0.000977, total loss:0.000599\n",
            "[Train] epoch:1041, lr:0.000977, total loss:0.000590\n",
            "[Train] epoch:1042, lr:0.000977, total loss:0.000568\n",
            "[Train] epoch:1043, lr:0.000977, total loss:0.000550\n",
            "[Train] epoch:1044, lr:0.000976, total loss:0.000611\n",
            "[Train] epoch:1045, lr:0.000976, total loss:0.000577\n",
            "[Train] epoch:1046, lr:0.000976, total loss:0.000585\n",
            "[Train] epoch:1047, lr:0.000976, total loss:0.000583\n",
            "[Train] epoch:1048, lr:0.000976, total loss:0.000560\n",
            "[Train] epoch:1049, lr:0.000976, total loss:0.000567\n",
            "[Train] epoch:1050, lr:0.000976, total loss:0.000579\n",
            "[Train] epoch:1051, lr:0.000976, total loss:0.000582\n",
            "[Train] epoch:1052, lr:0.000976, total loss:0.000536\n",
            "[Train] epoch:1053, lr:0.000976, total loss:0.000546\n",
            "[Train] epoch:1054, lr:0.000976, total loss:0.000579\n",
            "[Train] epoch:1055, lr:0.000976, total loss:0.000547\n",
            "[Train] epoch:1056, lr:0.000976, total loss:0.000546\n",
            "[Train] epoch:1057, lr:0.000976, total loss:0.000542\n",
            "[Train] epoch:1058, lr:0.000976, total loss:0.000557\n",
            "[Train] epoch:1059, lr:0.000976, total loss:0.000566\n",
            "[Train] epoch:1060, lr:0.000976, total loss:0.000584\n",
            "[Train] epoch:1061, lr:0.000976, total loss:0.000567\n",
            "[Train] epoch:1062, lr:0.000976, total loss:0.000544\n",
            "[Train] epoch:1063, lr:0.000976, total loss:0.000544\n",
            "[Train] epoch:1064, lr:0.000976, total loss:0.000532\n",
            "[Train] epoch:1065, lr:0.000976, total loss:0.000554\n",
            "[Train] epoch:1066, lr:0.000975, total loss:0.000539\n",
            "[Train] epoch:1067, lr:0.000975, total loss:0.000559\n",
            "[Train] epoch:1068, lr:0.000975, total loss:0.000563\n",
            "[Train] epoch:1069, lr:0.000975, total loss:0.000562\n",
            "[Train] epoch:1070, lr:0.000975, total loss:0.000564\n",
            "[Train] epoch:1071, lr:0.000975, total loss:0.000557\n",
            "[Train] epoch:1072, lr:0.000975, total loss:0.000557\n",
            "[Train] epoch:1073, lr:0.000975, total loss:0.000596\n",
            "[Train] epoch:1074, lr:0.000975, total loss:0.000598\n",
            "[Train] epoch:1075, lr:0.000975, total loss:0.000566\n",
            "[Train] epoch:1076, lr:0.000975, total loss:0.000552\n",
            "[Train] epoch:1077, lr:0.000975, total loss:0.000546\n",
            "[Train] epoch:1078, lr:0.000975, total loss:0.000545\n",
            "[Train] epoch:1079, lr:0.000975, total loss:0.000545\n",
            "[Train] epoch:1080, lr:0.000975, total loss:0.000526\n",
            "[Train] epoch:1081, lr:0.000975, total loss:0.000560\n",
            "[Train] epoch:1082, lr:0.000975, total loss:0.000588\n",
            "[Train] epoch:1083, lr:0.000975, total loss:0.000571\n",
            "[Train] epoch:1084, lr:0.000975, total loss:0.000562\n",
            "[Train] epoch:1085, lr:0.000975, total loss:0.000564\n",
            "[Train] epoch:1086, lr:0.000975, total loss:0.000562\n",
            "[Train] epoch:1087, lr:0.000974, total loss:0.000551\n",
            "[Train] epoch:1088, lr:0.000974, total loss:0.000533\n",
            "[Train] epoch:1089, lr:0.000974, total loss:0.000579\n",
            "[Train] epoch:1090, lr:0.000974, total loss:0.000549\n",
            "[Train] epoch:1091, lr:0.000974, total loss:0.000557\n",
            "[Train] epoch:1092, lr:0.000974, total loss:0.000602\n",
            "[Train] epoch:1093, lr:0.000974, total loss:0.000632\n",
            "[Train] epoch:1094, lr:0.000974, total loss:0.000595\n",
            "[Train] epoch:1095, lr:0.000974, total loss:0.000560\n",
            "[Train] epoch:1096, lr:0.000974, total loss:0.000577\n",
            "[Train] epoch:1097, lr:0.000974, total loss:0.000547\n",
            "[Train] epoch:1098, lr:0.000974, total loss:0.000548\n",
            "[Train] epoch:1099, lr:0.000974, total loss:0.000534\n",
            "[Train] epoch:1100, lr:0.000974, total loss:0.000546\n",
            "[Train] epoch:1101, lr:0.000974, total loss:0.000563\n",
            "[Train] epoch:1102, lr:0.000974, total loss:0.000544\n",
            "[Train] epoch:1103, lr:0.000974, total loss:0.000520\n",
            "[Train] epoch:1104, lr:0.000974, total loss:0.000567\n",
            "[Train] epoch:1105, lr:0.000974, total loss:0.000548\n",
            "[Train] epoch:1106, lr:0.000974, total loss:0.000542\n",
            "[Train] epoch:1107, lr:0.000974, total loss:0.000505\n",
            "[Train] epoch:1108, lr:0.000973, total loss:0.000530\n",
            "[Train] epoch:1109, lr:0.000973, total loss:0.000530\n",
            "[Train] epoch:1110, lr:0.000973, total loss:0.000516\n",
            "[Train] epoch:1111, lr:0.000973, total loss:0.000585\n",
            "[Train] epoch:1112, lr:0.000973, total loss:0.000565\n",
            "[Train] epoch:1113, lr:0.000973, total loss:0.000557\n",
            "[Train] epoch:1114, lr:0.000973, total loss:0.000523\n",
            "[Train] epoch:1115, lr:0.000973, total loss:0.000551\n",
            "[Train] epoch:1116, lr:0.000973, total loss:0.000506\n",
            "[Train] epoch:1117, lr:0.000973, total loss:0.000545\n",
            "[Train] epoch:1118, lr:0.000973, total loss:0.000528\n",
            "[Train] epoch:1119, lr:0.000973, total loss:0.000550\n",
            "[Train] epoch:1120, lr:0.000973, total loss:0.000518\n",
            "[Train] epoch:1121, lr:0.000973, total loss:0.000536\n",
            "[Train] epoch:1122, lr:0.000973, total loss:0.000504\n",
            "[Train] epoch:1123, lr:0.000973, total loss:0.000522\n",
            "[Train] epoch:1124, lr:0.000973, total loss:0.000542\n",
            "[Train] epoch:1125, lr:0.000973, total loss:0.000575\n",
            "[Train] epoch:1126, lr:0.000973, total loss:0.000518\n",
            "[Train] epoch:1127, lr:0.000973, total loss:0.000543\n",
            "[Train] epoch:1128, lr:0.000973, total loss:0.000534\n",
            "[Train] epoch:1129, lr:0.000972, total loss:0.000546\n",
            "[Train] epoch:1130, lr:0.000972, total loss:0.000520\n",
            "[Train] epoch:1131, lr:0.000972, total loss:0.000535\n",
            "[Train] epoch:1132, lr:0.000972, total loss:0.000537\n",
            "[Train] epoch:1133, lr:0.000972, total loss:0.000542\n",
            "[Train] epoch:1134, lr:0.000972, total loss:0.000513\n",
            "[Train] epoch:1135, lr:0.000972, total loss:0.000567\n",
            "[Train] epoch:1136, lr:0.000972, total loss:0.000535\n",
            "[Train] epoch:1137, lr:0.000972, total loss:0.000563\n",
            "[Train] epoch:1138, lr:0.000972, total loss:0.000586\n",
            "[Train] epoch:1139, lr:0.000972, total loss:0.000511\n",
            "[Train] epoch:1140, lr:0.000972, total loss:0.000540\n",
            "[Train] epoch:1141, lr:0.000972, total loss:0.000554\n",
            "[Train] epoch:1142, lr:0.000972, total loss:0.000546\n",
            "[Train] epoch:1143, lr:0.000972, total loss:0.000545\n",
            "[Train] epoch:1144, lr:0.000972, total loss:0.000480\n",
            "[Train] epoch:1145, lr:0.000972, total loss:0.000537\n",
            "[Train] epoch:1146, lr:0.000972, total loss:0.000550\n",
            "[Train] epoch:1147, lr:0.000972, total loss:0.000528\n",
            "[Train] epoch:1148, lr:0.000972, total loss:0.000520\n",
            "[Train] epoch:1149, lr:0.000971, total loss:0.000502\n",
            "[Train] epoch:1150, lr:0.000971, total loss:0.000521\n",
            "[Train] epoch:1151, lr:0.000971, total loss:0.000482\n",
            "[Train] epoch:1152, lr:0.000971, total loss:0.000537\n",
            "[Train] epoch:1153, lr:0.000971, total loss:0.000532\n",
            "[Train] epoch:1154, lr:0.000971, total loss:0.000538\n",
            "[Train] epoch:1155, lr:0.000971, total loss:0.000516\n",
            "[Train] epoch:1156, lr:0.000971, total loss:0.000528\n",
            "[Train] epoch:1157, lr:0.000971, total loss:0.000520\n",
            "[Train] epoch:1158, lr:0.000971, total loss:0.000546\n",
            "[Train] epoch:1159, lr:0.000971, total loss:0.000540\n",
            "[Train] epoch:1160, lr:0.000971, total loss:0.000532\n",
            "[Train] epoch:1161, lr:0.000971, total loss:0.000489\n",
            "[Train] epoch:1162, lr:0.000971, total loss:0.000511\n",
            "[Train] epoch:1163, lr:0.000971, total loss:0.000550\n",
            "[Train] epoch:1164, lr:0.000971, total loss:0.000521\n",
            "[Train] epoch:1165, lr:0.000971, total loss:0.000515\n",
            "[Train] epoch:1166, lr:0.000971, total loss:0.000537\n",
            "[Train] epoch:1167, lr:0.000971, total loss:0.000517\n",
            "[Train] epoch:1168, lr:0.000971, total loss:0.000498\n",
            "[Train] epoch:1169, lr:0.000970, total loss:0.000519\n",
            "[Train] epoch:1170, lr:0.000970, total loss:0.000503\n",
            "[Train] epoch:1171, lr:0.000970, total loss:0.000506\n",
            "[Train] epoch:1172, lr:0.000970, total loss:0.000482\n",
            "[Train] epoch:1173, lr:0.000970, total loss:0.000515\n",
            "[Train] epoch:1174, lr:0.000970, total loss:0.000509\n",
            "[Train] epoch:1175, lr:0.000970, total loss:0.000513\n",
            "[Train] epoch:1176, lr:0.000970, total loss:0.000467\n",
            "[Train] epoch:1177, lr:0.000970, total loss:0.000529\n",
            "[Train] epoch:1178, lr:0.000970, total loss:0.000525\n",
            "[Train] epoch:1179, lr:0.000970, total loss:0.000504\n",
            "[Train] epoch:1180, lr:0.000970, total loss:0.000538\n",
            "[Train] epoch:1181, lr:0.000970, total loss:0.000521\n",
            "[Train] epoch:1182, lr:0.000970, total loss:0.000502\n",
            "[Train] epoch:1183, lr:0.000970, total loss:0.000506\n",
            "[Train] epoch:1184, lr:0.000970, total loss:0.000512\n",
            "[Train] epoch:1185, lr:0.000970, total loss:0.000482\n",
            "[Train] epoch:1186, lr:0.000970, total loss:0.000487\n",
            "[Train] epoch:1187, lr:0.000970, total loss:0.000465\n",
            "[Train] epoch:1188, lr:0.000970, total loss:0.000504\n",
            "[Train] epoch:1189, lr:0.000969, total loss:0.000532\n",
            "[Train] epoch:1190, lr:0.000969, total loss:0.000512\n",
            "[Train] epoch:1191, lr:0.000969, total loss:0.000492\n",
            "[Train] epoch:1192, lr:0.000969, total loss:0.000507\n",
            "[Train] epoch:1193, lr:0.000969, total loss:0.000489\n",
            "[Train] epoch:1194, lr:0.000969, total loss:0.000497\n",
            "[Train] epoch:1195, lr:0.000969, total loss:0.000475\n",
            "[Train] epoch:1196, lr:0.000969, total loss:0.000494\n",
            "[Train] epoch:1197, lr:0.000969, total loss:0.000499\n",
            "[Train] epoch:1198, lr:0.000969, total loss:0.000538\n",
            "[Train] epoch:1199, lr:0.000969, total loss:0.000509\n",
            "[Train] epoch:1200, lr:0.000969, total loss:0.000521\n",
            "[Train] epoch:1201, lr:0.000969, total loss:0.000517\n",
            "[Train] epoch:1202, lr:0.000969, total loss:0.000481\n",
            "[Train] epoch:1203, lr:0.000969, total loss:0.000481\n",
            "[Train] epoch:1204, lr:0.000969, total loss:0.000460\n",
            "[Train] epoch:1205, lr:0.000969, total loss:0.000498\n",
            "[Train] epoch:1206, lr:0.000969, total loss:0.000532\n",
            "[Train] epoch:1207, lr:0.000969, total loss:0.000518\n",
            "[Train] epoch:1208, lr:0.000968, total loss:0.000475\n",
            "[Train] epoch:1209, lr:0.000968, total loss:0.000455\n",
            "[Train] epoch:1210, lr:0.000968, total loss:0.000548\n",
            "[Train] epoch:1211, lr:0.000968, total loss:0.000532\n",
            "[Train] epoch:1212, lr:0.000968, total loss:0.000536\n",
            "[Train] epoch:1213, lr:0.000968, total loss:0.000512\n",
            "[Train] epoch:1214, lr:0.000968, total loss:0.000522\n",
            "[Train] epoch:1215, lr:0.000968, total loss:0.000522\n",
            "[Train] epoch:1216, lr:0.000968, total loss:0.000500\n",
            "[Train] epoch:1217, lr:0.000968, total loss:0.000510\n",
            "[Train] epoch:1218, lr:0.000968, total loss:0.000523\n",
            "[Train] epoch:1219, lr:0.000968, total loss:0.000488\n",
            "[Train] epoch:1220, lr:0.000968, total loss:0.000503\n",
            "[Train] epoch:1221, lr:0.000968, total loss:0.000490\n",
            "[Train] epoch:1222, lr:0.000968, total loss:0.000478\n",
            "[Train] epoch:1223, lr:0.000968, total loss:0.000524\n",
            "[Train] epoch:1224, lr:0.000968, total loss:0.000503\n",
            "[Train] epoch:1225, lr:0.000968, total loss:0.000539\n",
            "[Train] epoch:1226, lr:0.000968, total loss:0.000513\n",
            "[Train] epoch:1227, lr:0.000967, total loss:0.000500\n",
            "[Train] epoch:1228, lr:0.000967, total loss:0.000471\n",
            "[Train] epoch:1229, lr:0.000967, total loss:0.000481\n",
            "[Train] epoch:1230, lr:0.000967, total loss:0.000494\n",
            "[Train] epoch:1231, lr:0.000967, total loss:0.000484\n",
            "[Train] epoch:1232, lr:0.000967, total loss:0.000477\n",
            "[Train] epoch:1233, lr:0.000967, total loss:0.000532\n",
            "[Train] epoch:1234, lr:0.000967, total loss:0.000483\n",
            "[Train] epoch:1235, lr:0.000967, total loss:0.000496\n",
            "[Train] epoch:1236, lr:0.000967, total loss:0.000480\n",
            "[Train] epoch:1237, lr:0.000967, total loss:0.000456\n",
            "[Train] epoch:1238, lr:0.000967, total loss:0.000479\n",
            "[Train] epoch:1239, lr:0.000967, total loss:0.000526\n",
            "[Train] epoch:1240, lr:0.000967, total loss:0.000486\n",
            "[Train] epoch:1241, lr:0.000967, total loss:0.000449\n",
            "[Train] epoch:1242, lr:0.000967, total loss:0.000486\n",
            "[Train] epoch:1243, lr:0.000967, total loss:0.000460\n",
            "[Train] epoch:1244, lr:0.000967, total loss:0.000490\n",
            "[Train] epoch:1245, lr:0.000967, total loss:0.000486\n",
            "[Train] epoch:1246, lr:0.000966, total loss:0.000460\n",
            "[Train] epoch:1247, lr:0.000966, total loss:0.000468\n",
            "[Train] epoch:1248, lr:0.000966, total loss:0.000461\n",
            "[Train] epoch:1249, lr:0.000966, total loss:0.000475\n",
            "[Train] epoch:1250, lr:0.000966, total loss:0.000469\n",
            "[Train] epoch:1251, lr:0.000966, total loss:0.000476\n",
            "[Train] epoch:1252, lr:0.000966, total loss:0.000470\n",
            "[Train] epoch:1253, lr:0.000966, total loss:0.000513\n",
            "[Train] epoch:1254, lr:0.000966, total loss:0.000439\n",
            "[Train] epoch:1255, lr:0.000966, total loss:0.000449\n",
            "[Train] epoch:1256, lr:0.000966, total loss:0.000476\n",
            "[Train] epoch:1257, lr:0.000966, total loss:0.000484\n",
            "[Train] epoch:1258, lr:0.000966, total loss:0.000468\n",
            "[Train] epoch:1259, lr:0.000966, total loss:0.000479\n",
            "[Train] epoch:1260, lr:0.000966, total loss:0.000486\n",
            "[Train] epoch:1261, lr:0.000966, total loss:0.000472\n",
            "[Train] epoch:1262, lr:0.000966, total loss:0.000462\n",
            "[Train] epoch:1263, lr:0.000966, total loss:0.000467\n",
            "[Train] epoch:1264, lr:0.000966, total loss:0.000475\n",
            "[Train] epoch:1265, lr:0.000965, total loss:0.000461\n",
            "[Train] epoch:1266, lr:0.000965, total loss:0.000464\n",
            "[Train] epoch:1267, lr:0.000965, total loss:0.000443\n",
            "[Train] epoch:1268, lr:0.000965, total loss:0.000452\n",
            "[Train] epoch:1269, lr:0.000965, total loss:0.000467\n",
            "[Train] epoch:1270, lr:0.000965, total loss:0.000491\n",
            "[Train] epoch:1271, lr:0.000965, total loss:0.000455\n",
            "[Train] epoch:1272, lr:0.000965, total loss:0.000464\n",
            "[Train] epoch:1273, lr:0.000965, total loss:0.000474\n",
            "[Train] epoch:1274, lr:0.000965, total loss:0.000494\n",
            "[Train] epoch:1275, lr:0.000965, total loss:0.000463\n",
            "[Train] epoch:1276, lr:0.000965, total loss:0.000440\n",
            "[Train] epoch:1277, lr:0.000965, total loss:0.000437\n",
            "[Train] epoch:1278, lr:0.000965, total loss:0.000490\n",
            "[Train] epoch:1279, lr:0.000965, total loss:0.000452\n",
            "[Train] epoch:1280, lr:0.000965, total loss:0.000496\n",
            "[Train] epoch:1281, lr:0.000965, total loss:0.000524\n",
            "[Train] epoch:1282, lr:0.000965, total loss:0.000457\n",
            "[Train] epoch:1283, lr:0.000964, total loss:0.000477\n",
            "[Train] epoch:1284, lr:0.000964, total loss:0.000439\n",
            "[Train] epoch:1285, lr:0.000964, total loss:0.000519\n",
            "[Train] epoch:1286, lr:0.000964, total loss:0.000488\n",
            "[Train] epoch:1287, lr:0.000964, total loss:0.000457\n",
            "[Train] epoch:1288, lr:0.000964, total loss:0.000448\n",
            "[Train] epoch:1289, lr:0.000964, total loss:0.000471\n",
            "[Train] epoch:1290, lr:0.000964, total loss:0.000456\n",
            "[Train] epoch:1291, lr:0.000964, total loss:0.000469\n",
            "[Train] epoch:1292, lr:0.000964, total loss:0.000464\n",
            "[Train] epoch:1293, lr:0.000964, total loss:0.000443\n",
            "[Train] epoch:1294, lr:0.000964, total loss:0.000473\n",
            "[Train] epoch:1295, lr:0.000964, total loss:0.000439\n",
            "[Train] epoch:1296, lr:0.000964, total loss:0.000450\n",
            "[Train] epoch:1297, lr:0.000964, total loss:0.000417\n",
            "[Train] epoch:1298, lr:0.000964, total loss:0.000449\n",
            "[Train] epoch:1299, lr:0.000964, total loss:0.000455\n",
            "[Train] epoch:1300, lr:0.000964, total loss:0.000445\n",
            "[Train] epoch:1301, lr:0.000963, total loss:0.000440\n",
            "[Train] epoch:1302, lr:0.000963, total loss:0.000461\n",
            "[Train] epoch:1303, lr:0.000963, total loss:0.000474\n",
            "[Train] epoch:1304, lr:0.000963, total loss:0.000436\n",
            "[Train] epoch:1305, lr:0.000963, total loss:0.000480\n",
            "[Train] epoch:1306, lr:0.000963, total loss:0.000470\n",
            "[Train] epoch:1307, lr:0.000963, total loss:0.000523\n",
            "[Train] epoch:1308, lr:0.000963, total loss:0.000467\n",
            "[Train] epoch:1309, lr:0.000963, total loss:0.000506\n",
            "[Train] epoch:1310, lr:0.000963, total loss:0.000532\n",
            "[Train] epoch:1311, lr:0.000963, total loss:0.000477\n",
            "[Train] epoch:1312, lr:0.000963, total loss:0.000467\n",
            "[Train] epoch:1313, lr:0.000963, total loss:0.000474\n",
            "[Train] epoch:1314, lr:0.000963, total loss:0.000456\n",
            "[Train] epoch:1315, lr:0.000963, total loss:0.000446\n",
            "[Train] epoch:1316, lr:0.000963, total loss:0.000481\n",
            "[Train] epoch:1317, lr:0.000963, total loss:0.000495\n",
            "[Train] epoch:1318, lr:0.000963, total loss:0.000462\n",
            "[Train] epoch:1319, lr:0.000962, total loss:0.000493\n",
            "[Train] epoch:1320, lr:0.000962, total loss:0.000447\n",
            "[Train] epoch:1321, lr:0.000962, total loss:0.000464\n",
            "[Train] epoch:1322, lr:0.000962, total loss:0.000448\n",
            "[Train] epoch:1323, lr:0.000962, total loss:0.000501\n",
            "[Train] epoch:1324, lr:0.000962, total loss:0.000557\n",
            "[Train] epoch:1325, lr:0.000962, total loss:0.000480\n",
            "[Train] epoch:1326, lr:0.000962, total loss:0.000464\n",
            "[Train] epoch:1327, lr:0.000962, total loss:0.000478\n",
            "[Train] epoch:1328, lr:0.000962, total loss:0.000437\n",
            "[Train] epoch:1329, lr:0.000962, total loss:0.000475\n",
            "[Train] epoch:1330, lr:0.000962, total loss:0.000482\n",
            "[Train] epoch:1331, lr:0.000962, total loss:0.000474\n",
            "[Train] epoch:1332, lr:0.000962, total loss:0.000477\n",
            "[Train] epoch:1333, lr:0.000962, total loss:0.000503\n",
            "[Train] epoch:1334, lr:0.000962, total loss:0.000464\n",
            "[Train] epoch:1335, lr:0.000962, total loss:0.000474\n",
            "[Train] epoch:1336, lr:0.000961, total loss:0.000490\n",
            "[Train] epoch:1337, lr:0.000961, total loss:0.000472\n",
            "[Train] epoch:1338, lr:0.000961, total loss:0.000426\n",
            "[Train] epoch:1339, lr:0.000961, total loss:0.000429\n",
            "[Train] epoch:1340, lr:0.000961, total loss:0.000426\n",
            "[Train] epoch:1341, lr:0.000961, total loss:0.000436\n",
            "[Train] epoch:1342, lr:0.000961, total loss:0.000442\n",
            "[Train] epoch:1343, lr:0.000961, total loss:0.000432\n",
            "[Train] epoch:1344, lr:0.000961, total loss:0.000427\n",
            "[Train] epoch:1345, lr:0.000961, total loss:0.000465\n",
            "[Train] epoch:1346, lr:0.000961, total loss:0.000428\n",
            "[Train] epoch:1347, lr:0.000961, total loss:0.000434\n",
            "[Train] epoch:1348, lr:0.000961, total loss:0.000449\n",
            "[Train] epoch:1349, lr:0.000961, total loss:0.000460\n",
            "[Train] epoch:1350, lr:0.000961, total loss:0.000431\n",
            "[Train] epoch:1351, lr:0.000961, total loss:0.000443\n",
            "[Train] epoch:1352, lr:0.000961, total loss:0.000449\n",
            "[Train] epoch:1353, lr:0.000961, total loss:0.000440\n",
            "[Train] epoch:1354, lr:0.000960, total loss:0.000463\n",
            "[Train] epoch:1355, lr:0.000960, total loss:0.000459\n",
            "[Train] epoch:1356, lr:0.000960, total loss:0.000465\n",
            "[Train] epoch:1357, lr:0.000960, total loss:0.000450\n",
            "[Train] epoch:1358, lr:0.000960, total loss:0.000442\n",
            "[Train] epoch:1359, lr:0.000960, total loss:0.000438\n",
            "[Train] epoch:1360, lr:0.000960, total loss:0.000412\n",
            "[Train] epoch:1361, lr:0.000960, total loss:0.000412\n",
            "[Train] epoch:1362, lr:0.000960, total loss:0.000438\n",
            "[Train] epoch:1363, lr:0.000960, total loss:0.000452\n",
            "[Train] epoch:1364, lr:0.000960, total loss:0.000442\n",
            "[Train] epoch:1365, lr:0.000960, total loss:0.000451\n",
            "[Train] epoch:1366, lr:0.000960, total loss:0.000459\n",
            "[Train] epoch:1367, lr:0.000960, total loss:0.000424\n",
            "[Train] epoch:1368, lr:0.000960, total loss:0.000465\n",
            "[Train] epoch:1369, lr:0.000960, total loss:0.000414\n",
            "[Train] epoch:1370, lr:0.000960, total loss:0.000424\n",
            "[Train] epoch:1371, lr:0.000959, total loss:0.000434\n",
            "[Train] epoch:1372, lr:0.000959, total loss:0.000418\n",
            "[Train] epoch:1373, lr:0.000959, total loss:0.000451\n",
            "[Train] epoch:1374, lr:0.000959, total loss:0.000415\n",
            "[Train] epoch:1375, lr:0.000959, total loss:0.000428\n",
            "[Train] epoch:1376, lr:0.000959, total loss:0.000414\n",
            "[Train] epoch:1377, lr:0.000959, total loss:0.000412\n",
            "[Train] epoch:1378, lr:0.000959, total loss:0.000441\n",
            "[Train] epoch:1379, lr:0.000959, total loss:0.000425\n",
            "[Train] epoch:1380, lr:0.000959, total loss:0.000420\n",
            "[Train] epoch:1381, lr:0.000959, total loss:0.000448\n",
            "[Train] epoch:1382, lr:0.000959, total loss:0.000462\n",
            "[Train] epoch:1383, lr:0.000959, total loss:0.000470\n",
            "[Train] epoch:1384, lr:0.000959, total loss:0.000450\n",
            "[Train] epoch:1385, lr:0.000959, total loss:0.000420\n",
            "[Train] epoch:1386, lr:0.000959, total loss:0.000425\n",
            "[Train] epoch:1387, lr:0.000959, total loss:0.000428\n",
            "[Train] epoch:1388, lr:0.000958, total loss:0.000414\n",
            "[Train] epoch:1389, lr:0.000958, total loss:0.000499\n",
            "[Train] epoch:1390, lr:0.000958, total loss:0.000423\n",
            "[Train] epoch:1391, lr:0.000958, total loss:0.000437\n",
            "[Train] epoch:1392, lr:0.000958, total loss:0.000420\n",
            "[Train] epoch:1393, lr:0.000958, total loss:0.000423\n",
            "[Train] epoch:1394, lr:0.000958, total loss:0.000424\n",
            "[Train] epoch:1395, lr:0.000958, total loss:0.000431\n",
            "[Train] epoch:1396, lr:0.000958, total loss:0.000415\n",
            "[Train] epoch:1397, lr:0.000958, total loss:0.000430\n",
            "[Train] epoch:1398, lr:0.000958, total loss:0.000418\n",
            "[Train] epoch:1399, lr:0.000958, total loss:0.000433\n",
            "[Train] epoch:1400, lr:0.000958, total loss:0.000432\n",
            "[Train] epoch:1401, lr:0.000958, total loss:0.000405\n",
            "[Train] epoch:1402, lr:0.000958, total loss:0.000406\n",
            "[Train] epoch:1403, lr:0.000958, total loss:0.000417\n",
            "[Train] epoch:1404, lr:0.000958, total loss:0.000430\n",
            "[Train] epoch:1405, lr:0.000957, total loss:0.000420\n",
            "[Train] epoch:1406, lr:0.000957, total loss:0.000409\n",
            "[Train] epoch:1407, lr:0.000957, total loss:0.000441\n",
            "[Train] epoch:1408, lr:0.000957, total loss:0.000410\n",
            "[Train] epoch:1409, lr:0.000957, total loss:0.000379\n",
            "[Train] epoch:1410, lr:0.000957, total loss:0.000393\n",
            "[Train] epoch:1411, lr:0.000957, total loss:0.000456\n",
            "[Train] epoch:1412, lr:0.000957, total loss:0.000450\n",
            "[Train] epoch:1413, lr:0.000957, total loss:0.000457\n",
            "[Train] epoch:1414, lr:0.000957, total loss:0.000442\n",
            "[Train] epoch:1415, lr:0.000957, total loss:0.000401\n",
            "[Train] epoch:1416, lr:0.000957, total loss:0.000427\n",
            "[Train] epoch:1417, lr:0.000957, total loss:0.000494\n",
            "[Train] epoch:1418, lr:0.000957, total loss:0.000447\n",
            "[Train] epoch:1419, lr:0.000957, total loss:0.000424\n",
            "[Train] epoch:1420, lr:0.000957, total loss:0.000461\n",
            "[Train] epoch:1421, lr:0.000956, total loss:0.000401\n",
            "[Train] epoch:1422, lr:0.000956, total loss:0.000425\n",
            "[Train] epoch:1423, lr:0.000956, total loss:0.000424\n",
            "[Train] epoch:1424, lr:0.000956, total loss:0.000393\n",
            "[Train] epoch:1425, lr:0.000956, total loss:0.000399\n",
            "[Train] epoch:1426, lr:0.000956, total loss:0.000379\n",
            "[Train] epoch:1427, lr:0.000956, total loss:0.000406\n",
            "[Train] epoch:1428, lr:0.000956, total loss:0.000413\n",
            "[Train] epoch:1429, lr:0.000956, total loss:0.000418\n",
            "[Train] epoch:1430, lr:0.000956, total loss:0.000397\n",
            "[Train] epoch:1431, lr:0.000956, total loss:0.000421\n",
            "[Train] epoch:1432, lr:0.000956, total loss:0.000403\n",
            "[Train] epoch:1433, lr:0.000956, total loss:0.000415\n",
            "[Train] epoch:1434, lr:0.000956, total loss:0.000461\n",
            "[Train] epoch:1435, lr:0.000956, total loss:0.000401\n",
            "[Train] epoch:1436, lr:0.000956, total loss:0.000422\n",
            "[Train] epoch:1437, lr:0.000956, total loss:0.000457\n",
            "[Train] epoch:1438, lr:0.000955, total loss:0.000417\n",
            "[Train] epoch:1439, lr:0.000955, total loss:0.000462\n",
            "[Train] epoch:1440, lr:0.000955, total loss:0.000403\n",
            "[Train] epoch:1441, lr:0.000955, total loss:0.000410\n",
            "[Train] epoch:1442, lr:0.000955, total loss:0.000405\n",
            "[Train] epoch:1443, lr:0.000955, total loss:0.000457\n",
            "[Train] epoch:1444, lr:0.000955, total loss:0.000476\n",
            "[Train] epoch:1445, lr:0.000955, total loss:0.000464\n",
            "[Train] epoch:1446, lr:0.000955, total loss:0.000447\n",
            "[Train] epoch:1447, lr:0.000955, total loss:0.000433\n",
            "[Train] epoch:1448, lr:0.000955, total loss:0.000442\n",
            "[Train] epoch:1449, lr:0.000955, total loss:0.000408\n",
            "[Train] epoch:1450, lr:0.000955, total loss:0.000388\n",
            "[Train] epoch:1451, lr:0.000955, total loss:0.000399\n",
            "[Train] epoch:1452, lr:0.000955, total loss:0.000395\n",
            "[Train] epoch:1453, lr:0.000955, total loss:0.000379\n",
            "[Train] epoch:1454, lr:0.000954, total loss:0.000430\n",
            "[Train] epoch:1455, lr:0.000954, total loss:0.000402\n",
            "[Train] epoch:1456, lr:0.000954, total loss:0.000401\n",
            "[Train] epoch:1457, lr:0.000954, total loss:0.000397\n",
            "[Train] epoch:1458, lr:0.000954, total loss:0.000397\n",
            "[Train] epoch:1459, lr:0.000954, total loss:0.000395\n",
            "[Train] epoch:1460, lr:0.000954, total loss:0.000437\n",
            "[Train] epoch:1461, lr:0.000954, total loss:0.000440\n",
            "[Train] epoch:1462, lr:0.000954, total loss:0.000421\n",
            "[Train] epoch:1463, lr:0.000954, total loss:0.000423\n",
            "[Train] epoch:1464, lr:0.000954, total loss:0.000407\n",
            "[Train] epoch:1465, lr:0.000954, total loss:0.000422\n",
            "[Train] epoch:1466, lr:0.000954, total loss:0.000421\n",
            "[Train] epoch:1467, lr:0.000954, total loss:0.000408\n",
            "[Train] epoch:1468, lr:0.000954, total loss:0.000408\n",
            "[Train] epoch:1469, lr:0.000954, total loss:0.000390\n",
            "[Train] epoch:1470, lr:0.000953, total loss:0.000379\n",
            "[Train] epoch:1471, lr:0.000953, total loss:0.000378\n",
            "[Train] epoch:1472, lr:0.000953, total loss:0.000417\n",
            "[Train] epoch:1473, lr:0.000953, total loss:0.000439\n",
            "[Train] epoch:1474, lr:0.000953, total loss:0.000428\n",
            "[Train] epoch:1475, lr:0.000953, total loss:0.000404\n",
            "[Train] epoch:1476, lr:0.000953, total loss:0.000423\n",
            "[Train] epoch:1477, lr:0.000953, total loss:0.000383\n",
            "[Train] epoch:1478, lr:0.000953, total loss:0.000392\n",
            "[Train] epoch:1479, lr:0.000953, total loss:0.000426\n",
            "[Train] epoch:1480, lr:0.000953, total loss:0.000436\n",
            "[Train] epoch:1481, lr:0.000953, total loss:0.000412\n",
            "[Train] epoch:1482, lr:0.000953, total loss:0.000412\n",
            "[Train] epoch:1483, lr:0.000953, total loss:0.000398\n",
            "[Train] epoch:1484, lr:0.000953, total loss:0.000381\n",
            "[Train] epoch:1485, lr:0.000953, total loss:0.000378\n",
            "[Train] epoch:1486, lr:0.000952, total loss:0.000399\n",
            "[Train] epoch:1487, lr:0.000952, total loss:0.000359\n",
            "[Train] epoch:1488, lr:0.000952, total loss:0.000391\n",
            "Early stopping at:  1488\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compare metrics, plot loss curve and save best model"
      ],
      "metadata": {
        "id": "g-tnMUW8KwNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "if overwrite:\n",
        "  torch.save(pinn.best_model, \"/content/drive/MyDrive/Machine Learning Project (ECE 228)/Battery Dataset/pinn_second_derivative_extended.pth\")\n",
        "  pinn.load_model(\"/content/drive/MyDrive/Machine Learning Project (ECE 228)/Battery Dataset/pinn_second_derivative_extended.pth\")\n",
        "true_label, pred_label = pinn.Test(loader['test_3'])\n",
        "mse = metrics.mean_squared_error(torch.tensor(true_label), torch.tensor(pred_label))\n",
        "mae = metrics.mean_absolute_error(torch.tensor(true_label), torch.tensor(pred_label))\n",
        "r2 = metrics.r2_score(torch.tensor(true_label), torch.tensor(pred_label))\n",
        "mape = metrics.mean_absolute_percentage_error(torch.tensor(true_label), torch.tensor(pred_label))\n",
        "\n",
        "print(mse, mae, r2, mape)\n",
        "\n",
        "print(count_parameters(pinn))\n",
        "result_dict = [{\n",
        "    'run name': 'second derivative extended features',\n",
        "    'mse': mse,\n",
        "    'mae': mae,\n",
        "    'r2': r2,\n",
        "    'mape': mape,\n",
        "    'param count': count_parameters(pinn)\n",
        "}]\n",
        "df = pd.DataFrame(result_dict)\n",
        "try:\n",
        "  results = pd.read_csv('/content/drive/MyDrive/Machine Learning Project (ECE 228)/Battery Dataset/results.csv')\n",
        "  results = pd.concat([results, df], ignore_index=True)\n",
        "except:\n",
        "  results = df\n",
        "\n",
        "if overwrite:\n",
        "  results.to_csv('/content/drive/MyDrive/Machine Learning Project (ECE 228)/Battery Dataset/results.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z82QEwseNUg8",
        "outputId": "5e18b02a-17b9-459a-c77f-7b12e718e32c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0008996331617869602 0.017487332733968894 0.934149046543683 0.024556102570090818\n",
            "16122\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the loss curve\n",
        "import matplotlib.pyplot as plt\n",
        "x = list(range(1,len(losses_second_derivative_extended) + 1))\n",
        "plt.yscale('log')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Log-Scale Loss plot for second derivative and extended features')\n",
        "plt.plot(x, losses_second_derivative_extended)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "nHkh_5cmNRZN",
        "outputId": "0192d028-9288-493d-c9db-1dd68da744ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7d352f205790>]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot loss values together"
      ],
      "metadata": {
        "id": "H3h11SI9_wjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_second_derivative_extended = np.load('/content/drive/MyDrive/Machine Learning Project (ECE 228)/Battery Dataset/loss_second_derivative_extended.npy')\n",
        "\n",
        "loss_second_derivative = np.load('/content/drive/MyDrive/Machine Learning Project (ECE 228)/Battery Dataset/loss_second_derivative.npy')\n",
        "loss_regular = np.load('/content/drive/MyDrive/Machine Learning Project (ECE 228)/Battery Dataset/loss_regular.npy')\n",
        "loss_extended = np.load('/content/drive/MyDrive/Machine Learning Project (ECE 228)/Battery Dataset/loss_extended.npy')\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (15,8)\n",
        "plt.rcParams.update({'font.size': 15})\n",
        "plt.plot(np.arange(len(loss_regular)), loss_regular, label='regular')\n",
        "plt.plot(np.arange(len(loss_second_derivative)), loss_second_derivative, label='second derivative')\n",
        "plt.plot(np.arange(len(loss_second_derivative_extended)), loss_second_derivative_extended, label='second derivative extended')\n",
        "plt.plot(np.arange(len(loss_extended)), loss_extended, label='extended')\n",
        "\n",
        "plt.legend()\n",
        "plt.yscale('log')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Log-Scale training loss plot for all models', fontsize=15) # remove this when exporting for latex\n",
        "plt.savefig('/content/drive/MyDrive/Machine Learning Project (ECE 228)/Battery Dataset/loss_curves.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        },
        "id": "YuOBPylq_52o",
        "outputId": "75b5f0ee-bd4d-45ff-d092-4eb3b784440c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x800 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABPgAAALLCAYAAABkeTyjAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQABAABJREFUeJzs3Xd4VFX+x/HPTU9IofeQ0DFShSAI0hGkCNKkqRQ7uxawo4ALv0VlBXXNsqJSFGRXOoICKkVAQRBpG6Q3EZGaUFJnzu+PMCNjJpCQSYaE9+t58jg599x7v3NzJ7v5cO45ljHGCAAAAAAAAECB5OPtAgAAAAAAAABcPwI+AAAAAAAAoAAj4AMAAAAAAAAKMAI+AAAAAAAAoAAj4AMAAAAAAAAKMAI+AAAAAAAAoAAj4AMAAAAAAAAKMAI+AAAAAAAAoAAj4AMAAAAAAAAKMAI+AABywLIsWZbl7TJyxG63a+rUqWrdurWKFy8uf39/lSlTRvXq1dPDDz+sWbNm5XtNrVq1kmVZOnToUL6fO69FR0d77B7x5LE8zbIsRUdHe7uMfHOj3LMnT57UAw88oHLlysnX11eWZWn69OlerSm7Vq9eLcuyNGjQIJf26dOny7IsjRkzxit15QVPvqcb5d4DANzYCPgAACjEUlNT1blzZw0dOlTffvutatasqZ49e+qOO+7Q+fPn9eGHH2ro0KHeLjNPjBkzpkCFH7g5ZBVyZdfQoUP1ySefqHTp0urXr58efPBBVatWzbNFAgCAAsfP2wUAAIC8895772nZsmWKjIzUihUrVKtWLZft//vf/zRjxgwvVVc4ffPNN0pLS7vhjoWCLzU1VV988YWio6P1008/yceHf6sHAAAZCPgAACjE5s2bJ0kaNWpUpnBPkm699Va9+eab+V1WoVa1atUb8lgo+H777TfZbDZFRUUR7gEAABf8PwMAAPLQ0aNH9eijjyoqKkqBgYEqXbq0evTooU2bNmW5z/z589WkSROFhISoZMmS6t27t/bt23ddj5yePHlSklSqVKnrqv3JJ59UjRo1FBwcrOLFi6tRo0Z67bXXlJiY6Ox3/Phxvfnmm2rZsqUqVKiggIAAlS1b9prvMytnzpzRSy+9pJiYGAUHBysiIkJt2rTRkiVLsn2M6Ohovfbaa5KkwYMHO+dOtCxLq1evluQ6R9aePXvUt29flSlTRj4+Plq4cKEkOa9706ZNVbZsWQUEBKhixYp64IEHtGfPnizP/ed58w4dOiTLstSqVSslJSXpxRdfdN4T1apV0xtvvCFjTJ4eS5LWrFmjNm3aKCwsTMWKFVOnTp20efNmj8+B9sUXX6h9+/YqVqyYgoKCVLNmTb344os6d+5cpr7GGM2aNUvNmzdXmTJlFBQUpMjISLVr105xcXEufVNTU/Wvf/1LsbGxKlGihEJCQhQdHa0uXbroP//5T7Zqu/L6JSYm6qmnnlJkZKSCgoJ0yy23aNKkSbLb7Tl6v/Hx8RowYIDKlSungIAAVahQQQ888IB2797t0m/QoEFq3bq1JGnGjBku9+W1rn10dLSioqIkZfwcHfv9eR7E77//Xt26dVOpUqUUGBio6OhoPfHEE/r1118zHTM7n4GrWbp0qYYMGaJbbrlF4eHhKlKkiOrVq6e///3vSklJueb+ueF47+np6Ro7dqyqVaum4OBg3XLLLZo2bZqz38qVK9W6dWuFh4erWLFieuCBB3T69Gm3xzx9+rSee+45Va9eXUFBQSpevLg6duyoFStWZFnH+vXr1a5dO4WFhalo0aLq0KGDNm7ceNXa09PTNXnyZDVt2lTh4eEKDg5W/fr19fbbbys9PT3b1+Dw4cN6/PHHVaNGDYWEhKh48eK69dZb9eijj2a69wAAhR8j+AAAyCM7duxQmzZtdOrUKdWsWVM9evTQkSNHtGDBAn3++ef69NNP1bt3b5d93nnnHT399NPy8fFRixYtVLZsWW3cuFGNGzdW165dc1xDZGSk9u7dqw8//FCdOnWSv79/tvZbu3at7rnnHp07d07R0dHq2rWrkpKS9PPPP2vMmDHq1q2b6tevL0latGiRXnjhBdWsWVN169ZVeHi49u7dqwULFmjJkiVasmSJ7rrrrmydd8+ePWrXrp2OHj2q6OhodejQQefPn9eGDRvUtWtXTZgwQc8+++w1j9OrVy99/fXX2rZtm5o1a+YyR1nZsmVd+u7evdsZGLVu3Vpnz551XqcPP/xQb775pmrXrq3Y2FgFBgYqPj5en3zyiRYtWqS1a9eqbt262XpvUkZAdddddyk+Pl6tWrXSxYsXtWbNGr344os6f/68xo0bl2fHmj9/vvr06SObzaYmTZooOjpaO3bsUPPmzTV48OBsn/daxo8fr5dffll+fn5q2bKlSpYsqfXr1+uNN97QggUL9O2336pMmTLO/s8//7z+8Y9/KDAwUC1atFDJkiX122+/afv27dq3b5+GDRvm7DtgwADNnTtXYWFhuvPOOxUeHq5jx45p3bp1unDhgvr27ZvtOlNSUtSmTRvt379fbdq0UWpqqr755hsNHz5c27Zty3aQ/s033zg/Hw0aNFCrVq30888/65NPPtGCBQv0xRdf6M4775QkNW/eXL/99puWL1+uqlWrqnnz5s7jOD5PWenVq5cOHTqkefPmqUyZMurYsaMkqWTJks4+M2fO1KBBg2Sz2dSsWTNFRkZqy5Ytmjx5subPn6/Vq1e7Hcl7tc/A1QwdOlRJSUmqXbu26tatq4SEBP3www8aOXKkvvnmG61YsUK+vr7ZuYzXrU+fPs4Qr2rVqlqzZo2GDBkiSQoLC1O/fv3UpEkTdejQQd9//70++eQTHTx4UN9++61LeH7s2DG1aNFCBw4cUKVKldS9e3edPHlSX3/9tZYvX66JEyfqmWeecTn3kiVLdO+99yo9PV2NGzdWlSpVtG3bNrVo0SLLORaTkpLUuXNnrVq1SsWLF1eTJk0UFBSkjRs36plnntGqVau0YMGCa47QPHr0qG677TadOXNG1atXV6dOnWSz2XT48GF98MEHatq0qWrWrJm7iwsAKFgMAADINkkmO//zabfbTZ06dYwk8/zzzxu73e7cNnfuXOPj42NCQ0PNr7/+6mzfv3+/CQgIMAEBAWblypXO9rS0NDN48GDnuadNm5btemfPnu3cr1KlSubpp582//3vf82+ffuy3Of06dOmVKlSRpKZMGGCsdlsLtu/++47c+LECef327dvNzt37sx0nGXLlpmAgABTtWpVl/dvjDEtW7Y0kszBgwedbenp6c5r9uabb7qcd+/evaZy5crG19fX7NixI1vvffTo0Ve9XtOmTXNem7/85S8mPT09U5/vv//eHDhwIFP71KlTjSTTunXrTNuioqIy3SMHDx50nqtly5YmISHBuW3Tpk3G19fXhISEmPPnz+fJsRISEkzx4sWNJDNr1iyX47366qvO440ePdrNlXJPkomKinJp++GHH5z39oYNG5ztycnJpnfv3kaS6dmzp7M9KSnJBAYGmrCwsEzXOS0tzXz77bfO7w8cOOA856lTp1z6JiUlme+++y5bdV95/erWrWtOnjzp3LZv3z5Tvnx5I8ksWLDAZT939+yFCxdMmTJljCTz3nvvufSfOHGikWQqVqxokpKSnO2rVq0yksyDDz6YrXrd1d6yZctM244cOWKCg4ONr6+vWbRokbPdZrOZp59+2kgyjRo1ctknO5+Bq1m4cKG5dOmSS1tiYqLp0qWLkWRmzJjhsi2r9+6oI6f3nyRTu3Zt8/vvvzvbV65caSSZcuXKmRIlSpglS5Y4tyUkJJhbb73VSHL5HWuMcdbcv39/k5KS4mxfu3atCQkJMb6+vuann35yeZ+O35NTp051ttvtdvPCCy9k+Zl64oknjCRz3333mXPnzrkcr1OnTkaSmTx5sss+7u69UaNGOX9uf3b48OGr/o4HABROBHwAAORAdgM+xx+ZlSpVMqmpqZm29+jRw0gy48aNc7aNHDnSSDJDhw7N1P/s2bMmNDQ0xwGfMRlBQ5EiRZy1O76io6PN+PHjXcIHY4x54403jCTTsWPHHJ3HnQEDBhhJZvv27S7t7v5gXbBgQaYA6Erz5883ksyTTz6ZrXNnN+ArVaqUuXjxYraOeaVmzZoZy7Jc/kg35uqhnI+Pj/n5558zHcsRLqxatSpPjvXBBx8YSaZt27aZ+qelpTnPk9uA74EHHjCSzEsvvZSp/4kTJ0xwcLDx8fExR44ccbZJMvXr17/m+TZu3Ggkme7du2e7RneuDPhWrFiRafvkyZPdXit396wj6G3atKnbczVs2NBIMjNnznS25VXA5wh8+vXrl2lbcnKyM7hct26dsz23n4Gs7N2710gyPXr0cGnPi4Dv66+/zrStQYMGRpIZOHBgpm3vvPNOpnPt37/fSDKhoaHm9OnTmfYZPny4kWQeeughZ5vjZ9+iRYtM/VNTU03FihUznefEiRPG39/fREZGZgpGjTHm+PHjJiAgwNStW9el3d299/jjjxtJZuHChZmOAwC4OTEHHwAAeWDt2rWSMh4fc/eo2/333+/ST8qYy0lSpsd2Jalo0aLZfsz1z5555hkdOXJEU6ZMUf/+/VW9enVJGXORvfTSS2rdurWSkpKc/b/++mtJ0qOPPprtc6SkpGjRokUaOXKkHnnkEQ0aNEiDBg3Sjh07JEl79+695jEc81z16NHD7XbHY44//PBDtuvKjnbt2ikkJCTL7RcuXNDs2bP1wgsv6OGHH3a+t+PHj8sYo/3792f7XFFRUW4fm6tRo4akjPkM8+JYV7u3/Pz81LNnz2yf92oc9/OAAQMybStdurTuuusu2e12Zz2lS5dWxYoVtXXrVr344os6cOBAlseuVauWihQpoqVLl2rChAlu55TLieLFi6t9+/aZ2vv16ydJ+u677645F9/V3q8kDRw40KVfXrpaLYGBgc6fvbtarvUZuJq9e/fqnXfe0V//+lcNGTJEgwYN0tixY53b8pK/v79atWqVqb1KlSqS5PZ3pmPblZ+PdevWSZI6duyo4sWLZ9rH3e9rx2t3j4X7+/urV69emdpXr16ttLQ0dezYUcHBwZm2ly1bVtWrV9eOHTtcfie707BhQ0nSyy+/rCVLlig5Ofmq/QEAhR9z8AEAkAcc4cOfJ8B3cLQfO3bM2eb4gzMyMtLtPpUqVcrU9vrrr+vnn392aatVq5ZefPFFl7bixYvr4Ycf1sMPPywpY3L2uLg4TZw4URs2bNDEiRM1cuRISRlzO0nZX8F1x44duueee3To0KEs+5w/f/6ax3HsP2DAgCwDE0k6depUturKLnfX1WHlypXq27evc7ESd7Lz3hwqVqzotj0sLEyScrQwQU6OdT331vW4nvt+xowZ6tu3r9544w298cYbioqKUsuWLdW3b1/dfffdzn7h4eH64IMP9Mgjj+j555/X888/rxo1aqh169a6//771axZsxzV6liw4s8iIiJUtGhRnTt3TmfPnlWJEiU8+n7zSm5quZ6fvzFGzz77rCZNmpTloi45+Wxcj7Jly7qd4y80NFSSVKFChSy3Xfn5uJ5r59gnq/vI3bEcv+M++OADffDBB273czhz5ozb+h0GDRqkFStW6LPPPlPXrl0VFBSk2NhYdezYUUOGDMk01ygAoPAj4AMAwAv+vDLq9Vq2bJnWrFnj0tayZctMAd+fRUVF6c0331R6eromTZqkpUuXOgO+nDDGqE+fPjp06JAee+wxPfbYY6pSpYpCQ0NlWZZefvlljR8/PssA4EqO0VIdO3Z0WYThz65cVMATgoKC3LZfuHBBffr00ZkzZzRq1Cj17dtXUVFRCg4OlmVZ6t+/v2bPnp2t9+ZwrYnzc8KTx8ov7u77Nm3aaN++fVqyZImWLVum1atX6+OPP9bHH3+snj17au7cuc6+/fr1U7t27bRo0SKtWLFCa9as0fvvv6/3339fw4cP11tvvZWfb+eaPPU594Sr1ZLVZ+Bq/vvf/2rixImKjIzUpEmT1LRpU5UqVUr+/v5KTU1VYGBgjj4b1+NanwFPfUY89XN0/I6rX7++6tWrd9W+gYGBV93u6+ur//73v3rxxRe1aNEirVy5Uhs3btTatWv1+uuva9myZbrjjjs8UjcAoGAg4AMAIA+UL19eUsZIOXccIzmuHKFRrlw57d69W0ePHlVMTEymfRwj6660evXqXNXZpk0bTZo0yWVUXGRkpH7++Wft379fderUuer+P//8s37++Wc1atRIkydPzrT9ao9c/pljRNpDDz3ksUdGc2Pt2rU6ffq0evXqpddeey3T9py8N28rV66cJPf30NXac6p8+fI6ePCgDh8+7PYednffSxmj8/r376/+/ftLkjZs2KDevXtr3rx5+uKLL9SpUydn31KlSumhhx7SQw89JGOMli9frvvuu08TJ07UkCFDdOutt2ar1iNHjrhtT0xM1Llz5xQcHKyiRYte8/1KOfuc55Xy5ctr9+7dOnz4sNtr4OlaFixYIEmaPHmyOnfu7LKtIH02pOv/fX21fdy1O37HNW/eXP/85z+vu94rNWjQQA0aNNCYMWOUmJioMWPGaNKkSXr66ac9Pp0BAODGVvD+6RcAgALAMV/cnDlzZLPZMm2fOXOmSz9JzkcM582bl6l/QkKCc466nLjWCJp9+/ZJcv3DtV27dpKkKVOmXPP4Z8+eleT+cdGzZ8/qq6++ynatjvnQHMFBbgUEBEiS0tPTr2v/q723ffv2acuWLddfXD672r1ls9k0f/58j5zHcT/Pnj0707aTJ09q+fLlsizrmo/TNmnSxDnv2c6dO7PsZ1mWOnbs6AyY/ve//2W71tOnT+ubb77J1P6f//xHktS0aVO3j39e6WrvV3L/Oc/tfXk9taSmpmrOnDmZasmNq30+PvvsM4+cI780b95cUsaI6HPnzmXa7u7n6Hjt7r2mp6e7/ay1bt1avr6+WrJkidLS0jxRuovw8HCNHz9elmVd9XMDACicCPgAAMgDrVq1Up06dXTo0CGNGjXKJWhbsGCB5s+fr9DQUA0ZMsTZPnjwYAUEBOjjjz/Wt99+62y32WwaMWLEdc1ndc899+jdd9/VmTNnMm3buHGjczL8KyeEf+ihh1SyZEl9+eWXevvttzOFhBs2bNDvv/8uSapWrZp8fHy0cuVKlwn1k5OT9dhjj7k9b1Z69uypmJgYzZo1S2PHjs00H50xRuvXr3cu0HAtjlE5u3fvznYNV3IsVjF//nyXOfjOnTunoUOH5skf6Hmld+/eKl68uL766itngOUwbtw4HTx40CPnGTZsmHx8fPTuu+9q8+bNzvbU1FT99a9/VVJSknr06OGcC/DIkSOaPn26Ll265HKc5ORkrVq1StIf8wb+9NNPmj9/vlJTU136njlzRhs3bnTpm13PPvusTp8+7fz+4MGD+tvf/uZ8L9fSp08flSlTRuvWrcsUiDuuQYUKFVxGpOb2vszK0KFDFRwcrP/85z9aunSps91ut+vll1/WsWPH1LBhwxzPVZgVx+djypQpLr8j1q5dqwkTJnjkHPmlSpUq6ty5s86fP6+nnnrK5bP9/fffa/LkyfL19XW5J3r37q0SJUpo9erVmjFjhrPdGKPRo0e7HSFaoUIFDRkyRIcOHVK/fv104sSJTH327dvnNhz8s08++cRtiPfll1/KGJPjzwIAoODjEV0AAK5DkyZNstzmeHxw1qxZat26tf7+979rwYIFql+/vo4cOaL169fLz89PH330kfMxLyljUYs333xTTz/9tFq3bq2WLVuqTJky+uGHH3TmzBkNHDhQM2fOdI4Ayo6jR4/qqaee0ogRI1S/fn1VrlxZdrtd+/fv19atWyVJXbt2dVkxt3jx4pozZ47uuecePfPMM3r33XcVGxurpKQk7dq1S/v27dNPP/2k0qVLq3Tp0ho6dKg++OAD1atXT23atFFwcLDWrl0rm82mQYMGafr06dmq1c/PTwsXLlSHDh00atQovffee6pbt65Kly6tU6dOaevWrfr99981adKkbIUUd911l4KCgjRp0iTt3LlT5cuXl2VZeu6559yuPvtnjRo1Uvv27fXVV1+pRo0aztU6V69erZIlS6pbt25atGhRtt6bt0VEROiDDz5Qnz591K9fP7377ruKjo7Wjh07tGfPHj3yyCOaMmVKju4tdxo3bqyxY8dq5MiRatq0qVq1aqWSJUtq/fr1Onr0qKpXr664uDhn/zNnzmjw4MEaNmyYGjVqpIoVK+rixYv67rvvdPLkSTVq1Mi5qvLhw4fVs2dPRUREqFGjRipbtqzOnTunb7/9VufPn1fXrl3VtGnTbNfapEkTpaamqlq1amrTpo3S0tL0zTff6NKlSxo4cGCWqzlfqUiRIpo1a5bzMzRlyhTVqFFDP//8s3766SeFhoZq9uzZLnPcRUdHq27dutq8ebMaN26sW2+9Vb6+vrrnnnt0zz335OBqu6pUqZLef/99DRo0SF27dlWzZs0UGRmpLVu2aPfu3SpTpoxzJJonPPnkk5o+fbr+9a9/afXq1apbt66OHTumdevWacSIEfrHP/7hsXPlh/fff1933nmnPv74Y61Zs0ZNmzbVyZMntXr1atlsNr311luqX7++s39YWJg++ugj9ezZU4MGDdLkyZNVpUoVbdu2TXv37tXDDz/sdiGNd955R4cOHdK8efO0bNky1a9fX5UqVdLFixcVHx+vffv2qVu3btecpmDevHl64IEHVLVqVdWpU0fBwcE6ePCgNm7cKB8fH40bN87TlwgAcINjBB8AANdh48aNWX798ssvkqQ6depoy5Ytevjhh3XhwgXNnTtXu3fvVvfu3bV+/Xr16dMn03GfeuopzZ07V40aNdKGDRu0fPly1a9fXxs3bnSGBFdb1fPP5s6dq3fffVd33323EhMT9eWXX2rx4sU6ceKEOnfurE8//VSLFi2Sn5/rv/m1atVK27Zt02OPPSZjjBYuXKj169crIiJCf/vb31xW2J08ebLeeustVa5cWd98843Wrl2rdu3aafPmzVmuMJmV6tWr66efftK4ceNUsWJFbdiwQfPnz9eePXvUoEEDxcXFaeDAgdk6Vvny5bVo0SI1adJE69at09SpU/XRRx85V5TNjkWLFmnkyJEqVaqUvvzyS/3444/q27evNmzYcM352W40PXr00Ndff61WrVpp+/btWrp0qcqXL6+1a9c6V1HNyb2VlZdffllLlixRy5YttWnTJs2fP1+BgYF6/vnntXHjRpcFVKpWraq33npLrVq10pEjRzR//nytW7dOUVFRmjRpktasWeNcbKBJkyYaN26cGjZsqN27d2vOnDnavHmz6tatq6lTp2Zr1NOVAgMDtXLlSvXv39/5WYuMjNQ//vGPbIfSktS2bVtt2rRJ/fr10y+//KK5c+fqt99+08CBA7V582a3j8TOmzdP3bt314EDB/Txxx/ro48+8sgj3/fff7/Wrl2rLl26aNeuXZo7d66SkpL0+OOP68cff1StWrVyfQ6HGjVqaPPmzeratatOnTqlxYsX68KFC3r//fcL3Ag+KWN03aZNmzRixAj5+flp/vz5+vHHH9W2bVstX75cw4cPz7RPt27dtGrVKrVu3Vo7d+7U0qVLVa5cOa1ZsybLBS6Cg4P15ZdfasaMGbr99tudP6fNmzerVKlSeu211/Tmm29es97hw4dr2LBhCgsL09q1a7VgwQL9/vvvuu+++7Rx40b17t0719cEAFCwWCavl7cCAAC5ZrPZVLduXe3atUu//vqrypYt6+2SUIh07NhRy5cv14YNG3T77bd7u5w8dejQIVWuXFktW7bM9SI1AAAANwpG8AEAcAPZv39/pkneU1JS9Pzzzys+Pl5t27Yl3MN1OXbsWKY5v+x2uyZNmqTly5erRo0aaty4sZeqAwAAQG4wBx8AADeQOXPmaPTo0WrYsKEiIyOVmJiobdu26fjx4ypZsqTee+89b5eIAmrt2rUaOHCgGjRooKioKKWkpGjnzp06dOiQQkJC9OGHH8qyLG+XCQAAgOvACD4AAG4gbdu2VY8ePXT8+HEtXbpUq1atUnBwsB5//HFt2bIlW4tDAO40bNhQDzzwgM6dO6cVK1Zo+fLlstlsuv/++7Vp0ya3c8UBAACgYGAOPgAAAAAAAKAAYwQfAAAAAAAAUIAR8AEAAAAAAAAFGItseIHdbtevv/6qsLAwJrMGAAAAAAC4yRljdP78eZUvX14+Pjkfj0fAl4/i4uIUFxen1NRU7d+/39vlAAAAAAAA4AZy9OhRVaxYMcf7sciGFyQkJKho0aI6evSowsPDvV0OAAAAAAAAvCgxMVGRkZE6d+6cIiIicrw/I/i8wPFYbnh4OAEfAAAAAAAAJOm6p3JjkQ0AAAAAAACgACPgAwAAAAAAAAowAj4AAAAAAACgACPgAwAAAAAAAAowAj4AAAAAAACgACPgAwAAAAAAAAowP28XAAAAAADIOWOM0tLSZLfbvV0KAOAKvr6+8vf3z9dzEvABAAAAQAGSmpqq33//XZcuXZLNZvN2OQAANwIDA1WyZEmFh4fny/kI+AAAAACggLh06ZKOHj0qX19fFStWTMHBwfL19ZVlWd4uDQCgP0ZXJyQk6NixY5KULyEfAR8AAAAAFBCnTp2Sv7+/oqKi5Ovr6+1yAABuBAcHKywsTL/88otOnTqVLwEfi2wAAAAAQAGQnp6uixcvqnjx4oR7AHCDsyxLERERSklJUVpaWp6fj4APAAAAAAqA9PR0SRnzOgEAbnyOhTbyY75UAj4AAAAAKECYbw8ACob8/H1NwAcAAAAAAAAUYAR8AAAAAAAAQAFGwAcAAAAAAAAUYAR8AAAAAAB4waBBg2RZllavXu3tUgAUcAR8AAAAAAAAQAFGwAcAAAAAAAAUYAR8AAAAAAAAQAFGwAcAAAAAKLAOHToky7LUqlUrJSYmavjw4apcubL8/f319NNPS5LOnDmjl156STExMQoODlZERITatGmjJUuWZHnc+fPnq0mTJgoJCVHJkiXVu3dv7du3T2PGjJFlWZo+fbpLf8uyFB0d7fZY06dPl2VZGjNmTLbe09atW/X888+rYcOGKlWqlAIDA1WlShU98cQT+vXXX6/rGgAo3Py8XQAAAAAAALmVlJSkli1b6vDhw2rZsqVuu+02FStWTHv27FG7du109OhRRUdHq0OHDjp//rw2bNigrl27asKECXr22WddjvXOO+/o6aeflo+Pj1q0aKGyZctq48aNaty4sbp27Zrn7+X111/XvHnzVLduXTVv3lxSRug3efJkLVy4UJs3b1b58uWzfQ0AFH4EfAAAAABQCBhjlJRm83YZ2Rbs7yvLsjx2vB9++EFNmzbVgQMHVLRoUUmSzWZTgwYNdPToUb355psaMWKEfHwyHmTbt2+f7rrrLr344ovq2LGjateuLUk6cOCAnn/+eQUEBGjZsmVq3bq1JCk9PV2PPPKIpk2b5rGas/Loo4/qnXfeUZkyZZxtdrtd48aN0+jRo/XKK69o6tSpmfZzdw0A3BwI+OAZe7+S0i5JlVtKwUW9XQ0AAABw00lKsylm1HJvl5Ft8X/roJAAz/5J+u6777oEW59//rl27Nihnj176rnnnnPpW61aNb311lvq0aOHPvjgA73zzjuSpKlTpyo1NVVDhw51hnuS5Ofnp4kTJ2rOnDm6cOGCR+v+syvP6+Dj46NRo0ZpypQpWrx4cZb7/vkaALg5EPDBMxb/VTp/XHp0LQEfAAAAgHxXrlw5NWrUyKVtxYoVkqQePXq43efOO++UlDHyzWH9+vWSpN69e2fqX7RoUd11112aP3++R2q+mtOnT2vx4sXauXOnzp07J5stY3RmWlqaTp8+rTNnzqh48eIu+7i7BgBuDgR88JDLQ+uN3btlAAAAADepYH9fxf+tg7fLyLZgf1+PHq9SpUqZ2g4dOiRJGjBggAYMGJDlvqdOnXK+Pn78uCQpMjIy2+fxtNmzZ+uRRx656kjB8+fPZwr48qM2ADcmAj54huVYkNl4tQwAAADgZmVZlscfeS1IgoKCMrXZ7RkDEDp27Ogyn92flSxZMs/qurKO7Dh8+LAGDRokSXr77bfVuXNnVahQQcHBwZKkO+64Q99//72Myfy3l7trAODmcPP+9odnOQI+RvABAAAAuEFUrFhRkvTQQw+pZ8+e2dqnXLly2r17t44ePaqYmJhM248ePep2P39//yxH3GW1jztffPGFUlNT9eyzz+qpp57KtP3AgQPZPhaAm4fPtbsA2eBY/crNvyIBAAAAgDe0b99ekrRgwYJs79OsWTNJ0rx58zJtS0hIcM7r92flypXT6dOndfr06Uzbvv7662yf/+zZs5L+CCev9O233+rEiRPZPhaAmwcBHzzDYg4+AAAAADeWnj17KiYmRrNmzdLYsWOVkpList0Yo/Xr1zsX1pCkwYMHKyAgQB9//LG+/fZbZ7vNZtOIESN0/vx5t+dq2bKlJGncuHEu7W+++abWrVuX7Zpr1KghSZo5c6YuXrzobD927Jgee+yxbB8HwM2FgA+e4XxElxF8AAAAAG4Mfn5+WrhwoSpXrqxRo0apUqVKat++vQYMGKAOHTqobNmyat68uTZt2uTcp2rVqnrzzTeVkpKi1q1bq02bNurXr59q1KihefPmaeDAgZKkgIAAl3O98MILCg4O1ttvv60GDRqoV69eqlmzpsaMGaMnnngi2zXfc889uvXWW7V582ZVq1ZNvXr1UpcuXVSjRg0VK1ZMd9xxh2cuDoBChYAvh7Zu3ao777xTwcHBqly5st577z1vl3RjYA4+AAAAADeg6tWr66efftK4ceNUsWJFbdiwQfPnz9eePXvUoEEDxcXFOUM7h6eeekpz585Vo0aNtGHDBi1fvlz169fXxo0bnQtZlChRwmWfW2+9VStXrlSrVq20Z88effXVV6pataq+//57xcbGZrvegIAArV27Vo8//riCgoK0ZMkS7dq1S3/961/11Vdfyd/fP/cXBUChYxl3S+/ArZMnTyomJkaNGzfW8OHDtWXLFr300kuaNm2a7r///mwfJzExUREREUpISFB4eHgeVpyP/tlIOr1XGvSFFN3M29UAAAAAhU5ycrIOHjyoypUrs1qql9hsNtWtW1e7du3Sr7/+qrJly3q7JAA3sJz83s5tVsQqujnw73//W5Zlac6cOQoJCVHbtm118OBBjR07NkcBX6HkGMEn8mIAAAAABdv+/ftVokQJFS1a1NmWkpKil19+WfHx8WrXrh3hHoAbCo/o5sDy5cvVqVMnhYSEONt69+6tvXv3slQ5j+gCAAAAKCTmzJmjMmXK6I477tB9992nu+++W5UrV9bEiRNVsmRJpmoCcMMpNAHfjz/+qNdff109evRQxYoVZVmWLMfKrleRlJSkUaNGqUaNGgoKClL58uU1ZMgQHTt2LFPfPXv2qFatWi5tju93797tmTdSULGKLgAAAIBCom3bturRo4eOHz+upUuXatWqVQoODtbjjz+uLVu2qGbNmt4uEQBcFJpHdMeOHatFixblaJ/k5GS1adNGGzZsULly5dStWzcdOnRI06ZN05IlS7RhwwZVqVLF2f/s2bMuQ7QlqVixYs5tNzVW0QUAAABQSMTGxmr27NneLgMAsq3QjOBr2rSpXn31VS1evFjHjx9XYGDgNfcZN26cNmzYoKZNm2rPnj3673//q40bN+qtt97SyZMnNWTIkHyovHBIOWtT8lk/2ZOTvV0KAAAAAADATaXQjOB74YUXctQ/NTXVOW9CXFycQkNDnduGDx+uGTNmaM2aNfrxxx/VsGFDSRmj9RISElyOc+7cOee2m9mReeeVfrG0Knc6oaBbvV0NAAAAAADAzaPQjODLqfXr1yshIUFVq1ZVgwYNMm3v1auXJOnzzz93ttWoUUM///yzSz/H9zf9HAzOKfhs3q0DAAAAAADgJnPTBnzbtm2TJN12221utzvat2/f7mzr0KGDvvjiCyUlJTnb5s6dq+rVq7vM1fdnKSkpSkxMdPkqdBzrmdhZZAMAAAAAACA/3bQB35EjRyRJFStWdLvd0X748GFn22OPPSa73a4+ffrom2++0T/+8Q+9//77evXVV696rvHjxysiIsL5FRkZ6aF3ceOwfC4nfLZ07xYCAAAAAABwk7lpA74LFy5IkkJCQtxuL1KkiCTp/PnzzrZSpUrpq6++0tmzZ9W5c2f985//1MSJE3X//fdf9VwvvfSSEhISnF9Hjx710Lu4gTgf0WUEHwAAAAAAQH4qNIts5Jf69etr3bp1OdonMDAwW6v6FmQXLKNASQkpF+Q+MgUAAAAAAEBeuGlH8DlWzb106ZLb7RcvXpQkhYWF5VtNBdkZn4yRe+dSzl+jJwAAAAAAADzppg34KlWqJEn65Zdf3G53tEdFReVbTQWZuTwHn51VdAEAAAAAAPLVTRvw1atXT5K0ZcsWt9sd7XXr1s23mgoyc/m/dhbZAAAAAACvmT59uizL0pgxYzxyvDFjxsiyLE2fPt0jx7ua1atXy7IsDRo0KM/PlVOHDh2SZVlq1aqVt0sB3LppA75mzZopIiJC+/fv19atWzNtnzt3riSpa9eu+VxZwWQu30l2GyP4AAAAAAAFi6eDUSC/3bQBX0BAgP7yl79IkoYNG+acc0+SJk6cqO3bt6tly5Zq2LChx84ZFxenmJgYxcbGeuyYNwrDKroAAAAAgFxo3Lixdu3apfHjx3u7lEwqVKigXbt26eOPP/Z2KYBbhWYV3aVLl2rs2LHO71NTUyVJTZo0cba9+uqr6ty5s/P7V155RV9//bW+++47Va9eXXfeeacOHz6sjRs3qlSpUpo6dapHaxw2bJiGDRumxMRERUREePTY3uYI+BjBBwAAAAC4HiEhIapVq5a3y3DL39//hq0NkArRCL6TJ09q48aNzi9jMmaFu7Lt5MmTLvsEBQVp1apVevXVVxUSEqKFCxfq8OHDGjRokLZs2aIqVap4460USI5FNgyLbAAAAADIRzt37tTAgQNVpUoVBQUFqVSpUqpfv76efvppHT9+PFP/Xbt2adCgQYqMjFRgYKDKlCmjvn376n//+1+W59i4caP69u2rChUqKDAwUOXKlVPbtm31wQcfZOp79OhRPfroo4qKilJgYKBKly6tHj16aNOmTZn6XjmvW1JSkl588UXnftWqVdMbb7zh/Nv2z9avX6927dopLCxMRYsWVYcOHbRx48YcXDlXixcvVtOmTRUSEqISJUqoZ8+e2rNnz1X3uXTpksaPH68GDRooNDRUoaGhatKkiWbMmOG2v2VZio6OVmpqqv72t7+pVq1aCgwMVPfu3SW5n4PvySeflGVZmjx5cpZ1NGzYUJZlafv27c62pUuXasiQIbrlllsUHh6uIkWKqF69evr73/+ulJQUl/1btWqlwYMHS5Jee+01WZbl/HLMPehuDr6JEyfKsiy98MILWdbWs2dPWZalxYsXu7SfOXNGL730kmJiYhQcHKyIiAi1adNGS5YsyfJYwFUZ5LuEhAQjySQkJHi7FI/5sm2Mia9Zy/zw9qPeLgUAAAAolJKSkkx8fLxJSkrydik3jM2bN5ugoCAjydStW9f06dPHdOnSxcTExBhJZtWqVS79FyxYYAIDA40kU79+fdOrVy9z++23G8uyTEhIiFmzZk2mc7z99tvGx8fHSDINGzY0ffv2Ne3atTOlS5c2ERERLn23b99uSpYsaSSZmjVrmr59+5o77rjDSDJ+fn7ms88+c+l/8OBBI8k0bdrUNG/e3BQvXtz06NHDdOjQwfm+Ro4cmammzz//3Pj5+RlJpnHjxqZv377mlltuMQEBAeaRRx4xkszo0aOzfR0nT55sJBnLskyLFi3MfffdZ6KiokxERIQZOHCgkWSmTZvmss+JEydM3bp1jSRTtmxZ06lTJ3P33XebiIgII8n85S9/yXQeSSYyMtLcfffdpkiRIqZTp06md+/e5rHHHjPGGLNq1SojyTz44IPOfTZs2GAkmebNm7utfdeuXUaSqVOnjkt7mTJlTHh4uLnjjjtMnz59TIcOHUyxYsWMJNOmTRuTnp7u7Dt+/HjTrFkzI8nUq1fPPPjgg86vtWvXGmP++Fm1bNnSud+xY8eMj4+PqVSpkrHb7ZlqO3funAkMDDQlSpQwqampzvbdu3ebyMhII8lER0ebbt26mTZt2piQkBAjyUyYMMH9DwoFTk5+b+c2KyLg84LCGPB90S4j4Nv41kPeLgUAAAAolAj4MnvggQeMJPOPf/wj07Zdu3aZX3/91fn9wYMHTZEiRUxoaKj56quvXPp++eWXxt/f30RGRpqUlBRn+5o1a4xlWSYsLMx8/fXXLvukpaWZpUuXOr+32+2mTp06RpJ5/vnnXQKfuXPnGh8fHxMaGpqpJknO4OjKvxE3bdpkfH19TUhIiDl//ryzPTEx0ZQqVcpIMlOnTnU5/wsvvOA8XnYDvkOHDpmgoCDj7+9vli1b5mxPTU01AwYMcB7vzwFfp06djCTz1FNPmeTkZGf7b7/9Zho1amQkmS+//NJlH8exqlWrZn755ZdMtbgL+Iwxplq1asayLHP48OFM+7zyyitGknn99ddd2hcuXGguXbrk0paYmGi6dOliJJkZM2a4bJs2bdpVr5u7gM8YY9q2bWskmW+//TbTPh9++KGR5AwwjTEmPT3deZ+8+eabxmazObft3bvXVK5c2fj6+podO3a4rQMFS34GfIXmEV14F4/oAgAAAF5mjJR6seB8ZfHoaU44pmFq165dpm21atVSuXLlnN+//fbbunjxosaPH5+pf8eOHfX444/r6NGjWrp0qbP99ddflzFGI0eOVNu2bV328fPzU6dOnZzfr169Wjt27FClSpU0btw4WZbl3NazZ091795dFy5ccDvXu4+Pj95//32Fh4c72xo1aqS7775bly5d0ubNm53tc+fO1cmTJ9WiRQvnY6VSxuOvY8eOVcWKFbO+YG5MnTpVycnJ6tevnzp06OBs9/f31zvvvKOQkJBM+2zdulVffPGFYmNjNXHiRAUGBjq3lSlTRlOmTJGkLB+rHT9+vCpUqJDtGgcMGCBjjD799NNM2z799FNZlqX+/fu7tHfr1k3BwcEubWFhYZo0aZIkadGiRdk+/9UMHDhQkjRr1qxM2xxtAwYMcLZ9/vnn2rFjh3r27KnnnntOPj5/xDLVqlXTW2+9JZvN5vbxb+BqCs0iG/AyxyIbrKILAAAAeEfaJenv5b1dRfa9/KsUUCRXh2jYsKG+/PJLDRs2TOPGjVPz5s3l5+f+z9wVK1ZIknr06OF2+5133ql3331XP/zwg+69916lp6dr9erVkqRHHnnkmrWsXbtWktSnTx/5+/tn2n7//fdr/vz5zn5XioqKUs2aNTO116hRQ5Jc5hJ07N+3b99M/f39/dWrVy+9/fbb16w3O8crUaKE7rrrLi1cuNCl3XEtu3fv7hJQOTjm5Pvhhx8ybbMsS127ds12fVJGQPbaa6/p008/1Ysvvuhs//7773XgwAG1bNlSkZGRmfbbu3evvvjiC+3bt08XL16U3W53zmm4d+/eHNWQlR49eujxxx/X3Llz9c9//tP5sz927JjWrFmj6OhoNWvWzNk/O/ehJLfXDrgaAj54hGMVXUbwAQAAAMgvzz33nNatW6fVq1erdevWCg0NVdOmTdW5c2cNGjRIERERzr6HDh2SpGuOHDt16pQk6fTp00pKSlLx4sVVrFixa9by66+/SpKio6Pdbne0Hzt2LNO2rEbdhYWFSZLLohCO80RFRV31PNl1PcdzXMuRI0dq5MiRWR47OTk5U1vp0qVdRvxlR/Xq1RUbG6tNmzZpx44dqlOnjiT3I+QkyRijZ599VpMmTcpykZLz58/nqIashIeHq2vXrpozZ46WLVvmDC9nz54tu92u/v37u4zmdFy7AQMGZKr7So77EMguAr58FBcXp7i4ONlshS8Eczyia08vfO8NAAAAKBD8QzJGxRUU/pkf/cyp8PBwrVy5UuvXr9fnn3+u1atXa+XKlfrqq680fvx4rV27VtWrV5f0x9NGDz744FWPefvtt+e6LneuDHn+zN0ouBuZ41o2b95cVatWzdG+QUFB13XOgQMHatOmTfr00081fvx4paen67PPPlNgYKB69erl0ve///2vJk6cqMjISE2aNElNmzZVqVKl5O/vr9TUVAUGBmYZ/F1vbXPmzNGnn37qDPiyCh8d165jx44qU6ZMlscsWbKkx+rDzYGALx8NGzZMw4YNU2Jiosu/JBUKjhF8hkd0AQAAAK+wrFw/8loQWZal5s2bq3nz5pKk33//XU8//bRmz56tkSNH6rPPPpOUMUpu//79euutt1SiRIlrHrdkyZIKDg7WmTNndO7cORUtWvSq/cuXz3g8+vDhw263Z3cE4bU45hXM6jxZtV/teLt379bhw4cVExOTreM5Rhx2795dI0aMyNH5rtd9992n4cOHa/bs2fr73/+uFStW6OTJk7r33nszjbBcsGCBpIw5ADt37uyy7cCBAx6v7e6771bx4sW1ePFiXbhwQUeOHNHWrVvVoEGDTNfUce0eeugh9ezZ0+O14OZVsP6ZADes9Mv/+JGSmubdQgAAAADc1EqXLq0xY8ZIknbu3Olsb9++vaQ/wp9r8fX1VatWrSTJuWjE1TjmTpszZ47bp7Zmzpzp0u96OfZ3BJdXSk9P17x58zx2vDNnzjjnjLtSTq+lJ5QpU0bt2rXT4cOHtX79+ixHyEnS2bNnJbl/9Nnd+5SkgIAASRnXMKf8/f3Vu3dvXbp0SQsXLrxqbd64drg5EPDBI9KVkfClpfGILgAAAID88e9//1sHDx7M1P7FF19IksvCCyNGjFBwcLCeffZZzZ8/P9M+KSkpmjt3rn755Rdn2wsvvCDLsvR///d/WrVqlUv/9PR053kkqVWrVqpTp44OHTqkUaNGuTwCumDBAs2fP1+hoaEaMmTI9b9hSb1791aJEiW0evVqzZgxw9lujNHo0aN15MiRHB1v8ODBCgwM1KxZs/T1118729PS0vTMM8/o4sWLmfa5/fbb1b59e61fv975lNqfbdu2TcuWLctRLdfiWLF2ypQpWrRokSIiItSlS5dM/RyLk0yZMsXl57B27VpNmDDB7bEdIzB3796dq9pmzZql2bNny8fHR/369cvUr2fPnoqJidGsWbM0duxYl/kVpYyf4/r167V+/frrqgM3LwI+eIZzkQ3PzWMAAAAAAFfz73//W1WqVNGtt96qXr16qW/fvqpfv76eeeYZBQUFadSoUc6+1apV0+zZs5WWlqaePXuqevXquueee9SvXz+1aNFCJUqUUO/evV0WN2jZsqXefPNNnT9/Xm3atFFsbKz69++vu+66SxUqVFD//v2dfS3L0qxZs1SiRAn9/e9/16233qr+/furefPm6tGjh3x8fPTRRx85H7G9XmFhYfroo4/k6+urQYMGqUmTJurfv79q166tCRMm6OGHH87R8SpXrqy33npLaWlp6tChg1q3bq1+/fqpRo0aWrRoUZYLQcycOVMNGjTQv/71L0VFRal169YaMGCAunTpokqVKql+/foeD/i6d++ukJAQffLJJ7p48aJ69erldsGOJ598UkWKFNG//vUv1a5d2/kzbtmypR577DG3x27SpIlKly6tuXPnqlWrVhoyZIgeeughfffdd9mqrVmzZoqKitKyZct08OBBtW7d2hkaXsnPz08LFy5U5cqVNWrUKFWqVEnt27fXgAED1KFDB5UtW1bNmzfXpk2bcnZxcNMj4INHmMsTxrKKLgAAAID8MnbsWA0ZMkSWZembb77R559/rqSkJD300EPaunWrmjVr5tK/W7du2r59u5544glZlqWvvvpKS5cu1e+//66uXbvqs88+yzRn2rPPPqs1a9bo3nvv1ZEjRzR37lzt3LlTderU0VtvveXSt06dOtqyZYsefvhhXbhwQXPnztXu3bvVvXt3rV+/Xn369PHI++7WrZtWrVql1q1ba+fOnVq6dKnKlSunNWvW6I477sjx8YYNG6YFCxYoNjZWGzdu1PLly1WvXj1t2LBB1apVc7tP6dKl9d133+ndd99VTEyMfvrpJ82dO1fbt29XlSpVNGHCBD377LO5fasuQkND1a1bN+f3WYWPNWrU0ObNm9W1a1edOnXKOTfe+++/n+UIvqCgIC1dulTt27fX1q1bNX36dH300Ufas2dPtmqzLMsl8L3aCrnVq1fXTz/9pHHjxqlixYrasGGD5s+frz179qhBgwaKi4tzjggEsssynlw6BtniWGQjISFB4eHh3i7HIz7rVkd1dqdrf6fK6jLxi2vvAAAAACBHkpOTdfDgQVWuXPm6VyIFAOSfnPzezm1WxAg+eIZzBB+r6AIAAAAAAOQnAr58FBcXp5iYGMXGxnq7FI8zzMEHAAAAAADgFQR8+WjYsGGKj48vlJNlOufgc7McPAAAAAAAAPIOAR8843LAJx7RBQAAAAAAyFcEfPAIHtEFAAAAAADwDgI+eITxYQQfAAAAAACANxDwwTNYRRcAAAAAAMArCPjgEXbL+YyudwsBAAAAAAC4yRDwwTN4RBcAAAAAAMArCPjgEca6fCuxyAYAAAAAAEC+IuCDZ/CILgAAAAAAgFcQ8OWjuLg4xcTEKDY21tuleJxxBHyM4AMAAAAAAMhXBHz5aNiwYYqPj9emTZu8XYrn+RDwAQAAAAAAeAMBHzzCOQefIeADAAAAAADITwR88AwfR8DHHHwAAAAA4C3Tp0+XZVkaM2aMR443ZswYWZal6dOne+R4V7N69WpZlqVBgwbl+bly6tChQ7IsS61atfJ2KchCdHS0LMf0YXmoVatWsixLhw4dyvNz5QQBHzzCXA74LB7RBQAAAAAUMJ4ORguTQYMGybIsrV692tul4Cr8vF0ACgnnCD7vlgEAAAAAKJgaN26sXbt2KSIiwtulZFKhQgXt2rVLISEh3i4FcIuAD55hMYIPAAAAAHD9QkJCVKtWLW+X4Za/v/8NWxsg8YguPMXxnDuLbAAAAADIRzt37tTAgQNVpUoVBQUFqVSpUqpfv76efvppHT9+PFP/Xbt2adCgQYqMjFRgYKDKlCmjvn376n//+1+W59i4caP69u2rChUqKDAwUOXKlVPbtm31wQcfZOp79OhRPfroo4qKilJgYKBKly6tHj16aNOmTZn6XjmvW1JSkl588UXnftWqVdMbb7whk8XfWOvXr1e7du0UFhamokWLqkOHDtq4cWMOrpyrxYsXq2nTpgoJCVGJEiXUs2dP7dmz56r7XLp0SePHj1eDBg0UGhqq0NBQNWnSRDNmzHDb37IsRUdHKzU1VX/7299Uq1YtBQYGqnv37pLcz8H35JNPyrIsTZ48Ocs6GjZsKMuytH37dmfb0qVLNWTIEN1yyy0KDw9XkSJFVK9ePf39739XSkqKy/6tWrXS4MGDJUmvvfaaLMtyfjnmHnQ3B9/EiRNlWZZeeOGFLGvr2bOnLMvS4sWLXdrPnDmjl156STExMQoODlZERITatGmjJUuWZHmsq1m2bJk6d+6sUqVKKTAwUFWqVNHw4cN1+vRpl34vvPCCLMtSnz59Mh3j1KlTKl++vHx9fbVu3TpJGT8zx8+zdevWLtfmz3PQZbcGyfWx32+//VZt2rRRWFiYwsPD1blzZ8XHx7t9n0lJSRo5cqQqV66soKAgVa1aVaNHj1ZqaupVr09OP/c2m03/+Mc/VKtWLQUFBSkyMlJPPfWUEhMTr3oerzLIdwkJCUaSSUhI8HYpHjPtibtMfM1aZmG32t4uBQAAACiUkpKSTHx8vElKSvJ2KTeMzZs3m6CgICPJ1K1b1/Tp08d06dLFxMTEGElm1apVLv0XLFhgAgMDjSRTv35906tXL3P77bcby7JMSEiIWbNmTaZzvP3228bHx8dIMg0bNjR9+/Y17dq1M6VLlzYREREufbdv325KlixpJJmaNWuavn37mjvuuMNIMn5+fuazzz5z6X/w4EEjyTRt2tQ0b97cFC9e3PTo0cN06NDB+b5GjhyZqabPP//c+Pn5GUmmcePGpm/fvuaWW24xAQEB5pFHHjGSzOjRo7N9HSdPnmwkGcuyTIsWLcx9991noqKiTEREhBk4cKCRZKZNm+ayz4kTJ0zdunWNJFO2bFnTqVMnc/fdd5uIiAgjyfzlL3/JdB5JJjIy0tx9992mSJEiplOnTqZ3797mscceM8YYs2rVKiPJPPjgg859NmzYYCSZ5s2bu619165dRpKpU6eOS3uZMmVMeHi4ueOOO0yfPn1Mhw4dTLFixYwk06ZNG5Oenu7sO378eNOsWTMjydSrV888+OCDzq+1a9caY/74WbVs2dK537Fjx4yPj4+pVKmSsdvtmWo7d+6cCQwMNCVKlDCpqanO9t27d5vIyEgjyURHR5tu3bqZNm3amJCQECPJTJgwwf0PKgsvvPCCkWQCAgJMs2bNTK9evUz16tWNJFO1alXz22+/OfumpKSYBg0aGElm+vTpLsfp3r17pnvuwQcfNFWrVjWSTIcOHVyuzcmTJ6+rBsdxJZnhw4cbX19fc/vtt5s+ffqYGjVqGEmmRIkS5vjx4y77pKSkmDvvvNNIMsWKFTM9evQwnTt3NsHBwaZr166mUqVKxl3MdT2f+759+xpJJiQkxHTt2tXce++9JiIiwjRs2NA0adLESDIHDx685s8mJ7+3c5sVEfB5QaEM+IZ1NPE1a5lFXQn4AAAAgLxwrT8U7Xa7uZh6scB8uQtEcuqBBx4wksw//vGPTNt27dplfv31V+f3Bw8eNEWKFDGhoaHmq6++cun75ZdfGn9/fxMZGWlSUlKc7WvWrDGWZZmwsDDz9ddfu+yTlpZmli5d6nL969SpYySZ559/3uX9zZ071/j4+JjQ0NBMNSljJnPTsmVLl78RN23aZHx9fU1ISIg5f/68sz0xMdGUKlXKSDJTp051Ob8jZMlJwHfo0CETFBRk/P39zbJly5ztqampZsCAAc7j/Tng69Spk5FknnrqKZOcnOxs/+2330yjRo2MJPPll1+67OM4VrVq1cwvv/ySqRZ3AZ8xxlSrVs1YlmUOHz6caZ9XXnnFSDKvv/66S/vChQvNpUuXXNoSExNNly5djCQzY8YMl23Tpk276nVzF/AZY0zbtm2NJPPtt99m2ufDDz80kpwBpjHGpKenO++TN99809hsNue2vXv3msqVKxtfX1+zY8cOt3X82WeffWYkmdq1a5u9e/c62+12uxk1apSRZO677z6XfeLj401wcLAJCwszBw4cMMYYM2XKFCPJxMbGmrS0NJf+jjDuz4F5bmpwHNPHx8csWLDA5fr07NnTSDKvvvqqyz6vv/66kWQaNGhgTp065Wzfu3evKV++vPP+utL1fO7/85//GEmmUqVKLiHeiRMnTO3atZ3nIeBD4Qz4/trZxNesZRZ3udXbpQAAAACF0rX+ULyYetHUnl67wHxdTL2Y62ty9913G0lm69at1+z71FNPGUnmn//8p9vtTz75pJFk5s+fn+n4fw6P3Fm5cqUzFLhytJZDjx49jCQzbtw4Z5sjNPLx8TE///xzpn0cYdSVwcrUqVONJNOiRYtM/VNTU03FihVzFPA5ApgHHngg07ZTp045R5VdGfD99NNPzjDoyoDKYcuWLUaSueeee1zaHcHInDlz3NaSVcA3evRoI8mMHz8+0z5VqlQxlmWZI0eOZOPdZoRBkkyPHj1c2q834HPs9+ijj2bap3Xr1kaScxSgMRmjySSZnj17uj3P/PnzjSTz5JNPZuv91KtXz0hyGwja7XZTv3594+vr6zLazhhj4uLinKNH4+PjTZEiRUxISIjZvXt3puNcK+C7nhocxxwwYECmfTZv3uz2WjtG6K1cuTLTPo5RqH8O+K7nc9+iRYtMAbrDl19+ecMGfMzBB8/wdayiyxx8AAAAAPJHw4YNJUnDhg3T6tWrlZ6enmXfFStWSJJ69Ojhdvudd94pSfrhhx8kSenp6Vq9erUk6ZFHHrlmLWvXrpUk9enTR/7+/pm233///S79rhQVFaWaNWtmaq9Ro4Ykucwl6Ni/b9++mfr7+/urV69e16zVXd3ujleiRAndddddmdod17J79+7y8ckcKzjm5HNcyytZlqWuXbvmqMYBAwZIkj799FOX9u+//14HDhxQixYtFBkZmWm/vXv36p133tFf//pXDRkyRIMGDdLYsWOd2zyhR48eCgoK0ty5c5WWluZsP3bsmNasWaPo6Gg1a9bM2Z7T+/Bqfv/9d23btk3Vq1dX7dq1M223LEvNmjWTzWbTjz/+6LLtiSeeUOfOnfX999+rSZMmunjxoiZNmuS857IrNzVIcnt/ubvvjxw5oiNHjqh06dJq3bp1pn369evntr6cXu+0tDRt2LBBknTfffdl6t+xY0cVK1bM7bG8jVV081FcXJzi4uJks9m8XYrn+fhKkizyPQAAAMArgv2CtbH/9S+ykN+C/YJzfYznnntO69at0+rVq9W6dWuFhoaqadOm6ty5swYNGqSIiAhnX8eCABUqVLjqMU+dOiVJOn36tJKSklS8ePFs/UH/66+/SpKio6Pdbne0Hzt2LNO2ihUrut0nLCxMklwWhXCcJyoq6qrnya7rOZ7jWo4cOVIjR47M8tjJycmZ2kqXLq3AwMAc1Vi9enXFxsZq06ZN2rFjh+rUqSNJmjVrlqQ/AkAHY4yeffZZTZo0KctFSs6fP5+jGrISHh6url27as6cOVq2bJkzvJw9e7bsdrv69+8vy7Eopf64dgMGDMhU95Uc9+HVOI61d+9el3Nk93gfffSRoqOjlZiYqLvvvjtbQbana3B371/PfR8REaGiRYvq3LlzbuvLyec+NTVVpUqVUkhIiNu+UVFROnv27FWP5w0EfPlo2LBhGjZsmBITE13+h6ZQcAR8di/XAQAAANykLMtSiL/7P0gLq/DwcK1cuVLr16/X559/rtWrV2vlypX66quvNH78eK1du1bVq1eXJNntGX+sPPjgg1c95u23354ntV4t/HA3Cu5G5riWzZs3V9WqVXO0b1BQ0HWdc+DAgdq0aZM+/fRTjR8/Xunp6frss88UGBiYadTif//7X02cOFGRkZGaNGmSmjZtqlKlSsnf31+pqakKDAzMMvi73trmzJmjTz/91BnwZRU+Oq5dx44dVaZMmSyPWbJkyWue13GssmXLqkOHDlft6y4YW7x4sTOE3b17ty5cuKDQ0NBrnteTNeT1ve/tz31+IuCDR1i+vpdfMYQPAAAAQP6xLEvNmzdX8+bNJWU8Mvj0009r9uzZGjlypD777DNJGSOF9u/fr7feekslSpS45nFLliyp4OBgnTlzRufOnVPRokWv2r98+fKSpMOHD7vdnt2RRNdSrly5q54nq/arHW/37t06fPiwYmJisnU8x6ir7t27a8SIETk63/W67777NHz4cM2ePVt///vftWLFCp08eVL33ntvphGWCxYskCRNnjxZnTt3dtl24MABj9d29913q3jx4lq8eLEuXLigI0eOaOvWrWrQoEGma+q4dg899JB69uyZq/M6jlWyZElNnz49R/vu3btXzzzzjEJCQtSxY0fNnz9fTz75pKZOnZpvNeTEte77xMTETKP3pJx/7kuUKKGAgACdPHlSSUlJCg7OPNL4yJEjOSs+nxSsfybADctiBB8AAACAG0Dp0qU1ZswYSdLOnTud7e3bt5f0R/hzLb6+vmrVqpUkacqUKdfs75jLa86cOW6nZZo5c6ZLv+vl2N8RXF4pPT1d8+bN89jxzpw545zD7Eo5vZaeUKZMGbVr106HDx/W+vXrsxwhJ8n5+KS7xz/dvU9JCggIkKSrzuOYFX9/f/Xu3VuXLl3SwoULr1qbJ69dxYoVVatWLcXHx2vPnj3Z3i89PV0DBw7UxYsXNXHiRM2cOVO1atXStGnT3N4/V7s211tDTkVFRSkyMlK///671qxZk2n7f/7zH7f75fR6+/v7O0fzubtXVqxYoTNnzmS37HxFwAfP8M0YDMocfAAAAADyy7///W8dPHgwU/sXX3whSS4LL4wYMULBwcF69tlnNX/+/Ez7pKSkaO7cufrll1+cbS+88IIsy9L//d//adWqVS7909PTneeRpFatWqlOnTo6dOiQRo0a5fII6IIFCzR//nyFhoZqyJAh1/+GJfXu3VslSpTQ6tWrNWPGDGe7MUajR4/O8eiiwYMHKzAwULNmzdLXX3/tbE9LS9MzzzyjixcvZtrn9ttvV/v27bV+/XrnNFR/tm3bNi1btixHtVzLwIEDJWUErosWLVJERIS6dOmSqZ9jkYYpU6a4/BzWrl2rCRMmuD22YwTm7t27c1XbrFmzNHv2bPn4+Lhd+KFnz56KiYnRrFmzNHbsWJd55qSMn+P69eu1fv36bJ331Vdfld1uV8+ePbV169ZM20+fPq0PPvjApe1vf/ubfvjhB3Xt2lWPPvqogoODNXPmTPn7++uRRx5xznfncK1rcz01XI/HH39cUsZn+cqQ7cCBA/rb3/7mdp/r+dw7zvPnz9OpU6f03HPP5fp95JnrWnsXuZLbpY9vRDNGDTLxNWuZL9vV8nYpAAAAQKGUlJRk4uPjTVJSkrdLuWHUq1fPSDIxMTGmZ8+e5r777nO2BQUFmXXr1rn0X7hwoQkJCTGSTLVq1UzXrl1N3759zZ133mmKFCliJJmffvrJZZ8JEyYYy7KMJNOoUSPTr18/0759e1O6dGkTERHh0nf79u2mRIkSRpK55ZZbTL9+/UyzZs2MJOPn52f++9//uvQ/ePCgkWRatmzp9v2NHj3aSDLTpk3L9D58fX2NJHP77bebfv36mZiYGOPv728efvhhI8mMHj0629fxvffeM5KMj4+PadWqlenbt6+Jjo42ERERZsCAAW5rOHHihGnQoIGRZIoWLWpatWpl+vfvbzp37mwiIyONJPPUU0+57CPJREVFZVnHqlWrjCTz4IMPut1+/vx5589Pkhk6dKjbfrt373b+PGNiYpw/Y8uyzLPPPuu2jqSkJFO6dGnnz2Pw4MFm6NChZv369caYa/+s7Ha7iYqKctbWtm3bLN/nnj17TOXKlY0kU7p0adOuXTvTv39/c9dddzlrmDRpUpb7/9nLL7/s/Pnddtttpnfv3qZXr16mQYMGxtfX1+U+/e6774yvr68pU6aM+f33312O83//939Gkmnfvr2x2+3O9s2bNxvLskxQUJDp1q2bGTp0qBk6dKg5derUddVgjDEPPvigkWRWrVrl9j25+xmlpKQ4P0/FihUzPXv2NF26dDHBwcGmS5cuplKlSsZdzHU9n/vevXsbSaZIkSLmnnvuMT169DBFixY1t912m2nSpImRZA4ePJjlz8QhJ7+3c5sVEfB5QWEM+D7+28MmvmYts6wNAR8AAACQFwj4Mlu8eLEZMmSIufXWW03RokVNSEiIqVGjhnnooYfMzz//7Hafffv2mSeeeMJUr17dBAUFmbCwMFOzZk3Tt29f89lnn5mUlJRM+3z77bfm3nvvNaVLlzb+/v6mXLlypm3btubDDz/M1Pfw4cPm4YcfNpGRkcbf39+ULFnSdO/e3WzcuDFT3+sN+Bw1tW7d2hQpUsSEh4ebtm3bmu+++85MmzYtxwGfMcYsWLDA3H777SY4ONgUK1bMdOvWzezateuqNSQlJZl3333X3HHHHSYiIsIEBASYyMhI07JlSzNhwgRz9OhRl/65DfiMMaZfv37OEG3lypVZ9tu1a5fp2rWrKV26tAkJCTENGjQwU6ZMuWodmzZtMu3btzcRERHOUNfxvq/1szLGmJdeeslZ29SpU7PsZ4wx586dM+PGjTO33XabCQ0NNUFBQSY6Otp06NDBxMXFmZMnT151/z9bs2aN6d27tylfvrzx9/c3JUqUMHXr1jV/+ctfzJo1a4wxGQFplSpVjCSzdOnSTMew2WymefPmRpKZOHGiy7ZZs2aZ2267zQQHBzvf458DruzU4HA9AZ8xxly8eNG89NJLplKlSiYgIMBER0ebl19+2aSkpDgDVndy+rlPS0szb7zxhqlRo4YJCAgw5cuXN0888YQ5d+6cadmy5Q0Z8FnGeHDpGGSLYxXdhIQEhYeHe7scj5j5+hNqOH2VfikntV+1y9vlAAAAAIVOcnKyDh48qMqVK1/3SqQAgPyTk9/buc2KmIMPHmH5BWb8l0U2AAAAAAAA8hUBHzzC8vWXJPkQ8AEAAAAAAOQrAj54hI9/xrLZBHwAAAAAAAD5i4APHuHrn/GILgEfAAAAAABA/iLgg0dYAYzgAwAAAAAA8AYCPniEDyP4AAAAAAAAvIKALx/FxcUpJiZGsbGx3i7F43z9M5Z7JuADAAAAAADIXwR8+WjYsGGKj4/Xpk2bvF2Kx/kFXDGCzxjvFgMAAAAAAHATIeCDR/gEBEuSfO2S7DbvFgMAAAAAAHATIeCDR/gFXPGIrj3du8UAAAAAAADcRAj44BF+gRkBny8BHwAAAAAAQL4i4INH+AWGSCLgAwAAAAAAyG8EfPAI/8sBn4+RjI2ADwAAAAAAIL8Q8MEj/IOC//gmLcV7hQAAAADATWbMmDGyLEvTp0/P0/NMnz5dlmVpzJgxeXoeADlHwAePCLi8yIYk2VKTvFgJAAAAAHjG6tWrZVmWBg0a5O1SAOCqCPjgEQGBfwR8acmXvFgJAAAAAADAzYWADx4REBDofJ3OCD4AAAAAAIB8Q8AHjwgMCJT98uv0FEbwAQAAAMg/R48e1V/+8hdVrVpVQUFBKl68uLp06aLvvvvO2Wfr1q0KDAxU8eLF9csvv2Q6xsMPPyzLsvToo49KkgYNGqTWrVtLkmbMmCHLspxff56DLjvnd7jysd8zZ87o8ccfV7ly5RQYGKjatWtr6tSpWb7PxYsXq2nTpgoJCVGJEiXUs2dP7dmz56rX5tKlSxo/frwaNGig0NBQhYaGqkmTJpoxY0aW+6xfv17t2rVTWFiYihYtqg4dOmjjxo1XPQ8A7/LzdgEoHIL8/GX3kXzsUlpKsrfLAQAAAHCT+P7779W5c2edPXtWNWvWVOfOnXXy5EktX75cy5Yt06xZs3Tfffepfv36+r//+z8999xzevDBB/X111/LsixJ0sKFC/Xhhx+qRo0amjhxoiSpefPm+u2337R8+XJVrVpVzZs3d56zfv36OT7/n507d05NmzbVhQsXdOedd+rUqVP69ttvNXToUNntdj300EMu/f/973/r8ccfl2VZuvPOO1WuXDlt2LBBjRs3VteuXd1em99//13t27fX9u3bVbZsWbVs2VLGGH333XcaNGiQNm/erH/+858u+yxZskT33nuv0tPT1bhxY1WpUkXbtm1TixYtmIsQuJEZ5LuEhAQjySQkJHi7FI9JTEo1W2rXMvE1a5mj6+d6uxwAAACg0ElKSjLx8fEmKSnJ7Xa73W5sFy8WmC+73Z7ra5KQkGDKlStnfH19zcyZM122bdq0yRQrVsyEhoaa33//3XmN2rRpYySZCRMmGGOM+fXXX03JkiWNv7+/2bRpk8sxVq1aZSSZBx980CPnv/KYkkzfvn1NcnKyc9uCBQuMJFOpUiWXYx06dMgEBQUZf39/s2zZMmd7amqqGTBggPN406ZNc9mvU6dORpJ56qmnXM7z22+/mUaNGhlJ5ssvv3S2JyYmmlKlShlJZurUqc52u91uXnjhBed5Ro8e7fZ6AHB1rd/bV8ptVsQIPniEn4+P7Jcf+E5NZQQfAAAAkN9MUpJ239bQ22VkW80tP8oKCcnVMaZOnarjx49rxIgRGjBggMu2Ro0a6dVXX9Xw4cM1c+ZMPfPMM7IsSzNmzFDdunU1cuRItW3bVi+++KJOnTqlcePGqVGjRnl6/iuFh4frvffeU2DgH/OZd+/eXbVr19bOnTt16NAhRUdHO8+TnJysBx54QB06dHD29/f31zvvvKMFCxbo0iXXqZK2bt2qL774QrGxsZo4caJ8fP6YoatMmTKaMmWKbrvtNk2ePFkdO3aUJM2dO1cnT55UixYtNHjwYGd/y7I0duxYzZo1y+3jzQC8jzn44BG+PpZsl++mdB7RBQAAAJAPVqxYIUnq0aOH2+133nmnJOmHH35wtlWsWFHvv/++UlNT1apVK61YsULNmzfXiy++mC/nd2jYsKFKlCiRqb1GjRqSpOPHjzvb1q5dK0nq27dvpv4lSpTQXXfdlWVt3bt3dwn3HBxz8l1Z29XO4+/vr169emVqB3BjYAQfPMLPx3KO4EtLSfFuMQAAAMBNyAoOVs0tP3q7jGyzgoNzfYxDhw5Jkpo1a3bVfqdOnXL5vnfv3rr33nu1YMEChYSE6JNPPpGvr2++nV/KCBrdCQsLkySlXPF31a+//ipJioqKcruPY6Sfu9pGjhypkSNHZllbcvIfAzSu5zwAbgwEfPAInytH8PGILgAAAJDvLMvK9SOvBY3dbpck9erVS0WKFMmyX61atVy+P378uHO02qVLlxQfH39d4dX1nl+S21F1nuSorXnz5qpatWqenguA9xHwwWOcAV8aI/gAAAAA5L2KFStq9+7devHFF9WwYfbmHzTGaPDgwTp16pT69eunzz77TEOGDNGOHTtUqlSpPD//9ShXrpx2796tw4cPKyYmJtP2w4cPu61NynhEd8SIEdk+T1bHu1o7AO9jDr58FBcXp5iYGMXGxnq7lDxhZw4+AAAAAPmoffv2kqQFCxZke59//vOfWr58uZo1a6ZPPvlEI0eO1IkTJzR06NBMfQMCAiRJ6enpHjv/9XDM5ffZZ59l2nbmzBnnfHu5re1q50lPT9e8efOyfSwA+YuALx8NGzZM8fHx2rRpk7dLyROOEXz2tFTvFgIAAADgpvDoo4+qdOnSevPNNzVlyhTnY6kO6enpWr58uXbu3ClJio+P1wsvvKCwsDDnvHuvvvqqGjdurM8//1z//ve/XfYvX768JGn37t0eOf/1Gjx4sAIDAzVr1ix9/fXXzva0tDQ988wzunjxYqZ9br/9drVv317r16/XsGHDlJiYmKnPtm3btGzZMuf3vXv3VokSJbR69WrNmDHD2W6M0ejRo3XkyJFcvQ8AeYeADx5jLt9NtjRG8AEAAADIe0WLFtWiRYsUERGhRx99VNHR0erUqZMGDBigtm3bqlSpUurYsaP27dun1NRUDRgwQMnJyXrvvfdUuXJlSZKfn59mzpypIkWKaMSIES5hXnR0tOrWravNmzercePGGjx4sB566CEtXrw4x+fPjcqVK+utt95SWlqaOnTooNatW6tfv36qUaOGFi1apAEDBrjdb+bMmWrQoIH+9a9/KSoqSq1bt9aAAQPUpUsXVapUSfXr13cJ+MLCwvTRRx/J19dXgwYNUpMmTdS/f3/Vrl1bEyZM0MMPP5yr9wEg7xDwwWNsPlbGf1lFFwAAAEA+adKkiXbs2KHnn39e4eHhWrNmjRYuXKjDhw+rZcuWmj59utq1a6dXXnlFW7duVe/evfXAAw+4HKN69eqaOHGiLl26pAEDBigtLc25bd68eerevbsOHDigjz/+WB999JG2bNmS4/Pn1rBhw7RgwQLFxsZq48aNWr58uerVq6cNGzaoWrVqbvcpXbq0vvvuO7377ruKiYnRTz/9pLlz52r79u2qUqWKJkyYoGeffdZln27dumnVqlVq3bq1du7cqaVLl6pcuXJas2aN7rjjjly/DwB5wzLGGG8XcbNJTExURESEEhISFB4e7u1yPGZpmxhV+dXo+JBmavP8h94uBwAAAChUkpOTdfDgQVWuXFlBQUHeLgcAcA05+b2d26yIEXzwGPvlEXwmlRF8AAAAAAAA+YWADx5j8834r2GRDQAAAAAAgHxDwAePSffNGMFnv2K+CgAAAAAAAOQtAj54jP1ywKdURvABAAAAAADkFwI+eIxjFV2Tlu7lSgAAAAAAAG4eBHzwGJtjBF86j+gCAAAAAADkFwI+eIzNN+N2YgQfAAAAAABA/iHgg8c4RvBZ6QR8AAAAQF4xxni7BABANuTn72sCPniMzdc34wWP6AIAAAAe5+fnJ0lKSUnxciUAgOxIS8vIR3wdeUkeIuCDx9j8Mm5YHx7RBQAAADzOz89PRYoU0ZkzZ2Sz2bxdDgDgKowxSkhIUGBgoPz9/fP8fH55fgbcNJwj+Pg/GwAAAECeKFmypI4ePaqDBw8qIiJCwcHB8vX1lWVZ3i4NAKCMYC8tLU0JCQm6cOGCKlSokC/nJeCDx9j8MhJpK52ADwAAAMgLISEhqly5sn7//XedPXtWp06d8nZJAAA3AgMDVaFCBYWHh+fL+Qj44DH2yyP4LJvdy5UAAAAAhVdAQIAqVqzoHCVit/P/vwHgRuLr65svj+VeiYAPHmO7POmvZWNVLwAAACCvWZalgIAAb5cBALgBsMgGPMbul/F/Lgj4AAAAAAAA8g8BHzzG+GeM4PMh4AMAAAAAAMg3BHzwGLtjkQ2mAAEAAAAAAMg3BHzwGLt/xiO6PjYjGUbxAQAAAAAA5AcCPniMuRzwWXZLsqV6uRoAAAAAAICbAwEfPMbuHyhJsmyS0lO8WwwAAAAAAMBNgoAPnnM54POxi4APAAAAAAAgnxDw5aO4uDjFxMQoNjbW26XkjYDLj+jaJKUne7cWAAAAAACAmwQBXz4aNmyY4uPjtWnTJm+Xkics/4xVdH1sFiP4AAAAAAAA8gkBHzzGcqyia5dkI+ADAAAAAADIDwR88Bjr8iO6PjyiCwAAAAAAkG8I+OAxPpdH8Pmyii4AAAAAAEC+IeCDx/gEXLmKLiP4AAAAAAAA8gMBHzzG93LA52u3ZFIJ+AAAAAAAAPIDAR88xjcwwPnaJF/0YiUAAAAAAAA3DwI+eIxjBJ8kmeRLXqwEAAAAAADg5kHAB4/xCwhyvjYpBHwAAAAAAAD5gYAPHhMcGKD0y3eUSU7ybjEAAAAAAAA3CQI+eEygn7/SfTNemxQCPgAAAAAAgPxAwAePCfYPIOADAAAAAADIZwR88JiwgOA/Aj4e0QUAAAAAAMgXBHzwmLDAEGfAZ09J9m4xAAAAAAAANwkCPnhMWGCQ0i4HfOk8ogsAAAAAAJAvCPjgMeFXjOCzJTGCDwAAAAAAID8Q8MFjgv2DnAFfOo/oAgAAAAAA5AsCPnhMkO8Vj+impni3GAAAAAAAgJsEAR88JsA3wDmCL5WADwAAAAAAIF8Q8MFj/Hz8ZPOxJEmpaQR8AAAAAAAA+YGADx5l880I+NLT0rxcCQAAAAAAwM2BgA8elU7ABwAAAAAAkK8I+OBR6b4Zt5QtPd3LlQAAAAAAANwcCPjgUX8EfDYvVwIAAAAAAHBzIOCDR6X7ZiyjS8AHAAAAAACQPwj44FE2v4xbyp5u93IlAAAAAAAANwcCPniUzddPkmS3EfABAAAAAADkBwI+eJTNzxHwGS9XAgAAAAAAcHMg4INHpfv6Z7ywGckQ8gEAAAAAAOQ1Aj54lM0vI+AzdkuypXm5GgAAAAAAgMKPgA8eZXcEfDZJ6cneLQYAAAAAAOAmQMAHj7L5BUq6PIIvPcXL1QAAAAAAABR+BHzwKOPvmIPPYgQfAAAAAABAPiDguw6bN2/WAw88oGrVqsmyLL3yyiveLumGYfwzRvBZdkm2VO8WAwAAAAAAcBMg4LsO69ev14YNG9S8eXNFRER4u5wbi39Axn9tki6e9GopAAAAAAAANwMCvuvw17/+VXv27NH06dNVtGhRb5dzQ7EcI/hslnR0o5erAQAAAAAAKPwI+K6Djw+XLUvOgE9S0jmvlgIAAAAAAHAzuKGTqh9//FGvv/66evTooYoVK8qyLFmWdc39kpKSNGrUKNWoUUNBQUEqX768hgwZomPHjuVD1Tc3yz9IkuRjt6S9K7xcDQAAAAAAQOHn5+0Crmbs2LFatGhRjvZJTk5WmzZttGHDBpUrV07dunXToUOHNG3aNC1ZskQbNmxQlSpV8qhiyC9jBJ+PTdKJnZIxUjZCWQAAAAAAAFyfGzrga9q0qerWravY2FjFxsYqOjpaKSkpV91n3Lhx2rBhg5o2baoVK1YoNDRUkjRx4kSNGDFCQ4YM0erVq539z507p99+++2qxwwJCVGlSpVy/X5uBkcv2CVJvumXG+zpkq+/9woCAAAAAAAo5G7ogO+FF17IUf/U1FS99957kqS4uDhnuCdJw4cP14wZM7RmzRr9+OOPatiwoSTpP//5jx5//PGrHrdly5YuoSCy1qRmBUmSr+1ygy2VgA8AAAAAACAP3dBz8OXU+vXrlZCQoKpVq6pBgwaZtvfq1UuS9PnnnzvbHnvsMRljrvpFuJd9UWWKS7oy4EvzXjEAAAAAAAA3gRt6BF9Obdu2TZJ02223ud3uaN++fXu+1SRJKSkpLo8WJyYm5uv581NQSBFJBHwAAAAAAAD5pVAFfEeOHJEkVaxY0e12R/vhw4dzdZ6TJ09qzZo1kqRLly7p559/1ty5c1WkSBHdfffdmfqPHz9er732Wq7OWVCEhIRJknyNJWOXLDsBHwAAAAAAQF4qVAHfhQsXJGUsiuFOkSIZo8vOnz+fq/P873//U+/evZ3fz5s3T/PmzVNUVJQOHTqUqf9LL72k4cOHO79PTExUZGRkrmq4UYUUCZW5/NrYLVm2VK/WAwAAAAAAUNgVqoAvv7Rq1UrGmGt3vCwwMFCBgYF5WNGNIyQkTBcvvzZ2Sbb0q3UHAAAAAABALhWqRTYcq+ZeunTJ7faLFzOip7CwsHyr6WYTEPjH6EljszJW0QUAAAAAAECeKVQBX6VKlSRJv/zyi9vtjvaoqKh8q+lmE+QXpNTL40LT7JbEHHwAAAAAAAB5qlAFfPXq1ZMkbdmyxe12R3vdunXzraabTYBvgAIuP5WbnOzLKroAAAAAAAB5rFAFfM2aNVNERIT279+vrVu3Zto+d+5cSVLXrl3zubKbR6DvH3MNJh4OJuADAAAAAADIY4Uq4AsICNBf/vIXSdKwYcOcc+5J0sSJE7V9+3a1bNlSDRs29Ep9cXFxiomJUWxsrFfOnx98fXydr+2+Yg4+AAAAAACAPGaZnCwHm8+WLl2qsWPHOr//4YcfZIzR7bff7mx79dVX1blzZ+f3ycnJatWqlTZu3Khy5crpzjvv1OHDh7Vx40aVKlVKGzZsUJUqVfL1ffxZYmKiIiIilJCQoPDwcK/Wkhcm9a+rjlvS5F/7oqqN/0iq3t7bJQEAAAAAANywcpsV+eVBTR5z8uRJbdy4MVP7lW0nT5502RYUFKRVq1Zp/Pjx+vTTT7Vw4UIVL15cgwYN0tixY1WxYsU8r/tml+6XMTA03W5Jae5XNAYAAAAAAIBn3NAj+Aqrwj6Cb/zghur+/SWZWkmKee11qV5fb5cEAAAAAABww8ptVlSo5uDDjcHmlzEPn81uSakXr9EbAAAAAAAAuUHAB4+z+2Y8+W2zS0pL8m4xAAAAAAAAhRwBHzzO5pcR8Bnm4AMAAAAAAMhzBHzwOLt/RsBn5xFdAAAAAACAPEfAl4/i4uIUExOj2NhYb5eSp4yvf8Z/bZaUdNbL1QAAAAAAABRuBHz5aNiwYYqPj9emTZu8XUqesvsHSLoc8G2ZIV087eWKAAAAAAAACi8CPnhcWkiQJMkn1cpo+Hq0F6sBAAAAAAAo3Aj44HGpocGSJJ/kywHf+eNerAYAAAAAAKBwI+CDx9lCwyRJfimXAz6L2wwAAAAAACCvkLzA4+xhEZIkvzRLdpsI+AAAAAAAAPIQyQs8zic0XPbLg/dsqT6S5evdggAAAAAAAAoxAj54XEhAEV3IWGdDthQfybK8WxAAAAAAAEAhRsCXj+Li4hQTE6PY2Fhvl5Kngv1DdD5jnY2MgM+W5t2CAAAAAAAACjECvnw0bNgwxcfHa9OmTd4uJU8V8QtRUkDGa7vNkuzp3i0IAAAAAACgECPgg8cV8Q9Rml/Ga2OzpGLRXq0HAAAAAACgMCPgg8cVCSiiNL+MefeMzZJsKV6uCAAAAAAAoPAi4IPHhQaEKO3ywrnGLinlglfrAQAAAAAAKMwI+OBxYQFFnI/o2m2WdOF37xYEAAAAAABQiBHwwePCrwj4jM2SLvzm3YIAAAAAAAAKMQI+eFyRgCLOR3TT7ZaUdM6r9QAAAAAAABRmBHzwuACfIOcIvlS7JdlSvVsQAAAAAABAIUbAB4+rWCxMqb4Zt1aazUdKZxVdAAAAAACAvELAl4/i4uIUExOj2NhYb5eSp0ID/ZTqnzGEL81uSfY0yW73clUAAAAAAACFEwFfPho2bJji4+O1adMmb5eS51KDAiRJ6WmXbzEbo/gAAAAAAADyAgEf8kRKoL8kyeYI+NKTvVgNAAAAAABA4UXAhzyRGhgoSTJpVkZDOgttAAAAAAAA5AUCPuSJtKAgSZJPoq+MEY/oAgAAAAAA5BECPuSJoxWLS5L8LvkoPYmVdAEAAAAAAPIKAR/yRFqRMF3MeEpX9nRLunTauwUBAAAAAAAUUgR8yBMBVpCSMxbSlT3dRzq+zbsFAQAAAAAAFFIEfMgTgb7BSs5YSFcm3ZJSznu3IAAAAAAAgEKKgA95Isg3+IoRfJZkYxVdAAAAAACAvEDAhzwR7PengC892bsFAQAAAAAAFFIEfMgTIf4hSva3JDkCPkbwAQAAAAAA5AUCvnwUFxenmJgYxcbGeruUPBfiH6KUKxfZsKV4tyAAAAAAAIBCioAvHw0bNkzx8fHatGmTt0vJc0X8Q5yLbNjTLCmdgA8AAAAAACAvEPAhT4QGhPxpDj4CPgAAAAAAgLxAwIc8UTIk7I+Az8YiGwAAAAAAAHmFgA95om6F0n8sspHmwwg+AAAAAACAPELAhzwRFlBESYEZr+1plpT4q3cLAgAAAAAAKKQI+JAngv2CdfFywGdL85F+/5+UnurdogAAAAAAAAohAj7kiRD/EF26HPClp2U8qqsvnvVeQQAAAAAAAIUUAR/yRJBvkC4GZQR7qWmXb7MtM7xYEQAAAAAAQOFEwIc84evjK5+gSpKk9DRuMwAAAAAAgLxC8oK8U6SUJMkn1ZIxXq4FAAAAAACgkCLgQ55JDw6TJPnYLRmbJVVp7eWKAAAAAAAACh8CPuSZoxf9ZLu8voYtzZJCSni3IAAAAAAAgEKIgA95xtiDdTEo47UtxUeypXq3IAAAAAAAgEKIgC8fxcXFKSYmRrGxsd4uJV9YJlgnIzJep17wk2xp3i0IAAAAAACgECLgy0fDhg1TfHy8Nm3a5O1S8kWX2pV1rETGM7qpiX6M4AMAAAAAAMgDBHzIM61rROl8SMbrk9vDCfgAAAAAAADyAAEf8kyof6gqnPrje5NGwAcAAAAAAOBpBHzIM6EBofr+Fsv5vUlO8mI1AAAAAAAAhRMBH/JMmH+Y1tT5I+CzH9sp/TjdewUBAAAAAAAUQgR8yDOhAaGy+1hK88343tgs6fOnvFsUAAAAAABAIUPAhzxTxL+IZCyl+Gd8b7d5tx4AAAAAAIDCiIAPecbH8pGPgpTql/G9sVlX3wEAAAAAAAA5RsCHPJWeHugM+OwEfAAAAAAAAB5HwIc8ZWxXjOBLt6SilbxbEAAAAAAAQCFDwIc85atgXQrMeG1L85F8A7xbEAAAAAAAQCFDwIc8FVOmtC4FZTyaa0+zpLQkL1cEAAAAAABQuBDwIU+FB4bpomMEX6qPlHbJuwUBAAAAAAAUMgR8yFOh/qG6GJTx2pbqI6US8AEAAAAAAHgSAR/yVFhAmJIvT7tnbJZkS5HsNu8WBQAAAAAAUIgQ8CFPhfgHK+3yKrp2W8ZcfMzDBwAAAAAA4DkEfMhTexP+p1S/y4tsEPABAAAAAAB4HAFfPoqLi1NMTIxiY2O9XUq+6XdLH6X5Zry2mctD+dIueq8gAAAAAACAQoaALx8NGzZM8fHx2rRpk7dLyTd3VmzmfEQ33X456WMEHwAAAAAAgMcQ8CFP+fv4K/VywGdzBnyspAsAAAAAAOApBHzIUz6Wj9J8M+bes9mZgw8AAAAAAMDTCPiQ59L9Mkbu2W2Xb7dURvABAAAAAAB4CgEf8ly6f8Yzuvb0yw08ogsAAAAAAOAxBHzIc2eLZAR85oI9o4FHdAEAAAAAADyGgA957vjlgM8vxcieZknr35EunfFyVQAAAAAAAIUDAR/y3KUAf10IyniddslXOrlLWvCod4sCAAAAAAAoJAj4kOeKBgfrVHjG67SLGQtuaO8K7xUEAAAAAABQiBDwIc81j7xNJyMsSVKqI+ADAAAAAACARxDwIc/Flr1NJyMyXqcT8AEAAAAAAHgUAR/yXIWw8leM4PPzcjUAAAAAAACFCwEf8lx4QLhzBF/apcsj+HwI+gAAAAAAADyBgA95roh/EZ0KzxjBl+4I+AKKeLEiAAAAAACAwoOAD3kuNCBUyQEZr+3pGUGfSt3ivYIAAAAAAAAKEQI+5LmwgDClXn4i157mI2MkGbtXawIAAAAAACgsCPiQ5wJ9A5Xm98ece+f2h0jpSV6sCAAAAAAAoPAg4EP+8A9xvvx9W7iUcsGLxQAAAAAAABQeBHzIF1bAHwGfPd2SUhK9WA0AAAAAAEDhkauA79KlSzpy5IguXrzo0n727Fm9+OKL6tKli5544gnt378/V0Wi4PO/ctVcY0nJicqYjA8AAAAAAAC54XftLlkbO3as3nzzTf3www9q2LChJCklJUVNmjTRvn37ZC4HOHPnztW2bdtUrly53FeMAinIN9S1wZ4mpadI/kHeKQgAAAAAAKCQyNUIvpUrV6pq1arOcE+SZs6cqb1796p169Zavny5nnzySZ06dUqTJk3KdbEouIoHuAl3eUwXAAAAAAAg13IV8B05ckTVq1d3aVu8eLEsy9K0adPUvn17vf3226pRo4a+/PLLXBWKgq10YLQ+bZlxu/kVsWc0ppz3YkUAAAAAAACFQ64CvrNnz6po0aLO740xWrdunerWravIyEhne7169XT06NHcnAoFXK0SNbSxpiVJsqVm/FfJCV6sCAAAAAAAoHDIVcBXtmxZHTx40Pn9jz/+qLNnz6ply5Yu/SzLys1pUAi0q1FN54MzXps0S8YmHtEFAAAAAADwgFwFfPXr19cPP/yghQsX6vz58xo7dqwsy1KXLl1c+u3du1fly5fPVaEo2IL9gnUxSLr8cK5sqT7S8W1erQkAAAAAAKAwyFXA9/zzz0uSevbsqaJFi+rzzz9XvXr11KZNG2efEydOaNu2bS4Lcdys4uLiFBMTo9jYWG+Xku+C/YJlfCxdvLxori3VR/pqlPTTLO8WBgAAAAAAUMDlKuC74447tGDBAjVv3ly1atXSwIEDtXjxYvn4/HHY2bNnKywsTB07dsx1sQXdsGHDFB8fr02bNnm7lHwX7J/xfK7jMd30lMv3yLdveqkiAAAAAACAwsEyxhhvF3GzSUxMVEREhBISEhQeHu7tcvKFMUb1Pq6n1z5JU61fpAp3nFF4pWSpRDXprz96uzwAAAAAAACvyW1WlKsRfEB2WZalQN8gJRTJWHDFOYLPx8+LVQEAAAAAABR8uQr4Tpw4oW+//VYnTpxwad+/f7/69u2r2rVrq1OnTvr+++9zVSQKh/CAcJ0LyXhtS/bNeEHABwAAAAAAkCu5Cvhef/11tW7dWgkJCc62xMRENW/eXHPmzFF8fLyWLVumdu3aae/evbkuFgVbzeI1/hjBl3z51rPbvFgRAAAAAABAwZergG/16tWKiYlRjRo1nG3Tp0/XiRMn1K9fP+3evVsTJ05UUlKS3nrrrVwXi4KtRHAJJRTJeO0M+NIueq8gAAAAAACAQiBXAd+xY8dUpUoVl7alS5fKz89Pb7/9tqpXr66nn35a9erV05o1a3JVKAq+QN9AZ8DnfETXbvdeQQAAAAAAAIVArgK+8+fPKyQkxPm9zWbT999/r4YNG6pkyZLO9lq1aumXX37JzalQCARduciGYwTf+eNerAgAAAAAAKDgy1XAV758ef3888/O79etW6cLFy6oVatWLv3S09MVEBCQm1OhEAj0C3QuspGacvl+MDbJGO8VBQAAAAAAUMDlKuBr2rSptm/frrfffls7duzQK6+8Isuy1LVrV5d+u3btUoUKFXJVKAq+jBF8Ga+tdLvsaRmj+WRL815RAAAAAAAABVyuAr6XXnpJgYGBGjFihOrXr6/169erVatWuuOOO5x9Dh06pPj4eN1+++25LhYFW6BvoJIDpBS/Pz2mm57kxaoAAAAAAAAKNr/c7Hzrrbdq3bp1euedd3Tq1Ck1bNhQzz33nEuf5cuXq169eurevXtuToVCIMgvSLIsnQuxVCbRKD3FRwFhNiktSQqK8HZ5AAAAAAAABVKuAj5Juu222zRjxowstz/66KN69NFHc3saFALBfsGSpIRQu8okOlbSTZPSLnm3MAAAAAAAgAIsV4/oAjlRPKi4JDlX0k0zl0ftXTjprZIAAAAAAAAKvFyP4JOkEydOaOrUqVq7dq2OHTsmSapQoYJatGihwYMHq0yZMp44DQq4ooFFJcm50EZ68uXbb+pd0iu/S36B3ikMAAAAAACgAMt1wDdv3jwNGTJEFy5ckDHG2b5jxw4tX75cr7/+uj766CP17Nkzt6dCAVc6pLQkKSEk4/v0i7Y/Nl44IRWt5IWqAAAAAAAACrZcPaK7efNm9evXTxcvXtS9996rBQsW6KefftLWrVu1cOFC9ejRQxcuXFD//v21efNmT9WMAqpUSCmF+Yfp3OVHdF0WzzV27xQFAAAAAABQwOVqBN/48eNls9k0d+5c3XvvvS7b6tatq3vuuUcLFixQz5499frrr2vu3Lm5KhYF313Rd+nY9jmSpJQz1h8b0pK9VBEAAAAAAEDBlqsRfOvWrdMdd9yRKdy70r333qtmzZpp7dq1uTkVColQ/1DnIhvp5y2lp1wO+X7b7sWqAAAAAAAACq5cBXwJCQmqVOna86ZVqlRJCQkJuTkVColK4ZV06Io1V9IuXh5EOv9h7xQEAAAAAABQwOUq4Ctbtqx++umna/bbunWrypYtm5tToZCoFF5JSYGWjhf1lSQZm3WNPQAAAAAAAHA1uQr4OnTooN27d+vll1+WzWbLtN0Yo1deeUU///yzOnbsmJtToZAoFlhMkmTzyVhxOflcrhdyBgAAAAAAuKlZxhhzvTv/8ssvatCggc6cOaNKlSqpT58+io6OliQdPnxYc+bM0aFDh1SiRAlt2bJFFStW9FTdBVpiYqIiIiKUkJCg8PBwb5eTr05cPKF2c9vps/HpzrZb+v6a8WIMj3EDAAAAAICbT26zolwNn6pYsaJWrlypAQMGaOfOnZowYYIsK+ORS0duWKdOHc2aNYtwD5KkYkHFst5ojGTxyC4AAAAAAEBO5Pr5yDp16mj79u1avXq11q5dq19/zRiNVb58ed15551q1apVbk+BQiTAN0BKqSTpgLPNmevZ0iS/AK/VBgAAAAAAUBB5bAK0Vq1aZRnmTZ06Vb/88otGjRrlqdOhAKtQpJKOFzugcmczvrenWfINMJIthYAPAAAAAAAgh3K1yEZ2ffDBB3rttdfy41QoAFpWrazRA32d36ddvPw6PdVLFQEAAAAAABRc+RLwAVcqF1pa50ItHS4ZLElKT74c8NlSvFgVAAAAAABAwUTAh3xXvVh1SVJQWsaIvV+/v7zwRjoBHwAAAAAAQE4R8CHflQstJ0kqk2CTJNlSL9+GaZe8VRIAAAAAAECBRcCHfBfqH+p+Q9K5fK0DAAAAAACgMCDgQ75zBHzz7rBcNySd9UI1AAAAAAAABRsBH/JdsF+wfCwfralz+fYLuBz0EfABAAAAAADkWI4CPl9f3+v6+uGHH/Kqfq/47LPP1LlzZ5UrV04RERFq0aKF1q1b5+2yCgzLslTEr4hS/DO+N2km4wUBHwAAAAAAQI7lKOAzxlz3V2Hy9ttvq2TJkoqLi9OcOXNUoUIFtW3bVtu2bfN2aQXG+bTzSvXLeG0ZyZ4uAj4AAAAAAIDr4JeTzna7Pa/qKFA+//xzlShRwvl9u3btVKdOHcXFxWnKlClerKxgSQ744/WFX4MUfuF37xUDAAAAAABQQDEH33W4MtyTJB8fH9WuXVsHDx70UkUFk833j0U2Eg4HS1tnerEaAAAAAACAgumGDvh+/PFHvf766+rRo4cqVqwoy7JkWdY190tKStKoUaNUo0YNBQUFqXz58hoyZIiOHTuWJ3XabDZt2rRJ1apVy5PjF2ZnMhbU1YVjwRkv0pK9VwwAAAAAAEABlKNHdPPb2LFjtWjRohztk5ycrDZt2mjDhg0qV66cunXrpkOHDmnatGlasmSJNmzYoCpVqni0zvfee09HjhzRE0884dHjFmaRYZE6ev6ofqhhqeOWK+ZovHRKiqjovcIAAAAAAAAKmBs64GvatKnq1q2r2NhYxcbGKjo6WikpKVfdZ9y4cdqwYYOaNm2qFStWKDQ0Y4jYxIkTNWLECA0ZMkSrV6929j937px+++23qx4zJCRElSpVcrtt48aNevHFF/XKK6+oTp06OXuDN7Ep7afo7vl3a0ljH3XcYvtjw4UTBHwAAAAAAAA5YJkCtMRtUFCQUlJSslyVNzU1VaVLl1ZCQoK2bNmiBg0auGyvV6+etm/frs2bN6thw4aSpH//+996/PHHr3reli1buoSCDocOHVKTJk3UokUL/fe//83W48OSlJiYqIiICCUkJCg8PDxb+xRG939xv/Yf/kkfvZMR8NW671dZlqR735fq9fVucQAAAAAAAPkkt1nRDT0HX06tX79eCQkJqlq1aqZwT5J69eolKWMVXIfHHntMxpirfrkL986dO6fOnTsrOjpaM2bMyHa4hz8E+gYqxf+P7y/8GpjxYsGj3ikIAAAAAACgALqhH9HNqW3btkmSbrvtNrfbHe3bt2/P1XlSU1PVo0cPXbp0SStXrlRwcPBV+6ekpLg8WpyYmJir8xcWAb4BSrviDvxlbQmVa3xWRaskea8oAAAAAACAAqZQBXxHjhyRJFWs6H4ON0f74cOHc3WeJ554QmvWrNEHH3yggwcP6uDBg5KkwMBAtyMHx48fr9deey1X5yyMAnwDZP408vH4D8UUVjFZvl6qCQAAAAAAoKApVAHfhQsXJGUsiuFOkSJFJEnnz5/P1Xm+/vpr2e12DR061KU9KipKhw4dytT/pZde0vDhw53fJyYmKjIyMlc1FAYBvgFu2+1+EQR8AAAAAAAA2VSoAr784i7Eu5rAwEAFBgbmTTEFWKBvxjUZOLCVZs5c7Ww3wWW8VBEAAAAAAEDBU6gW2QgNDZUkXbp0ye32ixcvSpLCwsLyrSZkLcAnYwRfauQ6ra5Yz9luEn6XbOneKgsAAAAAAKBAKVQBX6VKlSRJv/zyy/+zd9/hUVRdAId/s7vpJKH3DtJ7EQFpgg0BEWzY4EOsYO8dFRUbYkEFpYmFXgRpgvTei/TeIRDSk63z/THJluxsSSEJeN7n4cnMvXdm7oaQsCfn3qPbn9VerVq1ApuT8M2muoJ43/a2khSqFStxpKVA/JHCmpYQQgghhBBCCCHEVeWaCvA1baplgW3dulW3P6u9SZMmBTYn4ZuCq8CGMfoAqZFapWHVAcwcBPsXgDlv+yUKIYQQQgghhBBCXOuuqQBf+/btiY2N5fDhw2zfvt2rf/r06QD07NmzgGcm9CjZKuhaQxwAqDYFzu6AP+6H6QMLY2pCCCGEEEIIIYQQV41rKsAXGhrKkCFDABg8eLBzzz2AESNGsHPnTjp16kTLli0LZX6jRo2iQYMGtG7dulCeX9TEhsZ6nFeN0z4mHHGrgnxwcQHOSAghhBBCCCGEEOLqo6iqqhb2JHz566+/+PDDD53nGzduRFVV2rRp42x75513uOOOO5znGRkZdO7cmQ0bNlChQgU6dOjA8ePH2bBhA2XKlGH9+vXUrFmzQF9HdklJScTGxpKYmEhMTEyhzqUwJWQk0GFKB+f51E9ce/LVv/+Ma+DQxIKclhBCCCGEEEIIIUSBymusqEhn8MXFxbFhwwbnn6xYpHtbXFycxzXh4eEsW7aMd955h8jISGbPns3x48cZMGAAW7duLfTgnnApHl6ckZ1HOs/ntNGW7Joi7IU0IyGEEEIIIYQQQoirT5HO4LtWSQafi81ho/mk5gBUOh/KV+PSMIY6qNPnnGuQZPAJIYQQQgghhBDiGnZNZ/CJa5/JYGJyj8kAJIWFAWC3KFolXSGEEEIIIYQQQggRkAT4RKELM2iBvRTn0lyF+ANRhTchIYQQQgghhBBCiKuIBPhEoQszagE+R2i6s+3C9lhk8bgQQgghhBBCCCFEYBLgK0CjRo2iQYMGtG7durCnUqQUDy8OgKJ4RvRUu1IIsxFCCCGEEEIIIYS4ukiArwANHjyYPXv2sGnTpsKeSpESHRrtPL7oOsRuyQzwzRkCdmsBz0oIIYQQQgghhBDi6iABPlGkpIW5jlPOhGsH2ybBzimFMyEhhBBCCCGEEEKIIk4CfKJImdLJ9SWZeDzC1ZFyoRBmI4QQQgghhBBCCFH0SYBPFCmb6hg4FVUagNBom6tDdRTSjIQQQgghhBBCCCGKNgnwiSJnXr36ADisbl+eUlJXCCGEEEIIIYQQQpcE+ESR0KFSB+extcIaAJJPRrB3ckVSzoZJBp8QQgghhBBCCCGEDxLgE0XCZx0/cx4nR3r2nVxRSgJ8QgghhBBCCCGEED5IgE8UCcVCizmPD1ZUvAekXYR988Fu8+4TQgghhBBCCCGE+A+TAF8BGjVqFA0aNKB169aFPZUiqUxEGQBSI3QCfJt+hsn9YP2oAp6VEEIIIYQQQgghRNEmAb4CNHjwYPbs2cOmTZsKeypFksVh8dlnTc/8Ut01rYBmI4QQQgghhBBCCHF1kACfKDJUt0q5D71sZG+ZMs5zc0KIdqAYC3paQgghhBBCCCGEEEWaBPhEkfF5p88JM4YBYAlReG/QZVJKRABgN2d+qRokwCeEEEIIIYQQQgjhTgJ8oshoV7Ed6x9Y79FmjTQBYLdkfqme3gI2c0FPTQghhBBCCCGEEKLIkgCfKFJMBpPH+amISADOb40lPT5zme7y4QU9LSGEEEIIIYQQQogiSwJ8okg7XPqC8/jE8lLawenNhTQbIYQQQgghhBBCiKJHAnyiyHmuxXPO4xC7q91hMRC/PwpsvqvtCiGEEEIIIYQQQvzXSIBPFDkDGw10HofaPPvOb4sFW0YBz0gIIYQQQgghhBCi6JIAnyhyDIrry3JrLcV7gJqZ1uewe/cJIYQQQgghhBBC/MdIgK8AjRo1igYNGtC6devCnspVY0tthUU1G3i0qQ4V4o/AZzVg6QeFNDMhhBBCCCGEEEKIokECfAVo8ODB7Nmzh02bNhX2VK4eisLmGlEeTRl798Pc5yAjEVZ9WUgTE0IIIYQQQgghhCgaJMAniqSKURWdx1HFtnj02c0GOLqyoKckhBBCCCGEEEIIUSRJgE8USZ91+sx5HJKt0IbdqrMvnxBCCCGEEEIIIcR/lAT4RJHUtExT5/H6egpHS4U7z9MvhnoO3jKhgGYlhBBCCCGEEEIIUfRIgE8UWY1LNwbAHKrw2uM24otp7ZcPFiPxWAQ2c2Ym39zn4OLBQpqlEEIIIYQQQgghROGSAJ8osn6+5WdUe5jzfEEr15frmfUlOL60tGtwygXt49FVsP5HUNWCmqYQQgghhBBCCCFEoZIAnyiyIkMisSa5lurOu95z7z1LUojrJO2S9nFiD1j4GhxZVhBTFEIIIYQQQgghhCh0EuATRZo1/kbnsd3oXVzDmag39WFIOOnqiD96hWcmhBBCCCGEEEIIUTRIgE8UaSuevw8TET770867Fdw4uNitR5boCiGEEEIIIYQQ4r9BAnyiSKtSMpIv20/w2X9ieWlXFl/cfleH7MEnhBBCCCGEEEKI/wgJ8Iki7/oqdbCl1vTZn3I6XDvYOLqAZiSEEEIIIYQQQghRdEiArwCNGjWKBg0a0Lp168KeylWlWJiJcKUUAH+1VjCbPPtTz4XpXCWEEEIIIYQQQgjx3yABvgI0ePBg9uzZw6ZNmwp7Kledt2+6E4CJXQ387wWjR59i0lmOK0t0hRBCCCGEEEII8R8hAT5xVWhVoZl2oCjYTAqrb6jo7IvfV8w7nme3FNjchBBCCCGEEEIIIQqTBPjEVaFaTDUalWrkPP+hs40S773vPI/bFe15QUZCAc1MCCGEEEIIIYQQonBJgE9cNe6uc7fz2KrE03lnivP80p5o0uJCXYPjj4AlrSCnJ4QQQgghhBBCCFEoJMAnrlr22GMsrdzCeW5Ocqu+sXsGfFwBVn9VCDMTQgghhBBCCCGEKDgS4BNXjaoxVT3OFVMiiWHFnOcGg05hjSVDr/CshBBCCCGEEEIIIQqXBPjEVaNVuVZ0rtzFeW4IO0+6Kcx57rAr+hee2XalpyaEEEIIIYQQQghRaCTAJ64aiqLwVZcR2NMrARAS8y9LWyU7+89tLs7lw5HeF47pDA5HAc1SCCGEEEIIIYQQomBJgE9cVUwGE2nHn3Sep1TdzNZarsy9c5uK619oToKVX8C53Vd4hkIIIYQQQgghhBAFSwJ84uqjhnicJkQFcc3y4fDPh/Bj+yszJyGEEEIIIYQQQohCIgE+cdVLDfc8t2XofFlv+KFgJiOEEEIIIYQQQghRwCTAJ646I+5tSonEl5znf7YxcCna1X9wdnksycZCmJkQQgghhBBCCCFEwZMAn7jq9GlRmZXPDmBo26EAJBZTeGqIyWPM4b/Koao+bmC3XdkJCiGEEEIIIYQQQhQgCfAVoFGjRtGgQQNat25d2FO5JnSp2sVvvzXVRxbfgle0IN/JjWC3XoGZCSGEEEIIIYQQQhQcCfAVoMGDB7Nnzx42bdpU2FO5JpQML0mJkMo++1OTK+p3bB4HcwbD2Jth4RtXaHZCCCGEEEIIIYQQBUMCfOKqVj2qkc++cyvsXD4Uqd+5c7L2cdNPkBYPG0ZrH4UQQgghhBBCCCGuMhLgE1e1Dzq+ji25PgCnSnn3n9tcnISjEWQkmLw7s/xxPyx4VbL5hBBCCCGEEEIIcVVSVNVnKQJxhSQlJREbG0tiYiIxMTGFPZ2r3tnEdG785gfKlR5HndMqBge8MtPhNa7+/Wf83yi8OLx+/MpMUgghhBBCCCGEEMKHvMaKJINPXPUqxEZgT61NcqTClusMbKpr4JumfXN+o4wEmPoIOOz5PkchhBBCCCGEEEKIK0UCfOIa4VkxN77sydzdZs8cOLQ0H+YjhBBCCCGEEEIIUTAkwCeuCY/eWMPjXC27xWvMiRUlg7uZLT0/piSEEEIIIYQQQghRICTAJ64Jr99ej9TDLzjPQ2zeW0umng0n8XhE4Jsp8s9CCCGEEEIIIYQQVw+JZIhrQojRwIKn73Genyqt6I47s65E4JtJgE8IIYQQQgghhBBXEYlkiGtG3fLR1I5qB0B8jMK54rm8keK5nx9ntkPyubxMTQghhBBCCCGEEOKKkQCfuKY8Vu8NrInNAFjTQD+L78nkZwFQvVfxauIPuzov7IUxneDLuvk8UyGEEEIIIYQQQoj8IQE+cU3pVq8qjUKeorztXma1MzD3eoW3HvbMyLtn+TIGHxvMzplVST4d5n2TRW/Cqi+14xPrCmDWQgghhBBCCCGEELknAT5xTQk1GZj+VDv+fvQdLCEKk7oaOVjZM5OvYuolhqyfRajVxqlVpfRv9M+HmQf6WYBCCCGEEEIIIYQQRYUE+MQ1y2GLdh6vre87UJcW3prLhyL1l+wqbtf981E+zk4IIYQQQgghhBAif0iAT1yzLBducR6PvNPAy48adccdn3Cac5uLk3wy3LMj5QIeGXwrP7sCsxRCCCGEEEIIIYTIGwnwiWuWNbE16Scf1k4UhdM+VuNmyUgI8WxYPdIzg08IIYQQQgghhBCiCJIAn7im2VIakrx/KLaUOtiNCmvrhPoc6xXLs2XgtQefw57vcxRCCCGEEEIIIYTICwnwFaBRo0bRoEEDWrduXdhT+U/o07ySduAIJ+PMfQCM7OtgZA+dyrkASrZN+DaPhT+HeLbZrfk8SyGEEEIIIYQQQoi8kQBfARo8eDB79uxh06ZNhT2V/4ThfZsw6MYaAKj2KOzmMgBsqmdjWWPvpbdBrca1W+DEepjQA87t9uy7eBD2zc/rtIUQQgghhBBCCCFyRFFV3dqh4gpKSkoiNjaWxMREYmJiCns617x0i53EdCtdJjyDqfh6Z/vUT2we4wxtK1K3+lZQHb5vVrUtnFinHRcrBy8fcPUNjdU+/m8hVGvrf1LJ5+D4WqjfC4ymnLwcIYQQQgghhBBCXGPyGiuSDD5xzYsINVI+Npwnb2zqd5xj3Rky+vwDN3/oe1BWcA8g5bzrOOmM6/jsdu2jNQMmPwibxnrf58cbYfr/YP33gV+AEEIIIYQQQgghhB8S4BP/GcZsX+3T2ytYjHDKrbru0XseIONMUvA3PbYaUi/CmM6uNsWofdz+G+ybB3+96H1dapz28cCi4J8lhBBCCCGEEEIIoUPWBor/DBXP1ehTOxqZ0V6lbAJ8PcZVHffoO79R4roYyrcMItA34Q7vNkNmgM8cxPVBbfwnhBBCCCGEEEII4Ztk8In/DPftJi2XtUrGdqPC2VIKn7Tu5zH28sFiTEi6OW8PzMrkE0IIIYQQQgghhLiCJMAn/jM6V+kMgMMag/mCZ+ZdZNk5XuPnXr6Bhy2vc33GqJw9yJoO8Uch/bJ+/7+zXceSwSeEEEIIIYQQQog8kgCf+M9oUqYJUXGvknrkeXCEePRtrWMmvpjn+GHrxlJr1ymKJ6SQo1rTl4/CN81g9QhXmyOzMm/qJZjW323wNRbgO7AIFr4JdlvgsUIIIYQQQgghhMgXEuAT/ykmeyVwRAJGyofVc7anhys8PdjI+rqeAbf7Dv7D18u/JvV8WPAPObLcu82WoX00J3q2X2sZfL/fC+tHwfZfC3smQgghhBBCCCHEf4YE+MR/yoh7mxERYuTD3o14ufE32FJqO/scBoWRvQ28NsB777zkE+HBPyQt3rstK8DnlbF3jQX4siSeLuwZCCGEEEIIIYQQ/xlSRVf8p1xfoyS7ht6CyWhAVVXsC6tgKnbI2e8wKJws470eN+FIFGHFrViSQihxXSphsX6WoKbrBPgST0JkSe/2iwch9SJElc7NyxFCCCGEEEIIIYSQDD7x32Myal/2iqLQqcz9mC92xp5e2dlvMyle+/EBnN9anMuHojiwtALW8NKccJQJ/qGjO8LeeeCwe7YnnYLPa8HJjTDvRTi2RmvP0aZ/RdHVPn8hhBBCCCGEEOLqIQE+8Z/21X3X81PPd7mr3GeEnH/G2T7sfiNf9zIwsWkbr2tMFjub+qxlraNhzh426wlwWPX7xt4Mm8fChO4w+2n4tgVYUnN2fyGEEEIIIYQQQvwnSYBP/KcVCzPRqU4ZPrqrMVtffZyH6j8EwKkyCmsaGkiqskn3ukujRhHzdxJ/JHfm59rfBvcwgxHsPgJ87rb/BvFHYM+fwb6Moueqz0AUQgghhBBCCCGuHhLgE8KNmm1p6YHK+kUwai+aRo3Ec5zYU4Zhu0txLrJOEDdXwWYOfjL2HIwVQgghhBBCCCHEf5YE+IRw079Bf4/z8yUUXhlo5OO7Q3XH9zq6hk9W/8D5YxZUR4Cbm5Ngcr/gJxNMtp8QQgghhBBCCCH+8yTAJ4SbCsUqeLUdL6ewvbadFY30s/maXTxM6D8Wjq/0vtZLalzwk5EAnxBCCCGEEEIIIYIgAT4hsvmzt87ed4rCqJ5G9lX27sqSfk5hQOQoLFEVodmDeZ+I3eLddmYbxB3I+72vONmDTwghhBBCCCGEKCgS4BMimxqxNTzOVbtree7qBv7/yVw4mMDS6RXY5ujF9502UydjIqlqWO4mkj2DLy0exnSGUa2liIUQQgghhBBCCCGcJMAnhB8tyrYg/dQA5/nfzRU+ucfAb531/+l8uWoU1RPPEv7mC0zbchoLIXxgeyR3D182DIbGgjlZO3df3pvVVlRJAFIIIYQQQgghhCgwEuATQsdPt/xE+4rt+ejGj/hn8ED61RoMgGpQ2FbbwF+tFdbX1d+TL8vxOC0IV47LeZvMP8O0j4rR1ZZ4Ehz2vN1XCCGEEEIIIYQQ1wQJ8Amh44YKN/DjzT9SOboy1UpF8eaNTxKW0drZbzMpjOhjJCPE9z1GL/mMp3fM5NjJMnmbzJnt2kf3Pfl+aAcTeuTtvjllTYf4o0EOlgw+IYQQQgghhBCioEiAT4ggLXrwB26t5Fk849HnjT5GQ+XUi/Q8upanNs7htFrKs7NGx+AfbE3TPmYvunFired5RiKs/xGSzwV/75z4sQN80wxOb7ky9xdCCCGEEEIIIUSuSIBPiCCVKhbGF91eZ3iH4c42q0mh36tG5l7vf7nusFMPMH9bS3Zbq/F58bfh3knBP/jcTu3jys/9j5v3Aix8DX7p7WrbPB6m9gebTkXenLp0UPu4e2bgsbIHnxBCCCGEEEIIUWAkwCdEDt1R8w6W37vceW43Kkzq6juTD+DZNTOosf8syzc0ZtS5Bnyx8jzNMkbn7MH75gXon699jNvrapv3POyZDTv+yNmzhBBCCCGEEEIIcdWQAF8BGjVqFA0aNKB169aBB4sirVREKT7v6JlR9/oAI3YFVjdQ+LqXgVU1vffe63pqK40uHua7ZYdIIJq3rAMBOB1Zj701/+f7gVN9VOL9uhlsGA0LXgdbuu/rMxK822wWmPUk7Jjs+zo9kp0nhBBCCCGEEEIUKYqqyrv1gpaUlERsbCyJiYnExMQU9nREHhxJOMKdc+702X/9tjK8vPCsV3uqKZwnur7CpYhY7m0YxdR/U+llWMM3oaPyZ2L954Fqh18y59ZtKNz4Am/M3IVBgY/uagybxsJfL2r9QxMD33NorPbxhqfhtk/8j2n/PNz8fl5egRBCCCGEEEII8Z+R11iRZPAJkQeVoiv57d/a+IJue5Qtg7sOrQDgvDUSABv+l/nmyMQeruAewJaJXEox88fGE/y24QSJaVZIOpO7ewf1OwH5vYEQQgghhBBCCFFQJMAnRB6EGcNY0GcBoWfewpZa06vfZlLYXVW/AEevo2uokHKRMwnplExPpHhC8pWb6OWj2B2uoJtDVb2r8jo7HdpHq48lv6ojnycnhBBCCCGEEEKIvDAV9gSEuNpVjq7M6pfuId1yF2fNh7h/3v0e/Z/3NRByoRVfztpAbJqrPcRhZ9yS4R5jzd2NhMXYyVBDCFes+TpP95w6e/YAX0YihMfCrKfg6Aq49WOY1t+5tNfnnVIuwOynofE9UKKa2xDJ4BNCCCGEEEIIIQqKZPAJkQ8iQo2ULBZGw1INaV62uUdferhCUtUtvNlfW4KbEu77PmnnwwCYY2+f73O0mtNpqeznJdNUbBYz2DJcncOrQtJZ2PE7JJ3WgnsAS4Z638g9eLfwDTj0N8x6HMbdmu9zFkIIIYQQQgghRGAS4BMin31707e67XHFFe59w8TA533vtXfqRCkuq8WYkHaLbhJcQoX2qMWr5mpekVvHMCPsfZ4xzSZ8y2itiq67EfWCvJPbxJJO6w85uTFXcxRCCCGEEEIIIUTOSYBPiHwWGxbrf4CiMKOd/r58pjg756bEMGL+d+ybUpHUC6Ee/Xsavco3jWbmal4hF/e6PWdP7vfSO7YGtkzUju0+lhGfXA8ZSbm7vxBCCCGEEEIIIXJEAnxCXAHTe0732z+lU3AVc8/8WwF7WHHnebyxNGNWHnYNCI2G658I6l4Ot5RA1VdgTo8lFU5vdZ3H7YW5z2pZeg6b7+vMV7BoiBBCCCGEEEIIIZwkwCfEFVC3ZF129d/ld8zRcoHvYztvZX/Zd9mytiYvJT5OaopKlUsnucv8Ps9lPM3FUu+QXulBeONUwHupDlfGXvThuZCREHgCAL/0hp+6eLdfPuY/wGc3B3d/IYQQQgghhBBC5ImiqlLusqAlJSURGxtLYmIiMTExhT0dcQWN2TmGE0knmHN4jlefyaYSYYHyl6HdHgd3bA7+n+IznZ+n7uUTDNmhLdetsXMXe4a1oYXhkM9rrOGlCcm4mPMX4UvfsTDjUd/9T62Dcg20ohyK/pJkIYQQQgghhBBC5D1WZLoCcxJCZHq8yeMA7L+8n33x+zz6bCaFZBMkR8LBSkZmt1X56Vs7AIsbxXLL7kSf9x254huMbnvoXU630tcylGrKeZaHvaR7Tb4G9wDO7vDfb0uHVSNg/Q/w6GIoWSN/n59LqqqiSMBRCCGEEEIIIcQ1RJboClEAvur8FX2u68M7N7zjc0xiMYV+rxp58GUjP/dM9Xs/Y7YCGUfjUlExcEytwPZei3nM8iLT7R3zZe4+rf3Gf781A5a+D6kXYMl7V3YuQfpk/l5u/HQZCWmWwIOFEEIIIYQQQoirhAT4hCgAlaMr836797m37r3s6r+L3pVe0R1nNypYQ7Tssn2VtLZz0eEB7//Azxucx9/uNLA19TqqrDxDwpGIvE8+t2zpbsduAbX9C+CrRlo13gI2euURTiek88u64wX+bCGEEEIIIYQQ4kqRAJ8QheCtTg8GHPN1byMjeht47mkr54r7H1s2Ld55vHzPWX5d9CEx59M4u7FEHmeaB9YM17HDrWrvH/dD4kmY0B3i9hf8vNC2BRRCCCGEEEIIIa4VsgefEIUgPMTIun7reHftu/x9/G/dMZdiFC7FaNl8Lw0yYnTALyPsumO/XPkdqqJQJj2R0Y165WguE2y38Ke9HU1iUqiTuoUHTMty9mJ8ca/Sa88M8GUkeY4ZdT0M9b3XYI5s/wP2zYM+P0FoZP7cUwghhBBCCCGEuApIBp8QhaRYaDFGdB7Bzkd2MvaWsX7HWkMUMsIUttXULw5ROiOJMulaoOyJ3X969Dls/gtKnFdLslWtw99Ke4opGX7H5siFvW6TyAxM/nZP/t0/u9lPagG+jWOu3DOEEEIIIYQQQogiSAJ8QhQyRVG4vsL1zLlzjrPti05fMLqbd6Dqm14GxtxmYF09hWMxpUkzhQW8vzXN/z/zyMyg3umEdMbY7sjh7P1Y953r2GEDcwqcXO89zuK/oEiOZfjPCAzDQsn0o/n7TCGEEEIIIYQQohBJgE+IIqJm8ZpM7TGV1fev5tbqt1KreE2vMakRCkuaG/jqLiOvDk4gJcwY8L6rVjSgZdL3DLNq+/5tcNTz6I/ClbW3W63JMnvTPL4SHeZkmHSXfl/qRf12VYWdU+H8vzl7ljHEb/dvoR/z8JZ7YP/CnN1XCCGEEEIIIYQoomQPPiGKkPql6juPy0SWCTg+3JEWcEyl1Ev8On8YAF9U78PpYqUZXvl+JivDCI2xsdVxncf4U2rg5+bYBT9BOpvZ8/zSYSheDQ7/AzMf09qGJsL0gZBwEgYuBEO2wOa++a5jg/8AXyvDAe1g6y9Q97YgX4AQQgghhBBCCFF0SQafEEWUQTHQrWo3v2NG9TDgACZ0NbCvcuB73nFsPY/vnsfHC8dwZEFZft5zK3852jj7Y83JRB9JwWH1v28frR4F4G7zu8yw3+jZV71D4Im4c6+wu2oEfNsCPiwFJze42lUVds+AUxvhzHbvMriT+7mOjUH+3sJhy9k8hRBCCCGEEEKIIkoCfEIUYU81e8pv/7baBvq/ZGT+9QZG3hl4uW52N+7aher2bWDo+nHU3XqC89tifF7Tz/IW9BgBQxPZrNbjJevTngNaD8rRHE5fSuLp37awf/8eWPq+q2PVF67jlZ+7jpe+D1/Wg6Sz+jf0lcGXftnz/GoN8Kmqlt2YciH4a5LOwqEl3oHR/JAWD3Ofh5Ob8v/eQgghhBBCCCGCIgE+IYqwWrG1aFG2BTdXu9nZFhPqGXwzh2rZdvExClszq+zGF4P3Hgwu4Pf+up+dx/UunwQg8Vik17h/7M2Yab+RdY6GXn3xMa6lxYREBPXcLB/P24n133nU/aOt70HLPnIdH10BKedgzUj9sXp78O2eCZ9W5znjDFdbVoDv7E44uMT3s9d8Dd+29B1QLGh752r7GX7TPPhrRjaCX/vC/vmBx+bUwjdgy3gY6z/bVAghhBBCCCHElSMBPiGKMKPByMTbJzKi8whnW5MyTXyOH36vgX6vGnnyGRN7qyrsqRL4Gdef38eC2S/T4sJ+Z5vqULjf8jaX1GgAbKqBgdZXeTEzW2/36UQcDlc22B9V32elvTH3mt/h2KWcVcV9MeUrfgodEXhgdtn37nPSWV4893kAXghxD/DZtY+jO8BvfeHiIVffjimw/Q/t+O934dIhz6rAhenwUu2jJSX4a7KCmYf/yf/5XNwfeIwQQgghhBBCiCtKAnxCXCV+6/4bPWv25P127/O/hv/TH6Qo2I2uANeXfYyMvdVI/xeMPP20kSPlfN//o7U/eZyvdzTg8dQX2HK4Fg+lvOHR1+Pb1Yxbc9R5vvBsFI9Y32CjWp8Rczfn6HXVMuQyM061g90GKXGe7ZvHaR9tZvh3tpbxZk70vtx97z+Ay8e0j+YUmPU4zH4SMpJc/Q4bLP0ATm7M3XzziylnGZIeVEf+zcMpwH6NQgghhBBCCCGuOKmiK8RVokmZJs7svRdbvciLrV4kyZLEn4f+5NNNn+pekxypsKiFdpweDq8PNDH1k+D2nhu/+CPKp2n71t1ebh3r23ouzf166UHnscEtxvOvWj3IV5RHW3+BIysg4bhne9xeSDwFG0bD2m98Xq6c3KAF87JkFeewuGUguh9vnQTWVFj1pVbVN4vDAbunQ9UboHjVPLygIIWE5/7arKzF/KRIgE8IIYQQQgghCpsE+IS4isWExvBQg4foV68fRoORSylmBs35lEO2aT6vef8BA123KdQ57aBsku+iC1nBPdCW8WZnsbmywXaccgW8DquVuNP8AefVEpRUkjHiYG7Y2zl9acHJHtzLknQWtv8W+PrFb7mOs4pz2NJdbenxrmOrj6XHW8bDXy9CdAV4yfvzBGhZf/FHoel9gecUiHsGX/I5iC4f/LWSwSeEEEIIIYQQ1yRZoivENcBo0ApqlCoWxqwH3+XjGz+mS5UuumP/rWbgm94KX92Vs3/+L2ydwvXn9mDK3M/NbPMdLNqh1uYcpdijVmeXWjNHz9nqqJ2j8brGdgObJfC4LRNcxxO6gzkZLGmutl/7Br7H7pnax2Q/S43H3qwt+z29RTsPZm6+uGfwZS1HDtaVqKIrGXxCCCGEEEIIUegkwCfENahnrZ4MaT7E75jDFXMWmLnlxCbeXz+Oz1d9n5ep+WW942u+td2VPzezJOf4kosrx8CRZa4Gf0G7LHZfxT4yuQfVLh/TqvZ+XAH+Gaa1JZ7S9vYLtkpviFuFY1MOl+tKBp8QQgghhBBCXJMkwCfENapOiTo82/xZPmz/oc8xZ0p4nq+tHzhYU+/yCTqf3Mq3y77ingP/OANYJoctTxlij9tfJbXB/ax2NGaHoyaXageRPZfPZq4/AIveDDxw13Tt47KP4dQmV7tdZ39D94ChMRT+fkcr2LHyc61tUh9tX79pA4KbpDHEdRxaLLhrslyJAJ9k8AkhhBBCCCFEoZMAnxDXsMeaPEbv2r19D3CLzfx8i4GxtwT3LeG1Lb9TO/E0A/fMZ8Zfb9P8wgF+WTSMtzdO1B2fomqZZvsdlXX7X7U+xjpjK9JtKlZM3GkZxpEbvwhqLvnpcfvk4AbOeBT+fAZWZCtucnaH99j5r7iOHToBwIv7tY8n1wf3bPdCGb72BfQlq1KwEEIIIYQQQohrigT4hPgPWH7vcubfNZ/6Jet7tI/vpn0LmNVWYXFLA8mROc/GirSZ+XjtGEqYU2h/drduFl9fy1Bm2dvzuPVF+lne4lPr/c6+abaOTLV3IcxkIM3iCl6ZrTnPNttOnRxfk2tbf/FuG3uzd1vl1q5jS6qfLMcgP/fuWXju+wUGI9ggYo5IBp8QQgghhBBCFDYJ8AnxH1AqohRVYqrQv2F/AOqWqEvv2r3ZUcvAgBeM/NHZ6Bw7+jbPbwtJJXK2DLSEWdv7LtaczJ2HV1HMksZ+tSovWAdzXC3POkdDfrD34lbzcH6w9eRD28MAhIcYSUq3Ou9jttnpahjL69ZBQT97uDogR3PNd2pmgNKRGYSzpEJ6gqvf7GdfQFOYd5vNDL/eDWu+drV5ZPDlMMAnhBBCCCGEEOKaZCrsCQghCk73Gt1pU6ENpSNKY3fYqV28Nl9s9lwKu7S5gYOVFHpucJAeCmHd29Nl2KKgn1Ej8SyXw2OYvOB9AFpc2M97bb2DdPvVqnxqq+o8P3U5nbu+X+s8f3TiZiACq9Iw6GcnWhTmmW6gh/FKZKoF6e93Yfvv8MRKmHAHxB9x9S18HUrXdZ0nnnYd6wX4dk6BQ39rf9o/p7WpbgE+9+ChL44cZkImnoZp/eH6J6DJPYHHyx58QgghhBBCCFHoJINPiP8QRVEoHVEaAKPB6Mzoy+5EWYVRPY2Mu9XID/alfHSfgZcGGXnmCaPueHcfrfuJtzdMcJ5ff34fpYMJRPlgU30/c4DlFY/zA2plXrI+metn5Ys1X0NqHKz/wTO4lyVrzz2AxW+7jvUq4qbFe7e5Z/DtnQs7p3qPSTqjZQ8COFxZkR4VeC8dhmOrva9d+LpWOGRmsJmTEuATQgghhBBCiMImAb5cmDhxIq1ataJ48eJERUXRokULJk8OcnN+IYqYTzp8EnDMjpoGTpZROF9S4dkggnztz+72OJ+0aBj14o/z3LaphNnMutcY3QNXbqx4Pq9Rxs9sd9TkM+t9nFDLOdvvMb+LHSNmQr3uMdx6P9/Yegecd74yBJEgfWS56zjlvHd/+mWPU1VVcbhX6jUnwszH4PA/rrZ/Z8OI+vBxRS0AaHcL8Dns8Pt9MGMQfNtCyzC8sM/vMwOSDD4hhBBCCCGEKHSyRDcXLl++TO/evWnWrBnh4eHMnj2bfv36ER4eTu/evQt7ekLkSI+aPWhcujE9ZvUIavy5kgpPDDEy+jv9gJwvX638FoDbjm/k9t5f8FHbUhg/H8asWh2pknKBB/ct5rUbn+JAiaoe16luv4f4wPowKUTS2zIMgCjSnX0pRPh89nm1BPPtbUhUixGjpPGcaWaO5p4rq0cEHpOeLUPv8jGw2+DPIRBTEXbPcPXZbawf+QBVU3ZQKft9zu6AWjdpxzMfc7XPfBxe3ON2DzMcWOh57Yl1EFECosuROxLgE0IIIYQQQojCJgG+XHj++ec9zrt168b27dv57bffJMAnrkrVYqrRs2ZP5h6ZG9T4y9GuoI7NAKYcbvM2YsW31J99HICmFw872wfvmMlznZ/3GJuEa1npRPstHn2pRLDW3oDSSiIHVa+wFwBxaiwLHNdjJpSx9u7cH7IyZ5MtSJY0WPaRFnTL7t+ZtE32sReie5ae3ZKtz4Zf857X/rxxCsKiA05x6uaTmG0OHr6hmtYgGXxCCCGEEEIIUegkwJdPSpUqhdVqDTxQiCLq4w4f83GHj1l7ei0Lji3gcsZlVpxa4XP8mNsMdN/k4JN7jYz6IWfZfPUvH9dtV3WywSyE0Drje+wYsOO9PPgB61soqB6Zfu5am3/wOE8LKQk5DEgWmKTTvivj+ls6mz2o584R5Pel+CNQoanfIRlWO69O3wlAj8YVKBHlvRxaCCGEEEIIIUTBK9J78G3ZsoXhw4fTp08fKleujKIoKEFki6Snp/Puu+9Sp04dwsPDqVixIgMHDuT06dMBr80Jm81GUlISU6ZM4e+//+aJJ57I1/sLURjaVWrHh+0/ZGSXkQxtO5T5febr7tO3pLmBFx83EVdc4bse+fOtxG7Qv08cxYknxsdVildwr6/5PXY4atLHPNRr9K6I6xlp68Mgy0vOttV2z0q9vcwfOo8PV+kb3OQDqdAs8JjFb4PRR9DsqJ/Mw5Wfa8t7133v3WcPMsCntwdi6iVtmXDmvokpZlc2oNWeGSUtQhl8B88n8+LU7Ry7mFrYUxFCCCGEEEKIAlWkM/g+/PBD5syZk6NrMjIyuOmmm1i/fj0VKlTgzjvv5NixY4wfP5558+axfv16atasmee5nTt3jgoVKgBgNBr5/vvvuf322/N8XyGKCpPBRN86WnArzBjmbL+9xu0sOLrAY+zKxgZ21lB4ep6DZkdVZ3t8MSieBoYgM+YaxB+nx5E1zKvRjnqXj/Ps9umMadSL7WXrYHDYeXfDBI7GVmRiA///1raodbkzc5++7GIiQhh56W6PtvWOBvxsv4Nqynm2OWqzU63l7NsV3opazMh+GwC2OmrTwnAouBd323AYf5v/MXH7tD969s3zf+3Xetl3CjgCLNHNkjUuI9HV9sV1oNqh02vQ5U3SLa4goEOlyOn7w1qSMmzsPp3I4hc6FfZ0hBBCCCGEEKLAFOkAX9u2bWnSpAmtW7emdevWVK9eHbNZvwJnlmHDhrF+/Xratm3L4sWLKVasGAAjRozgpZdeYuDAgSxfvtw5PiEhgXPnzvm9Z2RkJFWrem78X7p0aTZt2kRycjILFy5kyJAhlCpVir598ynbR4gipGxkWab0mEKIIYTrSlxHhagKjNs9zmNMQjGF1HDP637sbuCWrSqtDmnRILsCxgCBocE7Z3G63lo+XqlVlf1k7Ri63/kZ15/fS5vMP3oBvjsqh3Js+z7+LV2TyskX6Ld/Cb/X7cbp6LIe4+qUi+bIxVSSM2z0Nn9AN+MWfrLfoVt9F+Bkmu+qwTH4WE6rxxQGgzeCJVWrsDupN6RdCv763LCmwuyngxvrsMG+v+DcTlebmhnQ+3cWdHlTP4PP3W/3QqdXoXKrPEw695IytPkdOJ9SKM8XQgghhBBCiMJSpAN8r732Wo7GWywWvvvuOwBGjRrlDO4BvPjii0ycOJEVK1awZcsWWrZsCcDkyZN56qmn/N63U6dOHkFBAJPJRKtW2pvYLl26EB8fzxtvvCEBPnHNalCqgfO4W9VujNs9jnKR5ehcpTNT9k8B4EAlhfZ7tQhe/xeMpIcrnC6lUjzVzrzrDaxtYMBkU/lsnJ3KfmJbH08973H+0N7FxId7FoBQVAdR4aHOoNPTY17BYDHzevsnGLJ9BpVTL9Lw0hEG3Pq2x3XhIUba1CjFkr3n2a7WZruttu4cfrV1pZZylo008jnPaCWNRDWSWCWIQJ+iQJm6rvPIUq4AX/kmnoG1/HRyfXDjHDaYM1i/L0QrdJLqFuCzOVP43JboHlyk/RnqlgVYCEKNRXr3CSGEEEIIIYTId0U6wJdTa9asITExkVq1atG8eXOv/rvvvpudO3cyd+5cZ4DvySef5Mknn8zzs5s1a8b48ePzfB8hrgaNyzRmVq9ZlI8qj121U6t4LXbE7WC1cQU3l2jA6LANpIdrgZ+44gpvDnB9q7GZFF4daMRugNs3qwxYGnj97gMHlnic9z24nEH/Zi5ZjYzCNGUWttladm/zCwepnHoRgHLpCV73cqiqfvZZNm/bHtUODicQ1/V9yqx5z2vMfkcVnrI+z1umX3nAtMzv/c6kKpw8conmVUsQajJoe+ZlCfO1v2ABsmb4LuSRFeBzW6Jry/ocnt3uPd5uhZQLEJutsnHqJYgoAT72WvQp8RRMfhDaPAnN+gUcHmIsOvsCCiGEEEIIIURBuKbSHHbs2AFAixYtdPuz2nfuzP9MmbVr11K9enXdPrPZTFJSkscfIa52tUvUplhoMWLDYulXrx/DOwxn9UPruPnDsdzc8xm/19pMCqpB4UgFVyDmk3sMnIouHtSzncE9gLRUyu53/Zv2VagjS2xCHHV2rwU1+E3kblhaE0u1jiymLRbVyEFHJX633cRr1sdJJcJjzz4a36N7j3ZjT3HfmPV8OG9P5kTdKt9GFA96LlfM7/rzBkDVgnkZVleAz2pX4dxuzz37skz/H3zVAEY2hhWfaW2nNsPnNeG7VjDqBtg9M/i5LXpLCyTODu6XMWEhvpdV63JkBitPb4WfboLja3N2vRBCCCGEEEIUsmsqg+/EiRMAVK5cWbc/q/348eN5ek6XLl3o27cv9erVIyMjgzlz5vD7778zZswY3fGffPIJ77//fp6eKcTVpH2l9ny3/TuPtkhTJGk2z6Ws+yrDb50NWI2wrbaB8m2S+J9nsl5QzIddhS4e2O95A4PqINKagdkYgtUYwm0ThhFy/iznmvRmbs0bg7q/HSN19mvBpTAsmAnBfWnqDHtHmimHuKVXP0Lq3sLbW8pgQOWr0B8AuN/ytnP8pPXH+bB3tmW/d42GT7Jlu+VFhWb6mXW55dAq8ZptrsxHm8MBp9bpj987V/uYcAKWfQQ3vgA/d9Xa4g9rH6f/Dxr1Ce75ekFEP3K0RHfpB7B5HDyxUtsXMSMRxt9e6MuMhRBCCCGEECInrqkAX0qKtrF6ZGSkbn9UVBQAycnJeXpO06ZN+fbbbzl58iRRUVE0aNCAuXPn0qNHD93xb7zxBi+++KLzPCkpiSpVquRpDkIUZY1KN2LOnXPYEbeDd9e+C8Dy+5Zz/W/Xew5UFOa0dQXKFrZUKJauEB+jsLaewphv7YQFUQT20g8/+uz7a86rAJyOKs3jXV8h5PxZAHocWUuzCwdZVakpy6voZ/3q0SvGYcXE67bH+XCekdSZ6wAtcDgro0NwNw0rBl3fg6U5+EVA9y9g/ssAJNa5m9gD0119j/4Nw8oEf69AzNr3TItbgM9qV8EQZKbcwcVBDdt3LokSkaGUi8lWrcW9EvDRlVCjo9/7RIXlIINv1ZfaxxWf5jiQKIQQQgghhBBFxTUV4CsoI0eOZOTIkUGPDwsLIyws7MpNSIgiqGbxmtSIrUG6LZ06JeoQYYqgQakG7Lm0x+c1qkFhWkdXcObhV0x02OXgmXmB98wLpFLqRaomX3CeV025QNWUC7Q792+OAnz+uO9RF5BidFWpBSjuWambt85BSIRW2XbyA97Xu42fvieVR7O+mzd7CEz6FYF59zJ8UCLw3O4c5Vlw4+IBsFkw27Lmq0LyGe01BCPhZMAhJ+PTuG3kKgCODb/D1bHyczi2ynU+sadudp3NbV/FUlG5+H4b/IptIYQQQgghhChyrqk9+LKq5qal6Ve0TE1NBSA6Olq3XwiRvxRF4YH6D9CqvFZxevyt47m3zr0AtC7fmoGNBga8x6rG+fdt6odlX+q2L5j9Mp+t+p7yqZdAVXl90yTe2jgxR/v05Vjzh7SPlbSCP4RGObv6mIey5Uw6qWYbKw77yCqr3oGtpXryqfV+klRX1nL6aT97jBoMMHBR4LnF6CwXXjIUs1ULon1sGkvL6e1gz+zA9wI4tSngkF2nfbzOf4YFvr/dSvKev4kgA4CYCBMcWwNndT4X5hT9e6h5DyKLq0xGEvz9Hlw8FHisEEIIIYQQRdw1lcFXtaqW0XLq1Cnd/qz2atWqFdichBAukSGRvNP2Hd5p+w4Aqqqy7cI2tl3Y5ve63zsZeGDFlQ3ANL50hPF/f8LmsnVpdWE/AMWs6aSERoKqMmDPAs5FlWRh9Rvy/Ky1hy/S7rZPoGIzqHsHhy4kc2B/Mt0z+0+pZXj8ly3c0rAcJzefoJNeQp4pjD6ntYqyg4x/OZtX1HyJ2/w9vOoNEF0Bks/6HhOuU9V3/ShaVb7McNMZ7jct19oO/+PvSS67p+u328ww4Q4o1wiqv+ZstjtUjIYcVMJd/gklVn3JNyEtecz6EhHWBJhwt9b35lkIzQyAHl0FE3vAjS9Ct+xVkQs4hS/5PBQrC4pU/C00wzO3ylgzUvZcFEIIIYQQV71rKoOvadOmAGzdulW3P6u9SZMmBTYnIYRviqIw8baJbHxwI83KNHO2R4d6ZtmubOQZBBl7s4F3HzIy54b8D45kBfcAGpfQfgdS7/IJ7jv4D89tn06ZtMvO/uF9GgcMRJWL8V4u+sBPG7SMvVYDIboc3UasZNw61zLWdMK4lGph0b/nsbt9m37GMsR5PGb1MefxCbWs8zixVPPAL3LIJtQ6vsOARy5boVIrr/Ymp353Bffyw/E1WnbflvEoqoObDFtZGPoaU+ZmBizt1uDus24UADcbtwAQZb3k6jvnlsW38A3t4+oR3vc4uTGns8+9/Qvhyzrw55DAY8WVcSWzc4UQQgghhCgE11SAr3379sTGxnL48GG2b9/u1T99upZF0rNnzwKemRDCF0VRiDBFMKn7JLY8tIX5feYzved0qkS7CtFYs+UaJxSDfVUUFjf3/S3s36pwoU7eCk28v+J7vupUjtL2dGfbWxt/cR47VNj7wW0MurGGz3uEmfT3qTsS57lU1OH27Tg9s5BHVJiR86prz7x9qmvfvY/n73MeL3G05FdbV4ZZHyTEx/PoN8V5uP60hef31vM556d+26Zl111pWyY6D6OTDzIu9AvqGU7SfmtmUaK9fwZ3H7e5tlL2YbKluvrS4rVgzqElcH6X73tkVfctCMsylx1v+zVn11nTA4/5rzqxAb5pDgeCK+gSdPBYCCGEEEKIq8Q1FeALDQ1lyBAtI2Lw4MHOPfcARowYwc6dO+nUqRMtW7YslPmNGjWKBg0a0Lp160J5vhBFXagxlCrRVahYrCLz+8znofoP0aVKFyxuAb4zJWFjXS1rLi7W8/rVDVzZdONuNjKk72XufSP3OxHYT52i3gsPUz4qxNlWN7NgRKWUOIqv/psQAzzRqRYVY8N17zG8b2Pd9pu+XEGLD/+m+utattpptbSzz5a5e8LJ+HSOqBV5zfoYAy0vc1CtzJvWRxlkecnjXg4MvG17lJ/tdxBiNDB9yynMhgjXgAHzoa6WsZdmsXH/mPUk2H0XoohTi2M26+9lmq/c9vArd3qp87g4yZB+GWY+7vtah10L6ljTcV9eOz3sA06cPe8aN7mftgz41775OPE8cg8uLRkKe+cFvmb9D/BRea3oivD2290QfwR+vye48bYMz3PJ6BNCCCGEEFe5Ir0H319//cWHH37oPLdYLADccINrD6x33nmHO+5wVVx8++23WbJkCWvXruW6666jQ4cOHD9+nA0bNlCmTBnGjRtXcC8gm8GDBzN48GCSkpKIjY0NfIEQ/3GvXa/ty9bkRCNn2+jbjahZ+5YpCg+/ZOSudQ4OVlTYcp2BqR1USqTAybKuYN+i5gq3btPewP/VWuGOTTl7M//o4h88zrsd38RL27SMuEPzfqPMs8+y5vU7uenpn6mQeom1FbWgXs+mFWlXyxW4K2ZJw+SwkxCuLUGOT7U4+85Tktci32ffZe8lv1PsXZzHv9u7+p2r0aDw8rQdfK18zMgb7bTsdj+ERWOxObjnx7VcSNay3dJUtwBfywGwZYLzNJ4Y4i4nUrkAt4cz2bIVv5j2P3DYdMeeTkin0p6fYfHbUKGZV38xsmW6HV+TT7PMg5VfwPl/oe9Yz+zI1V9pHwPtAbfwde3jjMfgrTNXZo55YbeBsRD/S2FJDTzGnd3iee6wgTFEf6wQQgghhBBXgSKdwRcXF8eGDRucf9TM37C7t8XFxXlcEx4ezrJly3jnnXeIjIxk9uzZHD9+nAEDBrB161Zq1qxZGC9FCJEH3Wv1cB6fy1yx+lD9hxh/63jMoQqTOxnZcp327excSYW9VT0jUxNudn2ri8qWuJMbWcE9ANvZs5x94w0UReH7ZSN4Z+NE6sYfB6BDbVdwD1Vl2vx3+WPh+4T7WP66IK0+O9TaeZqbzaF9nzypluMfUwdOpmpLdlceiGPHqUTOJmqfgDTcAnwt+nvdJ4yCXcIYnu7KulMAjq/1OTY5w6oF9wDObvfqL6YU0lJWuxUOLoFLh2HNN5B60dX3z4fw70w4ssw7uJSdqsLGn/T3BTQUwd/LrRoBH1eA0/r73xYIJYf/ncmewVcQS9KFEEIIIYS4gop0gG/AgAGoqur3z4ABA7yui4iI4IMPPuDQoUOYzWbOnj3L+PHjqVy5csG/CCFEnr17w7uc/P4lSv/yE5ejXcG7VuVbsav/LrY8tIU5veew7N5lFAsp5nW93ahwpBzYDDDnBs9ve1M6GJjZLu+panvr1Xce10jSKtQWj9CCMaN71+Htja795sq6Fepwl5Shn7GWEz8sd+0lN2rZYTp8tozEdKtXjdiTboU5MmKqe91nou0W7eC6WyGqrFd/fqtw0rX0NEZJA7vvgEta/Dm/94rOnsHny8El/vtVFRw5qN68bRL81he+bQF/vwNTtcBphtXuGmNN0w8mXdjrOt47F+a/DGNv9h5nLoLVXpe+rwUt579SeHPIcYDP7P9cCCGEEEKIq0wRTAUQQghPkSGR3HLTIO1kr3d/qDGUmrFadu66B9ax5fwWBiwcQJcqXVh2chkAb/U3EmqD9DCFx58xcsM+lbqnVP5so2ANMaBip+/a/NmHq3bCKb5Y+R1VZh/j4vPP0/TiRaqe3e3sf/TfebzXdlC+PCu7vWeTvNrOJ2VgylbtN4koupo/x0wIF4Zv4EC21Yk/2HuxyVGPyfc8hZJ6kTMjO1NRidd95r3md6hmOM/dxlW0Mej8BeWzFlP972Pa1RBEJlniKS0YpycqszjLr321cU+tCW755u6ZnufHV+NwqDR5fzEHsn7aTnlI/9rvb9CW6TrsMPVh/89JOqstSS2dt2xPVBU2jIaDi+C+3yA0Mo/3y0EwNL/lNcDnJ6AshBBCCCHE1aBIZ/AJIYQvqldOmkvLci3Z1X8X39z0DW+30ZZy2o0K6WFakCuhmMLCVga+7m3EGqK1Tb/RwOSOrm+Ja+vnPqvvjmPraRh/DIC4kSNJnDvXo//68/t4ZfPvtDvjp6qrm1hzCnccWUOURT8zLcSocGvDcj6vV9D25svusFqJU2pZLHYHz1meBuBZy2AA7BjZoNbn6an7oEQ1djpq6d7bqhrZqNZnmr0zA9Shfl/H45YXWGxvyWFHBQ47Kvgd66HlgODHAu2MewIPSo3z3RdZWgt+HV4KF/fDmW3BPTiypFdTRsJpmtiDmE+WtEuBx4yoB9+1hIsHvftyUixi6iOw8DU4/A9szof9ac9shfijMLGnVrW4IOV5iW4+rN0XQgghhBCiEEmATwhxVVKDDGTcV+8+Vt+/mr/u+ouV961k+8PbaVOhjdc4u1FhZnvXt8QFrfS/PY66w8DKhjkL/jkSvZdV3nRqK89un+bRVufyCXoeWe0VpHl740SG7JzFm5sm6d5fVSH0wlmaxekEfICbv1rJy9N2+J3jHMeN1MmYyJ+O9h7tC3afo/rrfzHV3kn3OgOurC29IGKW8bZbWexoxePWl+hq+ZIp9s5+5+OhdrfgxwbL4qdKcNxeSLngOvf3taaqfvsjv2nI9LAPgptT0hmwpAQel+XYKs/znVPhizr6e/e5O7ICTm2BvX+62szemZ+5MvtpOLqy4KsWG4w5G599H0RbgH0RfUiKO8UjP69j6qaTubpeCCGEEEKI/CJLdAvQqFGjGDVqFHa7PfBgIYQuk8GEzWGjXcV2QV8TGxZLbJircvVPN//EtgvbGLBwgFcm4DNPGqkQr7K/ssKUDgbuW+VgSgcDm+ooXIqG1AiFlY1V9lVReXxh3pYkxlrSCLHbiDWnUCfhJO9k7tN3IaIEGyo0BCDEbqXRpaMAtIg7QK/Dq/izVgeP+4x5pCUVencG4IWOz7CvZDWvZ2VVz/XHgu9lqP84mnOreTitDfsZFjLe2W5UXJ+/NIsN99odWd60PupV/dfoJwPTS0wlPio7grcuvBj8NYGkXfTfr7eUNi0e5j4LzR6CurcBkDGuF+mJcRQbssLPZy9II+oHHuOLzQwzH9OOZzwKz/vIDk06C7/08m7PSeafP8mFVOFXyWHGbfYluhm52Nvw8DJiJvWml70jLx96kntbV8n5PYQQQgghhMgnksFXgAYPHsyePXvYtGlTYU9FiKvWor6LGN1tNB0rd8z1PRRFoUW5Fvxz7z+M6jqK0hGuarfnSyhsr6V9a5zRXtuvb8aNBk6UVUiN0IIIqqKwpHn+fPscv/odJi0e5gzuAbQ7u5t+daIBuPfAPx7jn9o1ByXkEmClmCWN57tdx031XMtzG188zJWhsF+tyu/2rrxsfUJ3hMNHjCh7cA88M/8COWaqyU8nyvOYJR8DfFMf8d9/yj0LTgVzMsx8XCuA8cd9WrPdRvjJlZRI2svCBbPyb27uFr0Ff7/rf4zDoS2zdT8HLSCZXaLvTLNZ206xfP8Fn/1BKYx9+OzWnAfoHNkK2iwblvPnrvoSgLuNKwEVm70Q9yAUQgghhBD/eRLgE0JcVcpGlqVdpXYoOc3Y0VE6ojQdK3dkyd1LeLiBVtjg5mo306pcK22AopBQzPdzJnc0YDHCm48YufcNE/1ezeEyQaDUZatX2y0nNvHIZ09QzJLGQ/v/9uovVvtzXjs3kmnz36Xn58+i2lzBitIh+ZSJ5YMDA9N9LNd1N8F2C2Nsd9DR/JVufyJRgR9mCocX9zHgF20PPHPec+Ryx26FT6vDIdffxanLaWBz7YkYf2w37JmT/89e9x2s+dp3v8MOYzrBH/e72hQDLHkfPqvhXfgj+9LUTJfTLLwwZQcDxufxF1AJJ/J2fW5s/Cnn12QPRGZWvs6REFdRkg6GXSTnQxXsXNv+u1a12Sp7CQohhBBC/FdJgE8I8Z9nNBh5tfWrbH5oM192+pIvO3/p0V8jtgYju4xk1X2rGHfrOG6qchMAM9spDHwtgkOVtCCg3eg/6HiqFAy/O/hvu9Pm62dutdvjoPP68wBYjh8neYmroMGAlhW4vVH5oJ9xpZxVS/Gx7UFOqPrFP6YFESTkqbUQU4Fjl7T98jY56ubnFINnTffK+Lrx02WkpaU6z/tf/ragZ6WJPwLndnq2WdNg9QjteMFrnn0+AnwpGa52h69UTIDze+Df2bmY6BWQeArmPgc7J+fsupMbYcsEz7bcVBA2uHY5KUkSVkchZvDNfgr2zIYt4wMOFUIIIYQQ1yYJ8AkhRKYwYxiKolAyvCSbH9rMzkd2Mv+u+UzrOY2uVbtSPLw4rcu35uubvmblfSt5odWLLLx7Eb1qufY0Sw7Xv/eh8vDaQCNbrzPw1OCcZ/q5e36OZyDBkeoKNKkZ6Xx1X7M83T8Yr1q1/d6etDzv0b7BUQ+Aufa2fq83E8pTlufYmC1ot95R33mfi2GVPfrS8fHJzYOE4o08G8o39hqzeNsBr7ZhprGYd8/1ai9YipbBl51bEM/mcJCY5pYlavfOGAXP5dUWH0tNp246CT+0hWn9r0y2Yk5NH6gF6s76LyDjZezNsG+eZ1tOq/ACxFZyHsYTg9V+ZbNng5JyvrBnIIQQQgghCokU2RBCCB1hRq1aRJUY/Y3zS4SXYGCjgQAMbTeUVuVacSnjEg0W3sboue+SaDBTdcFOfu2kcinGM7MvPTR/53px8Xzn8e7zO4m4sD5P9/voLi3oZbY6+GDeHt0xU+1dmGNvjxnPF9PP8jaRZJBC4IyoBY42LLC04Vj4A862wZZnuce4ghn2jkxKMRNi8Ay8XFRjKK3kU8VX4FJsQ4on7HY1hBf3GnPL3re82h4yLYWlS/NtHrlm1ymeUrsr/KvtCZiQZqHHyJWsfzNzH8TsxSUyOdyyz8xWB2EmA0v3XqBhpRgqRIeBwcCrM3Zyb1aMNWsPw/cS8umF5MKZ7fl3Lx+BT7/cgoJGHFhtRWAPvux7CwqRV8fXwrHV0OGlnFerFkIIIUSBkgw+IYTIoxBDCHdddxeDGg+iWPnKvPTYOD549Dd+7B3mFdwDsOj8auWzvv6/HQ98zvcbK+uKtc7jMyf2MPrHx1FUB4rqINacgjHyEFXCZ3D70TWE2LUAwOu31/N5vwfbVOPBNtXoWKe0zzGAV3APtD36ggnu+XKJWH609yKO4lxOtfLLumMe/b0tH/KJtV+O7qkOcgvE/W8Bu8NbOE8PJYd4BGosoTG5mndO/RkgwzEoigKWNO92g2uvQgWVc0lu+7JZ073HAzX2/khng7bXYYbNzrydZxn0y2Z+/uJ1+LQanN6qP4fTW3I9/TwzBPE7SocdUuJ892f93fsIfPpldX3ujdix6mQ+nrqchuqvQvG++XBgcc6f7c79/oW5TFgUTZZUSPBdXCeg8bfDso9g+2/5NychhBBCXBES4BNCiCvErJddBdiMsLuaZ+BvWy2FWW319/C7GA0pkcEVFWl9UOWNaQ5m103m+7T1TF4wlHm//8i309fx7I5Z/Dn3de44upYq1iQMYadB0ZZzlosJo8N1pdnyVldOPDqIM6+95rMqbn77ydYdgN9sntV2X5+5kzOJngGpU2oZRtt7et0jQw1hmb2p7v07/JaE+tJ+eDceqrVjRnJDZ9+28zaPggtnMvJ/GbCeQEuYg2ZJ9W5z+7pTyPaXaNUZn2lC6OcA2C4eo+nCu+hhWMc7xglgToK5z+pflH45pzPWZclN9puvAJ97wGtiT/iiNpzZ5t0HYIrQPmYua76camH7yYTgnu8WLDXh8FraPHHtMW78dBkfz9+rf336ZZjcD36/J3cBxizu+ypKBp/I7pvmMLIRXMpjhfWLB/NnPkIIIYS4YiTAJ4QQV8jwDsM9zkuGlyQ6JBoUhQ/6GXhiiCsrz25UmNnOwOd9DPR71cigZ119l4tpHz+920BqWHDPNo3+iup/uyqohrm97x+yYyY1hzzAT4u/orXha4b3acyGN7sx6dE2FIs7S+qaNSTO+ZPqEdCkfCT99yyg8UXtzeFnfZvk8LMQ2Ke2++lrfo+htv4AdKtfFoDjl9L4Y2NwmSfhipX/WV8jwVgKgDg11tl36nI6H6+Mdy4vS8f1SUwlHDXSlal43hqRtxcTpHg1Ou83UVX9gJ3b/njZw8JHz1wIeNvYle9QNX0f34W6FQ45vweyBwshuMCUww7LPoGjK3W7V07/jp+H9mftQR+ZdjunwYQekHpRO1dVOLsTVJ39BwGSMyvibpkAx9dox5szi0/YslWZDckK8GlLdDt9vozeo9aw7vClwK/LLbBmwOG1B9+H8/YQipWfVh3Vv96c7DrOS4BPL8h7tdgxBWY9CTb94i8iH2Tty3j4n8KdhxBCCCGuOAnwFaBRo0bRoEEDWrduXdhTEUIUgDtq3sGGBzZ4tP19z9/8fffffNd1FCFly7Hujds5OuIp1vZby3UVGrOprgG7USE9OpRDmcVw57fWvlVvuc7AE88Y2VI7cDafITEl4JiKl+GDGedp/dEz7F0/n0OXD/Had67sOPuZ03w54z3uP7CUz1b/gEF1cG/rKkSHmTA47ET7CCyYDAqG4BIOAbBhYotaF2vmtrDFIwNvUvig5Q2m2jox294OgO9sdwLwadUf+Cj6bW4yf8nf9pY8axkC4BFkSVNdAb7LajT9ed95npSRi73YcuGsWirvN1HtAYM72TP4Nh44FfC2EWd09nBU7XwdMsq73UeWqocdk2HFcC2bzpLmFczpuPstnjb9yZ9TfnQ1psXDsTVaMG/mIDi2SlsmCFqF4NEdwOLja9yc2T73OfcXoH3IvkQ5JNLjdSRlaJHwZfsDB0LdC5yYdJbofm8awYHw/txjXK5/vXs2oa9gZTCOu5bo+6qSXGTNehx2/AE7fi/smVz78rx/XhEoIiOEEEIIvyTAV4AGDx7Mnj172LRpU2FPRQhRQCJDIrmpyk0APNLgEaJCoigfVZ5OVTqx9N6lDOw/gu7dnyU6NJr32r1HhagKPNX0KbY+vJU9b/bhg34G1jRwRcssIQorG+UgehYEy/4DMOAlrG178sR8V6Ah5fRxbHGurKr/HfgDu8POmEda8cma0Uyd/x6Vks/T6dQ2SmS4Cl8c+rg7hz7qTrkYLZAWbUmlzdl/MehVfNVxITlw0GiNozGv2p7gZeuT9DJ/yAjbPQBsiAvjpWdfJJlIHrO+xJ+Ods5r7JlrjjPc9g68oBZnZXwJTjjKAHBU0S+qkht7HVV99p2hNC9ZnszbAy4dgfXf+x2ioNLLsBZOaXvlhTr09+DLMsQ4C6NFv4jJnca13o2+Ms/ci5VcdstgG1FPWy6osy9dM/u/rpMxnWBCd89qvemXtUy7pR/4eQV4Z+mB63leAT7PDL6gqKoWpHT7ejbg4KGfN/DenMyCLZZUbjFoP+s/DxnjfY9NP8PXbtmwmXtjYrdC8rng5wIw5UHXsVVnT8ZcSDUX8FLftPiCfd5/kSIFMoQQQohrnQT4hBDiCvu046f8cvsvDGg4wO+4eiXrsajvIp5u9jQAr93yEburG7RiCpkalWpEeNcu/NVa4dseBlY2VJjfSuGTewzsqK6wt3L+zTtx5XKP8xvPbuP3fb9TuUQETS4dAeD75Z/z+ubf+HHDjx5jzf/u5peNP/DB2p/4cekXDN0wnjuPrA7quReSdAI0PtgwsVOthSPzx1moyUCYSf9H2z0/rmXHyQRsuN7oxlEcgMetLzGjxlA+OdPM2TfA8krQ89AzynYn1TO8M5M2OLQCJzMcHfN0f9aPgnO7/A6JVdL4JvQ7+PkmSL1IqMP/5/blkGk5m8M6naw+AGMoVruDpXvPk2F1CxZlJGpLBnUyDyuoWtbcl4v3Q8IJrXH6QNcAxRjw9QL6WWynt0D8Ee2Pu5Bw5zXuxTAU0AKKekUrZj8Nw6tCoisb8qOQcVSxn2DiuuPafTIS/c/xr5c8zx2ZAcYJPeDLelr2Ym4Eem4WVfUZ1Jy97TQN31vEhDU+lhZvHg9zBnsEOPMsj9llk9YdCy7r8r/GPZAeqCjNpcPacum4/Vd2TkIIIYS4YiTAJ4QQV1i4KZzmZZtjDOJNrKJ4ZueVDC/pcV4puhIfdfqEid2MrGps4LteRibcbGRbbQMf9TPyfY/8y9KwTpntcZ4eBrz6CSntWjrbQjPf48dc9nxzffrFl3Ds20PrC/spmbnXWKdT24N67js9GuR2yoD35zDL1hMJ3DlqDadV1557x9RyAOxTq/LS3jqoGLg+YxR3mj9guaO5c9xeRxVOqf6rCgPEq8Wcx4sdrbz6R9r6MNjynFd7sA46KuX6Wj6vRT3rntxfr+f8bv12Swo/LjvIoxM3E77uK+/+rOIcbgG0UiRwOC6Fb/855BrnvnRVUbSCH4HoZRVe2KMVGzjjoxqwzczbs1zBwwqp/8JntWCezt/Vjt/Blg7nXeOLKRksCXuVd0yTyLA6gg+0ZbFbSEizwMn1gOq5X9rid2DFZ8Hd58BCWPy2boakh1lPwOe1IemMV9fzU7YDMHSuj6+Vec/Dtl9h90z9/tzIQ3bZ9pMJvDPnX/43vgisjki9BOf/DTwukJObYNd0rXr10g/1q2UHwz3YHSjAN+kubbn0hDv0+wN9TYn/trR47RcU22W5vRBCFCYJ8AkhRBE26fZJPNfCFWRQUIgOjWbZvctoV7Gd1/jkK1gjosZ5aHXIz5s8xfVm0npSvzjGs9um8dWKrwmxWzGEnQFDGhhcWWXD+zSmUUWtQEanU9u4/lzOAlK2IEr/7lerMsDyCl3MX6Lq/Bi8QAl2qLUBOObQAoADLK9xr/ndwPd2W5ZrIcSrf6Ttbi4S69UeLFsef2zXsh8JPCg/WNN4dHUHWigH9PvTM5dkugUgFNVB8WVv8GfoW/rX7JoGq74M/OydU3z3pSd4nitZn0+VGRtdgcX6l5ZqwcWtvwR+nptHTQtIMdsgI4hApDu7jRczA2uAqwDHpcOw9htt/8FglxGv/RYOLfE/ZucUyEiA5Z/kbJ7uLh0KPCZYSu6/rk9f9r/svEB92xx+aJf3IN/YbjDjUfipC6z6Qtt3Mjfcl6QH+gVTwnHtY6qPYjdCY5dK1bqWf6LtlTr7qcKeiRBC/KdJgE8IIYqwqjFVGdR4kPO8TYU2AJSOKM3om0fzeJPHeaLJE/S9ri8AaQGq7E7samDAC0YeeMWII3+38mPBrDf5/p8vufjjaN1+Y8RJbj++gXqXT/Ln3DeIqvkN0XU/oEz194iM2AbA/ddXJSzEQImMJF7f/Bvvrx/Hy91qOe/x55D2fHFPU59zcPgL8BnSCCs/G0P4SZY7mnNUrRDwNd1q+ZRWGT9wnpKYFM8lifPsNzDH7gqyjrb5yHxxn0IePufDrA9iuAIb3b9ifTzf7wkQiZmZYUP1O0d31AJWbkU6FFRK7fmFJgYfS0PBZyVeD1sn+u5LybaM02ACgxaILYmrqm16SHHXmBxWeE1NS/fINNzq0ILFnNigmzEHQOJJVux323svq4CI+3zdq+66Cy3m3Za1lDktHrZO8h1wTAxcdMWnvASCHHbPJb55WKLrKEqZZVmZm1sn5W9l4HM+MmUDcc9m9ZHZfDX4YO4eun+9igxrPi4Lz41tv8HwKnBkeeHOIz+oKix4Hbb/kT/3y/7LEyGEEIVCAnxCCHEVmH/XfIa1H8Zdte/yaH+m+TMMaT6ERxs9SvGw4jzR1HfhBlPFitR4/FnSwhVsJoXX/mfkh+4Gvrwr/34U1Eg6S9zIkfrPz/7eTFWpdl7llxF2ZvzxG+VSLwEQZjIQaXVl9dWJMTH58Rv4c0h7mlQuTs+mvgNzdzbzXsJap5wWAAkvP5fQEuuJquFj3zgdZkKdGXcn1TLO9s+t9zLE+ix/213LlT+xPcB+1f8miNvfu8XjfKqtk+64sbbbvdp+tt/hVRU3P0yzd873ewblyHKPAEQlCiBzKDFbZqktw7n/XX/TosxG1TOjzOq/UnF2+0/F8fGczc7zFoZDcGozjLsFRtTXv2hSb4y47feXdEarBuweRPNVNVjV2Scwq9jG1EfgzyHannl6clJcJLvcBvgSTsJnNWHBq662PGTwFakAX5YNP8DYm3N3rTX4PUgDsrll8OU186wQP8/j1hxlz9kk5u7wESAvKHOe1v5t/fFA4c4jPxxaon2dzs5jsSchhBBFigT4hBDiKlAlpgp31r7T5z5+VWKqsOK+FQxpPoQ/2ygcKwsb3uxOlZ9+ovqM6ZR743Vq/72YJ5o+wRvXvwHA8XIKy5oa2FDPwLZZ77Dlaf1gU34JzRZLCLPC5+NcUb8J5vUAWI4c4d6Dy5ztlWf/yg01S9GoUox2H6P3j67Vr3VhzMMtebqLK9uvfOolKqbE0aCCdp0x3C24Y8gAdAIj2bSrVcp5rGKgesZvtM/4mlH2O7V74h61VPjSdi+/2G6mj3mos/UPWxdAK64REx7Cb4PaOPtetT3O9RmugOMMeweesjzHh7aHdeejl4MzqtLwgK+jSIo/4hHgi1VyFkjLjYwz2TKh3IIpvYzrKEMCm8OeovOJ71xjzu6AIyuCfsaI6Ut4Jflzz8Ztk1zHx3WqEQMh7hmiR5ZpQTD3oF7aJS0z7Pf7tOBfFodO4MYYCsnntSVzAHv/1J+sryrIvrgXHdn7p7ZP3Let4Mz24O+x7CNtefCmn11tATL4pm85xcytOtmG5/cQleRaKqwWZrAve0GWs9tdxxcPwd/vQUoQQVHdTM1cvi73YKEjD8HcIsJsC/A9Oy8B65y4EtmQqprzpf15kXYpn29YBAPtQgjxHyQBPiGEuEYYMrNgkh/rw5fPVKL3ve9QrMONRDRsSMn+/VGM2pvoB+o/wN93/035qPLOa/vV68dDz/5InU0bSRw2JF/msy9bMltYtjjEpC+zpfStXIaqqhx/4EFuOeHaMF+ZOYXxu8fT9JemPDj/QeLS4zg2/A6qlbMRUWUspmL/UrlEJLc0LE9IZvDP5LAx/u9PGLvkU0JsmW/6DK43f9F1hxJR9Wf0hNht/D6oDbuG3kLxyOz76CmcpgxZobaljhZcUIuzyK4V1Egmkndt/2OrWsd5xQe2h3nJ8iRPWp7XpuHx5lDhAiXobf6Ar219eMM6iAWONmR3v+XtzNGeb6Im2G5h4oXrnOeWap11X1Mgy+2uZc/L7E3Z7aieq/sArLQ3Dm7ggldzv7dYAPtG99dtDzdne1NrTYMGvQFYaW9CT+M6SivZ3mT/cif80ivo6qKvm/7wDNaBZ6BlYi/d60Kzx7jsZm2JbZbVX2nZeAcWeu5FqBfgO7UZJnQPPNlTG7WP5hQY0xmW+diTT1W1DLDsQaIZj8KlgzCmU/BVdd1fk/v9fUjKsPLytB28OHUHaRa312pJhR/a0u2fXpjQ2q32QgoypMW7sib1jLsV1owMbn+yYArJ+JJ4Cua94Ppadc/g0/s6yZHCD+BY7X4CfGd3wLCy8M9HBTeh/DTveW3579mdBfTAq3fJthBCCN8kwCeEENeYD9t/yOK7F1M8vLjPMeWjyvNoo0ed51mVZ43R0dxw92BKTBqT53lcLub5BqKUjy3E3B14uB/2RO8KpCM3aQGNnXE76TqtKwB1GyzGVOwgEVUmeY3/5wlXBdti9sz7KZ5vcE1R3gUnHti3mD/nvk6z5FNEh4fw2m31/M43hUjamb/hCesLPsekE84MR0cuo2USlowK9RqzXa3NV7a7dQtzfGG9h/UOrbJw9gDfBbU4F5JdWVgrzxr51dbV75z1vG97xHlsxEEjw7Ec32OlvTF9zENJJ8BGkO42j8vxc4JR7+zs4AamXUKtpC2zvs+0nHdDvL+WnC7sCWqZY33DCe/GrKrB4DOTqone59x9GeyeOa7jrOrFqqq/RPfgIu8iGOt/BODnVdm+7s/t0qrqntkGK3SyQTeNhfeLw4elIPWi7tyB4Ktnun8usvgJPrnvu2a2OuDycS1Tyy3TLRotuGbLnkVXEE5vhc9qwK99fI9Jy/y8ZWVU+jP9f95twWYmTh+o/Zv6uZt27l5ko6Cy264gu789Vhe/rf1bWBlkxemiZssE7WMwhYTyyOFQUbMHbM0pcGxN8IF6IYQQRZIE+ArQqFGjaNCgAa1bty7sqQghrmGKojiz+fy5s/addKvajWHth3n1lW/dwe3EtffchVjYVwnnvn17/Ww5lxYGJ0sHP28Ax+Yduu1dt7vejHTZ4WBP48YkHtzubEu3pROf4coMKhfqeqO/LPEF3uxajK/HJvHIEjuoKrdscVD/hOcbnDBbOg/vWwzA+U+0TKZqpaK4o7H/Yhw2TOQkG6Ju+WheurkOn93dJKjxGXgHBLNEK9ob+IGWl1lib87rSXcTT3TQc0lQowA8Co6YyN0bvOG2fmxV65CkRubq+kJhy+CndUHu67VkaFD78dnQWW56cJF3WzYTDR94tVmS3IpsNHXb9ysr8JeTN+MLXwNg2F97Pdv/6Af75jlP3RNMrSnx8NeLrgZ/FYqzCnsEYtdZFuwn+KS4/dsynlwLXzeBX3p77NsXpWgZkh4ZfKqqVSHO74BF4int+Qe07xVszPxlyMkNga8NZkn0Wf3vgUE5oxUrcmYBumcV5jWDrwjsdVhoGZoF6QoXQ7E7VLp/s4rvlh327Pjjfi3rd/tvubtxEfj6EEIIAabCnsB/yeDBgxk8eDBJSUnExsYW9nSEEP9xEaYIvurylc9+0zsvYPluLDW/HUVIxYqkrlnDhYaR7Di/mjcbPsIjVXqRHAEVL0F6KHwz2k6I23vpC7EKc24w8M3ovL/Bbn1AZWMdlXArPDXfATgY+l08b/Q3criiwt1/3s2J5BPcWvpGHn1jDVhcAYNQGxz882k6xKtUigeD6qD7Zu3NyN3dU0kNjaL/ngXce2Cp8xrV6rq+dDFXgO2JTjVxOFTqlItmxN8HOJvovSF+qMlA6ahQzuj0ZXmmq7as9tXpvpdjnXSUoYohjiWOFsx6uh13fb/WK4Mvq8jHP44W/ONoAcBpt2IgegZYXsFMKD0M6xlr9y7msVOtSTv2+L0HQIYaQrji+jzZM39n+IXtXu4xBVHtNr88NAN+7ZurS09UuI1DJ2zoJE56u3wMtv4ScFhlxU+WWw7tOXiIZlkn7sEaS2ag0T1ooxhBzcW/tWyFR7Lep1dVzpOw6BM8vpr8BcuC3eNNLxDg51r3eEfYjszP//HVHq89JiuDz+5g8b/n+GzRfiY0O0DllS9D035w14+B52W3QtJpKFHd95hpA+DfWdrxkWUwNFHb7zBo+kEQs83OfT+uo1fpswzMwd28GELA7la9131p+DWQwWfzt0S3QF3BIFyCTgZwPjp0IYV955Kpa0jB43dHWdmlu6ZBi0d0r71mXDqsZUy2HQLR5Qp7NkIIka8kg08IIYSu6x58nAZr1xPRuDGmUqWI7dWLTrW6MbTdUCoXq0xilILDoHB9u75MG7SMz/oa2OdWxPZQRThXUuHzvnn/UWOyw1c/2fn2R88AwycT7XTY5SAu7jhGu8qjLy33CO4BPDPXTokU1xvrrOAewGtbfuWW4xu5/8BSjx+IqtX1JnlQh5qEl/mbug0X8Ppt9Xjrjgbc06oKC5/r6BwTUnw9ppjtAHxxT1Mi3DZUm/ZkW+pXiGH8AO/s7eurl/T5mm+2fEbbjG85plagfmahkAuUcPb3NA/z2Osvyz/25rr3W21vSIOMcSx3NGedoyFv2R7lckQ1j/uNsd3BeNttDLS87Gz/zHqv7v0s2X5H6Mj8DJ7H92vKL9sdNclQQzjTfQLU7ga9vmWVvVGO73P70Xswq8FE9zRHDuwOPCgf2dPclrPqBfjcA3qGYH9n6z/TpoNhJ+WIZ2XYC5TZlW2pvp8ssPUHz7HthM7y22Ce72fps3uVXIfB7e/KLdhYDC2T1eZQeXzSFg5dSEFZlVnoZMcfQcwJ+O1u+LopHFqq35+R5AruuTPlYEm6D4v/PU/VM/MZuG+QjxFBZkdl/xqY97zr2Nffnc0Mu2cEd/9C4HBblmv1t0T3auYe9D695Yo+yhAoNhnjXYn+mvNzN1j7Dcx6vLBnIoQQ+U4CfEIIIXxSfCwXCjWGMqXHFH7v/jvvt3ufMpFl2FHLwLuPmPjlJgOOQffzwMPDCTeGk5bt/e/a+q57Lmqu8NDLRjbU9f+uo9EJlWI+EuKemedg4ld2/vhMP7uo9lm4a53+G8PW5w/ywrapXu2WQ4dxWLQgX+USEYSUXsoZxwoOXD7gHBMVpgXxFNNlwivMJqLSZKY8fgM9m1TgkmMnpmgtGNS6ekkWPNeBLvXKej3np0daebVlySCMs2hVfMNDjCx4rgOrGrzPantDHra8zi61pu51cRT3OD+jlqRVxg88ZH2LNMI9+qqWinIe71Jr8rHtQc5Tkn8cLWib8S1DLM/wvf1O5tm9C3+8afUMRjh0sloW2oPbkmKDox7vW/UrB+v5zd6NhuZxHCvVQQsAtHiEh61v8rZVZ/8yP1KJwOxnCXR2qw7ld+VJ/yJx+6J330/NkpJZddNtv8oAlWizlMN/EG5S6HCaGLz3pwQ8izZkczo+ibu+168Q7EHvn6J7Bl/8UVg3CixaQNN9Wz2PKrluAatimUvV3YswpNndgl0X9gWe15Hl2seVn+v3+wqQ5SiDz8etVZUHTP/4HqCqcHQVpFzwPQbAnG3/Uvc9HH3Nf9nH2t59AeVjcM2cDPvme2YY+mBzC+r5z+DLQ1bd0ZUw5SFIOpv7e+SFe7XsK8yQGeFTfX2+gtje46qXnrmdx8mNhTsPIYS4Av4D38WFEEJcCQ1KNaBxGe+KqbtvrknDl9+jR62edKjcweN94bhvbmHajQYyQmDu9QqHB3XFEqLwZR8js28oWlX9tj56Py8uf5HbZtzmbEuyaHtbLT62mB6zu3NbcxvlirvedDauEoFDdWAv+xMRlX9FMfp/4xYbGUKnOmWICDFSwqtir6f6FWLofENrHrK+xSqH5/59lYpHeJy3zvjeeTzLfiMX0d8WIibcd9bXWUoxz9EWULiglvDqX+NoSCezqwqu+xvGH2w9uajG8J5Vv5ptdmlqGNYc7BqSpoZjx8hPK4/QdcQKBk3Uqi7PsbcP+h5fWbVlveag1udqIrB4tX1hvSfo63OqvsG1fFaNdwu6ZSTCsHIwor6rTQkc4FNVlc9DRgccZ8BHIOX0Vp/XhOjt3aiqWkVf9+WhekVB3Pt/7ACL3oSl2p6EWRl81yt7idzjtgeg231CMqvo2tz2aPPIML3oCsw7nd0Bv90D57JlZZ5YB/sXeI/3FSAzheu354BBUZyvQdehv2FiD/i2pVakxKKzF6Qt29dm9qXQvpbo6mXvHV7m3ZaXPdayB/KmDYDJ/bTCGAG4F9bI9/y9LRO0v+uJPWHvXM+Mx4KUnuA6DonyOSw/GPV+aedRnKZo/Ry+ooL4nimEEFcbCfAJIYTIV1VjqjqP377hbTp20jKzlLAwHmg2gNOlFf73gpFJXY181OEjGpXSllbObeP5I2najQqDni28/4BHbdrL/k2LOZN6hsgMlZhUlYGLBnLo3dc5+cLznE4+xWbrh4x+WNsHr9NOB6f7PUzqiaPOe9zUMHDBi3EDWrPp7W7MHtyeDteV5oE2VX2ObZ1tSW+nOmV4s3s93unRwKPdPYtvg6M+voQYff83YNnLnfll4PXc0biCR6GPT6z9+ND6IJeJ8Sio4b4/4Ke2frQ2f0+5yjV83t9dGmHOPfyy+7vq815tF1UtYLlsfxxHL6ayZK+W2ZRMJDUzfg34vFvNw/nargX4ElU/b6iLV/M4vde0wmuIryIoo2y9+NbWO+BcgqUknfZsyF6sIojMG4cKrQw6wa5sDL5CKXoVcDPpFmfZ9DP83BXLnOc8quF6cd83zpJZHfeIFmTKCvCMCP3B8xq3gFsYNkDF7hb4amg47hobpvPvcNxtcHCx/v6NC1/3atp8+Jz+3P0t0U2J827TyRIzGgIE+LKYk+DT6vB5be++7Flg2QOSKz+DEzqFQPT2VZzU27tt60StympOzX8FPioHF9yKuxxaon0Mooq21S34FJN+Es7/m/M56Ll4COY+pxWXyHL5WP7c25fTW7Rg4pntnu3umbimvGeE+qP7L9t9qX+u43tX4fJpg7wNFkJce+Q7mxBCiHwx5uYxdK7cmXdueMfZVjK8JE93fp3a/yzlupUraF62ObdVvw27UXsXERMaw8guI/mkwye8evMwTg/oxsY6Ck8/bWRaByNJUQpjby68H1VfjLXTe62DcSPt/PyNnVu2OLBOnUO7vSqVL4LZbsawfQdDf7Ux+C8Htt17uPDqG5RJUHlsgZ2X6npm1jlSXQGInXE72XZhG0aDQrEwE9VKRTHp0Ta817NB9mnoqlQ8gokDr+fxjrXQfXP1wr/w4AxWOnxX67XYfC95q1E6io51ytCpbhkyVNebztH2noy13wF4BreyB3hUDBSPDIXSdQO+llQ1gkVuy3mPO8qy2VGHBy1vsLJEH8bbbiVFdWVKXci2DNmdw+2/Ng9bvAM12ee9Vb3O571SunzEKod3lqq7UB+Bmc9t97HM3szvtb4stzfN2QVBZqLYHSqRSuBKrl2N2/Q7/CwlrKVkq0bscMB8bS/H0J2/cfuwaZkdekU2tM/hxRS3uWUGnrIy+LyKl5hdcwnBxpchP1JrzHVUU87pPyO7rD0NU3QCdzrZai9O3qx/n5AI/XaAL2rD9mx7ALovAV72MfxyJ0bVGlyAL4v7foxZsgfq/ujnPWbcLd5tehmVemwZWpVVIC7ZzGO/bGbp3vOBr8uqMuxr6XMAdreszKd23A0/tINUnaXyOa0+m5Ho3RZsxeXcVrr9pbe2HHjCHZ7tyz/J+RxyyZUR6fYa3LM7twX+Bck1I7cZfOYUbT9Oc8EtrRZCiGBJgE8IIUS+aFuxLd92/ZbyUeW9+kIqVsSYWT28RbkWHn3losrRo2YPetfuTbfXv+XSu4O4GOu2T18rA46+t3lc83snAyf8F4vNNw+scGDIfE80aLHrzfAjSx2gqpie+5AGboVIU/bvYdQPdm7erqIMehGAM6+/wd569dnfshWXxo4lbupkHp73AI8seITvt3/P/CPzndeHmYx0b+z9Ocwytn8rritbzJk5CFpmlrueTStCbGW4rhs/P+IKnDWt7LlU170YiC8VYyM4rFbU7XMPlOkFukKNCjw4jfG2Wxlvu5V9jirUz/DO2jlDKeKJoX7GOGplTKKT5SvutgxljaMxkzac4n1bf+ba2zrHX1CL+53zU5bneM/a32spcxbPwhq+36y/OO8kRx2+/y4ASipJPnqUHC3/dWfUy4bzx2CCji8HHndgUVC3u9voowqy+1LCbCoo2YIu2bL9oixxWnBBJ6B0MTGF80kZtBq2xNWoZgX4QNFbMpzhmkuoYqWvUasCOtC4AGP28bbAQU0PCcchwbO6sM8AXPYlurHZMnBnP+l57h5YWvEpHFlOhbNLvecciNcS3Gyv8dDfQd7Gx9eanyW5P686wt97zvPoRC3o+c++83z01x7/e+T5KdDijyuDz20+7svVcytEZ2l1bqpQ54Q583tF9kD5vnmu41x+noLl0Pt7zf61k5vl2HlZwl1Ygty31MvsJ7Vl5nOeztfpCCFEfgh+wxshhBAiH9xd526MipHW5fULMDzT/BmalmlK6cjSTNk3hUFNBlHVXpzjW/YRUrECkde3IbzOSU5/M5OqcYX3pqLZUZWmR72fH2Z2e5ObnsHlKVNJnD3b2XTh8y8AeLyJwo93GPlhh7b08PYat2NPSOD08y/w+m3dmY/+0tGu9cvRtX45jzaHqmKMPERo6aWYz93FSzd3dvZ1a1COuUNuJCLUACh0G+FaZtqmRkn+3uM/C6d51eI87LiekbY+bHNo2W71ykez71wyqtvvCdPwXqq49UQClGhN6k0f8cVi30tD4zIDdun43s/MfdloKn6ypoAFDu+iIO4uE3jpNMCu5GJs5G4eMfkOliy0t6aacoGbjd7VL8PItu9ZpVacPXWECoq2yftQ6yOsdjTiRdN0uhtdG76bchrsMZig7RDY/hvE+S4oYVz1Wc7um51bUC27GCWdX0I+wb7birFRbzjguY9dFGYyrHaidAIBZy8n0/PjbNVrMwMddodKMbyLMaQmXXL+C3EPvnU1buMz2/2eg7MHMDz2EvQR4J0xCB51BUR19xh0m6drYIA9+XSyII12M3ZyGGxQHZ4ZSDkNYgITf/mJ/qk6y4hBWzbtY/mxQ1WJJo3kzH+HAydogb6mMSn0OPgu3PAUNLgz20W5C55lZZx5ZAjbAhfncNowBi7sgR5fZcu809uLLtg5el6bmGZlzeGL3FSvLOEhedxWIq8BvtUjtaDzDU9C6kUtiBXh2kPVrleJ2JqteI6fv/vCYLU7cKgqYaZ83rIjtxl8e+dqH/fMyb+5CCFEPpEMPiGEEAUqxBDCvXXvpUas/v5sIcYQulbrStMyTfm4w8fUjK2JqWRJai1cQNVx4yj95BO80OFNKnXvA8DJ0jDgBSMTuvr/kZZSyzsTa0FLhcPl4bO+uftx2GZ/4ADjuffe022/aafKdadc12fYM7j088+kbdhA6vve19gddvZc2oPV4b1ZfouqJYis9jOmqKNE1RpBsuqZ4dK4ciy1y0ZjNHi+Me3frnrA+UeFmVAxMNJ2NyscTVn8QkdmPt2OAe2qU6tMFOl3fMdn1vs4rFbyujY+VdtbbchN13ntE+huhZ9lxFns+bj5u7/KuWvsDQFwhMVylpIkEE1n85e6Y2fYO7BZrctj1pd09/47lD3zsWJzPrI+6DydYL+NQ2plVjg8l+QalJwG+IzaflL95/ofdiGf9i/zoaNxF8bp/Zm2+aS2v5mbkkoS6Va77vJGxWbmu5Cved30u6sxM3MrK5iU3bAZ653HZZUE53Fl5SLDQjyzRFOWfEbGH/1h7bcs/vcceye94Oo0+siyPLvd41Q3gy8jySsopAYquqGzvDbcHIctp/8lzx4I8lVEw4+HDr/iu1NvGXCmOhm72BU+iGGmcR6VjRtte18rUjL1EZIzrJ7BJIcd7LYc7+OXVTjFI8MxNY6NR+O5kBxEoG/BK7BlPBzPVuFZL5CWcNy7LYufDLX/TdjI079t5bOF+/3PxRBERm9eAnwpcbDkPVj4GqTFw+e1tD0b3fYxzPo78Xg1q7/yvI+fv3tddqtnFmI+UlWVzp8vp+7bC2k//B9+33Ai/27+X6gYLIT4z5HvbEIIIa46kSGR3DLoAz6838C7DxlJC1f434cz+PuTXqhuAYRPB7kyF0qVrUa5d1xVG+fcoDD+FiNv/M/E5jqePw4/vD+4H4/dtuctg/CjSXbCzdo9Xpz7BAkzZzr7Bt1YgxC7lbfvqI+qqozZNYb75t3Hpxs/9biH1W7FEOK5n9QD8x+gz599MGfLXHJVULShhMQTYjSw8c2u1Ki2n/YN0pj7VFualw3jl4HXe1z39h1aoY4XutWhTrloIkNNDO3VkKUvdSai9cN8b8+WrZOpeilXEY5IH8uB50Xfyym1rEfbG7fXo2y0ZwbJaHtPMtQQJtm6USY6jOtreBYcyQ9/2a9noPUV3rb+j6d4i6xMnTRVP2jzla2vc4wDg9fS4SSKee4DGFWGvxxt+NJ6Nw9Y3nQ2T7N38rgu+3LNozUf8DvvrNGJhhL8aOvpc5ySPZPtCpk+Y4pXoOKH0K9Jt9h1AxhVL66kh3EDT5rcggSqaw++Ykq61zUxbkG/ISbPTJq7jJ5BpGLxuwnfPxsWv83jk7YQn+oWDDP6CPZmZT2d2ABfNeIO43rvMX+96LWs81ya/0C0xa7y0E9r2D/lLWdbrR1f0NhwzO91XrJ/Hs3JObo8zWLD5i9r0M9y7rYnfgTgIdPSzICRSitlHyXTXL9YaDlsCf1+cvucqXZY+r5zHz9de+fB0g89gmk2vQy+6f9j8OgF3OCR9anzeXcPeto9qwwnpHp/TXlJueCai58Mya0nEgCYsfWU//tl36/xyAr481nPNtWBGmwmoSUVfrkTZj2lBfHcvybclzG7ZTzqLtHN2ifRed8cBvjWjPT6/Pq0fLgWUNTbR1GH3aFyOkH7uzqdkM6bs3blbG7ZZbhtqZDbJbpCCFGESYCvAI0aNYoGDRrQurX+sjQhhBDBUwwGGtzWj9QIhf4N+lOvZD2evetTqv8y0Tnmtf+NdR6HV6hEyQcfpPqqZWx6ugMz2/r+EbirhoEj5Xx2+5QSIHlHT8V4qHFO5ZFhm3BcTnC2P3ZhHXPmvcnFdS+xp/utGN//FoAp+6fw4nJtbz9bfDwjP+xFj9+7ed334OWD3P3n3Xy0/iNnlo1dVSlmSaNX4teUrfwpG89u5HDKNi5Gjmen+gGlfx7Jx2Ofp43JM1jw6I01mDO4PU91rqX7Gh5p66o2q4TGEVZ2PooxhTGPtHK2h7pV7F2G9nPwoKMSs0o+5v05KR7BF/d4ZrUdV8vT1PwT79j+h6qq2j6DQVhU/gkwhTPA+AmfW+/lfosW5H2sgyuDtJf5Q36ydecV65OYCeVX+80sSqzs7E/XWX4MrqXFWfT2zjupum0WGV0OFQPf2vuw1tHI2ezAwDz7Da7nqZ5BpwXH/AeSDZl7e/1vwkZ+8BPgKyhTwj7UbTceXAQ270BAjKITUEg+C5Y07A79DL5YJdX7miB5fDZDIvSXZkaW0j5O7geJJz2Dj1l2z/Cq6hqV7H9/uGMXUyl/bA51936Xs0ln5z7nNV/DWO/vAf6kW+yk+lkSz6wn9LPWHHaUbAG4ToadTA/7gJgMV6GVGx2b2Xg03u06G6z9JtvNst1/yoOw6guPbDB7ZvbZU6Y/PYZ2NOz02nvUi/ueke7BNVXl5EVfe2fCvnNJ/PHT5/DFdbAic1l7EMFxNdA+dNmzV3/ppVUnzuaVaT6K3GS37GM4shx2/A47J3v2JbsVj3EL8GVl8Bn8bQOQfcluILtnBTcu6axWUGTJUPi8Jhz+J+Al9vze2y/OLcsyp5mKebThyCVOXS7YZwoh/nskwFeABg8ezJ49e9i0aVNhT0UIIa4Jr7V+jYm3TeS5lq7lgBHNmhFWvz7RN99M/VL1qfDxx0TdeCOlBj2q9ZcpT8sHniM93PPN1h+dtB+Jxe+/j886fsayd25l5S9P8fTTRma0C2556L/Vcr6M9NXpdj4db6d4tnhF/GdfoKgqd47bh+HoSTrsUWl9wEHXbQ5Orl7M3MNzOT5wID0mH+PhfzzfrJVKUql+TuVY0jEm75/M0cSjAGRY7byxaRJPLTvLM3MdTN4/mXVn1wFgcKgkTJsGdjvJixd73E9RFJpWKU6oycCqU6u4c/adHoVBapct5jyOqvE1oaVWElFuAXXKufa6a+xW4ONV9VnuM7/DbZbhNK5S3OtzUiIyVHcvK21prcKb3esTFUSBEIB1FfvD6yfZ5qjNKHtv1ju0pcIdrnMF3naqtfjI9hBpPgIeevsLNskY47XU1z3L6LaG5albLhqLe0GPYr4Ldiy2u4KhE+y3evSdN/teUuxu64kEEinm0ZZYsSP7S3TyGnuk/pNebcH6xXZzrq6ruGBAUIESp0Vv4LCrROsEANsa9uRqDkNNE7jR6BaUCy0G37XyHli5NawbBWl+Mo1UB+yc4tEUo/rPpLPa7FRXdCr35lRWtpY5Bf5+N8eX2x0qKar/vSx1A592q0fRE9O0h3ja5L0X2bjQLzyDSDnZg8+twIk1c4lu9izNgGGfXdM9Py9Zz1dVmNiTxgvv1r/OYafP92vpdSqz6u/yj7WP7hl8WYViVJXEs4edswk4J/cB53b7HDZ320mffR6OuvZTZfZT4L59wxTXVgB6AT6/RV1yHPgKMgiXff/JlV8EvMQRaLeCWU/BD+2D34My0B6ZV8jOUwncN2Y9N366rFCeL4T475AiG0IIIa5aIcYQr6q8SkgINWbOQMnMlije5y6K97nLY0zD0g15u83bjNk1hvfavsfHGz5mVrvTNHzhXe6rdx8V0IpeTNoziYuxClM6GVnUUmXMt77fpB4vA8fKKs59+S5FQ6kgVs2V9N5z36dXZrje7dxb5U2m7tPe5N+yTWVKR5WxX2vzuxALZRPhg34Gdlc3kGTRslWqlIyEuIMAtDykstcUwaV0LYBRzC1pwxCplS9QVZXdF3dzXYnrCM/cW+zppVrlwNdWvUbD0g2pFlON+1pXITnDRqc6Zei3RJuTEnbBY+4LT48noupS0k8OJM5s4q37+lF673me6FiLkUsOeowtHhnCjovbMMVsx5bUzNn++6A21K8QQ4moUOZsPx3U5yzUZABTKBab63PXompxSkYFFzQDsGNksOVZehvXcLNxC+9bHyYpWyANIMRt77xqpSM5m5jOJbdKuofSowD9zKE/HW0Jt5q5GF2ffzJcgUAHiv9MK39M4bQ+8TTl7GdZFeYKBvxiuxlbpSepuffHXN12hr0DdQ0naaEc5F7Lu8wK099nEiBJjSDGbYmtak4JfjfFLROoeWIHpfAumtLccCiHs9YMMHkGr7l81Mezx+fq/gDpxa8jIuGgbl9iupUaytlc3zvLxiMXSDFZuengxzm6Ln7KYOZUfplu9ct5VMHWpbPfJw4riltAx3RgPm18pAvUVdwCVTrVkz3adk13Hbst97Q7VO4yrPK+1N9XUcJJmPGoZ1tWYNmSAse87+dkM5NmsXMutCS13P+e3ANIWfNb8zWxS97jWDhUz/gtZxE+P0uVfVZszs6Y7RcPGT6yEt0y8rICfI9k/3fgY3yuqKqWrfjPMEg4AXeN1s5HZ/9Fg+Ia/8f92rL4uzy/JwXM4NuRuW/n4WVQ97bAc3PPIK7eIfD4fLL1+OXAg4QQIh9IBp8QQohrjqKzkX9299W7j6X3LKVj5Y5M7TmVkZ1H0vu63h5jUqyu6NtHPXwvqRvfzcB7DxlJcCt8Gyi4t6xx3opG3L/CM9iYFdwDLbgH8O4fDkokq5xMOsE9c++h7eTmHtf0fn0RpRdq1V/fnOK6Pn6G9mZ7+sHpPDD/AV5Z4bkZf/NDDsrFqzy2+DF2xu0kzGRkcJfaNKrkytJT7K5Pxr8X/+WnXT9hijqCMeIYAB13L+PjqJNE6GTi1a8Qw/DtzxBRaTKm6B3OdptDpURmYC7dElxGUIhR+zxb7K5gwtf3N9eCnTpuqKm/t99fjht4zPoS1TN+Y7z9dt0xl4lxHsclmwkzGbG67XPWf/JhPzNVmGrvAuU9C46kq6Eku2Va7XdUzn4hAE//5l3FVy3bEIvNwUm1HKfU0s72GfYOfDAvdxlwAPvUqvSzvE0j81i2qdfxV4mHfI5dZPfclkSx5mxpbVTcNl4NmRx4YBHS7NxbfrMc73CrmJxbQ37dzMAJm1G3eRd3cbdJaeRxXnLvr7w/dw+/bzyBNdDv+fX2VbNbPZbo+lNRueg6ubAXnxWLwTMg55ZxZrqwk69Cf/Aa7hXgc/+en64TTMkK7ATKJMx8zYnZq5h7BPjMWlBqiSuw3UA5HmwemyYj0WeX3+w65xx0goC+smPdM/hUlVIk0szgZyl5TjP4sn89ZAVuV36uZbieyaxanf3fftbf2aXDcGAh7PjD6+9Ht+qv7hyCLEzk/jUd7DU5tWu6tteg2+fF6/8kcQdgVBvYOe3KzKGQDf3zX27/ehUZ1txVzxZC5J4E+IQQQvznxYTG0LVaV8KyZUScS3UtpetYrbPP6zv2HsLInj9xT1vXfnJz2ijMb+X9hjY9FN59yMiOmnkL8PVZG9wbn9Hf2and4zVOn9pL3ZOe1xS7mMpt07XKkTXPu9qt+w+wb+1f/LbpJwCWn1ru7Kt7SuWNaQ6+HW3nbOpZnvnnGQDsiYkcvr07/ZZr/6GvVsr1Bvn+v+53HqtqCGUyznD+o48489rrqA4H393XhE7lTDStHMvvg9p4VPuNqPyH89g96y6NMxhCtUm7j//r2Rt5/fZ6zvOsKpzubxSrlIwkNkK/ouUfj93AjbVL6/b9NqgN/oIUz6kvs91Rkwctb3AxxUKoyUASkSQVq8kxRznOUkr3urlDbnQeZ81rj0Pb13Cs/XZScQX43rQ+ytvW//GHrQt9ze9xXi3Oc5anmb/Le9nn8URXEKCyW7Al1xmBwO+2LpgJxYHBuUR51nnfG1a+b3uEvY4quX4eQDm3Krk5cVbNeSGWNFV/v8VAsgqsfGq9HzOh/GzXz9BqZ8x9YNVd1nJwf5lsd5nf5y/Fe3k2qGw/keARfNalt+zRboVgAlBABG7BlLSLBL2Uc/knzmW6piT9whUOFEDlcrJOMEqvMvIf98HWXwJXqc0MAHkFP7NdN2SSZ9GVMkpi4D34gmTS2cvTw6ax8FF5OL3Zs91X5p3VrciGAxaGva4/LtB9fMr2uh12z6BfoArP7oVqds+A01tdt3KoROKaf1vDv6jJeVji7h4Ezcmy8ZyY8aj2NXxinbPJ63eOv/SCuH0wcxDsmQM/dYV4H9nEV6EJa4+x92wS83flPVtZCJEzEuATQgghfLi/rhaY6lGzBwDlh77H+VolOFccFjVXsGS+P27U5CbaVmxL8+s6Oq+d18bAhJuNfNXbwMqGrv/dP/qckX1VFPZXyluAL6dGjrbz4a/Bv6FRB77MU2O8l8HWOOf2xk1VSUm+RJo1jcS//sJy9Ch3rdP6a5T2XsIKoCg2ylR0bbavWiw0+f5DXh/9PFNvK0+72qW93ijf1bwSQ3s2cGYIWh1Wvtr7GFG1vqLrjesIN7n+O9OwYixPdnIVAykeqQWh7mhSAfDM0Fv0UnOaVfXM5G8T1DEAAIchSURBVFMUxfvNWKbqpaP0OzLtslelt2UYaxyNiQo1EmoyoGKg5cWhdLV8gcPHf7tKFnMFLmPCtcDCIMtLvGJ9nK9tfT0yeg6oVfjVfjNv2B5ji1qXNubvmeO40eueACeT9P++s1cF9pUVqMeCd/DkRLYqyFkuqjGkEMkbVu9CKgWhghIfeFA20+wdAw/S8ae9LfUyxvODvRcQdCgr14yKnSjSMegto820Tb0Ou04QLxQbNocjcAafXoDPYQ068ymopaYnfexLvXqENgXV11sVhe9CvsXwdRPUjETPJDJfAaU/nwkcbMoK8KnZPm/ZqiUv3+MZeBwb8jlvqmPJDwEz+P56UX/5tK/AnM1tia6qUkbxnT2o3cctaJp8Hv54AA4t8T3eK4Mve7XsHPysm/kY/NQFZjwG1gxMO35hT/hA7jau4BbDJv4I/Qh1Qg/v64LI2gc8//4DBXvzKslVdMZrdsluga+pj2jB2vmZmfI7p8FfL1+5AGQBsgWbgSmEyDcS4BNCCCF8qF+qPqvvX83HN2r7XJW4/35Wv9OdZ58yMfY2I489Z2Tgc0ZKFdcquppKuoJHQzq9CcC6+gbKvaUdp4SDzaT9V/9SrMLWZ27yeua6elcm8Bed4bvv4aX6bySqnne90bRkvvFNd9u2a+pwO+NG2un/083Yw1zBAqNd9ViSVKlYJedxaKkVYHD9p181m0ldpe2JdfkPbSmmOdtSs6/ua8aA9q6qtxluS842XpqjW5Djze716Fa/HP3baZlww/s05rO7mzD6Ia2gwqX0S9w9/1aSywyjdDHPvcjKROtncZWI1M/6c74WFcb/rzU31i7NW3fUp1yMFkizYtINtGQJcwtQVi6hBRzPUJpp9s7YMfKvo7o2ZzWaZCJ9LiPOzj0Yt8HhymrMvvfaCTX4ktHz7d774R1QqzDQ8jIjbX082odYn9XmTbTXNStv+ImZdv3AZGHSC2AGw2SADLdiLAELWPiRWrJhwDFGHDxm+stnf1Lm8206QbwXTdPAZqa14YD/h+gt0Y0/Qon0EwHnBxCqBBFAGdtNqwKc3eZx4LBj9/FWJUyx0MO4nlhbHMrwqmw45BYw8RPEe2XKZp99gDOo6fV5yxYMCs0WvDQpDh40LvYfQAwywy9gBp8vekuTAS659qs0pvspGpPFPVC44FXY/xf82tdzzNkdsPANSNMJotutnp8HJRdvN3dNhU0/Eb34JQC+CBnNAOMiAAyXdPa3DDZ70v1r+koH+JZ9FMQgt5/3WUVIZg6CTT95VJP2kHoxcKC6qJD4nhAFTgJ8QgghhB+xYbEewarHmzxOnRJ1uK/ufczot4Bf+80hOlQLYIRWr07ZV16mwkfDaFK+mfOaHs3u54khRp550hXkebftu/TsMMjjWdG33MKm6/QDfB/fe+V+ZPfc6Pt/4d22OahxTqXlry1Zemgh1c97jg21QdkjCfyy17UXWItDKgaH9jpOJp/kdIorE9BU7AAG9+KaGa5gnj1RyyxZfXq1s61hKe9ghzVb9opRJ3b2eMda/Ny/FZGh2hv16PAQ7m1VhdjMIN22C9sAuJB2ARRQjCkoIdoyVvclvu4iQozc3dJ3tltYiIEudcvy66A2VC4RSfva+ktysysZGUrzqsVpVa0E7XWWB18klvYZX9PFrGU1TX68LVVKBg4gbXVchzHiCGHl5jIoM9i2zN6U+MyA24OWN1hsb8lb1oEMtT5CohrJPWZX5dEvrPdw2FHB454b1fq6z/rH0YL4li94tKmq9jWQoHoH+P4NacTLVt+VfB+1vKTbvisz2BmsJywvBB7kxpzLAF+o4pl1dZkYz+yz7AUR/DjS7FWP83TVuxhGE+UIZfCdiZUVUD6hVPDqe9I0j+eTPgs8kZmPe7dN7EmYPbh9FN2XVvrlqwpw/BFsqv73w89DxnicN1fcgj5+KjXvPXLM/1wyAydl3ZeF75gMl497DAvFR4BFLyiaQ0YlM8C3YwrMGRx8MGemj0zZ85nLwpPPc+Ms7wB9dqp7Bt/lY/qDRneE9d/DgtfwiuLYzJ6fB8WgH4ALlHWXGudxGpO9mnbAErvAnj9hyfuubFS9YilXSvwR1+v29Voj3X5GGLP9O9fbp/HSYfi8FozX3we2qFElwidEgZMqukIIIUQOlI0sy4xeM3z2l3pU2yw+2mGnWZlmlIsqh8lg4nK053/wW5ZrSUSqa6lk1IhhVOx0GxU+2g5oFWgTIyE28z3N9loGLhdzUCIHVXdzY2tNhRZHXP8pf3yh9iYqORyiP3mBijrXDJnnAFxvsF+Z6WCtcgK6Qa9ZvZztYRaVJsdU0tzexzgyXNki1oR47A47Lyx3BWWOn9xN3Pc/UPzOXoRU0jIBrdne8KZb0sj6L82++H1sOLuBB+s/iMng+785IQa3QE7YYYpVHgXA/vgbqFuyLqaYHZiK7SPjbB/Wv34boSYDiqIwrLdWtGD6Fu+9wbJnEtYt5x3Y0mMwKMx6uj2qqnI4zvUX/HjHmoxZqW2Gf5oyHtfEhIcA+kvybjd/QhfDdn62dyey+jsAmB1hVI/73WPcGkdj1jgaAzDBfhsT7bdgcHtDtslRj+/sd/FzyOd0M24L+DrqVozlbXUlw3Zqy1xjlFRQIZkIVtob09G4yzk2xargwMBQ6yMMDfnF616H1EpebW9aH0UFPjG4lkKuszegrY997dYX68r6i/oBSV8sau4CfCGKd9ZVE/PP7AkfqJ1UbIbl1DZCVf9Bhd+qf0StcM+MysbmnzkU/ohH2zeho/wW8rBlBvj2KzV1+9tb1vqdB+C9x1sORQUb4PPDlBYXeBAQ5p4tuPorn+OilQD7y9nNhGGhgcEtoDfrCa9hPrMTV34BB/+GNk9Ai4f9P8uHsiRkPjczwFr5emjZXzvOzbLN5DNaVt6XdYIavmbvSW7MigMGynI7t8s7eGfLwKGqriwSxdd9sqroBrfk2ytU5H5PvWDY6S0wNfPvILIUtBviGSw9tkqr8lu8alDPz5W0S7BlPLGWlvr97kE9U5hn0FLvlwL/ztQ+nvKxtL2IyadtKYUQOSAZfEIIIcQVYDQYmdR9El90+gKA7jW6E2oI5Zsu3/BNl2+oGVuTkIoVqfz9KKpP/oOq3ftiiIpiUB3Xm8K4N1xv6lffv5qQit5Bj9z66k4Dkzt6/jcgKQK21tbPNPC3xFdP0/kHsZw6zdffZTD1ExutDzgYtMjBKzMcvDbd9Sam97Q7nMd7Tm9n/L/jPe4zaLGDi998w7EHH2LJ8SUsPLoQi8MzSFKmhOuN3j1z7+GLzV8w8+BMrzlZ7BbSMrNT3DMLzGVGOY//OqIte4yo9AchsduoUXMr5WPDnQU+wkOMtK2pn5kXHuL5+byuXDRv+MgGzFK6mOtNnKIoziW6AH1b+M4WtNp9vyneq1bje/udHstNDaEX/M4DQMXgsZTYkhk0HWnTluf9auvq9/oT8WmYTCa2OWqTqoax3tEgs0fhEevr3JDxrXPsqOVa4HJVZoAxu/NqCa+2CDK8lkYur/2q17gsN9SvyTcPtfVqt6pGHrO8qHuNQQku2JBd9nkBpLkVMzl5OZ11Nv9fCwAWixWzw/PrSG+ZLcAjpr993iergEaK2co4220Bn3slFFPyFuBzzH+VhpsCFITQc3Cxz66Hjb4/ZwDYzJRVfCx1dRPmK4Nv9Qg4vwv+HKLTGVy0Y0bY+1guuJbVuu/lluMKtwB758LSD4IevvmQ296rAbMHVbxel93Ckn/d7uGww4FF3pdmZbUFuVTWo6DM2R1w0W2J+WydbOCtbr842DgGRt0AJz2LozCyMSwZ6vuhB5doRTAy2bJ/37WkwuqR8M8w/esXvQn/DKPX6rsAKMNlOOleRdvtc2cM9aw0bNIJ8IW6/dKogKNn649cot0nS1m693zgwZkkvidEwZMAnxBCCFEAhncYztoH1tKlahe6VO3ibI++6SYimjVznsfUuM553POeN6j09dfUnP8XsWGxNP76p3ybj6rArLYKr/7PFdDJCIWIpk3z5f5R6Q5ODh9GmSTt/JUZDjrt1v67H+72nnHEz66MFHtGGl9v/Zpq51VqnlWpcU6l3V7tGtu5c7yw/AVeWfkK3Wd6Vih94dZKtK1Ziqd7uYJYH2/4mPVn17P53GbOp2pvSPr+2ZcOkzuQZk1zBvqyCzd5FqBIDPfe58zmlmXRuFIsDSrEAPDyLXW9xg5oX93j/OnOtRjSpbbzfPELnkUdwkOMbHvnZna8dwuRob737TP4Wd7240MtdFpz8Far7RCo14PtqlasZLdak0YZP/O2baDfy3o1rUioyfD/9u46PIqriwPwb9bjrsRIggULHjS4S3G30hYoFChSwaVCkZa2WGn5sBZKcXd312DFNXg8WZ3vj83uzmRmJSFAA+d9nj7NztyZnd3cDNmTc89BW814VFTPQSqcIZUwOPJ1XQAMEuGDtupxaKyebD7mESseLM2CAsO1/MypbYZKvKWRn2kG4tMOzdFDNQOH9OJ16+JLCjv4ztc3xQ5DRazVVwMAnDZYvh92GxwA5uO4GDtBiocpGpw2FLE5BgA0Wi2SVY43PeH6WtvH/LUuu0lEltaAiboeiM4SZkm+biGMY9l31khu7s6nK7FoKj1ue8DVLYiU2A+Gz5DPsjuGR6e21FhzwPUdnCXInJqj0OQhwAcYaxo6qI90C3D7oDGA9Oxq7p9Ll4UnL1Mtjw06YJPYkvvcBfjKSDhdZn+rBcytzh+gz3EeBafRU9Id4Oll8feBm/GZ+hjYO9kYVH1+A/irrbEJRnZ9w6uPkvjHJl4Ado4D9k8F0p4Ig243+HP4hGoAMJ+TectttsEwxucykYjc/5Wc16ROEe5/jTrNO4qHyVnos8jxzF7K4CPkzaMAHyGEEPIGMAwDpQN1uFxq1ULAqFEIX2pcTuneqCGUkcZldqrChRG+dClUZcvAOS4OUTv5nQ2LHj0ies7bIk1OXbMAVsLgdiCDdXHGD1qxY6ZhUu8/8ai8MCiSFy8O7s3V+OhHQOe9ekz9nx6TF+rxwwL+crQSd1lI9cZPDAzLot0BA2JvGODqnIlKFQ5jyb8/msd6JemgbdUba8f3RP2V9dFjSw/cTrkNjUGDS88vIUMn/kH5aSY/KKE1CJdURvubPmSxiIhZjkIllmD/F/FoFVsIWbosJGUlITE9EVqDFkoZ/0PaF42LY3ijYjj0VV2sGBSGp+pbgvN7uSjg4SSHu5Nwuei6AcYPtV2qCJeVMQywoFcl1CqavZyXsQTDInydBeOtavQt0OkvcBbZIQ3O4BaELx/myTukV7UIlAz2AMuyMECCzOzstaENiiLIw1Iv8BRbDFdYy7Vzs9xyvBqs1Mfzttxn/fGS07Bjg6Ea3FUyFCpaAR9qR2CjPg6j8SnnFAzAMLhdby6GafrhPmusb7hVXwkAMEQ7EBFZS9FGMxEDNIMwXNtXtN4dYOwI3EA9BSWy/ofZulaC/c8N4h2WTcHAObqWuMf6iY7hWnQvAJlaPXbpy/G211ZPx0hOAE/MPU43Y12Opi46yHDC4NgSzfzSXHrU/qD/moM/YpH8e7vDSkru2B3Ds2FIroYzGk4m16EZwLPsEghax+ofCujsZ1Pqs+sdujGZwMJmxqXGHClZ2feTnI01BEt01ZBxMxwNOmF9Oa786hZrCnipU4GHZwGF7a7nPKbXsPJDYO/3wNKOwAvOvVltDFgqn1/mH/f8huVrg1643NhaZ2MxlzfwA4J6zvt9ZBaQ9pTfsCQzyfFzO2jlqfsY+s9ZmxniufGma/Dp9Absv/bUMldJgZeU8ZprZb6DKMBHCCGE/IcwDAPv7t3gXL6c6H7n8uVQePlyhC9cAEVIIRQ9dhSB48aiyJHDkHp6ImDUKMiCgvDwi84AAGWRIlDOmwZtldII+mYSfPr3gyI6itet9+M5B1DiymV4NG8GRiJBnb+2QTplzCu/Ftc8rNBrfcT6B4IJf+nReZ/xg0fsDRYdDhow8h8DBu0ZhPWH/ofRy/Qod90Al0wWfTcb4JsCdN1rHG9qqgEYu/SmW/mgvPLaSiy7soy3bcapGfjn6j/mxxXCvfFr53KY1SsQe+/vxdFHR9Bha30kZSWh6eqmqLm8JhqsbID+O/sDAOqXCIBE+Qgu0d9h1TVj/UYvFxYf7uiCdhvaYcapGUhMTxRci4eTHAt6V8L4FjGI9HPBnK7lUTrEmC3YtUo4dg61BMB8XRW49X0z1CnuD2eFDH9/EgeFz37zfpbzwbNsiIfguZZ+bCy6JZU41sV59af87Jly2QG/LC3/g6HBYP8DXmv1BEzQOlavbK2+Opbp6uAzzUBIGOPPy8imJaCGAgO1g3DApSFntPG1SGJaYZWhFhqpf0A99VScZaMF591kiMNKfTycGOGHiYispaionot/2RBkQiXoQgwAFw2FBdsA4HPtp6icNQt7DbFIhe0gaw31z3gIX/x59C4yczzHbTYIS/W2l0jf5DREyRngA4Ak1lWwjbw+2xMSoTewxuDRuaX2D+BwYnMEhn7P/t5r8hjgc0AacjTuWdqe93CVqe7o7DjLxqdXeBnNAIwBPpYTtDPoAKlIbUt7S3QvW+kia40pwLegCTAvHribiwCzqeHGnewGT4nn+c1asoNtbM7GHNxOuXqNMFiZlyXVOa9p9SfGpb7LOvGbhOQiI9RRw1ecw+rTD7D2zAP7gx3wpjP45h24iR7/O45ufxx7s09MXoulx+4iduIOzN573f5gYkYBPkIIIaQAk3p4wKtzZ8i8jDXLvLt3Q5E9u1Hvw7GI2r4NEf8sR93izVBm0T/wbNcO/oMHI2rjRsDd+GG/dXRr+Djxl0kyDIOw6Ng3/VIc0vKYcfkuN3j43QId5szWo8xtFl+vMGDOLD3K3uZ8ssjxKePay2uYdnKa1ef47th3vMfzL87HpKOTeMt6m5cJwlfHepsfp2vTsTBhIS8D8NijY9DoNYjyc4EqaCUk8hSMPzIeAJCiSeGdv8HKBrifamncodar0XZ9W+x7Phu9qhfG7mG1ofK4gurLqmP33d2QShhOJqEwMFc0wA1Sp7vmxw+1p8xfT25bRvCaq0X54ujXdTGzj8LYWRjA0a9tB5Tql7A0g2hRxth+JUvL/4B7LOlP9N3RF+XCrDccOcMWwQaRZa8A8Ij1BgAc1hvr+ekgw9e6j3HBqz4SJhhry7mpLMEDKXfpcvbXni7G/elwwg2R5h1czrDegdUkK0eW3059Oaw21BAdy0KCJzD+bKawwoyiHfryGK7tiwbqKbifneF34UEy1CJBRAA4LxJI7Kv5HC3U3+AhLB2YzxmiBOMM9Gu/wDhtz9d27k+WnMLyE/eMzRxySWHIERhSZzeRyOsSXQfYC0DLTPeYNH4NNllSjizkzSPgrOPUMTToxevJ3dgNHJltvc7fixvi263JzrJDYnYzn1v7HD9WLHDK3aZTA6mJCN2XY6lxCicQdnOvMIPPwQYiokwBvuvZmZQPTvIDfOpU4TH5JDnTegack9x66QgAYDn/3r7pFbqrTxu/H+fvW+8w/i5iWRYTNiTgt325/Jl5Cy4/SsHh688cGjtyjfFnecrWPJQKeI/Rv/SEEELIO0oRFgaJk5PovuXNl2Ng7ECMqDRCdL+yRAl4tm8vuu/EYEvgR+LhDl2vNq9+sbnwzWI9fDm/v0fnSH5T5fhs0n+TASXusnDJNH7c+PHUj8iLRQmLcO2lsbD7rru7BPsfpj8UbKvwZwVsfDkYjMwS0Dv04BDUemEgaebZmdBkf6g7kXgC115ew6p/V5k/MA3aMwhp2jQM3jPYfMxHNYwBn9HNYszb9AY9HmRchbuneBZGYV9LoGl4w6Lmmn0nnu3EFwc/R/sNxu97oIcKp0bXx/T24nUZ6xQ3BqQ8neWQZH/4jy/GX4Z6Lm01Dj88jE+b6LF5UE2sH1hdcB4A8PeyXFN79Vjz1901X+EvXT0M0/bnjY8N9YSTSI1CiUgGopvSejflnI4bhHUUc+Jmx50xRONj7TDecmZrUkQCKBKwWKmPx79sCOpw3jtrS4Vn6j4QbNtmqIQLrHEZfxP195ira45JOmFGpAHi2Zlq1vL+3DPYX0acHx5Yqb34Jm3WV8YxQ+66LOfWtoREGDaKN3QBAA0rHixh1Fays67vEN+eBznnejprbbm8kYujP0ePLyDu2lTLY4MOCKkoPnbb1zaXsRpk4v9+iZpbA9iSh6YsgPg13OF0mtZrgK1fQZVsI4CyYRCw+iOruxkH6ntyPU0WCeBxl1q/xgCfLMd99EW6JXPRTWV7HvCStt9wCl/O635fXHqUggWHbuP7LVfe9qXY1fSXA+jyxzGcuP3C/mCSJxTge4NmzZqFmJgYVKpU6W1fCiGEkPdcuHs4+pbtCzeFeGYVwzAImjQRfkMswSStjzsCvv4K3ftZuqFCb0Dpr76FPExYE45LHpo/df0AQGYAOu93/MNSnQssJvylx6Qlr1brafa52Wi7vi0epT3CvPPzBPtvJ98WPS5F9wgSueXDWL+d/bD2+lrBuE03N+G7Y9/h0vNL+GzXZ+btq/5dhX47hF0in2c+hzJgE1Z8FokWZYPN2+dfnI8um7sgQyceJFDJpVj+SRyWfxKHgXWLoHEp49LOHXeMwYMXWZZfvH1clWhbIQSbB9UUnKdTpTDM7VYe24dYGoU0LxOMYQ2Etd4kEgNigt1RJsQTu4bF8z6IfVo7CnO7W343usupJXedDcEoXR88Aj8g9Fk98YYVUX6cLLnsIvuMjYYkOW0zVIKu/Z+4bDDO1/V6YRfeJLjiliEA9wx+aKsZ71BwDwBuswGCbdymHnuuWrI/5+mbQ88yOB/wAW/8STsByMtsOCbruiAFwmzBnfoKgm2LmZY4brB0922vGSsYk9826qvAyYFMydcphXXCp9rBuMKGoYX6G/yuEF8ivkjXQHS7I7yQgp63RkDy/F+rY9bohT9XAKBOt9LJd/9U8e0cgzQD7I454N4Cdwz8+ShYopuDV6bjdQe90yyvmdVrgaR71gfbWGrKMrn8qHpsTu7Gm4gF+E5xOrrr1MCVzfbPc3mD1V2NJI43pwCAFceMncbBcILA3OvM5yYb3Mw7mZT/vs/bf9P8tb0An97w9jL4bDWfepdxMy7Z/3hnE9Pl7bjkeDdmkjsU4HuDBgwYgEuXLuHEiRNv+1IIIYQQh/j2swSWAtt2gnfPnmAYBq7xxvpvnh07AADC5v1m8zyudWrbfa5jxV7vL+chz/PnPA1XNcTlF5cF28W2WfPHhT9Et6/6dxU6buwIHWupSzXhyAQceniIN27MoTEYsX8EFl9ajE/39ODt+/XMrxDj5n0ZH9csjMT0RBx5uQRqRQIAY2By6N6hOPvkrNXrjQl2FzTWkEoYNC4VBH93fubPwLrR2Z2FOQFVzrc2ys8VAZxjvmhcHGFBQUB4DSCsKh5nL2ktHigefC4a4IooP349uZ87xaJZmSBMblMGaDIFCKkMVLMESc2NRwD81LEsvvmgFII8VBjZtDj4GMhKtkAfzQiM0fbCcC0/sDq6WQkYIEFDzVTU0UzP1bLXTJGGIpLsAN+opvxMsrtsAD4O3QDP9rN521/AHaO1lqXhucm4W22oYV7qDAA3DEGYKesJNSxLnBPhg/m6JubHXTVfY7y2B0ZpP8QkbTccNZTAZ5qBOJu9BFjPMiiZNR8Vs+YgImspIrL+wg1OLcCc+mmGYKB2kNWl0Mms9WWiq/Xiy6BN1KxInTcrHrPeME3KC2wkTqYJMwpvGQIEdQ9ZxvbyRBMDy2C1YhzqSs/aHJdupcmMi14YvHn6Ismh5+Z+P5+H1Bcd80QVgYgcf5Rxgu1i9jHnvgWblfugkv7WAeDmHusDbAT4pHltKmJXjn9rNGnGpcISK8Gr/zXk1+TLgzAmdwGN9PTs5djcBiX7LB3IYS3LM4+49VNzZsJxSy+4qmz/nBk4AaZ0dT41UHHA7WfpeJici6Ym7xCd3vKe6xyoe/tfoC8g11kQUYCPEEIIITaFzJ4N9+bN4fPJJ+ZthX6cjtDf5sJvsDHDTxERgeIXziNkzmy4N2smPInI73KawsEIX7YUMn9/MCWLQfWWknp+qv0T+pbp+3aePI/WXl+LE4nGPxhm6Y3LtiYfn4zSi0pbPyhgEUY1i0GDlQ0w/+J8DNg1AOnadHy+93PsuLMDL9VWsoayTWlXBqUKueO37sJMMC6GYbBpUA2cGGPJTpJAgvU31uOX07+AZVlhFgjDAL02Ar23YNew2pjXvQIqhHsBjAZK/41Q+FqWRC/oXVnwnK1iC2FWl/LwclEAVfoCH+0AnDzN++M5Ab5GJQPRLS4cR76uh09qRaFPDWFdu1/6tcBO15boUYOfjWj67KqFDCqlCt+3sfF+i2ignoK/dPWA4s0BAD/rjMvbaxb1xYyOsbyx5xOzEObrghGNrGfttdGMd/i5WUjwidayXHS4tp9oNs53ui44aSiKjfoqOGQohYX6xvhLXx/z9U3RSTMGGwzV8KXXDJTI+h/KqP9AOpzwDKbGLQzOsJbsSlPHYpNthooAGFxiw0WvsZ/2c9Ht91TFeF2CxbTVjLO5n2u/gV+HUqz+XDvNeEFDlSzWsWWqEoZFYYn9gA43wPePztI0x099VzC2z9TFDj03dwl5+xuNsVEfJxjDShQo32UCb9seg/hSfBOpJg1nfunk0DVwyU4Is515LqzI9TlfmX+Opdnz4oFJvtYbfuQDHRwvFQAATkz2P4hiDUqAfF+im6GxvPacpQ7cOfcJ91xk8P2w9c0sGc3S6lF72l4kZRSM7rlLj921NK3JB9yux/nVAfl14GYXGv7jmYYFWe7uNIQQQgh577jVrQO3unV42yQuLuYsPhNGLodbnTpwqVIFjEKB5DVrzPukHh7w6dcXUjd3eHbsgBRGDS8XY9ZM9K6dgESCC035wZLCq1fhhOEWfNsNz7fXEviCRaK35cPLihYrUNy7OOqH18dv521nIf6XnUw8ib8u/2V3XM76g8uuLMP1JMc61EX7u2HjZ+JLCnNiGAYsY/mwZWANGHVwFACgQkAFfNu6NDr/PR3R4feRpasHlUxlbooR5WfM0Ft//jbciluWjGqe10L94oVQyDMXdbmy1S/hjylbryDQQyUoEj+meQyWHL0Djc7ywahihDeOZDcZ+eOgpYlAk9KBiA5wxaSNlzCtfVmUD/PCtoRE7OUsrxXSQaJKhCErGP+yIdhbdCS6dqyA0l+vNAeWVDIpKkZ45TjO+H50qxKOqduucrZaPhg9Bf+YLlXCsPSYMDhkkgZnRGQthQpqZEGJCs4KIIk/Rg8p2tkJHAZ5qnD1sXj22TfarrhtCECxcjUx9KQn/pVaMkxNy5kHaT/DF7K/0VJ6hHfsI9Yb9wx+CJVY3s/ThmjsLPYzFh17gMGy1aLP+Y8uHpkQaeTAUSFrDgKZl6glOY+l+rq8fakimYMZUELF8rPatKwETvmYaKzn5FpsNMShA6w3higncezn1AAJKmbNgR+ThJtsMET/uiKVQ+rqi8uyEiihM2Ye3xFZQs7lm3SO08YlH93ab39MfvOOBJ5ceqNPWZyx/nMpxh/Zf3CxllWYzwG+TE6WHjcjLEurx7XHlmxBpcx2flBeAzcsy2LThUcoU8gTYT7Cn8cnKVlwd5JDJdLk42WG7ezT/5LnaWpz84jmZYOglDmWFWyLlvP94n79X8PNLqT43utDGXyEEEIIyVcSZ2cETZoItwac5WEGPfyHDIFPnw8hdXU1B/cAY2CQkUqxpqrxk7MmMhglrlyGKiYGNUuJZANyeHbqmKtrG7PNsrTzcOfDKO6dc4kmX3n/8rk6/6sel1d/Xv7T6r5Al0Dz1wsuLuDt+/n0z6LHJKYn4uqLq5h8fDJ23dmFuefmIj2Xy+UydZblUqYsQ8DYxbhCuBeUgetwT31KtB4hADzKusZ7XL2IOyZ9UCpX12AS7uOCQ1/VxS89gtFjSw8cfnCYt9+R2uzbhtRCiJcz6hTzx+5htVE+zBhcG9m0BNxVMjQvE4TP6kYLjlMFr4RL4ZlQ+OzDXx9VMWbqMQwva0wll8I1RxMD0zUpcnyg3pudaZWmEGa0fftBKVyZ1Bi3J9v+ucnKDoa5qWRgrTTfsMXLWbwJCAAkwQ2xXb9FfPMu0EKGr7V9oGWlaKMebx5zn/XDIO1nKJ61AH01lqw9bvYZACzV1UFnzWgwzl5oE1ccW3JkBP6pq4fumq8wRtcbmawlwHfWECm4Li1kSGAjMEffEsngL/EWqz+XBYXgenIuY+2oHoMyWb/ztulZx99PHafJht7Ox7KvZMss1+sUbHWcDHo8gwcuZ2dJSkQCfIkuxqzQFKklQGyABPfZ1xLC++9xtR3MfB06yHLR1RdAAPPCuGw400ozgnwO8HGDehqdJdjX83/HsTXB0snK3hJQQx4TyDZdeISBS8+g1lT+cu5p266i5NitqPzdLjT4KXfvoSPe9FLRDI3lvbX33CzL4oetV7DurHjDLBNNAcjg0+oNvMYalMH3+lCAjxBCCCH5jpHJEPLrr/D9bCDk4WHw6trV7jEXCkvQ/1MpXJbwC6Uv+1C4hNIkaPx40e3evXohePo0wXa/28loGN4QH0R/IGgwIhbsG1JhiN3rzum3+r+hRwy/Lt7RLkdzfZ7cEOvqa9IwvGGuz9dgZQO029AOf13+C0P2DsGss7MQtzQOyepk0SLeV19cxdQTU/Eyy7LMN4vT8ZHbuGPxJf5Swwxdhug1dKvMb6TxfbtoBHk4QWfQWW1oYouvqxKjDg/D2adn0XenZUl2YnoiJE7XAegg8ziJR2mPeMcdH1kPO4fWQjFOTcALTy+g9brWOPzgMIoGuOHoyHr4tXM50YYeco+zAACFzx5Uj/YV7UaqkksE200F43NmzFQoWw74PAFJffjZb4Axc1Isw8UaN5Uc50WCYfZ4Oluvw9W1ShjqxwTAPbtW1zJ9PRRRL8ZpVth8JQtKXGGNDU0yWCVesm68cNRI3cdQQwFnhQzOSimqSxJ4x7symThgKAM1FMjkLKcdqRV2M9XC+vsilsEn1jxFzlg+nGtZKY6xJRDgzw8WqWE9+LlGb+ki/YT1hJ5zTfbqOToxluDizJRaVscpwF9mKhXp3vrczRjgS5V5m7fpWQlaqL/BKYN4A5t3ipv1OpH/FV5IBY7Msj4g2UbjkjzgBu64AaNjt/gBRntBKX2Ofx9WnrqPZ2lq87FtZh9C3yXChiMnb/NLRCRnamEwsJi55zrSs4Ni916I19jLa4xu1p7riJ2wHdcev76OxADwLE2NpcfuIk3N/9m0l2039J9zmLP3Bgb/fdbmuCwr2Zf/JZM2XkKX34+ZH1OA7/WhAB8hhBBCXhu/AQMQvW0bZD7CIvY5rW21Ft+3+x3RPvxAQI32g7GtHIPnwa5wKldOcJwiIsL8deTGDSg0Ywb8v/wCHiK1ABVlSmN67emYVH2SYN+P8fzlq0ubLkU5f+Hz2RPrHwt1joLsLnIXDK9ofanx1PipiPGJsbr/VXgoPewPclCNv2tgyJ4huJNyBxlaY2AuWZ2MdhvaYfGlxai1vBYWJSwCy7K8DL4pJ6aYv36W+YwX/JOKNC7YcGMDJp8dwtvWbE0zZGgz8MPxH9BibQvMPTcXALD77m60WNMCCc8SBOfJ6U6KsBNoo1WNIC00D06hS+AUvBINVzWERq9BsjoZV15cwa8XvsX55B28wOanuz7F9aTr5kChs0IGhmEgtdHFkZHwAy2NSloCQyq5FHKphLf82JTBJ5EwOD++Ic6NbYjjo+rhxw6xuKpPR0KacYmhwnsflEErkXMp5pyu9rNIu1UJwzx9c0zRdsQvRRfYHW/i6cQPYnEbsJg+zPNZf1/usIGop56KhpofkG6lk6uTXIoANxXcGX4w+KrB2CiCYYAMTj07jUgVIq2NykSpOZ73O21nANabYBifwxjAXPoxv8adM2O9mOgRg+VnvJn6W15Q8hHrLXaIwFxdc15W3seaobwGIwrw65CJZfDJs7ukpsssGXxayPAS7tirt12Lr8CRiASjXez/e/S27NQb/80pw9wAdtqoK3l5PXB9p9XdLMvmqqMqN+BiK/BkN8CXY//wFecwYsU5AMCtZ2k4fTcJ2xIe80oiAIBcarlHRH69CWUnbEf/v045dO1aXd6y1qZuu4pUtQ7fbLLeICtLq8e6sw+Q9ArLgHv+7zhGrrmAUWsugPtPhM5Ott2aM7Yz90zedg2+nZceY+y6izafe/ER/r+91GPj9aEAHyGEEEL+E6I8o1A1uKpge8PCjdD6ty2I23kEEcuWosjhQ3Bv2hQhs2YCACJWrkDE38tQPOEilNHRcG/cSDSTCgAkNn4BDXUPxeDyxqYh1YKrobSfsSagaYmrhJHg8wriTQC4nOXOSNNaahbF+sUCALqU6IIKAcIGFXVC66BxRGMsb74coW6hds+fW+4K93w93+57u9F8TXNUWVoFXx/4Gn9f+Zu3f9rJadh3fx8vwJfTvvv7eONnnJrB2z/y4EjRzL6Tj0/i76vG55t1dhZuJt/E4D2DcTvlNvrt7IeDDw5iyaUlWHltpeDYpxn8OnknE0/i59M/w8Aa54TM1VLnru+Ovmi5tiXab2iPtdfXYtzhcei/sz823NiAFmtaIEmdJPq6Iv1crL5mMDpce3nN/KH7u9aWmpOmLD1uQ41OlS1dTt1Vcng4y+HvpoJEwqDdhnYYfnAAShdOgTJgCxSeJ9GrNj+QkbO7MdfRr+th42c1UCXSB2ooMFvfCj5RFTCvewVs/KyG3TqHKrmE91q5HThTs3LfqOAGWwj3s5toiC0ZVsmlqB7NXz6axqrwP31jAMCmz2pCDTmSWWcYWAb3WH+0Vk/AUYOlmcI3rWOtPj+38+wJQ1HM07fA2OYxSIMzemq+FD1G6eSC25Obwc/Ndu0/rj3ZwZub0kg8hRdSOJmDt9kgDNX0s3aomRYyHDNYso13GCqipHq++bGC4b//3KAiAHTWjDJ3Sc1QWIKKKdlLxjVwvBtxQWCQCjMqU7T/3Y/A9prJ8PzZFtAK77N9Fp5A4a83o/ykHTh7L8mhU3EzvzI11rvf2grw/XHgJv48Kvwjyp7sGqXcenMpWfxAtFRi+Z6YnmJbgrBRTc7AIADo8rouOJutEg0/7byGwX+fRc//Hc/z+RMeGrtPb7mQyHv/8qteHv+cbz7A99Hik1h85A7+Pu54nUl7wU17WJbFhA0JuXrO98V/9+5GCCGEEJIt3D0csuxi4zJvbxT6cTrc6hmbIEhdXeEUGwtGKswEi9y0EX6ff47Q34zZXvqkJJvP06tkL8xrMA8/1f7JvG16/HSU9y+Pv5r+hV4le8FbZfxQ3LGY9fp/3Bp8X1X5CgAgl8gxIHaAefvBTgcxLX4avqvxnXnbnPpz8FHpjzCx2kSb1ylGLHgIwByofB023tzIC9aZ/HP1HyQ8t55RN3wfP5tx/sX5KL2oNI49OmblCKPlV5fzHh+4f8D8dZI6Cf139seUE1Mw4cgE3Eu5h/kX5iNVY1x+tezKMt6xvbf1xh8X/hB9npOPT/KWFQPAoYeHMPLgSNxOuS16zNbbW3EpawmGNyxitdNk2/Vt8cX+L7D//n64O0nQt3ki5vYKMwekDSwLMFrIvQ6iUazVt8EsOHKv+euOlcJ4+7hZdQDQLc6yP9BDhVKFjJmd9UsEwFUpQ7PSQWhYMhClCnkgvpgf79i6xflBB5VcyksY9HG1BFFyfnDPrZwBPoYBYkM9USzQDQ85WW4rI8abl8Mas38YVFHPQmn1H1BDgTNsEUzSdjePb1JaWLeufgl/tCwbDG6GoanOXrS/sU7fPkNZ/KYzZgOfMFiyi1mZ9QAq112D5b18Ck+UzvoDY/yNf5xIAT8gvNpQC0MkX2GXvhzi1fyMYhMNK8Mpthg6a0ahWtYvAIwdWjfpK+MZ644ten6X6cX6Bhih/QRN1N8jImspjhhKQpadwZehsGSyJbPGa7GV6WhLzvqIeZVf5zFJ1gr/XRi94V/eXHpV9dRT8+1c91k/+4O4OBnQP2y9goivNmHXlScAgJcZWgxdftbqockZWoxddxFn7yXxMviSM7XQG1isOClcBmwtwPckJQvfbLqMn3f969BlX3nEXxbLzeCz5e4L4x9+HiZlYtae63iZroHaSgafo/X1JDYyr7ddNNYfPHc/2aFz2WJgWd5S6PwKxnEDhTpD7jI381NiSpbo9quJwiXQYoHa3Dhy8zkWHLqNr1ZfeKXzvIsowEcIIYSQd5YyKgq+fT+Bc1wconbuQOG1a2yOl0lkqBpcFc5yS2ZNGb8yWNRkEUr5loKEkWBr26042uUoRseNxtz6cyGTyDC84nA0CG+AqfHGD3rRXtFY2WIl9nXch5I+Jc3nqhhQEQNiB2Bu/bnwUHqgUUQjuCosBf/D3cMxuPxgtC7SGrVDavOurUPRDojyiDI/7le2HzoW6wh3hTv8nPzwv0b/4413ljmjenB13vO/DheeCX/BPvDggNUGHrZ8tP0jbLq5yer+/ff5HTennRTWWTRpuqYpZpyegU+2f4I0TRoUIpk8+WnEvhH488qfiI68iapR1pcAbr29FQN2DcCiS4uw9MYMfHm8m3mfgQVkrpegCtyIdhvb2H3Oww8tzUI0Bv4SMm4Wq4eTHCOblkCPquFYlmNJ6e89KuDUmPo4+ng3emzpgcT0RHzdpDhGNbVkv0lzpLikZGp5H6q//aA0KmV3AW5U2gXTT04XXQ7tiO2GigCAewY/TGtfFms/rY6YYGMWalfNKGzUx+F3989QqHJr8zFyqQSf1Y1GFpS8Zb4JbDjWGmpBW+1zeIjUDfyudWkU9uUH2ZxgXGIr4wQcljj1wCDNAPTVDDVvM8jEsxwP6PmNYHprv8AxQ3F00YwEAKTCGSqlcS4+ZvldkOd2q4Bj8sroox2BO2wgmqm/42XrAZYMuyOGknjI6Ws7QDsYceqZgqChDjKs0Nc2N90AAHn29zNFWci8zXSc2PJmR/TXfo45uhZ5OtZkn74MPtN+Jti+3fUDpPrxl5xP0XbAU9YdW+0EBLNY4evRQMZrcGJVVF3RzSc5gd4D+lK4wRYSHZcX9gJ832s7w8Bp5JLE+bGfs/eGYDy3PltO32y6hMVH7uCDWYd4gaekTC02nHuIESvPC46x1mTjZYb9wD434NZt/jEYOI9lEsfCElsuPEJichaqTd6NqduuoukvB9Dsl4OCcXP33UDZCdtxKTt7zhZbAb4AG5nQ1rAsKxpcNLAsL1PSXsMS3rE2xuo5GYwZGj2a/nIQ/ZY4trw5PzFWSjE8TxeWLdC8YnAzL5ni7wsK8BFCCCHknSdRKqEICYHUzc3+YDucZE5wkRs/DFcvVB3HuxxHz5I98WPtH9E4orF5XDHvYuZsPxOGYdCvbD9UL1Qd9vQp3Yf3+LNy/A++A2IHYHTcaGxsvRHrP1gPCcP/tW5Tm02YU9/YsKRbiW4oKL468FW+nu/i84uIXx6PWWdtFKzPR4/TH6NCuJfdcfvuGTMfTUuEAaB4oBskKsuyNLFMDJ1B/IONWme99ptGZ4CzQoaJrUoJgo8Mw0Apk2LE/hE48+QMppyYAjeVHB/XikRkdvCrRVl+9ptGb+AFDsJ8nLGgd2Us/agKzqnnYGHCQvTY0gNVI60HOs+NE2/+Ml3XHiO0n6CtZjwqRXihbKined8vA9pjd6kf0LzPKGSwL2FKI5RJGd711CziizrF/FCnmD9ajF8PecPxAIDFH1qy2yL9XODvrhJ8+FRDgaqRPrwP4r/1ror1hup4Actyd2sZfCO0fbFY1wAA8K+hEG6whdBRMxaHDcbAX6lC7nBSGINLF9jCuBLeBSs8+2B6+7JoXCqQ1zU5gY1A1+zAoIn1ABwDnYPBOVMGX7JrhHnbS9b4hwaxDL7h2r6CbWIecLrwPmE9HTrGZIM+DgO0gwSdiwFgpf9nuNZ8FW/bbP0HqKSeg+us9Y7CAHDCUEywTQspTrLC7QJV+otuTmUtwd2LrLEJ1D8RE+yfLxu3ZiIA6FjL9/wB62O1E3Nc1q/4Td/CHAQHgNiJO7DkyG2rz/UwOQs7LgmXugLAVU5zCW5AKilDg4sPxDPWrDVHEAvi8I4zsIIGHC8yNObAlczBDL40jQ5d/7A0rXqULJ41NnnLFaSpdRi3/qLdc0ptRES43c1/3ulYduJHi06i/o/7oNbxg6sGlr+cODfLVLU2liFzM/jO3UvC5Ucp2JqQ+EoZgmqdHhvOPcSLdMdrD1qLk4oFO181g49b7/ZtZSz+V1GAjxBCCCHkFcilr6dmVax/LPZ33I81LddgWbNl8FR5Il2XLhjnpfLiZQGa+Kh8zFlcfcv05S0bnlJrimD8m1DINf8yXXIjZ3ZbfsrZ1Vdr0KJ39cIY3yIGO4da73bKrVH49YGvsfnmZqhl1xEeasnC0RqEWTEavfhrydJn4fDDw6j7T10M3TsUzzKfoU154/v9ae0o3tjH6Y8x6uAonHlyRnAebifkzYNr4sAXdVDc1EGY0SI6+hSaxrqgfUVjvci4SGMQ21UpQ7VoX5x+YswceZH1AjO7lEPRAOHcDHBXwsNJjtuThY1w1FBghb42nsBLUAuwdIgHfuwYi9PPd2P0qQ5Q+m8BYMzgM10PYKxpuKB3ZSzoXZmXfVirqCU7qkph43U3y166u8nZmBG4UN8QWr0Bvq6W2noxQcI6lhp/TjOKiJoAjEt4E+GDsbpe6KwZhY+0w3gBuxJB7vijRyW4KExBAwb3q4xH+yE/om2FEACAIke0QQcZlujqmx/n7JKbF6aaj3KlM7ppvsYnms+RBOP3OIMV1hRcqY936Lx3OfXjRmt7Wx13XCTodt4QiTQ4A2BQS/0TWqsnYIGuEdqox8NJIYVMIsHPOuP3yBBr+oMFA61Ihh7XeG1PzNG1wLWSQ8zbNJBjvLYnFmUHYsUYouph5rEXovu4WZIyGIM4I69Y7/aeU1n1PLTSfY91+mpoox6POprp5n1JcIVe5CP6OG1PJMIYMF+b3Y05wWDMyhyzLgGXH1nPVPt4sbBrLQBw4yLcIMyTFDV8rdSWtNal1V4gKDVLJ8hC23DuIUqN34atFxMdXqL7NEWNG0+F/w5aY60eLzcoJGEY3H2egetP+EtJZ+7+17zcGTDW43MkKLfryhPcepaOdWceCvZx3z97WWxBHpY/Ijja+IT7up6n5f3fvVm7r+OzZWfQed5R+4PtEMtUtLasesbOa6KB1Jydjrn39dxkQr4PKMBHCCGEEPIf5aXyQrRXNEr5GrN/RlcZDQDoX1Y8s2RGnRnwUHpgdr3ZvA82nipPLGqyCLPqzcKYuDGoF1bP6nOOrzoeCokCLaNa5uMrMXKysqzRmjC3MPuD3rJfzvyC749/b36sM+iQmPEAH1TwQrS/GzyVnqLHXX1paeqx8eZGfHngS/Ta2guJmZalrdwA363kWzjz5Ixo0A8A1Ho1+u7oi6eZT7Hjzg6MOzwOP7Qtgw0Da2BAnWje2C6bu2D9jfXosaUH9AZ+lgnLKa6nkksR6u2MwOwPmsrAdXgsX4EJx4fj8wZFMK97BczrUZF/POcDpo+rEsMbWoI5k9uURhF/V/zZp4roa7gyqTHaljcGuvrXjjJnmuU0+fhkAIDCx7hkWyZhUDTAsezcqe3KoFKEl/m6qkb54OCXddB42P9QKusP/KOvA62BRUywOya0LIk/elTk/Sy1Vk/AIl0DPKs6ynLS9guBht+in8bUhIfBEUNJ3GED4aywZKR9Xr8IAj1UcOPUaHRR8gNU3ICgyWxdK/PXzox4xhIXt0OzGFNWUvMywThoKI3tBssy15wdhb/VdgEAjNTyM4q5TJ13DxhKY4WuFn7QdsIOQwX8pmuGdSEjcM69Dm/8p5ohOGUowtuWxekofJcNwBm2CCboeuI0WxQqmRQyKYNfdG3QUT0G5U81Mo8Vy/jjegE3/KDrjIlnLK9LCxlS4IJxut5oof4GrXTfo7vmK4zT9kTZrHnAyIdYFj0Nuy4lmo85GNTL/PUj1pKZWlpyK/s6ZLhmEP4BY52+mmBbFpQ4pwvHYO1AnGaLIp2TEZjGOkHBCJfVrtLXNH+91VAJ99uuQyfNGPO2wzee23wfxHB/1rlBojsv0q3WrzOwLE7efoF60/di/zVL46IMG405AKD9b4eRquYHpydsuIQMjR79/jzl8BLdJ6m2MwVzEmsY8sXKc6g33VI7VsIwaDv3MOr/uB8JD42Zi9efpGLa9muCY5+Kdgm34N7/vlh1Hree8YOR/Aw+64Gpk7df8LITbWW8cQNcyZmW91i8o7ljNl14BMCY5Wl6T/JKLbJMXOz1JGVoMGPnv/hp5zVBLdfvN/M7HUs4AT5rwcL3Vd6KLBBCCCGEkDcuPjQehzofstoZt15YPdQNrWs1a6FWiDCj7JMyn+DQg0PmphjNIpuhZXRLyCVydCnRBQaDAV02dxEcM+/8PNHn+KLSFyjpUxI9t/YEAHxX4zuMPGhcZhjpEYnrSdcde7EAfqrzE7xV3jj5+CRG7Bth3h7kEoRH6cYPIB2LdcSNpBs4+Vg8SyW3hlccbrO2X04LLi7gPX6U/ghNVzcFAFzoeQEeSg+rXXft2XxrM5LVyagWXA0dNxqbuixvvlx0bJaOH/jZf38/5FIJSod4CMY+ybBkpaj1ajhLLDUnxZY7uavk2DeiNlps/BosgITnCVDKpGhYMlAwlhs0MLAG3tK7DhVDed2Bc1LJpZjeoSymdyhrdYzpmrmsBQLFtK8Yysv2A4AQL+PrV0tdAD2Lytn1BHtWixAcf4YtgjO6Itgo4WQmuvgC1QYidfMWQGdAuTBPPEzKxHMchW/obSRdag6wcvMHcVNNQYC/BBAQD/A9giWg5Az7H9pDvZx5jye1Kokx6yxNb0zfE+7yZ5MM1pI1FK/+EXdY4/d4qb4e7rL++FNhCWZ/oJ6IOtKzmKn7AADAQoIROksn4O91XRGZ5IJbz8pinEyOXrLtAIBn8EBbzXjcVnU1j33BWu/27aSQQi6VQA8pjrElAM5nf27X4NuGAERIHuMR640gxpR9Z3ytmawlgKjhZP1dYCPB6I3ZbAdQBgBw7aUBzzP0OMtG4S9dPVxmw/DnrQaoK/FFUeY+Vurj0U+2wfhewhK8GK/riaUKS9MkAJina4bjhuL4Vm6skcrtnGx+7XDDen1VaCE1Z1Ka7NeXxlFDiezsRhMGH+2SIJWzbQcnGOko7qpP7nLOLK0BU7ddFTnCGEzqu+QUnqdr0ON/xzG6WQnoDazVpbIm1x6nYcGh21b3O5rB99hKIwdrLjxIhk5vMN8jWJbFPyfv88boDSyeZgcOj918gZLBHnhmJfvtcYoank4KjF57ES5KKSa24tfczBkYPXT9Ge/xdE7QUGcw4EpiCr5cdQESxljL1HRvaDf3CO+4dLUOnk5yHL/9AjHB7nBXWVYOcLMKf9ppOX9KZt4bHnE7Hjf75aBotnVO1r6Dz0WyO8WWD3OzFLU5gnbSHAFg7hJdjc4AON7M/J1HAT5CCCGEkALEWnDPxFpwL6ePSn+EOyl38GnZT9GmSBs0XmWsH6ji1BYzNei40PMCMnWZWPPvGjjLndGscDOrAb4MbQYCXSyBH+6y3A9Lf4jtd7abH3cs1hHLry5H88jm2Hhzo+BcMokMvk6+aBzRmBfgc1e4mwN8nYp1QrRXNPQGPWKXxDr02m0JdQu1P8iG1f+uNn/9/bHv89xsAgAmHjF2U+Y2LOE21uDK0GYItr3MeglPpScYhsGcs3Mw+9xsDCk/hDfGWkZgTuE+Lgh1C8Xd1LsAgElHJsHX2Rf9y/YHy7KcTsCWD2Yrr61EoMSSvSWRCOemr6vC6odpa7R6/jU7GhywZ/vn8diekIjuVcPtjvV3F36ivDqpMa4/SUO4jwsSk7PQbOPneGwA/IML4cmDyuZlwaYOvQDgouRnoHk4WT64NysdBImEwYZzlqV+YgG+pR9XQbSfK37bfxMrT91H7xqF8cfBW+b9NYv4Yf3A6mg585Dd19WyfBiQHQt8ml1Hz1khRYZGj4OG0tCyUsizM8xuKIvjbFa06HkYxhg0u/ksHYAE3+i64TYbiAMGU1dvBr00IzBMtgJ32ABsNVhvluGskEImMncAQMoJsLXUfIN4yTncZf2xTjmWN467rDZnHcOcMe1P/zqN1uUKgYUEo3SWzMXdhvK46lkdz5Isy+s9YMnQOmwoZa5XOE3+GwAgE0o85wQvP9TwO4gbMRjEaS4yTtsTbaQH8KlmMB5AvOnGlRydSY/eFF9ObAv3ZXPnmC1avQFpnEy8bzZdtjGaLymD/3MulzLmoM66s449f24DfAAwbMU5BHqo8HWTEqJLibcmWIKjpvfEWsbc0H/OokSQOzadN/77M7RBUXg6c4LHOQJX3AxegJ9pqdUbg6V3nhvv3f3/OoV9I/jZriY1p+zB0AZF8eOOayge6IatQyx/rLOWbWkvq9IWlTz/Fnqeuv1SsE1seTL3VxcWlnsIAEFnem49x5y1Dt93tESXEEIIIeQ9NLj8YPxY+0dIJVIUci2EPxr+gdUtV1sd7yRzQpcSXfBB9Ac26w76Ovmam5AAgIfSkkEW6ByIs93PYlSVUVjdcjVGx43Gme5n8F2N78ROBbnE8jw9Y3qav64aXNX8tZvCmPEilUixva0leGjLx6U/trovyjPK6r7c1hBcemVprsY7wlp34m+OfSPYVmt5Lcw4PQMAMPvcbAAwPza5/OIyDtw/YH58+slpnHps6cC4684u9NvZD88yn/GWG/9z7R/MPjsbQ/cORb0V9ZDwPAHbb2/nZfAtuLgA5cKMx5jq6R1+cBiXn1uCAos/rIK4SG+s7Mfv7pvTzeSbWJSwCGq9GjqWv9RPrc+0Wmh9UcIibLixAenadDzNeCo6xqSwrwv6xkfBWSHMgTj8laWj6rKP4+DvJmyywTAMigS4QSGTwMnJEvjpVt0P58Y2hE92Xb9ATm0tlZwfAIjwsfzstK8Ygl87lwMA7NQb/79aIqwbVzXSB/7uKoxpHoPTYxqgkKcTwn0s2V0Rvi4oE+Jp87UDwLmxDdGtumXpbGb2stlZXSz1OzczxqWiL7xjcXJ0fcQXFQ9AzenK73qrgwwL9Y15HWf3GsqhheY7DNQOht7GUtuYYHfIrWRpSmEJFKTABRsM1XCOjcYX2o/RS2P5o0Aya3lf7TUjuf4kzWoGm4+r8T0xLUtequd32l2pj8defaz5cSarhBqW+1gG7HdlXaRvhFaab6wG93LLVE/ywL9PsfOSeBOfFafuC44Tk6nRC5aVixGLx+b8EeXWUTt5RxgEEpOSh+6p684+xG/7buL6k1S7f0ww1QnMtNJ9+ObTdHNwDwBSMvnXo9XxX2TOAB9vrN6ARE7moynQZ61j7o87jNl5OYO71urzfbT4JO6/FP7hx54X6RpczNF9+M5zB+oeZkfosrR6dJp3BNO3X8X1J6lI1wi/Z2IBVG4DF4OB5c0X08+dCTdrUa2lJbpclMFHCCGEEEJQJUi8Lpo1K1usxJUXVxDtFY1OGzsBALxV3mgZza/d5yJ3wax6s5Cly4KPk3GpYafincz7ZRLjr6NucjekavkfXLgBvuGVhqNFVAucfHwStUNrY2HCQuNxCsuStiDXILvX7a3y5p23dmhteCm9sOb6GgDWMySn1pqK+NB4VP6rsuj+3JBJZFa74ea3/138H+JDrDdJ+Hi7MNjZa2svBLkY30tTpuSEwxNEm5XsuLMDAMxzgOt+2n3UWlEZcTVK4I8G/8PN5Jvou9OY4XSh5wUAxuDN6DZu+GRHCzR51AROMidUCqyE+FD+Nbdaa6xDl6ZN421X+OxGrX++yn6NTczbH6Y9RJ9tfXA/zRi4cJI5IVOXib+b/23OTM2NYE8nnB7TABIGvIwda77Y/4X5a2e5Ch7Oljnn66JEEX9XaPUG+OdoZhDBCcyZlu9OaVsGW25MR4V6QZiodUGzXw7yjuFm7ZqCJgt7V8bM3dfRv7YwYB3iJVwmKpcyxmtUlQWKNcVtrSfYS8agWmyoJ7YNqQU/NyX06qq4fGIVitdsC0YmxbT2ZTFhQwI2coIec7uV5y0hNCns68KrSeYkl1oNpHCFeTtb7bLKDfBx/aPnZ0IlczL4FMj70kVTE5R+2iEoqbuNM2wRwZgUztLZSR3jsGCFJTuNG+x7U1yUUqSrdeg+/zgAYMfntXDqzkuk5iJYVjzQDVcSUx0O8EX7u+LjmpEYsfK8eVvODrxZbzgwk6U1CIKMOZmywZIzHJsjOWvF5cxMs5VZqdOz8HCSC+oKXrLRMIX7vKafMb2NDru/7buJSR/wlxGnqXVYdPg2mpQKRKQfvwnSsZvP0VGksUbfJad4WYNiTD+h688+xNGbL3D05gv8ulu8JIdGZ8Cm849w/2UG+sZHZb8OyzcnZyA35x9CuHUHqQYfH2XwEUIIIYSQXCvmXQytolvxgiUjKo2AXCKHXCLHmLgxGFphKAJdAlErpBYaRjS0eb6Z9WYi0CUQX1SyBEZMwT/uc3Yt0RXBLsFoWrgp2hRpA2c5P1gxvKJlCVz14Oo43f00FjRagMYRjRHqFooptaagWaSlntCP8T9Cwlh+JRbrSOyh9EDjwo1z3SRETIxPzBsL7pmY6iHmxqP0R+bgHgDsvb9XUPvOETqDDgnPL+DnM9PNQToAvOYeO+7sQKomFf9c+weLLi3CwN0Dka5Nx72Ue9h/fz8v44+bXQgASn9j1ua++/vAyJIASRZuMQvQaFUjc3APsHQt7rSxE9ZdXwcAOPPkjGgnYQA4++Qsxh0ex+sq7O2icCi4B4BXa5IbUAaMS5W3DK6JbZ/XEtQPjPC1BKJMgZQOlUIxvVMFePkFczrwAt3jwvBzp1jR5y/s64LpHcrylgMv/rAyxjaPQeXspcIA0C3OWBPxk1qRposDOi/DuTKjeddRLNAN3i4K+Hl7o0Sjj8E4G8/h56bEzC7l4ccJVHo4KeDuJAxkOeX4kH7k67qCMWI8nRRWmzBwa+DldPirulj8oTEgr+Y08UiC8GfcUabaYVlQ4hRbDAaRj9MayNFRPQZdNV+jfrmiaFzU8keDp6ywJubrlpql42V9NfhpP75afQEPOEuN7fm8QVHjudQ60YysnEoFewhqXf4XaG0EwwBL0NHRpcCtZh3CrsuWrMicteUWHr5t/Vr0Bng6C39OWsw8KDKab9ae6+bsPK2NLrJiNT1/2HIFU7ddReMZBwT7xIJ7gDBr0BqWZfHFqvN2x2l0BgxYehrfb7mCK4nGgCa36UhajoYspow/jc6Aiw+See+zrQYk7yPK4HuDZs2ahVmzZkGvp3XihBBCCHm3dSjWIVfjyweUx452O/Ak4wmmnJgCAJAx4r+qMgyDH2r9ILqvR0wP1A2rixDXEHNmU8XAiqgYyO/2eqjzIbjKXSFhJLw6dDmDMdWDq6NfWUvjgE2tN2FRwiL8c+2fXL0+k3kN5qHvjr7mpiY5NYtshk03N+Xp3AAwqfokjDk0xv7APFDr8t6V8e+rf/Mep2pS4aH0wIj9I7Dt9jbB+Lil4kt2DawBEkbCq/Vn4lpkMmBQ4Alrexnerru70LRwU/TY0gMAsKfDHvg6+fLGdN/SHYAxQPltjW8BGIOSBx4cQLI6GU0jm5rnSrI6GVJGygsOc5cqp2uFy9tkUglkAG4k3cD/Lv4P/cr0Q6h7KCpGeMPbRQG1Vo9CXsKAsimTTVVoCc5DjbFlHJ+HtYr6oVaOJbWTWpVCy7KFUKoQP3OVu+xPLEjAxbIsFvWujKa/GAMGpuXYOSlz1PXiBkudFVKMbFoCa848wKkcyzU9XeRWly1KrGTwAUCQh4pXZ66LZiR8kYy7rO0uw7b0rx2Ffn+etjvuGFvCXNCte9s2wE9jcdVQCCmvEFzMq9QsrUNZYbZwayA6kvk3slkJAMCY5jGYtPESAOES3TdNrTPwlnaKmbnnOm49T4eLjaW1XHoDiz6LTmLzoJqICXZHci4aW6Rk6XDtcZpguyPv02/7buK3fTdx9Ot6vGW+OYll0h69aawDKFYHzxpH65w6mpX5glOP0RSg42Z45ux+bMrS+3LVeaw588Bcz9S4j2IrXJTB9wYNGDAAly5dwokTJ972pRBCCCGE5LtwN/vNCezhNvmQShz7kMXFMAxC3ULtNhtxV7ibM/e4TUG4aofUxtwGcxHrH2veFuYehjFVhQE0Pyc/zKw70+71eSg9sLjJYvQt0xfzGszD8a7HEeEeAW+VN9Z/sB7f1/je7jkAoEVkC8G2SdUnoVVUK5HR+SMxI/edOq1J1aTiWeYz0eCeLacen4K/s7/1ARL7DTvStGm8oNuB+wfM2YkZ2gxzcxMAWH9jvTmLb96Fefhs92cYfWg0/rr0FwBj8K7BygZov6E9r6ZZmJulW/Ccc3PMX6+8thKnH1sCRF02dcH6G+vx7XFjENFVKcP+L+pg74g6cBNZCnnx+RnIvQ9A7p6Am8nXkfBMPFBskpieiPYb2vOav3AxDIPKhb0FdQd1egPkXgehDFwtWt9wccJi9N3RF6maVHTY2AELr3+Df/pWxdxu5RHm4wx3J+G1cztf5sw89HFVoFtcOJqVFi6zd1PKrHZK5mbw7R4WzwvMMAwDP1dLZuFhQymsN1TnHR/kYb8mHlelCG9s/KxGro6BWyAw9DIKjTiEKe3K2By6a5j15fR5pdWzOONgjTtrShVyPPPQy1lurvsXG5r7jMVgD5VoDb9XpdbqHWo8sen8I0GnXXua/nIA919moMnPwqw4a4avOJer5xBz5u5L7L7yxOp+D5Gfw7zEWd1EAoUAv16ggWUdyu4E+IFAU31N7rLbrBxL903ByDVnHgAAjt2yLH2mJbp8lMFHCCGEEEJeyZ9N/8S91Hso7Vfa/mA73BXu+KLSF2BZlldf73X6qPRHeJj2EGX8jB++lzRZghXXVuDzCp/bPOaPC3+YH6/9YC3cFe5wlbua68R5Kj0xvOJwjD5kXO7oLDMuJ1ZIFRhYbqD52PUfrIfGoIFSyq/H1jOmJ9oWbYtum7uhZVRL/Hn5T/M+qUSKlS1WwsAa8FL9EheeXkDLqJYOd1HOC7Gsubw68ugIZp2dladjE9NfLdB4IvEEktRJ5sdjD4/F2MNjoZKq0CyyGVb9u4o3/rtj36F5ZHPMPjvbvO3gg4NoGNEQYw+PRaYuE/fT7iNNm2aeszlrBZqed8KRCQAsNQgzdMZldoceHEJieiICXQKRmHEbXx/8Gs8zn+PzCp+jblhd3Eu9hzC3MIw41A8qTgKaKUit1Wtx7eU1lPApwVty/uPJH3HlxRWMOzwObYq0cfg9qhjhDVWgsbP18cTjghqdU09OBQAM2zsMV15cwZUXVzCl1hTzfldBcNIADfMM24bUQZpahwrhXgCMwZyHyVmoEW3MoOR+WP+zTxW4KKVgGMZqF939hjLohl1gZSpE+rmiR7UIzNl7w7zfXvbh35/E4YetV9CiTDD6/2U/M8/TWQEfVyV+71ERL9M1WHv2AYY1LIa2c4TdrQfV5XQYdvWDK4DigbYDIFF+wgy/25ObITE5C30WnUDCw9xl4kkYwMDyAyK5tXVITQS4OxYIbV2uEMY2jzE/9nCyZGk+T3csA1guk0DCMIKafa9q2IpzeGQj2+1V1fhhT76cJ9TbCfdeOLZ82t73NV2jx/ITd1G5sA8KZy//5wbsuR3QbXmRroHBwAo6oXO72f66+zr+x+ne7SjT0lxusDBnINbWMlzK4OOjDD5CCCGEEPJKyvqVRfPI5vl2vu4x3dGjZI98O589TjInfF/ze3Qu3hkAEOsfi29rfCtYtsk1uPxglPe3dAo1Nefg1gTUG/RoFd0KfzX9C94qb6sBFoZheMG9jsU6wkPpgZ4le6KwR2Hs77gfX1b+Eme7n+Wdu5h3MZTwKYFqwdXQt2xfXmAnp8Ieha3u+6zcZ7jQ8wLWtFxjdUxufRr7qc39k45OwousvAcdAPACSrnVYq0wAzJLnyUI7gHA1ttbMXD3QN42tV6NRqsa4dijY+ZtTzOeos+2Pii9qDRuJfM/6OoNelx7ec38+OKzi4Ln+enUTwCA1utb48qLK3ia+RQjD45E7eW10X5De/Ta2ktwjOl7PvbwWHTa1MncfMYkWZMsOMYR3Np9jzMeY8ONDUjKShKMu5FkCaZxAwc5M+6UARtxy2kUziZtNgf3tAYtipZdhlpxhzGyqXFJJ/eDfI0ivigXZhyryHE+bxcFiga4YruhIrprvkLSx8YVUm3LhwAAYoIsS477xVvvjB3u44LZXSugaKD4HxMkqvvG2o7ZTE1MGsQEoEOlUCz9OA4Vwr1Em5n0rBYh2FYiyN3mY2sCPVTYNKgmb9vliY3tHmcKtOam3h7Xb90roHig8Rq9XezXn6wQ7gUvzrgoP0tNSbHlqGKCPFQQiznZ+j464nUG9/KDaRlskDt/ibu1OpsAcPKO7Xvo1G1X8eWqC/gyuy5eUoaGtwT469XGPzQ4EiQTq8Onz7F0Pt2BDMmcTHURuRl8Wy4+4o2xGeCjLro8FOAjhBBCCCEkD7qW6AoAiPKwfPAcVG6Q+WtTQK+MXxns67gPX1b+0qHzjo4bjb0d9sLP2VgvzbRUmbtkWcdazwRa1HgRr/nJrHqzsP6D9ehSvItg7JImS/BJmU8AQJBBaI1pvC0fl/4YwysOx/c1HVtynBdl/cq+tnPbc/bpWcG2Vuta4XjicdHxE49OhFZvqc/VeVNnTDsxjTfm8MPD+OX0L4Jjs/TGwMTlF5cF+0z1IzfeNGbbmbJKTyaeRMu1LXH4oTCzjGVZ/HrmV/Py6AxtBn4+/TOvmQk3WDflxBSMPDjSnInK9STTsjzQ1pxUeBuvY8bpGeZt115cw6mnx3AmeT3SdM8AWA80SCQMDn9lacrxS6dyGNu8JAAGBwxlIHE3Lu2N9nfFwS/rYGX/quaxXzUpjvk9K+Y8JY9YrTJG/gwuhWcaazvawV1+XDHcC9e/bQIfV+HPk1wqwd+fWOpLNioZgKHZDSy+bV1KMH7L4JqCbSZODtSJE2t24qgqhb3RoIQlXXRiK/vdp3N2O2UYBh65vIbxLUuCgTDC16SUeCmFN8HRGnSvQqtnodEZBD8DrWILWT3m4gPHMjqP33qBqt/vQuzEHbjJ6WT994l7WHb8LoqN3oqIr2zXfjUF87jXp7PR4COnBb0r4dw4YbMtUwYfN1jI7coN2A7w5aaW4PuAAnyEEEIIIYTkQYPwBljQaAEWNVlk3tYyqiX+bv43fqr9EwaVH2TjaNtydhDOyUNhvbZV+YDyvCYktUJqAQC+qvwVTnU7hXkN5mFli5XY02EPr76gxEqn0px6luyJPR32oEF4A5vX37NkTzSKaOTQOfPCU+mJIBdhzTYA6FOqD36t++tre+7cWv3vakw/NZ23bdGlRbzHSeok/H7h91ydt8eWHii9yLI0XsbIYGANGLxnsCCLcOjeoWBZFscSj2He+XkYvs/YcXrm2Zn448If6LCxA/rt7Id55+fxGs8kq41ZgPvu7zNep0gmHwBsvLGR93hml3KAJAvOhX82bzMFbh6mPUSnTZ3M228m3wQAVIsyZs3mXJH7LPMZnmqv4fP6RdEwxh9Vo3x4WV7cJbwhXs6CmoL1SgTYDPL5uSnxfRt+iQGp0wPeY1vLfblBj9hQT6s1AwHwOiGXDfHEZ3WjcXxkPXStYqxhalquXCbEw26G3+yu5RHh44zvWpdG3/hIwX5rtdOsifJzwfyeFTG5TWks71uVtySzSakgqw1UTMSWUpsyNh1VPNBdtFZhTLA7fLKzAztXfrMdegtzulu/TgkPk3HuvjDr1toS9dywlsFoyuKzR63TY/KWKyg2eisuZF+jXu94gE8ukYjWFdUZDGBZFt9uEv4Bw0SjN4jWAgUogy8nCvARQgghhBCSBwzDoGJgRXgoPXjbSvqURP3w+lBI7S9py63vanyHyoGV0T+2v81x4e7h+KXOL1jadCnv2hRSBaoGV0Ux72KCJciFXAuhUzFL0MVJ5oQQ1xDemLFVx8Jd4Q5fJ1+0jm5t93rlEjnGVh3ryEvjsRfgBIwNWdZ/sB4LGi0Q7BtSYQhqh9bO9fPmp5ZRLd/4c75Uv0Tb9W2RohFm9uy4swP3Uu/hWeYz87aGKxvi4IOD5seHHhzCr2d+5W3j0ug1aL5WfDn+2MNjkaHNwMKLC7Hk0hI0LxOM4a2zIFVZsnFStanourkrvjn6De/YB2nGYFqNIr5Y+nEVHPm6Hm9/izUt0G1zN9QonYJk72kYtu9zaA1qILuLrtSBAEi9EgHYO7w2elePEN3fubKlMQojTYNToWXmx93jwrHbRgOMSE7tPK2djCLTkkQA2YFKBv6cGne/dC6Hr5oUx+89bGcdAkDT0kHYO6IOulQJw9dNSqB9Bf7Pq7vK8ZL7kz4ohWUfx6FeiQB04rwXJlIJgxGNitk8ByvSwuHLxsVtHuPrarlPmr6NH5QrhG1DavHGyaUS7P+iDi5OaCQIXE5pa715ia9IJqWYvrWEAVKT3GSqvYohy8+Kbp/ewfFs5ZFNi+fq+25NzjqaqVk6zN1nXJLfYuZBXHucCp3B8eCaVMIIavgBxgy+wzee40h2d18xGp0BK0+JNz6hGnx8FOAjhBBCCCGkgGgR1QLzG82Ht8rb7tg6YXVy3fhkVNwo89dti7TFlrZbePvbF21v/rpGoRoYGDsQ46uOx6lup6yes35Yfd5jR7L6Piv3GUp4l7A5RsJIoJKpEO1paWbgJHPCxGoTbRyVex+W+tBqp2VbvJS5y1zKL9eTrlvdp2N1kHA+Aj5KfyTI9AOAwXsGix5/5OERc0afmEF7BmH6qemYcmIKumzqgt8uCZe3nn96Hgce8LuNmjoVA0BcpLegqYOpacnv53/HpeeXsOvuLnx+tAWcQpYAcDzDKcLXBb2rGetRVuRklj1Ie4DGqxpD7m28rrDCJ3jHTfqgFEK8nGENN7CmsZPVFBPkjgB3JWJDPQVLWgFjrbt+8VE2G1tYWzLKzTKc0TGWF3jk4mbVfVi9MD6tHYXuceG8QKO948SIxTaDPK2fs02OQB43jlYs0A0NYgJ4412UMrgqZeZlnSYtY4OtPoejfYdalA22GhjL+Xyvy53nGaLbbS3TBYDmZSyZzFUK++BV45Hd48KxZXBNhHlb5nxKlpY3ptsfx5CViw621ubs8VsvcPGB7VqhB68/w4iV50X3ZVEGHw910SWEEEIIIYSYLWy8EJtubhI0ygh14y+LYxgGfcv2NT/e3nY72m1oh7ZF2/LGuSvcjd1hs7vf9i7VG08znuL0k9PoHtMdSy4tMY/tVqIbingVQYvIFuhVshfqrajHyzgT46nyxOqWq6GSqhDqzr/G2fVm49tj32JklZG49PwSIjwi8PeVv3HqsTEgWcyrGK6+vGrz/J9X+By9SvZCreW1bI4Tu678UNy7OK68uJIv52q1thWcZLaXWdqSs9lITtymIxeeObb0DwDSdca6YE8ynqDt+raoH14fwS7BqFGoBkr4WAK93GCkjtVC5mZc1mfK4FPr1Tj04BAqB1aGq0I8uBXm44yzYxvwMpR+OvUTHqQ9gCrgAbQvakKl1JmSAwEYm6Ssv7EeYe5hqBBQQXBO7pJcexl8KrkU+0bUgdzGMl5rlDIJ1DoDmpcRD2gFeViCaR+UKwSGAZYdvwsAGNM8BhqdAVcTU/BDuzI4cuM5KkV4w0Vk2aQ1od7OmNExFukaHYr4u6FiuBcMLIvoUcY/BIh1vnVTyuCikIo2YOhcJUy0VqHJr53L4bvNlwWBPu7zeLsooLSxfNrR1a3OCimKBbrhxG1LsDnIQ4V2FUKw7Pg9x07yllSJ9DHXrVPJpehaJQy/7b+Z5/NNbFUSDMNg/cDqiJ24AwCQmeP79yRVjeqTd4seH+LlhPsv+Y1drGXZztxznZc9m1vn7QQH3zcU4COEEEIIIYSYVQiowAtiTKk1BT+f/hnT4qfZOAoIcg3C/o77ec1AAGNzkE2tN2HFtRV4nPEYMd4xWNRkEZKykuCh9IDeoMfSK0sxJm4MOhTrwDu2UkAlbLltDB60jGqJ9TfWiz53Ea8iottrhtTE1pCtACy1CBtHNMb229txIvEEvqz8JZZfXY7Jx203UvBSeeFw58O4+uIqRh0chfbF2uP3878jQyeecQMYawTmh88rfI6+O/raH+igTF3eOqq+Thla4/u4/OpyJKmTsPLaSgDAL2f4TUcepj8UHCt1vYKvDuzHmLgx+HDbh7j84jLqhtbFz3V/Fow18XQ2Lgvdemsr5FI5r6Nz9WIyBIc64+Edy/hdd3dh7GHjUvMLPS2ByycZT3Dh6QXecnCxhgAsy4LhpJKJZe45Ynnfqlh39gGGNxRfKtunRiQuPUpBwxhjxqmppiEAlAhy4z2uXcw/T9fwQTl+NpmE0xDDU6ShBsMwOPBlXZSftEOwz1TTL8rPBTeepqNyBD8zWSWXYmIrYfMRbkOGWkV8jeUSwr1w8s5LwViJlRS+HZ/XQoOf9psfuyplCPZ0AmA8R6i3E/aPqAOGYfDrbutZsWIkDF45iy43fDmdi5UyCYY2LIoN5x7iYR47B5vmqqezAs3KBGHT+UeYss32H0K4gj2FAT5TQNtJLkWmlh8sNAWh82J7QiKSM7W5bubyrqIluoQQQgghhBCrmhRugq1ttyLGJ8bu2JzBPROFVIGuJbpiaIWhlg+PKk8wDIOvKn+FQ50PCYJ7AFA5qLL56/HVxvO6FL+KhhENMSpuFGQSGbqW6IoVLVbYPcZN4YaKgRWxrd02fFT6I+xovwP9y/bHulbrRMd7qfJnia6/k/VAzMy6M/PlOd42U4DPkdqLOTmHLsTmW5vRf2d/c6fh3fd240TiCUw/OR1qvdpcoH/Nv2tw+IGxo2+yOhkj9o/AkD1DcCLRsiQ3vsIdgOEvRxy2b5joc3fY0AFD9g7BhpsbIHW5CmXAeuiUl8yv6UTiCRx/dBzxy+Ox4cYGGFgDktXJ2HN3D3QG8a7DGdoMLEpYJJq5GhvqiXEtSlrNunNSSDG7awVzEI5b387bJf9rgppMbVcG3eLCUL9EgOh+bxcFnLKDmtH+rljSpzL++qhKdkAN+POjKhhcrwhmdi3n0PNxa+KZ7id/fxKHq980xtxuFbD602poGBOAvz+JE+nHC/SoGo4iAW5Y+lEV8zZnpQxDGxSFSi5B7WJ+2Dq4lvncP3Yo61CdRxOlLHcB3A+rF8affSzXYspYDPGyZNvuHhaPsqGeosdzsyClEgZKmRQdKuVPI5LnaWoAwIt0jc1x9UtY7lNRIkvDTUFZ7rLfV+WmlKFrlXC7WbPvE8rgI4QQQgghhLw1DMPAXSHeLfSD6A/gLHNGOf9ykEvk6FqiK049OYW6oXXz9RqKe1saAfiofPA8y3rBdxN3hbt5GfOA2AGYdXaW+VypmlRUDarKG+/r5IsOxTpg9tnZvO0jq4zEymsrce3lNd52H5WxAUO4R7jVazBlJVrTILwBdtwRZk69qqpBVREXHIefTv2UL+fbcHMDqheqDrVOnedznH16lvf4w20fAgAWJiwEANQOrY299/YCAM71OGeu65eTi9wFWXrrmU+dN3bGbw1/A8uy5nky6cgkOIcZAyAHUg8D6IqBuwfyAocjD47E8qvLodFrcPnFZQwpPwR9SveB1qBFqiYVDBj8c/UfzDxrDNpOOzkN61qtQ6Sn9eYP9jAMg6UfVcG9lxkoHmi7I++raF8xFO0r2g4o/a9XJczZdwOTWpVEuA+/K22QhxM+b1DU4ecz8AJ8xv/LpBLIADQuZcxenJfdpIQRyeDrUTUCABDO6Y7rJJci3McFZ8c2hFIm4R3XpnwImpYOQvExWx2+RjHeLgrRQBkLllersEphb3zdpDgCOUuuI/1cUTHcC+fuJYme10QpN+Zw+bk51lwkp3JhnrzHz9JsB/ZMfuwYizLjtwMAiga4IjbUE2c515qapRNc66ta0b/qa53XBREF+AghhBBCCCH/STKJDE0jm5ofO8udMbf+3NfyXAwYsGBRP7w+DKwBK66tgIRxbMFTv7L9EOMTgwDnAER6RELH6uAkc0K0Z7S56cXHpT9G5+KdzQG++JB4zKxnDOasvb7WfK62RdpiRKURkDJSsGAhl8hxoecFJDxPQKeNli7Hi5ssBsMw+KTMJ/jjwh/wVfniSeYT3nX9UOsHJKxOEF3a+ir6lO6DKkFVUD+sPpqtaZbn8zSKaIRtt7cBAL468FV+XZ4oU3APALpu6oqLzy9aHWsr0Hjx+UUsv7Kct3xYY+AHQbR6LS+4Z3Lu6Tnz1+turEOf0n3QfXN3JDxPEH0uR5rp2FMt2tf+oDegapQPqkb55Mu59JwafIxojp7FsIZFMfSfc+hUKRQlg93xMkOLaH9jhlkhTyeMblYC7iq5OUPP2vJplVxqXkrco2o4Fh+5IzoOEK9F+F3r0mhXIQQHrz/FwKVnMKVdGUzccAlPUtVoGBMIZ4XleVVyqWiDlMqFvTH/oLApjp+bEiMaFYNGZ4C/mzEo6K4SLln1dJYjKUMr2M617OM43mOpA11KSgS5w10lx5yu5bH/32foWiUc3eLCUWSUpUmTqUGLKQCZH6wtv36fUYCPEEIIIYQQ8t5b3XI1tt7eit6leoNlWfg7+zvU8deEm00nh/HD9eImi3Ej6QbK+pU1ZwQd6HgA2+9sR+PCjc3je5fsjRH7R6BOaB2MrzZe9PwlfUri5zo/Y8qJKZhcczJi/WMBAANjB6JfmX6QSWQ48OAAvtj/BUr5lsKcenMgl8ix7oN1mH12NhYkLMjlO8I3PX66eamqi9yY+RTmnvfi+AAwruo4c4DvTbIV3EtRp+DIoyM2j7fVSRgAGqxsYPca0jRpeJrx1GpwDwDclZSdJIYbP7MX42lTPgSVIrxRyNMJEpFlth/VdDxDctnHcdh37SlalA22GeBTW+kuq5BJULd4AC6MbwSphEGton64+zwDpQp5ICnDEiS21jSkYUwA5narAAPL4tO/Tpu3uyplGFAnmjeWGzA0sddtuliAmyDA6cjyV1V20K5J6SA0KR0k2F+vuD+csq/HVkOU3BKrd/m+owAfIYQQQggh5L0X7RWNgV6WLrH9yvZ75XO6KdzMgTgTT5WnoN5go4hGKOpdVNCpOKe6YXVRN4y/PJlhGMilxoBirZBaONrlKG+/SqZC/9j+cJY7m5cRWyNhJCjvXx4nH58U7CvjV8b8tbM8d3W0pIwUelbYRdUUKPwvMdXxs8VWcxUADi3xfpr5FHVX2F5q7mgG6ftmSP0iWHX6PgBj8Mie0Hyq++bvrhIsRf6sbjR2XHqMP3pWRI0f9pi314j2xcHrljqK3KxDU7agu0qOUoU8APAzB32tdBZmGAaNSwUiK0eTCrH6gKbzcnWsFIpZe26gkKcTHiTxm2AMbVAUXasIA/Zag/0gmkjCIgCgV7UILDpyG8M4TWFsNZjpXzsKge4qjFtvPejNVdj3v3f/eNvojkEIIYQQQgghbxHDMIj0iIRc8no6QTrJnNCvbD8c63JMsK9OaB381uA3rPtgHc50P4O5DeZifsP5ON3NkiH0WbnPoJJa6oFxvxYLQvk7+WNExRHmx/s67sPSpktRyqcU3BRuNo8FgI7FOuJcj3Oi+w51PmTjlb66Pff22B2z4pr9piyvqmVUy9f+HAVVqLczjo+qhz/7VDHX3HvTzoxpgANf1MGwhsWwdUgthHg545NaxmzAjhVD8VPHWPSoaqmfWcRfuOSWi5vZViLIduamSi7Fh9UL2xwT4K7CtiH8Gp1ty4fgwBd1sGNoLYxuVoK3b1C9IrxmHSamjsxuVhq72LrecS1icHF8I8QEW/bHWmkUAgA1i/jygnbhPs5Y2LuS1fHWms28z+gdIYQQQgghhJD3gLPcGVUCq+BYoiXQ90vdX3hjlFKluXvxmLgxOPDgAHqW7GnuRAuAF6SbU38Ofjr1E668uAIA8FJ6YVeHXdAb9Fh/Yz0UUgXcFe4o7Vcay5ovw/77+zFg1wBUCrT+wV0ukQuCf25yNzSPag53hTsu9LyA0otKC47j1jzMLW49QLHzag1a3EmxviwzP53udjpPHYXfJ/5uKnO9ubfBy0UBrxwNI0Y0Koa6xf1RLswTSpkUE1uVQoeKobjxNA1xkbbrDzIMgw0DayBTq+c117DGkaa+xQItP6eh3k68un4dKoVi4eHb0BtY7Ble2+o5hjcshqIBrqhTzB+Vv9sl2B/kocLEViVFj2UYRhCE6x4XDrXOgLhIHyQmZ6JCuDcqfbvTuJMFqkX5wFUpQ5pah/k9KyLIw0lwXm8XBSa3Ef78EwrwEUIIIYQQQsh7Y3b92bj0/BL+vvq33SyxDsU68JYTz6o3C3qDnhfgqxZcDdWCq5kDbqblwlKJFMubLwfDMLyOpLVCamFdq3Uo5FbI6vOW8y/HeyyXyLG/035e0MvUFMWksEdhrGm1Bq3Xtc5TkK+cfzlegM9Z5oyZ9WaipE9JOMud0X9n/zcW4DO9h6RgkUslgkBeqUIeostlxZQOcWwcAPSpWRirTt+327346ybF8dexu1jel988w10lx8Ev7Xcjd1JI0bGScemu2NLej2pGQi51fGGoTCpBv/go44Mc2Xxs9v6LEyy1T1mR9b+nRtcX7Y5MaIkuIYQQQgghhLw3FFIFYv1jMbnmZFQLrparY2uF1EKdsDo2x3CXGUslUtFluJGekVBKxeuMfVvjWzQI5zepiPGJEWS0zagzg/d4XoN5AICmhY1dlyPcI7C5zWYEOAeYx3gqPc1fF3K1BBh3tNsBnUHHO9+g8oNQKbCSud4gN5jINbHaRPN1fxr7qegYQvJbkIcTTo1ugJFNS9gc1zc+Cvu/qCOaCZdbvatHCLb15CxDflVFA9wE23IG8r5sXJyCezZQBh8hhBBCCCGEkHyhkCrsD7KBm1U4o/YMrL2+FuOqjROM81Z5m79mwCDQxVgrrFepXgh1D0WlgErwcfLB383/Rp1/jEHJ3qV6o1uJbtAatHiR+QLdtnRD1xJdEegSyAtE1g2ti07FOvGfUCS+91Hpj9C6SGu0LtIaAHAv9R5mn52dq9e74YMNuJV8C4P2DMrVcYSIdQV+nT6sXhgVwr3QevZh8zZZLrL3rDk1uj7S1Xr4uYkH/U3alg9B/9pRr/x87zLK4COEEEIIIYQQki9y2yjkz6Z/orx/eTjJnLCo8SLevnrh9fBrvV/h6+QrOK6Ubynz19zsOrlEjsYRjeHjZFwqyc0UlDEyKKQKuMhdEOoeir0d9uKTMp8AAFpHt4aPygeFXAthWu1pkEr43T65z7GkyRJsbbsVg8sP5o0JcQ1B71K9US+sHja13oRmkc1svvZ1rdYhwiMCdcLqoGuJrgDAa05CyH+JRMKgXJgXFvauhAB3pc0GGLnh46pEmI/9TsdKOYWv7KEMPkIIIYQQQgghr6RGoRo4+OAgusd0z9VxZf3KYlGTRfYH5uBoEwpXuaWxAHdZLsBf/ueqcMXejnutnqdxRGMcfngYEe4RiPWPFR3DMAyGVhhqfjy55mRsurlJMG5ExRGI8YlBpGekedvwisPRuXhnhLmF2XtJhLxVtYv549jI+m/s+WoV9cP+a0/RpTL9bNhDAT5CCCGEEEIIIa/k5zo/42byTRTzKvbGnrNPqT6Yf3E+hlUYZnUMwzBY1XIVTiSesFs/0JZW0a0Q5BqEEt62a57Z4+vkiy4luggClDKJDOHu+VfPjJB3xYJelfAiXWN3CS8BGFasLQl5rVJSUuDh4YHk5GS4u7u/7cshhBBCCCGEkAKHZVncT7uPENeQ/2zh/X339mHN9TUo6lUUDcIbINQtFCqZ6m1fFiHkP+hVY0UU4HsLKMBHCCGEEEIIIYQQQkxeNVZEVQoJIYQQQgghhBBCCCnAKMD3Bs2aNQsxMTGoVCl/us0QQgghhBBCCCGEEEJLdN8CWqJLCCGEEEIIIYQQQkxoiS4hhBBCCCGEEEIIIe8xCvARQgghhBBCCCGEEFKAUYCPEEIIIYQQQgghhJACjAJ8hBBCCCGEEEIIIYQUYBTgI4QQQgghhBBCCCGkAKMAHyGEEEIIIYQQQgghBRgF+AghhBBCCCGEEEIIKcAowEcIIYQQQgghhBBCSAFGAT5CCCGEEEIIIYQQQgowCvARQgghhBBCCCGEEFKAUYCPEEIIIYQQQgghhJACjAJ8hBBCCCGEEEIIIYQUYBTgI4QQQgghhBBCCCGkAKMAHyGEEEIIIYQQQgghBRgF+AghhBBCCCGEEEIIKcAowEcIIYQQQgghhBBCSAFGAT5CCCGEEEIIIYQQQgowCvARQgghhBBCCCGEEFKAUYCPEEIIIYQQQgghhJACjAJ8hBBCCCGEEEIIIYQUYBTgI4QQQgghhBBCCCGkAKMAHyGEEEIIIYQQQgghBZjsbV/A+4hlWQBASkrKW74SQgghhBBCCCGEEPK2mWJEpphRblGA7y1ITU0FAISGhr7lKyGEEEIIIYQQQggh/xWpqanw8PDI9XEMm9fQIMkzg8GAhw8fws3NDQzDvO3LyRcpKSkIDQ3FvXv34O7u/rYvh/zH0PwgttD8ILbQ/CC20PwgttD8ILbQ/CC20Pwg9ryOOcKyLFJTUxEcHAyJJPcV9SiD7y2QSCQICQl525fxWri7u9MNkFhF84PYQvOD2ELzg9hC84PYQvOD2ELzg9hC84PYk99zJC+ZeybUZIMQQgghhBBCCCGEkAKMAnyEEEIIIYQQQgghhBRgFOAj+UKpVGLcuHFQKpVv+1LIfxDND2ILzQ9iC80PYgvND2ILzQ9iC80PYgvND2LPf3GOUJMNQgghhBBCCCGEEEIKMMrgI4QQQgghhBBCCCGkAKMAHyGEEEIIIYQQQgghBRgF+AghhBBCCCGEEEIIKcAowEdeSWZmJsaOHYuiRYtCpVIhODgYH374IR48ePC2L43ko9q1a4NhGKv/bd26VfS4hQsXonLlynB1dYW3tzeaNm2Kw4cP23yuQ4cOoWnTpvD29oarqysqV66MxYsXv46XRXLh1KlTmDx5Mtq0aYOQkBDz996eNzUH7t+/j969eyM4OBgqlQpFixbFuHHjkJWVlavXSfImt/Nj/PjxNu8pX331ldVjaX4ULBkZGVi7di369OmDYsWKQaVSwcXFBWXLlsXEiRORlpZm9Vi6f7z78jI/6P7x/vnxxx/Rpk0bFClSBB4eHlAqlQgPD0ePHj1w4cIFq8fRPeT9kNv5QfeQ99fz58/h7+8PhmEQHR1tc2yBvX+whORRZmYmGxcXxwJgg4KC2A4dOrCVK1dmAbB+fn7sjRs33vYlknwSHx/PAmDbtm3L9uzZU/Df+fPnBccMHjyYBcA6OTmxrVq1Yhs1asTKZDJWKpWya9asEX2elStXslKplGUYho2Pj2fbtm3Lenp6sgDYYcOGveZXSWxp1aoVC0Dwny1vag78+++/rK+vLwuALVWqFNuhQwc2MjKSBcBWr16dzcrKetWXT+zI7fwYN26c+fsjdk/5559/RI+j+VHw/P777+b5UKJECbZ9+/Zso0aNWDc3NxYAW7x4cfbx48eC4+j+8X7Iy/yg+8f7x8fHh1WpVGzlypXZ1q1bs61bt2aLFi3KAmDlcjm7YcMGwTF0D3l/5HZ+0D3k/dWzZ0+WYRgWABsVFWV1XEG+f1CAj+TZqFGjWABs1apV2dTUVPP26dOnswDY+Pj4t3dxJF+ZAny3bt1yaPyOHTtYAKyPjw977do18/bDhw+zCoWC9fT0ZF++fMk75vnz56y7uzsLgF21apV5e2JiIhsdHc0CYPfs2ZMPr4bkxeTJk9kxY8aw69evZx89esQqlUqbAZw3OQeqV6/OAmAHDRpk3qbVatnWrVuzANhx48bl+XUTx+R2fph+uV6wYIHDz0Hzo2BauHAh+8knn7CXLl3ibX/48CFbrlw5FgDbuXNn3j66f7w/8jI/6P7x/jl48CCbmZkp2D5r1iwWABsQEMBqtVrzdrqHvF9yOz/oHvJ+2rlzJwuA/eSTT2wG+Ar6/YMCfCRP1Go16+HhwQJgT58+LdhfpkwZFgB78uTJt3B1JL/lNsDXpEkTFgD7008/CfYNGjSIBcBOmzaNt/2HH35gAbCtWrUSHLN69WoWANu8efM8XD15HewFcN7UHDh27BgLgPX39xf8lSsxMZGVy+Wsl5cX7xc78vq9jgAfzY93z+HDh1kArFKpZNVqtXk73T8Iy1qfH3T/IFxRUVEsAPbcuXPmbXQPISZi84PuIe+fjIwMNioqio2JiWGvXbtmM8BX0O8fVIOP5MmhQ4eQnJyMqKgolCtXTrC/Xbt2AIANGza86Usjb1lmZiZ2794NwDIPuKzNjU2bNlk9plmzZlCpVNi5cyfVqygA3uQcMB3TokULKJVK3jEBAQGoWbMmXr58iYMHD77CKyL/BTQ/3j1ly5YFAKjVajx//hwA3T+Ihdj8yCuaH+8uuVwOAFAoFADoHkL4cs6PvKL5UbBNmDABN2/exNy5c81zQsy7cP+gAB/Jk3PnzgEAypcvL7rftP38+fNv7JrI6zd//nx8+umnGDhwIH755RfcvXtXMObq1atQq9Xw8/NDSEiIYL+1uWFrTikUCpQqVQpZWVm4du1afrwU8hq9yTlA96KCbffu3RgyZAj69euHb775BqdOnbI6lubHu+fmzZsAjB/AvL29AdD9g1iIzQ8uun+QJUuW4OrVqyhSpAiKFCkCgO4hxEJsfnDRPeT9cP78eUyfPh29e/dGzZo1bY59F+4fslyNJiSbKbAjNvG52+/cufPGrom8ft988w3v8fDhwzFmzBiMGTPGvM3e3HBxcYGnpydevnyJ1NRUuLm5ISUlBcnJyTaPCwkJwcmTJ3Hnzh2UKVMmP14OeU3e5Byge1HBtmTJEt7jMWPGoG3btli4cCFcXV3N22l+vJt+/vlnAEDjxo3Nf72m+wcxEZsfXHT/eP9MnToVCQkJSE9Px+XLl5GQkIDg4GAsW7YMUqkUAN1D3meOzA8uuoe8+wwGAz766CN4enpiypQpdse/C/cPyuAjeZKWlgYAcHZ2Ft3v4uICAEhNTX1j10Ren1q1amHJkiW4ceMGMjIycPXqVXz77beQyWQYO3as+ZdwwP7cAITzw3SMreNoThUcb3IO0L2oYIqOjsa0adOQkJCAtLQ03Lt3D3/99RcKFSqEVatWoXv37rzxND/ePZs3b8b8+fMhl8sxadIk83a6fxDA+vwA6P7xPtu2bRsWLVqElStXIiEhAeHh4Vi2bBkqVKhgHkP3kPeXI/MDoHvI++TXX3/FiRMnMHXqVPj4+Ngd/y7cPyjARwixa+LEiejWrRsiIyPh5OSEokWLYuTIkVi7di0AYPz48cjMzHy7F0kIKTC6deuGYcOGISYmBi4uLggJCUGXLl1w4sQJ+Pj4YO3atTh69Ojbvkzymly5cgXdunUDy7KYOnWqudYaIYD9+UH3j/fXzp07wbIsXr58if3796NIkSKIj4/Ht99++7YvjfwHODo/6B7yfrh79y5Gjx6N+Ph49OrV621fzhtDAT6SJ6a05YyMDNH96enpAAA3N7c3dk3kzWvYsCEqVqyIpKQkHDt2DID9uQEI5wc3DZ7mVMH3JucA3YveLUFBQejduzcAYOvWrebtND/eHQ8ePEDjxo3x8uVLDB06FIMHD+btp/vH+83e/LCF7h/vD09PT9SsWRObN29GhQoVMGbMGJw4cQIA3UOI7flhC91D3i0DBgyARqPB3LlzHT7mXbh/UICP5ElYWBgA4P79+6L7TdvDw8Pf2DWRt8NUtPbRo0cA7M+N9PR0JCUlwcvLy3zDcnd3h4eHh83jaE4VHG9yDtC96N2T854C0Px4V7x48QINGzbEnTt30Lt3b0ybNk0whu4f7y9H5oc9dP94v8jlcnTs2BEsy5q7WtI9hJiIzQ976B7y7ti4cSOcnZ3Rr18/1K5d2/xfp06dABj/oGTalpiYCODduH9QgI/kiWm5xOnTp0X3m7ZTM4R338uXLwFY6gQUK1YMSqUST58+xYMHDwTjrc0NW3NKq9Xi4sWLUKlUKFq0aL5eP8l/b3IO0L3o3ZPznmJC86NgS0tLQ5MmTXDp0iW0adMGv//+OxiGEYyj+8f7ydH5YQ/dP94/vr6+AICnT58CoHsI4cs5P+yhe8i7JSkpCfv27eP9Z1p1lpWVZd6WlZUF4N24f1CAj+RJ9erV4eHhgRs3buDs2bOC/StXrgQAtGjR4g1fGXmTnj59igMHDgCwtPJ2cnJC3bp1AQArVqwQHGNtbjRr1oy3n2vjxo3IyspC/fr1oVKp8u8FkNfiTc4B0zEbNmyAWq3mHfP48WMcOHAAXl5eqF69+iu8IvKmsCyLNWvWALDcU0xofhRcarUarVq1wvHjx9GoUSOrHQ0Bun+8j3IzP2yh+8f7ad++fQCAqKgoAHQPIXw554ctdA95t7AsK/rfrVu3ABjnhGlbREQEgHfk/sESkkejRo1iAbDVqlVj09LSzNunT5/OAmDj4+Pf3sWRfHPo0CF2zZo1rE6n422/desWW716dRYA27JlS96+HTt2sABYHx8f9tq1a+bthw8fZpVKJevp6cm+fPmSd8zz589Zd3d3FgC7atUq8/bHjx+z0dHRLAB2z549+f76SN4olUrW1j8hb3IOmObh4MGDzdu0Wi3bpk0bFgA7bty4vL5Mkke25seTJ0/YmTNnsikpKbztqampbN++fVkAbGBgIJuens7bT/OjYNLpdGzr1q1ZAGzNmjUF31cxdP94f+R2ftD94/1z8OBBdsuWLaxer+dt12g07C+//MJKJBLWycmJvXv3rnkf3UPeH7mdH3QPIbdu3WIBsFFRUaL7C/r9gwJ8JM8yMzPZKlWqsADYoKAgtkOHDubHfn5+7I0bN972JZJ8sGDBAvM/dk2bNmW7dOnCVq9enVWpVCwAtmTJkuzjx48Fxw0ePJgFwDo7O7OtWrVimzRpwspkMlYqlbJr1qwRfa6VK1eyEomEZRiGrVOnDtuuXTvW09OTBcAOHTr0Nb9SYsvGjRvZKlWqmP9jGIYFwNu2ceNG3jFvag5cu3aN9fHxYQGwpUuXZjt27MhGRkaa/wCRlZWV328HySE388P0i5Wrqytbp04dtkuXLmyDBg3M30NPT0/24MGDos9D86PgmTFjBguABcC2bt2a7dmzp+h/T58+5R1H94/3Q27nB90/3j+m30N9fX3ZRo0asV26dGEbNmzIBgUFsQBYlUrFLl++XHAc3UPeD7mdH3QPIfYCfCxbsO8fFOAjryQjI4MdM2YMGxUVxSoUCjYwMJDt1asXe+/evbd9aSSfXLp0ie3fvz9bvnx51s/Pj5XJZKyHhwcbFxfHTp8+nc3IyLB67IIFC9gKFSqwzs7OrKenJ9u4cWP20KFDNp/v4MGDbOPGjVlPT0/W2dmZrVixIrtw4cL8flkkl0y/QNn6b8GCBaLHvYk5cPfuXbZXr15sYGAgq1Ao2OjoaHbMmDFsZmbmq7xs4qDczI+UlBT2yy+/ZOPj49lChQqxSqWSdXZ2ZkuWLMkOGzaMvX//vs3novlRsIwbN87u3ADA3rp1S3As3T/efbmdH3T/eP/cvHmTHTlyJFu9enU2KCiIlcvlrIuLC1uyZEn2s88+Y//991+rx9I95N2X2/lB9xDiSICPZQvu/YNhWZYFIYQQQgghhBBCCCGkQKImG4QQQgghhBBCCCGEFGAU4COEEEIIIYQQQgghpACjAB8hhBBCCCGEEEIIIQUYBfgIIYQQQgghhBBCCCnAKMBHCCGEEEIIIYQQQkgBRgE+QgghhBBCCCGEEEIKMArwEUIIIYQQQgghhBBSgFGAjxBCCCGEEEIIIYSQAowCfIQQQgghhBBCCCGEFGAU4COEEEIIIQAAhmHs/terV6+3fZl2jR8/HgzDYOHChW/7UgghhBBC3gjZ274AQgghhBDy39KzZ0+r+2rUqPEGr4QQQgghhDiCAnyEEEIIIYSHMt8IIYQQQgoWWqJLCCGEEEIIIYQQQkgBRgE+QgghhBCSZwzDICIiAhqNBuPGjUNUVBRUKhUiIyMxduxYZGVliR73/PlzjBgxAkWKFIFKpYK3tzcaN26M7du3W32u58+fY9SoUShdujRcXFzg7u6O0qVL44svvsCjR49Ej7lw4QJatmwJLy8vuLi4ID4+HocPH86X104IIYQQ8l9BAT5CCCGEEPJKWJZF27ZtMXXqVMTExKBZs2Z48eIFJk2ahObNm0Ov1/PGP3jwAJUrV8a0adOg0WjwwQcfoFy5cti5cycaNWqEn376SfAcly9fRmxsLL777js8e/YMjRo1Qv369cGyLKZOnYpjx44Jjjl58iTi4uJw+/ZtNGrUCEWKFMH+/ftRr149XLx48bW9H4QQQgghbxrV4COEEEIIIa/k7t27MBgMuHjxIiIjIwEAT58+Rd26dbFr1y78+uuvGDJkiHl8v379cPPmTXTp0gULFiyAQqEAABw8eBCNGjXCiBEjUKdOHcTGxgIAdDodWrdujfv372PIkCH44YcfzMcAQEJCAlQqleC6Zs2ahZ9//hmDBg0yb/v8888xY8YMTJkyBYsXL34N7wYhhBBCyJtHGXyEEEIIIYSHYRir/61du1b0mLFjx5qDewDg5+eHqVOnAgBmzpxp3n7z5k1s3LgRrq6u+PXXX3mBuho1aqBfv37Q6/WYNWuWefvq1atx9epVlCxZEtOmTeMdAwAlS5ZEVFSU4JqqV6/OC+4BwOjRowEA+/fvd/DdIIQQQgj576MMPkIIIYQQwtOzZ0+r+8LCwkS3d+rUSbCtcePG8PLywo0bN/Do0SMEBQXh4MGD5n3e3t6CY7p3744ff/wRBw4cMG/buXMnAOCjjz6CVCp1+HU0bNhQsM3Hxwfe3t5Wa/YRQgghhBREFOAjhBBCCCE8CxcuzNV4Ly8vuLm5ie4LDw/Hy5cv8fDhQwQFBeHhw4cAgIiICNHxpu0PHjwwb7t37x4AiGbp2RISEiK63c3NDS9evMjVuQghhBBC/stoiS4hhBBCCPnPYBgm384lkdCvuoQQQgh5P9BvPYQQQggh5JW8fPkSqampovvu3r0LAAgODub9/86dO6Ljb9++DQAoVKiQeVtoaCgA4MaNG/lyvYQQQggh7xoK8BFCCCGEkFf2zz//CLZt374dL168QGRkJIKCggAYG2kAwNatW5GUlCQ45s8//wQA1KxZ07ytfv36AID58+fDYDDk96UTQgghhBR4FOAjhBBCCCGvbMKECebsOwB49uwZRowYAQAYMGCAeXtkZCSaNWuG1NRUDB48GFqt1rzvyJEjmDNnDqRSKe+YNm3aoGjRorh48SK++OIL3jEAkJCQgJs3b76mV0YIIYQQ8t9HTTYIIYQQQghPr169rO4LCwvDxIkTBdvKlCmDkiVLol69epDL5di9ezeSkpJQp04dDBo0iDf+t99+Q82aNbF48WLs27cPVatWxdOnT7F3717o9XpMnz4dsbGx5vEymQyrVq1CgwYNMH36dCxduhRVq1YFy7L4999/cfHiRaxZswaRkZH5+TYQQgghhBQYFOAjhBBCCCE8ixYtsrqvbNmyggAfwzBYuXIlJk6ciKVLl5o75g4YMACjRo2CTMb/lbNQoUI4ceIEvv/+e6xduxarV6+Gs7Mz6tWrh2HDhqFhw4aC5y1VqhTOnTuHqVOnYv369di8eTOUSiXCwsLw5ZdfIi4uLn9ePCGEEEJIAcSwLMu+7YsghBBCCCEFE8MwCA8P5y3PJYQQQgghbxbV4COEEEIIIYQQQgghpACjAB8hhBBCCCGEEEIIIQUYBfgIIYQQQgghhBBCCCnAqMkGIYQQQgjJMyrnTAghhBDy9lEGHyGEEEIIIYQQQgghBRgF+AghhBBCCCGEEEIIKcAowEcIIYQQQgghhBBCSAFGAT5CCCGEEEIIIYQQQgowCvARQgghhBBCCCGEEFKAUYCPEEIIIYQQQgghhJACjAJ8hBBCCCGEEEIIIYQUYBTgI4QQQgghhBBCCCGkAPs/AczI0vgdpjcAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper parameter sweep. The results were underwhelming so I'd just leav it un run\n",
        "\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "class args:\n",
        "  pass\n",
        "arg = args()\n",
        "\n",
        "arg.data = 'our_stuff'\n",
        "arg.batch = 10\n",
        "arg.batch_size = 256\n",
        "arg.normalization_method = 'z-score'\n",
        "arg.epochs = 1000\n",
        "arg.lr = 1e-3\n",
        "arg.warmup_epochs = 10\n",
        "arg.warmup_lr = 5e-4\n",
        "arg.final_lr = 1e-4\n",
        "arg.lr_F = 1e-3\n",
        "arg.iter_per_epoch = 1\n",
        "arg.F_layers_num = 3\n",
        "arg.F_hidden_dim = 60\n",
        "arg.alpha = 1\n",
        "arg.beta = 1\n",
        "arg.early_stop = 50\n",
        "arg.feature_length = 17 # use 17 for regular feature count\n",
        "arg.second_derivatives = False # use to not include second derivatives\n",
        "reader = DF(arg)\n",
        "df = pd.read_csv('/content/drive/MyDrive/Machine Learning Project (ECE 228)/Battery Dataset/features.csv')\n",
        "df = df[df['charge_CC_time_V']!=0.0] # these were giving strange results\n",
        "df = df[df['charge_CV_time_I']!=0.0]\n",
        "df.to_csv(\"clean_features.csv\", index=False)\n",
        "loader = reader.load_all([\"./clean_features.csv\"], nominal_capacity=2)\n",
        "results = []\n",
        "count = 0\n",
        "# Going to do a parameter sweep\n",
        "for batch_size in tqdm(range(128,512,128)):\n",
        "  for norm_method in ['min-max', 'z-score']:\n",
        "    for it_p_e in range(1,3):\n",
        "      for f_layers in range(3,9,3):\n",
        "        for hidden_dim in tqdm(range(45,80,15)):\n",
        "          for alpha in np.arange(0.5, 1.5, 0.3):\n",
        "            n=time.time()\n",
        "            print(batch_size, norm_method, it_p_e, f_layers, hidden_dim, alpha)\n",
        "            arg.batch_size = batch_size\n",
        "            arg.normalization_method = norm_method\n",
        "            arg.iter_per_epoch = it_p_e\n",
        "            arg.F_layers_num = f_layers\n",
        "            arg.F_hidden_dim = hidden_dim\n",
        "            arg.alpha = alpha\n",
        "\n",
        "            pinn = PINN(arg)\n",
        "            losses = pinn.Train(trainloader=loader['train'],\n",
        "            validloader=loader['valid'],\n",
        "            testloader=loader['test'], debug=False)\n",
        "\n",
        "            results.append({\n",
        "              \"batch_size\": batch_size,\n",
        "              \"normalization method\": norm_method,\n",
        "              \"iter per epoch\": it_p_e,\n",
        "              \"F layer num\": f_layers,\n",
        "              \"F_hidden_dim\": hidden_dim,\n",
        "              \"alpha\": alpha,\n",
        "              \"final loss\": losses[-1]\n",
        "          })\n",
        "            print(time.time() - n)\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(\"/content/drive/MyDrive/Machine Learning Project (ECE 228)/Battery Dataset/results_hyperparam.csv\", index=False) # VERY key\n"
      ],
      "metadata": {
        "id": "6Bm4Kvta8WNw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7205ace0-f98b-407e-8e9f-c2765b6001c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      charge_CC_mean_V  charge_CC_std_V  charge_CC_kurtosis_V  \\\n",
            "0             1.051718        -0.713588             -0.094252   \n",
            "1             1.224559        -0.542147             -0.040044   \n",
            "2             0.692781         0.079437             -0.421930   \n",
            "3             1.704675        -1.676956              2.159515   \n",
            "4             0.785932        -0.140048              0.086626   \n",
            "...                ...              ...                   ...   \n",
            "2420         -0.443082         0.720153             -0.611928   \n",
            "2421         -0.453659         0.645414             -0.614842   \n",
            "2422         -0.422004         0.627261             -0.612344   \n",
            "2423         -0.513857         0.688364             -0.596764   \n",
            "2424         -0.418204         0.602553             -0.612677   \n",
            "\n",
            "      charge_CC_skew_V  charge_CC_time_V  charge_CC_charge  charge_CC_slope_V  \\\n",
            "0            -0.723178         -2.570478         -2.557431           7.281268   \n",
            "1            -0.797171         -2.611115         -2.598165           8.519176   \n",
            "2            -0.471074         -2.599677         -2.586782           8.297809   \n",
            "3            -2.014463         -0.169714         -0.144630          -0.227694   \n",
            "4            -0.736379         -0.690659         -0.667485           0.054417   \n",
            "...                ...               ...               ...                ...   \n",
            "2420          0.417345         -0.381994         -0.356347          -0.103665   \n",
            "2421          0.423965         -0.398756         -0.373971          -0.101562   \n",
            "2422          0.427483         -0.422011         -0.396601          -0.092649   \n",
            "2423          0.499367         -0.168751         -0.142701          -0.206949   \n",
            "2424          0.405564         -0.445883         -0.420552          -0.081591   \n",
            "\n",
            "      charge_CC_entropy_V  charge_CV_mean_I  charge_CV_std_dev_I  \\\n",
            "0               -3.434897         -3.434933            -3.419796   \n",
            "1               -3.615999         -2.054186            -1.588874   \n",
            "2               -3.681913         -0.509941             0.287333   \n",
            "3               -0.517590         -1.430043            -0.720811   \n",
            "4               -0.822937         -0.144988             0.375235   \n",
            "...                   ...               ...                  ...   \n",
            "2420            -0.719246          0.431733            -1.386463   \n",
            "2421            -0.736011          0.417935            -1.654983   \n",
            "2422            -0.752977          0.522672            -1.230410   \n",
            "2423            -0.622616          0.321845            -1.698875   \n",
            "2424            -0.778813          0.738648            -1.126685   \n",
            "\n",
            "      charge_CV_kurtosis_I  charge_CV_skew_I  charge_CV_time_I  \\\n",
            "0                 3.165489          3.146038          1.305891   \n",
            "1                 2.716360          2.960147          1.264916   \n",
            "2                -0.067089          0.816123          0.149106   \n",
            "3                 1.469249          1.189845          1.432231   \n",
            "4                 0.212768          0.748297          1.322877   \n",
            "...                    ...               ...               ...   \n",
            "2420              0.047583         -0.033412         -0.581324   \n",
            "2421             -0.034865         -0.175776         -0.614544   \n",
            "2422              0.033223         -0.014649         -0.585731   \n",
            "2423              0.301483          0.234447         -0.397120   \n",
            "2424              0.042512          0.021309         -0.553738   \n",
            "\n",
            "      charge_CV_charge_I  charge_CV_slope_I  charge_CV_entropy_I  cycle_index  \\\n",
            "0               0.934968           1.224660             0.135281    -1.224588   \n",
            "1               1.084453           1.194460             0.096569    -1.204177   \n",
            "2               0.124126           0.454231            -0.968551    -0.836783   \n",
            "3               1.342167           1.251640             0.196704    -1.224588   \n",
            "4               1.413682           1.239426             0.128908    -1.204177   \n",
            "...                  ...                ...                  ...          ...   \n",
            "2420           -0.568314          -0.375006            -1.572750     1.347170   \n",
            "2421           -0.606129          -0.416189            -1.618723     1.367581   \n",
            "2422           -0.564282          -0.364575            -1.602036     1.387992   \n",
            "2423           -0.377817          -0.099872            -1.411697     1.408402   \n",
            "2424           -0.514577          -0.325104            -1.586194     1.428813   \n",
            "\n",
            "      capacity  \n",
            "0     0.532271  \n",
            "1     0.521953  \n",
            "2     0.350731  \n",
            "3     0.782358  \n",
            "4     0.779403  \n",
            "...        ...  \n",
            "2420  0.698253  \n",
            "2421  0.697567  \n",
            "2422  0.689922  \n",
            "2423  0.689690  \n",
            "2424  0.691855  \n",
            "\n",
            "[2425 rows x 18 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/3 [00:00<?, ?it/s]\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "128 min-max 1 3 45 0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/3 [00:14<?, ?it/s]\n",
            "  0%|          | 0/3 [00:14<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-f74e61b5b6d6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mpinn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPINN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             losses = pinn.Train(trainloader=loader['train'],\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mvalidloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             testloader=loader['test'], debug=False)\n",
            "\u001b[0;32m<ipython-input-4-c490a6c0a5d1>\u001b[0m in \u001b[0;36mTrain\u001b[0;34m(self, trainloader, testloader, validloader, debug)\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0mearly_stop\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m       \u001b[0mtrain_loss1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_epoch_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m       \u001b[0mcurrent_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-c490a6c0a5d1>\u001b[0m in \u001b[0;36mone_epoch_training\u001b[0;34m(self, epoch, dataloader)\u001b[0m\n\u001b[1;32m    253\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt_F\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt_solvU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt_F\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m                             )\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    242\u001b[0m             )\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    245\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    874\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    877\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as_real\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0mexp_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as_real\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}