{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivYQWBkf_o2h",
        "outputId": "2963a665-2075-4c77-e4ee-372982556913"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "# from google.colab import drive - Only for google colab\n",
        "import scipy.io\n",
        "import os\n",
        "# drive.mount('/content/drive')\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9td75KwOm7G"
      },
      "source": [
        "Extract Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fu3Qw9_Ny2-e",
        "outputId": "7c167724-c0e9-46d0-f9a0-5211e904851c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.io\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import kurtosis, skew, entropy\n",
        "from scipy.fft import fft\n",
        "\n",
        "base_folder = Path('Battery_Dataset')\n",
        "voltage_cycles = []\n",
        "current_cycles = []\n",
        "features = []\n",
        "\n",
        "for root, dirs, files in os.walk(base_folder):\n",
        "    for filename in sorted(files):\n",
        "        if filename.endswith('.mat'):\n",
        "            filepath = os.path.join(root, filename)\n",
        "            print(f\"Processing {filename}...\")\n",
        "            mat_data = scipy.io.loadmat(filepath, squeeze_me=True, struct_as_record=False)\n",
        "            var_name = os.path.splitext(filename)[0]\n",
        "            battery = mat_data[var_name]\n",
        "\n",
        "            cycles = battery.cycle\n",
        "            cycle_index = 0\n",
        "            if not isinstance(cycles, (list, tuple, np.ndarray)):\n",
        "                cycles = [cycles]\n",
        "\n",
        "            for idx, cycle in enumerate(cycles):\n",
        "                if hasattr(cycle, 'type') and cycle.type == 'charge' and hasattr(cycle, 'data'):\n",
        "                    data = cycle.data\n",
        "                    if hasattr(data, 'Time') and hasattr(data, 'Voltage_measured') and hasattr(data, 'Current_measured'):\n",
        "                        df = pd.DataFrame({\n",
        "                            'time': data.Time,\n",
        "                            'voltage': data.Voltage_measured,\n",
        "                            'current': data.Current_measured\n",
        "                        })\n",
        "                        cc_start_idx = df[df['voltage'] >= 4.0].index # when the voltage passes 4.0 for the first time that's the start of CC mode\n",
        "                        if cc_start_idx.empty:\n",
        "                          print(f\"Skipping {filename}_cycle{idx} due to missing CC start data.\")\n",
        "                          continue # Skip to the next cycle\n",
        "                        else:\n",
        "                          cc_start_idx = cc_start_idx[0]\n",
        "\n",
        "                        cc_end_idx = df[df['voltage'] >= 4.2].index # when the voltage hits 4.2 for the first time that's the end of CC mode\n",
        "                        if cc_end_idx.empty:\n",
        "                          print(f\"Skipping {filename}_cycle{idx} due to missing CC end data.\")\n",
        "                          continue # Skip to the next cycle\n",
        "                        else:\n",
        "                          cc_end_idx = cc_end_idx[0]\n",
        "\n",
        "                        # Check if CC_df can be created\n",
        "                        if cc_start_idx is not None and cc_end_idx is not None and cc_start_idx <= cc_end_idx:\n",
        "                          CC_df = df.loc[cc_start_idx:cc_end_idx]\n",
        "                        else:\n",
        "                          print(f\"Skipping {filename}_cycle{idx} due to invalid CC range.\")\n",
        "                          continue # Skip to the next cycle\n",
        "\n",
        "                        # Check if CV start index exists\n",
        "                        cv_start_indices = df[(df['current'] <= 0.5) & (df['voltage']>=4.0)].index\n",
        "                        if cv_start_indices.empty:\n",
        "                          print(f\"Skipping {filename}_cycle{idx} due to missing CV start data.\")\n",
        "                          continue # Skip to the next cycle\n",
        "                        cv_start_idx = cv_start_indices[0]\n",
        "\n",
        "                        # Check if CV end index exists\n",
        "                        cv_end_indices = df[(df['current'] <= 0.1) & (df['voltage']>=4.0)].index\n",
        "                        if cv_end_indices.empty:\n",
        "                          print(f\"Skipping {filename}_cycle{idx} due to missing CV end data.\")\n",
        "                          continue # Skip to the next cycle\n",
        "                        cv_end_idx = cv_end_indices[0]\n",
        "\n",
        "                        # without the skippin gthere were 1588 rows. we'll see how many there are after 1582 but some of them are still weird\n",
        "                        # Check if CV_df can be created\n",
        "                        if cv_start_idx <= cv_end_idx:\n",
        "                          CV_df = df.loc[cv_start_idx:cv_end_idx]\n",
        "                        else:\n",
        "                          print(f\"Skipping {filename}_cycle{idx} due to invalid CV range.\")\n",
        "                          continue # Skip to the next cycle\n",
        "                        filtered_df = df[\n",
        "                            (df['voltage'] >= 4.0) & (df['voltage'] <= 4.2) &\n",
        "                            (df['current'] >= 0.1) & (df['current'] <= 0.5)\n",
        "                        ]\n",
        "\n",
        "                        if not filtered_df.empty:\n",
        "                            cycle_id = f\"{filename}_cycle{idx}\"\n",
        "\n",
        "                            # Plot (optional)\n",
        "                            plt.figure()\n",
        "                            plt.plot(CC_df['time'].values, CC_df['voltage'].values)\n",
        "                            plt.title(f\"Voltage vs Time in Constant Current - {cycle_id}\")\n",
        "                            plt.xlabel(\"Time (s)\")\n",
        "                            plt.ylabel(\"Voltage (V)\")\n",
        "                            plt.grid(True)\n",
        "                            plt.tight_layout()\n",
        "                            plt.show()\n",
        "\n",
        "\n",
        "                            plt.figure()\n",
        "                            plt.plot(CV_df['time'].values, CV_df['current'].values)\n",
        "                            plt.title(f\"Current vs Time in Constant Current - {cycle_id}\")\n",
        "                            plt.xlabel(\"Time (s)\")\n",
        "                            plt.ylabel(\"Current (A)\")\n",
        "                            plt.grid(True)\n",
        "                            plt.tight_layout()\n",
        "                            plt.show()\n",
        "\n",
        "\n",
        "                            # Time-series data\n",
        "                            cc_v = CC_df['voltage'].values\n",
        "                            cc_t = CC_df['time'].values\n",
        "                            cc_i = CC_df['current'].values\n",
        "\n",
        "                            cv_i = CV_df['current'].values\n",
        "                            cv_t = CV_df['time'].values\n",
        "\n",
        "                            cycle_index +=1\n",
        "                            # --- Basic Features ---\n",
        "                            feature_dict = {\n",
        "                                'charge_CC_mean_V': np.mean(cc_v),\n",
        "                                'charge_CC_std_V': np.std(cc_v),\n",
        "                                'charge_CC_kurtosis_V': kurtosis(cc_v),\n",
        "                                'charge_CC_skew_V': skew(cc_v),\n",
        "                                'charge_CC_time_V': cc_t[-1] - cc_t[0], #np.shape(cc_v)[0], # basically just the length\n",
        "                                'charge_CC_charge': np.trapezoid(cc_i, cc_t),\n",
        "                                'charge_CC_slope_V': (cc_v[-1]-cc_v[0])/(cc_t[-1] - cc_t[0]),\n",
        "                                'charge_CC_entropy_V': -np.sum(\n",
        "                                                  cc_v/np.sum(cc_v) *\n",
        "                                                  np.log(cc_v/np.sum(cc_v))),\n",
        "                                'charge_CV_mean_I': np.mean(cv_i),\n",
        "                                'charge_CV_std_dev_I': np.std(cv_i),\n",
        "                                'charge_CV_kurtosis_I': kurtosis(cv_i),\n",
        "                                'charge_CV_skew_I': skew(cv_i),\n",
        "                                'charge_CV_time_I': cv_t[-1] - cv_t[0],\n",
        "                                'charge_CV_charge_I': np.trapezoid(cv_i, cv_t),\n",
        "                                'charge_CV_slope_I': (cv_i[-1]-cv_i[0])/(cv_t[-1] - cv_t[0]),\n",
        "                                'charge_CV_entropy_I': -np.sum(\n",
        "                                                  cv_i/np.sum(cv_i) *\n",
        "                                                  np.log(cv_i/np.sum(cv_i))),\n",
        "                            }\n",
        "\n",
        "                            # --- Additional Features ---\n",
        "                            feature_dict.update({\n",
        "                                #'duration': t[-1] - t[0], that's time already\n",
        "                                'voltage_range_CC': np.max(cc_v) - np.min(cc_v),\n",
        "                                'current_range_CV': np.max(cv_i) - np.min(cv_i),\n",
        "                                'voltage_rms_CC': np.sqrt(np.mean(cc_v**2)),\n",
        "                                'current_rms_CV': np.sqrt(np.mean(cv_i**2)),\n",
        "                            })\n",
        "\n",
        "\n",
        "                            # FFT features (voltage)\n",
        "                            if len(cc_v) > 5 and np.std(cc_v) > 1e-6:\n",
        "                                fft_v = fft(cc_v - np.mean(cc_v))\n",
        "                                power_v = np.abs(fft_v[:len(fft_v)//2])**2\n",
        "                                total_power_v = np.sum(power_v)\n",
        "                                if total_power_v > 0:\n",
        "                                    power_v /= total_power_v\n",
        "                                    feature_dict.update({\n",
        "                                        'fft_dominant_freq_v': np.argmax(power_v),\n",
        "                                        'fft_spectral_entropy_v': entropy(power_v)\n",
        "                                    })\n",
        "                                else:\n",
        "                                    feature_dict.update({\n",
        "                                        'fft_dominant_freq_v': np.nan,\n",
        "                                        'fft_spectral_entropy_v': np.nan\n",
        "                                    })\n",
        "                            else:\n",
        "                                feature_dict.update({\n",
        "                                    'fft_dominant_freq_v': np.nan,\n",
        "                                    'fft_spectral_entropy_v': np.nan\n",
        "                                })\n",
        "\n",
        "                            # FFT features (current)\n",
        "                            if len(cv_i) > 5 and np.std(cv_i) > 1e-6:\n",
        "                                fft_i = fft(cv_i - np.mean(cv_i))\n",
        "                                power_i = np.abs(fft_i[:len(fft_i)//2])**2\n",
        "                                total_power_i = np.sum(power_i)\n",
        "                                if total_power_i > 0:\n",
        "                                    power_i /= total_power_i\n",
        "                                    feature_dict.update({\n",
        "                                        'fft_dominant_freq_i': np.argmax(power_i),\n",
        "                                        'fft_spectral_entropy_i': entropy(power_i)\n",
        "                                    })\n",
        "                                else:\n",
        "                                    feature_dict.update({\n",
        "                                        'fft_dominant_freq_i': np.nan,\n",
        "                                        'fft_spectral_entropy_i': np.nan\n",
        "                                    })\n",
        "                            else:\n",
        "                                feature_dict.update({\n",
        "                                    'fft_dominant_freq_i': np.nan,\n",
        "                                    'fft_spectral_entropy_i': np.nan\n",
        "                                })\n",
        "                            feature_dict.update({'cycle_index': cycle_index,\n",
        "                                                 'capacity': np.trapezoid(data.Current_measured, data.Time) / 3600})\n",
        "                            features.append(feature_dict)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNW1aVj_t8K1",
        "outputId": "4fb202a3-daf2-43f2-aea5-d0f8a6f25ef4"
      },
      "outputs": [],
      "source": [
        "features_df = pd.DataFrame(features)\n",
        "print(features_df.columns)\n",
        "features_df.to_csv(\"/content/drive/MyDrive/Machine Learning Project (ECE 228)/Battery Dataset/features_EXTRA.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0mSP5vhU3L61",
        "outputId": "16b50336-60a4-4501-9054-3a2ff6197b3c"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a figure with multiple subplots\n",
        "num_features = len(features_df.columns)\n",
        "num_cols = 3  # plots per row\n",
        "num_rows = (num_features + num_cols - 1) // num_cols\n",
        "\n",
        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(18, 4 * num_rows))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, col in enumerate(features_df.columns):\n",
        "    axes[i].plot(features_df[col])\n",
        "    axes[i].set_title(col)\n",
        "    axes[i].set_xlabel(\"Cycle Index\")\n",
        "    axes[i].set_ylabel(\"Value\")\n",
        "    axes[i].grid(True)\n",
        "\n",
        "# Remove unused axes if feature count isn't a multiple of num_cols\n",
        "for j in range(i+1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flepdPsLPkHR"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtcgY73-U8Ea"
      },
      "source": [
        "## Model Architecture - Derived from PINN4SOH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSNkMPb1NbwR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.autograd import grad\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# apparently there's no torch nn sin activation function so we make one up\n",
        "# https://discuss.pytorch.org/t/sine-activation-missing-in-pytorch-nn/189470\n",
        "class Sin(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Sin, self).__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return torch.sin(x)\n",
        "\n",
        "\n",
        "##### Basic MLP #####\n",
        "class MLPBLock(nn.Module):\n",
        "  def __init__(self, input_dim=17, hidden_dim=50, output_dim=1, num_layers=4, dropout=0.2):\n",
        "    # init super\n",
        "    super(MLPBLock, self).__init__()\n",
        "    # assign all the properties\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim\n",
        "    self.num_layers = num_layers\n",
        "    self.hidden_dim = hidden_dim\n",
        "\n",
        "    # make the layers - first layer is Linear + Sin, hidden layers are Linear+Sin+Dropout and final layer is just Linear\n",
        "    self.layers = []\n",
        "    for i in range(num_layers):\n",
        "      if i == 0: # First layer\n",
        "        self.layers.append(nn.Linear(input_dim, hidden_dim))\n",
        "        self.layers.append(Sin())\n",
        "      elif i == num_layers - 1: # Final layer\n",
        "        self.layers.append(nn.Linear(hidden_dim, output_dim))\n",
        "      else: # Hidden layers\n",
        "        self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "        self.layers.append(Sin())\n",
        "        self.layers.append(nn.Dropout(dropout))\n",
        "    self.net = nn.Sequential(*self.layers)\n",
        "    self._init() # they have a preferred weigh initialization scheme\n",
        "\n",
        "  def _init(self):\n",
        "    for m in self.net: # dude the colab auto predict is so scary it alsmost go this whole block right\n",
        "      if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_normal_(m.weight)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8hY_9p_3DtH",
        "outputId": "c0f6bdf8-435f-4937-d1c1-aeb533673977"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Machine Learning Project (ECE 228)/Battery Dataset/features_EXTRA.csv\")\n",
        "print(df.columns)\n",
        "print(df.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDAHHFxjU_fs"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "##### Prediction Layer #####\n",
        "# just to separate the ML encoder from the actual layer that makes predictions\n",
        "class Predict(nn.Module):\n",
        "  def __init__(self, input_dim=32):\n",
        "    super(Predict, self).__init__()\n",
        "\n",
        "    # simple drop, linear, activation and linear\n",
        "    self.input_dim = input_dim\n",
        "    self.layer = nn.Sequential(\n",
        "        nn.Dropout(p=0.2),\n",
        "        nn.Linear(self.input_dim, 32),\n",
        "        Sin(),\n",
        "        nn.Linear(32, 1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layer(x)\n",
        "\n",
        "\n",
        "\n",
        "##### Solution model #####\n",
        "# Solution u is the first half of the architecture. This makes a guess as to the capacity which is then checked against the ODE in the loss\n",
        "class Sol_u(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Sol_u, self).__init__()\n",
        "    self.encoder = MLPBLock(input_dim=23, hidden_dim=60, output_dim=32, num_layers=3, dropout=0.2) # they said 17 but there's 18 features with the cycle index\n",
        "    self.predictor = Predict(input_dim=32)\n",
        "\n",
        "    # init the weights (used to be a separate method)\n",
        "    for layer in self.modules():\n",
        "      if isinstance(layer, nn.Linear):\n",
        "        nn.init.xavier_normal_(layer.weight)\n",
        "        nn.init.constant_(layer.bias, 0)\n",
        "      elif isinstance(layer, nn.Conv1d): # TODO: This is in the original paper but there's no Conv1d in the model and there's no difference so we can remove that\n",
        "        nn.init.xavier_normal_(layer.weight)\n",
        "        nn.init.constant_(layer.bias, 0)\n",
        "\n",
        "  # we'll see if this is used at all\n",
        "  def get_embedding(self, x):\n",
        "    return self.encoder(x)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.encoder(x)\n",
        "    return self.predictor(x)\n",
        "\n",
        "\n",
        "\n",
        "##### Utility Functions #####\n",
        "def count_parameters(model):\n",
        "  count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  return count\n",
        "\n",
        "# normally I'm not a fan of LR scheduler classes but this is a bit more complicated than the usual so I'm going to use that\n",
        "class LR_sched(object):\n",
        "  def __init__(self, opt, warmup_epochs, warmup_lr, total_epochs, base_lr, final_lr, iter_per_epoch=1, constant_predictor_lr=False):\n",
        "\n",
        "    self.base_lr = base_lr\n",
        "    self.constant_predictor_lr = constant_predictor_lr\n",
        "\n",
        "    # calculate the different stages of the scheduler\n",
        "    warmup_iter = iter_per_epoch * warmup_epochs\n",
        "    warmup_lr_schedule = np.linspace(warmup_lr, base_lr, warmup_iter)\n",
        "\n",
        "    decay_iter = iter_per_epoch * (total_epochs - warmup_epochs)\n",
        "    cosine_lr_schedule = final_lr + 0.5 * (base_lr - final_lr) * (1 + np.cos(np.pi * np.arange(decay_iter) / decay_iter))\n",
        "\n",
        "    # set out schedulers values\n",
        "    self.lr_schedule = np.concatenate((warmup_lr_schedule, cosine_lr_schedule))\n",
        "    self.iter = 0\n",
        "    self.opt = opt\n",
        "    self.current_lr = 0\n",
        "\n",
        "  def step(self):\n",
        "    #if self.iter >= len(self.lr_schedule):\n",
        "    #  return self.current_lr # don't do anyhting if we reached the end\n",
        "    for param_group in self.opt.param_groups:\n",
        "      if self.constant_predictor_lr and param_group['name'] == 'predictor': # we'll see if this working when we run it TODO never used a param_group object before\n",
        "        param_group['lr'] = self.base_lr\n",
        "      else:\n",
        "        lr = param_group['lr'] = self.lr_schedule[self.iter]\n",
        "    self.current_lr = lr\n",
        "    self.iter += 1\n",
        "    return lr\n",
        "\n",
        "  def get_lr(self):\n",
        "    return self.current_lr\n",
        "\n",
        "\n",
        "##### Actual full model #####\n",
        "class PINN(nn.Module):\n",
        "  def __init__(self, arguments):\n",
        "    super(PINN, self).__init__()\n",
        "    self.args = arguments\n",
        "\n",
        "    # the original has some logging stuff here but I'm going to skip that for now\n",
        "\n",
        "    # the actual guesser\n",
        "    self.solv_u = Sol_u().to(device)\n",
        "    # the ODE simulator\n",
        "    # NOTE: original 35\n",
        "    # Adjusted for new features input_dim = num__features * 2 + 3\n",
        "    self.dynamic_F = MLPBLock(input_dim=47,\n",
        "                              output_dim=1,\n",
        "                              num_layers=self.args.F_layers_num,\n",
        "                              hidden_dim=self.args.F_hidden_dim,\n",
        "                              dropout=0.2\n",
        "                         ).to(device)\n",
        "\n",
        "    # use to optimizers, one for solvU and one for dynamic_F\n",
        "    self.opt_solvU = torch.optim.Adam(self.solv_u.parameters(), lr=self.args.warmup_lr)\n",
        "    self.opt_F = torch.optim.Adam(self.dynamic_F.parameters(), lr=self.args.lr_F)\n",
        "\n",
        "    self.scheduler = LR_sched(opt=self.opt_solvU,\n",
        "                              warmup_epochs=self.args.warmup_epochs,\n",
        "                              warmup_lr=self.args.warmup_lr,\n",
        "                              total_epochs=self.args.epochs,\n",
        "                              base_lr=self.args.lr,\n",
        "                              final_lr=self.args.final_lr)\n",
        "    self.loss_func = nn.MSELoss()\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    self.best_model = None\n",
        "    # these are for scaling the loss terms\n",
        "    self.alpha = self.args.alpha\n",
        "    self.beta = self.args.beta\n",
        "\n",
        "  # NOTE: missing a bunch of logger functions but I don't really want those right now\n",
        "  def load_model(self, model_path):\n",
        "    chkpt = torch.load(model_path)\n",
        "    self.solv_u.load_state_dict(chkpt['solv_u'])\n",
        "    self.dynamic_F.load_state_dict(chkpt['dynamic_F'])\n",
        "    for param in self.solv_u.parameters():\n",
        "      param.requires_grad = True\n",
        "\n",
        "  def predict(self, xt):\n",
        "    return self.solv_u(xt)\n",
        "\n",
        "  # run a test\n",
        "  def Test(self, testloader):\n",
        "    self.eval()\n",
        "    true_label = []\n",
        "    pred_label = []\n",
        "    with torch.no_grad():\n",
        "      for (index, (x1,_,y1,_)) in enumerate(testloader):\n",
        "        x1 = x1.to(device)\n",
        "        u1 = self.predict(x1)\n",
        "        true_label.append(y1)\n",
        "        pred_label.append(u1.cpu().detach().numpy()) # dk why all this stuff but it works\n",
        "    true_label = np.concatenate(true_label)\n",
        "    pred_label = np.concatenate(pred_label)\n",
        "\n",
        "    return true_label, pred_label\n",
        "\n",
        "  def Validate(self, validloader):\n",
        "    self.eval()\n",
        "    true_label = []\n",
        "    pred_label = []\n",
        "    with torch.no_grad():\n",
        "      for (index, (x1,_,y1,_)) in enumerate(validloader):\n",
        "        x1 = x1.to(device)\n",
        "        u1 = self.predict(x1)\n",
        "        true_label.append(y1)\n",
        "        pred_label.append(u1.cpu().detach().numpy()) # dk why all this stuff but it works\n",
        "    true_label = np.concatenate(true_label)\n",
        "    pred_label = np.concatenate(pred_label)\n",
        "    mse = self.loss_func(torch.tensor(true_label), torch.tensor(pred_label))\n",
        "\n",
        "    return mse.item()\n",
        "\n",
        "  def forward(self, x_full):\n",
        "    x_full.requires_grad = True\n",
        "    x = x_full[:,:-1]\n",
        "    t = x_full[:, -1:]\n",
        "\n",
        "    u = self.solv_u(torch.cat([x,t], dim=1))\n",
        "    u_t = grad(u.sum(),t,\n",
        "               create_graph=True,\n",
        "               only_inputs=True,\n",
        "               allow_unused=True)[0]\n",
        "\n",
        "    u_x = grad(u.sum(), x,\n",
        "               create_graph=True,\n",
        "               only_inputs=True,\n",
        "               allow_unused=True)[0]\n",
        "\n",
        "    # THIS is where the magic happens. feed the solution of the regular predictor and the derivatives of the solution to the ODE simulator\n",
        "    F = self.dynamic_F(torch.cat([x_full, u, u_x, u_t], dim=1))\n",
        "\n",
        "    # d/dt (solution) = F so the difference should be 0\n",
        "    f = u_t - F\n",
        "    return u,f\n",
        "\n",
        "\n",
        "  def one_epoch_training(self, epoch, dataloader):\n",
        "    self.train()\n",
        "\n",
        "    # keep track of the losses\n",
        "    loss1_tracker = []\n",
        "    loss2_tracker = []\n",
        "    loss3_tracker = [] # they have average meters but whatever\n",
        "\n",
        "    for iter, (x1,x2,y1,y2) in enumerate(dataloader): # note uiqte sure why it does two ata a time but ok\n",
        "      x1 = x1.to(device)\n",
        "      x2 = x2.to(device)\n",
        "      y1 = y1.to(device)\n",
        "      y2 = y2.to(device)\n",
        "\n",
        "      u1,f1 = self.forward(x1)\n",
        "      u2,f2 = self.forward(x2)\n",
        "\n",
        "\n",
        "      # NOTE: without the physics loss the total loss improves significantly. It's because our dataset has multiple batteries in\n",
        "      # one file so sometimes the capacity will spike back up. Doesn't make sense to enforce that loss\n",
        "\n",
        "      # data loss\n",
        "      l1 = 1*self.loss_func(u1,y1) #+ 0.5*self.loss_func(u2,y2) NOTE: trying without the sequence info\n",
        "\n",
        "      # pde loss\n",
        "      f_target = torch.zeros_like(f1)\n",
        "      # the PDE and the guesser should be predicting the same output, so the f1 and f2 should be 0\n",
        "      l2 = 1*self.loss_func(f1,f_target) #+ 0.5*self.loss_func(f2,f_target) NOTE trying without the sequence info\n",
        "\n",
        "      # physics loss\n",
        "      # the capacity should be decreasing all the time\n",
        "      l3 = 0 #self.relu(torch.mul(u2-u1, y1-y2)).sum() NOTE trying without the sequence info\n",
        "\n",
        "      # total loss\n",
        "      loss = l1 + self.alpha*l2 #+ self.beta*l3\n",
        "\n",
        "      self.opt_solvU.zero_grad()\n",
        "      self.opt_F.zero_grad()\n",
        "      loss.backward()\n",
        "      self.opt_solvU.step()\n",
        "      self.opt_F.step()\n",
        "\n",
        "      loss1_tracker.append(l1.item())\n",
        "      loss2_tracker.append(l2.item())\n",
        "      loss3_tracker.append(l3) #, x1.size(0)) NOTE trying without the sequence info\n",
        "\n",
        "      if (iter+1)%50 == 0:\n",
        "        print(\"[epoch:{} iter:{}] data_loss:{:.6f} PDE loss:{:.6f}, physics loss:{:.6f}\".format(epoch, iter+1, l1, l2, l3))\n",
        "\n",
        "    return sum(loss1_tracker)/len(loss1_tracker), sum(loss2_tracker)/len(loss2_tracker), sum(loss3_tracker)/len(loss3_tracker)\n",
        "\n",
        "  def Train(self, trainloader, testloader, validloader):\n",
        "    min_mse = 10\n",
        "    valid_mse = 10\n",
        "    early_stop = 0\n",
        "    mae = 10\n",
        "    losses = []\n",
        "    for epoch in range(1,self.args.epochs+1):\n",
        "      early_stop += 1\n",
        "      train_loss1, train_loss2, train_loss3 = self.one_epoch_training(epoch, trainloader)\n",
        "      current_lr = self.scheduler.step()\n",
        "\n",
        "      state = '[Train] epoch:{}, lr:{:.6f}, total loss:{:.6f}'.format(epoch, current_lr, train_loss1 + self.alpha*train_loss2 + self.beta*train_loss3)\n",
        "      print(state)\n",
        "      losses.append(train_loss1 + self.alpha*train_loss2 + self.beta*train_loss3)\n",
        "      if epoch%1 ==0 and validloader is not None:\n",
        "        valid_mse = self.Validate(validloader)\n",
        "        info = '[Validation] epoch:{}, MSE:{}'.format(epoch,valid_mse)\n",
        "\n",
        "      if valid_mse < min_mse and testloader is not None:\n",
        "        min_mse = valid_mse\n",
        "        true_label, pred_label = self.Test(testloader)\n",
        "        # missing a logger thing here\n",
        "        early_stop = 0\n",
        "        self.best_model = {'solv_u': self.solv_u.state_dict(),\n",
        "                             'dynamic_F': self.dynamic_F.state_dict()}\n",
        "\n",
        "      if self.args.early_stop is not None and early_stop > self.args.early_stop:\n",
        "        print(\"Early stopping at: \", epoch)\n",
        "        break\n",
        "    return losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAwwwqavL9yN"
      },
      "source": [
        "Ok let's try making an args object and giving it the default values from the paper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wifZofk2MQ79",
        "outputId": "88b7b650-5723-4583-c336-b7c81155dd54"
      },
      "outputs": [],
      "source": [
        "class args:\n",
        "  pass\n",
        "\n",
        "# hehe tricks\n",
        "arg = args()\n",
        "\n",
        "arg.data = 'our_stuff'\n",
        "arg.batch = 10\n",
        "arg.batch_size = 256\n",
        "arg.normalization_method = 'z-score'\n",
        "arg.epochs = 1000\n",
        "arg.lr = 1e-3\n",
        "arg.warmup_epochs = 10\n",
        "arg.warmup_lr = 5e-4\n",
        "arg.final_lr = 1e-4\n",
        "arg.lr_F = 1e-3\n",
        "arg.iter_per_epoch = 1\n",
        "arg.F_layers_num = 3\n",
        "arg.F_hidden_dim = 60\n",
        "arg.alpha = 1\n",
        "arg.beta = 1\n",
        "arg.early_stop = 500\n",
        "\n",
        "arg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyRhSLY7NDKB"
      },
      "outputs": [],
      "source": [
        "pinn = PINN(arg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FVj2tWNONIr"
      },
      "source": [
        "So up to here there are no \"compilation\" errors let's see how it runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3JW52RYOQ-A"
      },
      "outputs": [],
      "source": [
        "# no clue how the trainloader works for their end so I'm gonna take a break and go swim\n",
        "# back now\n",
        "# messing about with their files to see how they loaded everything\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "class DF():\n",
        "  def __init__(self, args):\n",
        "    self.normalization = True\n",
        "    self.normalization_method = args.normalization_method # min-max or z-score\n",
        "    self.args = args\n",
        "  # very unclear on this\n",
        "  def _3_sigma(self, Ser1):\n",
        "    rule = (Ser1.mean() - 3*Ser1.std() > Ser1) | (Ser1.mean() + 3*Ser1.std() < Ser1)\n",
        "    index = np.arange(Ser1.shape[0])[rule]\n",
        "    return index\n",
        "\n",
        "  # think this is just drop entries where the values are 3 sigma off the average. interesting\n",
        "  def delete_3_sigma(self,df):\n",
        "    df = df.replace([np.inf, -np.inf], np.nan)\n",
        "    df = df.dropna()\n",
        "    df = df.reset_index(drop=True)\n",
        "    out_index = []\n",
        "    for col in df.columns:\n",
        "      index = self._3_sigma(df[col])\n",
        "      out_index.extend(index)\n",
        "    out_index = list(set(out_index))\n",
        "    df = df.drop(out_index)\n",
        "    df = df.reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "  def read_one_csv(self, file_name, nominal_capacity=None):\n",
        "\n",
        "    df = pd.read_csv(file_name)\n",
        "    #df.insert(df.shape[1]-1, 'cycle index', np.arange(df.shape[0])) # add the cycle index in the penultimate column\n",
        "    df = self.delete_3_sigma(df)\n",
        "    if nominal_capacity is not None:\n",
        "      df['capacity'] = df['capacity']/nominal_capacity # normalize the capacity\n",
        "      features_df = df.iloc[:, :-1] # get all the columns bar the last one\n",
        "      if self.normalization_method == 'min-max':\n",
        "        features_df = 2*(features_df - features_df.min())/(features_df.max() - features_df.min()) - 1\n",
        "      elif self.normalization_method == 'z-score':\n",
        "        features_df = (features_df - features_df.mean())/features_df.std()\n",
        "    df = df.astype({col: float for col in df.columns[:-1]}) # apparently putting everything back complains because everything just became floats\n",
        "    print(\"almost readf df\")\n",
        "    print(df)\n",
        "    df.iloc[:,:-1] = features_df.astype(float) # put everything back\n",
        "    return df\n",
        "\n",
        "  # they have a bunch of separate batteries but we only have one big file\n",
        "  def load_battery(self, path, nominal_capacity=None):\n",
        "    df = self.read_one_csv(path, nominal_capacity)\n",
        "    print(df)\n",
        "    # get the x and y\n",
        "    x = df.iloc[:, :-1].values\n",
        "    y = df.iloc[:,-1].values\n",
        "\n",
        "    #x1 and y1 are all but the last and x2,y2 are all but the first\n",
        "    # they're basically shifted version of each other at every point x2 is the next time step of x1\n",
        "    x1 = x[:-1]\n",
        "    x2 = x[1:]\n",
        "    y1 = y[:-1]\n",
        "    y2 = y[1:]\n",
        "\n",
        "    return (x1,y1), (x2,y2)\n",
        "\n",
        "  def load_all(self, path_list, nominal_capacity):\n",
        "    X1, X2, Y1, Y2 = [], [], [], []\n",
        "    for path in path_list:\n",
        "      (x1,y1), (x2,y2) = self.load_battery(path, nominal_capacity)\n",
        "      X1.append(x1)\n",
        "      X2.append(x2)\n",
        "      Y1.append(y1)\n",
        "      Y2.append(y2)\n",
        "\n",
        "    X1 = np.concatenate(X1, axis=0)\n",
        "    X2 = np.concatenate(X2, axis=0)\n",
        "    Y1 = np.concatenate(Y1, axis=0)\n",
        "    Y2 = np.concatenate(Y2, axis=0)\n",
        "\n",
        "    tensorX1 = torch.from_numpy(X1).float()\n",
        "    tensorX2 = torch.from_numpy(X2).float()\n",
        "    tensorY1 = torch.from_numpy(Y1).float().view(-1,1)\n",
        "    tensorY2 = torch.from_numpy(Y2).float().view(-1,1)\n",
        "\n",
        "    # they used multiple train/validate/test setups\n",
        "\n",
        "    split = int(tensorX1.shape[0] * 0.8)\n",
        "    trainX1, testX1 = tensorX1[:split], tensorX1[split:]\n",
        "    trainX2, testX2 = tensorX2[:split], tensorX2[split:]\n",
        "    trainY1, testY1 = tensorY1[:split], tensorY1[split:]\n",
        "    trainY2, testY2 = tensorY2[:split], tensorY2[split:]\n",
        "\n",
        "\n",
        "\n",
        "    trainX1, validX1, trainX2, validX2, trainY1, validY1, trainY2, validY2 = train_test_split(trainX1, trainX2, trainY1, trainY2, test_size=0.2, random_state=420)\n",
        "\n",
        "    # setup 1 - regular 80/20 split for train/test and 80/20 split inside train for train/valid\n",
        "    trainloader = DataLoader(TensorDataset(trainX1, trainX2, trainY1, trainY2), batch_size=self.args.batch_size, shuffle=True)\n",
        "    validloader = DataLoader(TensorDataset(validX1, validX2, validY1, validY2), batch_size=self.args.batch_size, shuffle=True)\n",
        "    testloader = DataLoader(TensorDataset(testX1, testX2, testY1, testY2), batch_size=self.args.batch_size, shuffle=True)\n",
        "\n",
        "    # setup 2 - no test set\n",
        "    trainX1, validX1, trainX2, validX2, trainY1, validY1, trainY2, validY2 = train_test_split(tensorX1, tensorX2, tensorY1, tensorY2, test_size=0.2, random_state=420)\n",
        "    trainloader2 = DataLoader(TensorDataset(trainX1, trainX2, trainY1, trainY2), batch_size=self.args.batch_size, shuffle=True)\n",
        "    validloader2 = DataLoader(TensorDataset(validX1, validX2, validY1, validY2), batch_size=self.args.batch_size, shuffle=True)\n",
        "\n",
        "    # setup 3 - only test set\n",
        "    testloader3 = DataLoader(TensorDataset(tensorX1, tensorX2, tensorY1, tensorY2), batch_size=self.args.batch_size, shuffle=False)\n",
        "\n",
        "    loader = {\n",
        "          'train':trainloader,\n",
        "          'valid':validloader,\n",
        "          'test':testloader,\n",
        "          'train_2':trainloader2,\n",
        "          'valid_2':validloader2,\n",
        "          'test_3':testloader3\n",
        "    }\n",
        "    return loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDukFKtufGTC",
        "outputId": "b19d565d-128c-4455-b23b-2c714175ae41"
      },
      "outputs": [],
      "source": [
        "print(\"NaNs :\", df.isna().sum().sum())\n",
        "print(\"Infs :\", np.isinf(df.values).sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "id": "BnYDA3yPtaW8",
        "outputId": "181abf4b-2466-45a8-a3ef-1ce02eecf057"
      },
      "outputs": [],
      "source": [
        "# little bit of hee hee haha data manipulation\n",
        "df = pd.read_csv('/content/drive/MyDrive/Machine Learning Project (ECE 228)/Battery Dataset/features_EXTRA.csv')\n",
        "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "df.dropna(inplace=True)\n",
        "df = df.drop(columns='fft_dominant_freq_i') # normalizing constants leads to div by zero\n",
        "df = df.drop(columns='fft_dominant_freq_v') # normalizing constants leads to div by zero\n",
        "df.to_csv(\"clean_features_EXTRA.csv\", index=False)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCU7xgx2zcU8"
      },
      "outputs": [],
      "source": [
        "# ok let's load a dataset\n",
        "reader = DF(arg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwsCIhys6wlI",
        "outputId": "8a836e49-001e-404b-8619-aa4199a1cdb7"
      },
      "outputs": [],
      "source": [
        "# let's get a loader back\n",
        "loader = reader.load_all([\"./clean_features_EXTRA.csv\"], nominal_capacity=2)\n",
        "    #['/content/drive/MyDrive/Machine Learning Project (ECE 228)/Battery Dataset/features.csv'], nominal_capacity=2)\n",
        "    #['./1-1.csv'], nominal_capacity=2)\n",
        "    #['/content/drive/MyDrive/Machine Learning Project (ECE 228)/Battery Dataset/features.csv'], nominal_capacity=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVTc1efD-GBd",
        "outputId": "c9269839-4f2e-4120-e7f1-2158a4e53acc"
      },
      "outputs": [],
      "source": [
        "loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fx_z5gMR-p_i",
        "outputId": "c1b36d42-6e16-41aa-fdcd-34ca9d72ad44"
      },
      "outputs": [],
      "source": [
        "losses = pinn.Train(trainloader=loader['train'],\n",
        "           validloader=loader['valid'],\n",
        "           testloader=loader['test'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "id": "f2deLPhrGLkR",
        "outputId": "e3507148-2445-4ed8-d8af-9c584859fd88"
      },
      "outputs": [],
      "source": [
        "# plot the loss curve\n",
        "import matplotlib.pyplot as plt\n",
        "x = list(range(1,len(losses) + 1))\n",
        "plt.yscale('log')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Log-Scale Loss plot for extra features model')\n",
        "plt.plot(x, losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below here is experimental code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Sz6unzeG0cM",
        "outputId": "2f9d3eb9-ed9c-4ffc-887e-4061ce85ec5d"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "from sklearn import metrics\n",
        "\n",
        "true_label, pred_label = pinn.Test(loader['test_3'])\n",
        "mse = metrics.mean_squared_error(torch.tensor(true_label), torch.tensor(pred_label))\n",
        "mae = metrics.mean_absolute_error(torch.tensor(true_label), torch.tensor(pred_label))\n",
        "r2 = metrics.r2_score(torch.tensor(true_label), torch.tensor(pred_label))\n",
        "mape = metrics.mean_absolute_percentage_error(torch.tensor(true_label), torch.tensor(pred_label))\n",
        "\n",
        "print(mse, mae, r2, mape)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKsnA0gohaCW"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flIn6FMOhoyG"
      },
      "outputs": [],
      "source": [
        "# try just doing cross testing\n",
        "#ll = pd.read_csv('./1-1.csv')\n",
        "#ll.insert(ll.shape[1]-1, 'cycle index', np.arange(ll.shape[0])) # only run this once\n",
        "#ll = ll.drop(columns=ll.columns[0])\n",
        "#ll.to_csv('./1-1.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "GhcFUxG5poPc",
        "outputId": "52255f2c-d8e2-4c08-9f13-42f45fd779b7"
      },
      "outputs": [],
      "source": [
        "# reader2 = DF(arg)\n",
        "# loader2 = reader2.load_all(['./1-1.csv'], nominal_capacity=2) # use one of the original batteries\n",
        "'''\n",
        "from sklearn import metrics\n",
        "\n",
        "true_label, pred_label = pinn.Test(loader2['test_3'])\n",
        "mse = metrics.mean_squared_error(torch.tensor(true_label), torch.tensor(pred_label))\n",
        "mae = metrics.mean_absolute_error(torch.tensor(true_label), torch.tensor(pred_label))\n",
        "r2 = metrics.r2_score(torch.tensor(true_label), torch.tensor(pred_label))\n",
        "mape = metrics.mean_absolute_percentage_error(torch.tensor(true_label), torch.tensor(pred_label))\n",
        "\n",
        "print(mse, mae, r2, mape)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "tmk44MRoqYDt",
        "outputId": "e2096b40-22e8-4021-db10-7ad934c6f1f4"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "pinn.Train(trainloader=loader2['train'],\n",
        "           validloader=loader2['valid'],\n",
        "           testloader=loader2['test'])\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8bkmaQVqlGF"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "true_label, pred_label = pinn.Test(loader2['test_3'])\n",
        "mse = metrics.mean_squared_error(torch.tensor(true_label), torch.tensor(pred_label))\n",
        "mae = metrics.mean_absolute_error(torch.tensor(true_label), torch.tensor(pred_label))\n",
        "r2 = metrics.r2_score(torch.tensor(true_label), torch.tensor(pred_label))\n",
        "mape = metrics.mean_absolute_percentage_error(torch.tensor(true_label), torch.tensor(pred_label))\n",
        "\n",
        "print(mse, mae, r2, mape)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBP0QlAqsPfL"
      },
      "outputs": [],
      "source": [
        "\n",
        "'''\n",
        "true_label, pred_label = pinn.Test(loader['test_3'])\n",
        "mse = metrics.mean_squared_error(torch.tensor(true_label), torch.tensor(pred_label))\n",
        "mae = metrics.mean_absolute_error(torch.tensor(true_label), torch.tensor(pred_label))\n",
        "r2 = metrics.r2_score(torch.tensor(true_label), torch.tensor(pred_label))\n",
        "mape = metrics.mean_absolute_percentage_error(torch.tensor(true_label), torch.tensor(pred_label))\n",
        "\n",
        "print(mse, mae, r2, mape)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnjZLhKaoZ_Q"
      },
      "source": [
        "# README:\n",
        "\n",
        "Ok so I built the model but it seems like our data is just insufficient to predict stuff with. The capacity that I calculate from integrating the current over the time doesn't match the nominal capacity that's available in the discharge cycle. Apparently that's kinda normal the charging was incomplete. But the original dataset has maybe \\~1500 cycles per battery whereas we have \\~1500 cycles between all the batteries.\n",
        "To compensate for that I removed the physics-enforced loss of u1-u2. Basically in their datasets the battery capacity is constantly degrading but since ours is a mishmash of several batteris it goes back up a bit whenever we change batteries. Basically the model does well on our data (\\~0.0010785406176327898 MSE loss) but not so great if you cross test on the original sets.\n",
        "\n",
        "TODO:\n",
        "\n",
        "- Figure out new features - dv/dt and di/dt are already calculated, that's the slope. min and max are just the range that we capture that's already encoded and are the same for every single value (in theory). The RMS is used to convert AC signals into a DC equivalent and the paper specifically extracts the linear section of Constant Current and Constant Voltage chargin modes so that data is already in DC. May have some value but not quite sure. The FFT is for periodic osciallating stuff, but as mentioned this is going up or down monotonically, idk how much info the FFT will have. I will try to add these in, but I doubt they have too much impact. Personally I think that discharge statistics of the same kind as the charge would work quite well, but again I never know.\n",
        "\n",
        "- Decide on the physics informed loss\n",
        "\n",
        "- Play with more Dropout - might make the model more resilient and let us train for longer.\n",
        "\n",
        "- find and analyze erroneous charging data (sometimes it doesn't find the CV and CC modes of a cycle). There's some cycles that error out (I skip them) and some cycles where the conditions find a teeny tiny window. find those in the features.csv file that has duration. The ones that are under a couple hundred features are weird. Oh shit they don't say which ones that is oops lol. About 6 values are lost, total.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-WSD_w4rA6_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
